{
  "search_index": {
    "5": {
      "file_id": "5",
      "content": "\n=== Page 1 ===\nCLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object\nRepresentation\nReza Abbasi, Ali Nazari, Aminreza Se\ufb01d, Mohammadali Banayeeanzade,\nMohammad Hossein Rohban, Mahdieh Soleymani Baghshah\nSharif University of Technology, Tehran, Iran\n{reza.abbasi, ali.nazari02, aminreza.sefid, a.banayeean, rohban, soleymani}@sharif.edu\nAbstract\nContrastive Language-Image Pre-training (CLIP) mod-\nels excel in zero-shot classi\ufb01cation, yet face challenges in\ncomplex multi-object scenarios. This study offers a com-\nprehensive analysis of CLIP\u2019s limitations in these contexts\nusing a specialized dataset, ComCO, designed to evaluate\nCLIP\u2019s encoders in diverse multi-object scenarios.\nOur\n\ufb01ndings reveal signi\ufb01cant biases: the text encoder prior-\nitizes \ufb01rst-mentioned objects, and the image encoder fa-\nvors larger objects.\nThrough retrieval and classi\ufb01cation\ntasks, we quantify these biases across multiple CLIP vari-\nants and trace their origins to CLIP\u2019s training process, sup-\nported by analyses of the LAION dataset and training pro-\ngression. Our image-text matching experiments show sub-\nstantial performance drops when object size or token order\nchanges, underscoring CLIP\u2019s instability with rephrased\nbut semantically similar captions. Extending this to longer\ncaptions and text-to-image models like Stable Diffusion,\nwe demonstrate how prompt order in\ufb02uences object promi-\nnence in generated images. For more details and access to\nour dataset and analysis code, visit our project repository:\nhttps://clip-oscope.github.io/.\n1. Introduction\nThe convergence of vision and language in arti\ufb01cial in-\ntelligence has led to the development of Vision-Language\nModels (VLMs) that can interpret and generate multimodal\ncontent. Among these, OpenAI\u2019s Contrastive Language-\nImage Pre-training (CLIP) model [13] has been particu-\nlarly in\ufb02uential, demonstrating remarkable capabilities in\nzero-shot image classi\ufb01cation and setting new standards for\nmultimodal understanding [3, 5, 18, 20]. The success of\nCLIP has catalyzed a wide array of applications\u2014from im-\nage retrieval and visual question answering to text-to-image\ngeneration\u2014signifying a paradigm shift in how models per-\nceive and relate visual and linguistic information.\nVisual Language Models like CLIP face signi\ufb01cant\nchallenges in understanding and reasoning about complex\nscenes with multiple objects and intricate relationships.\nCLIP struggles to identify distinct objects and model their\nrelationships accurately, especially when captions contain\nthe same objects but differ in their relationships. This re-\nsults in dif\ufb01culty distinguishing between similar captions\nwith different object relationships.\nSeveral benchmark\ndatasets have been introduced to elucidate the limitations of\nexisting models in capturing subtle relational nuances. No-\ntably, Winoground [20], VL-CheckList [23], ARO [21], and\nCREPE [10] have been instrumental in evaluating models\u2019\ncapacities to accurately match images with semantically ap-\npropriate captions.\nNumerous studies have addressed compositionality chal-\nlenges in multi-object scenarios, often through end-to-end\nmethods like \ufb01ne-tuning with hard-negative samples [21] to\nimprove model performance. However, these approaches\nhave faced criticism and subsequent re\ufb01nement, as seen in\nmethods like SUGARCREPE [8] and [17], which generate\nnegative captions with minor structural changes or LLMs\nto highlight semantic distinctions.\nWhile most focus on\nCLIP\u2019s ability to distinguish structurally similar yet concep-\ntually different captions, few studies, such as Dumpala et al.\n[4], explore CLIP\u2019s performance on semantically equivalent\nbut structurally distinct captions, revealing a gap in under-\nstanding CLIP\u2019s inconsistency with such prompts.\nWhile previous studies have advanced our understanding\nof CLIP\u2019s limitations, our work uniquely focuses on CLIP\u2019s\nperformance with semantically equivalent but structurally\nvaried captions rather than simply distinguishing conceptu-\nally different captions. This shift enables a deeper exam-\nination of the model\u2019s grasp of language and visual con-\ntent, where systematic errors reveal potential biases. Un-\nlike prior works that primarily propose benchmarks or end-\nto-end solutions, we investigate the root causes of CLIP\u2019s\nbehavior, delving into the mechanisms of both image and\ntext encoders to uncover why the model displays biases\nand lacks robustness to certain linguistic and visual varia-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n9308\n\n=== Page 2 ===\nFigure 1. Overview of our key contributions. Step 1: We create ComCO dataset for controlled multi-object experiments. Step 2: We\nidentify biases in CLIP\u2019s image encoder (favoring larger objects) and text encoder (prioritizing \ufb01rst-mentioned objects). Step 3: We\ninvestigate the origin of these biases, \ufb01nding a connection to training data characteristics. Step 4: We demonstrate the practical impacts of\nthese biases on image-text matching task, showing how they affect model performance in multi-object scenarios.\ntions. To support this analysis, we introduce the ComCO\ndataset, purpose-built for examining CLIP\u2019s performance\nunder controlled multi-object scenarios. Our study spans\nmultiple versions of CLIP trained on diverse datasets and ar-\nchitectures, ensuring the broad applicability of our \ufb01ndings.\nThis comprehensive approach aims to deepen our under-\nstanding of CLIP\u2019s limitations and pave the way for more\nadaptable vision-language models. Beyond CLIP, our in-\nsights have signi\ufb01cant implications for text-to-image (T2I)\ngenerative models and multimodal large language models\n(MLLMs), where decoding CLIP\u2019s encoding intricacies can\ninform advancements in arti\ufb01cial intelligence across do-\nmains. As shown in Figure 1, our key contributions are as\nfollows:\n\u2022 Development of Novel Dataset: We introduce ComCO,\na specialized dataset for creating controlled multi-object\nscenarios.\nUnlike previous benchmarks, ComCO al-\nlows control over object size and caption order, enabling\nprecise analysis of model performance across composi-\ntional challenges and enhancing understanding of VLMs\u2019\nstrengths and weaknesses.\n\u2022 Encoder Analysis: We conduct an in-depth examination\nof CLIP\u2019s image and text encoders in multi-object scenes,\nrevealing weaknesses in preserving information for object\ndistinction and identifying where compositional informa-\ntion is lost.\n\u2022 Bias Identi\ufb01cation: Our study reveals that CLIP\u2019s im-\nage encoder prefers larger objects, while the text encoder\nfavors \ufb01rst-mentioned and visually larger objects, high-\nlighting biases in CLIP\u2019s handling of visual and linguistic\ninformation.\n\u2022 Investigation of Bias Origins: We explore the origins of\nthese biases, showing that larger objects are often men-\ntioned earlier in CLIP\u2019s training captions, and are favored\nin embeddings due to the abundance of their visual to-\nkens. We substantiate this with analyses of the LAION\ndataset and CLIP\u2019s training progression.\n\u2022 Practical Impact: We show how these biases affect per-\nformance in multi-object tasks, with signi\ufb01cant drops in\nimage-text matching accuracy in ComCO and COCO [9].\nThese biases also extend to text-to-image models, in\ufb02u-\nencing object prominence based on prompt order.\nThese \ufb01ndings reveal how biases in CLIP\u2019s text and im-\nage encoders signi\ufb01cantly reduce its performance in multi-\nobject scenarios, emphasizing the need to address these bi-\nases to enhance vision-language models\u2019 robustness. Our\nwork offers key insights into CLIP\u2019s behavior and lays\ngroundwork for improving model performance in real-\nworld applications.\n2. Methodology\n2.1. Dataset Design\nTo thoroughly evaluate the performance of CLIP models\nin multi-object scenarios under controlled conditions, we\nconstructed the ComCO (Complex COCO Objects) dataset.\nUtilizing Blender software allowed us precise control over\nthe number, location, and dimensions of objects in the im-\nages (see Appendix 7.1). The ComCO dataset comprises\n72 objects derived from the COCO dataset. We generated\n9309\n\n=== Page 3 ===\nimages containing 2, 3, 4, and 5 objects. Each image is\npaired with a speci\ufb01c caption that accurately describes the\nobjects present. This approach ensures high control over\nthe dataset and minimizes confounding factors, providing a\nrobust platform for evaluating the CLIP models.\nWe deliberately chose not to use text-to-image models\nfor generating these datasets due to two main reasons. First,\nthese models often lack the capability to produce high-\nquality, fully controlled multi-object images. Second, since\nCLIP is used in many of these models, utilizing them could\nintroduce unwanted biases into our evaluations.\n2.2. Experimental Framework for Encoder Analy-\nsis\nThe main goal of this study is to evaluate the performance of\nCLIP\u2019s text and image encoders separately in multi-object\nscenarios. We aim to analyze the impact and contribution of\neach object in the \ufb01nal output of the encoders. To achieve\nthis, we conducted experiments using our designed ComCO\ndataset, with images and captions containing two to \ufb01ve ob-\njects. To ensure the generalizability of our \ufb01ndings, we also\nvalidated our results on the widely-used COCO dataset. We\ndesigned two sets of experiments: retrieval-based experi-\nments and classi\ufb01cation-based experiments. Given the con-\nsistency of the results in both types of experiments, we have\nincluded the classi\ufb01cation results in the appendix 7.2 and\n7.4 and explain the retrieval-based experiments bellow.\n2.2.1. TEXT-BASED OBJECT RETRIEVAL (TOR)\nThe Text-based Object Retrieval task evaluates how well\nCLIP\u2019s text encoder can identify individual objects within\nmulti-object captions. As illustrated in Figure 2a, this ex-\nperiment involves several steps: First, we use CLIP\u2019s text\nencoder to create embeddings for both multi-object captions\nand single-object captions. We then measure the similar-\nity between each multi-object caption embedding and all\nsingle-object caption embeddings. The single-object cap-\ntion with the highest similarity score is considered the \u201dre-\ntrieved\u201d object. To assess performance, we calculate re-\ntrieval accuracy for each object position in the multi-object\ncaptions. This helps us identify any biases related to an\nobject\u2019s position within a caption, such as favoring objects\nmentioned \ufb01rst or last.\n2.2.2. IMAGE-BASED OBJECT RETRIEVAL (IOR)\nThe Image-based Object Retrieval task is similar to TOR\nbut focuses on CLIP\u2019s image encoder. As shown in Fig-\nure 2b, this experiment involves several steps: We begin\nby using CLIP\u2019s image encoder to generate embeddings\nfor multi-object images and single-object images. We then\ncompute similarity scores between each multi-object image\nembedding and all single-object image embeddings. The\nsingle-object image with the highest similarity score is con-\nsidered the \u201dretrieved\u201d object. To evaluate performance, we\ncalculate retrieval accuracy for different object size cate-\ngories (e.g., large, small) within the multi-object images.\nThis allows us to determine if the image encoder shows any\npreference for objects of a particular size.\nWe also experimented with a variation of ComCO, called\nSimCO, where objects were replaced with simple geometric\nshapes from the CLEVR dataset. This was done to con\ufb01rm\nthat bias persists even with non-natural, geometric objects.\nFurther details are provided in Appendix 7.1.\n3. Results and Analysis\nOur experiments revealed signi\ufb01cant biases in both the\ntext and image encoders of the CLIP model. This section\npresents our \ufb01ndings, organized by encoder type and focus-\ning on retrieval tasks.\n3.1. Text Encoder Biases\nWe observed a consistent bias in the text encoder towards\nthe \ufb01rst object mentioned in descriptions. In the TOR ex-\nperiment, the retrieval accuracy (as shown in Table 1) was\nhighest for the \ufb01rst object, indicating its dominant in\ufb02uence\non the overall text representation. This suggests that the\ntext encoder prioritizes the initial object, leading to its more\naccurate retrieval compared to subsequent objects. The de-\ntailed results for the scenarios involving 2, 3, and 5 objects\ncan be found in the appendix 7.3, and experiments on longer\ncaption templates are in Appendix 7.6 and 7.7.\n3.2. Image Encoder Biases\nIn multi-object images, the image encoder exhibited a\nstrong bias towards larger objects. The Image-based Ob-\nject Retrieval IOR experiment, detailed in Table 2, shows\nthat larger objects were more frequently and accurately re-\ntrieved during single-object image searches. This \ufb01nding\nhighlights the image encoder\u2019s bias towards larger objects,\nwhich receive disproportionate emphasis in the \ufb01nal image\nrepresentation. Further detailed results, speci\ufb01cally for sce-\nnarios with 2, 3, and 5 objects, are provided in the appendix\n7.5.\n3.3. COCO Dataset Experiments\nTo validate the generalizability of our \ufb01ndings from the\nsynthetic dataset, we conducted similar experiments on the\nCOCO dataset, which comprises real images with accom-\npanying captions. This real-world dataset allowed us to in-\nvestigate whether the previously observed biases persist in\nmore naturalistic settings.\nDue to the absence of single-object images for COCO\nobjects, we approached the IOR experiment in two ways.\nFirst, we used single-object images from the DomainNet\ndataset [11] as retrieval targets. Second, we introduced an\nalternative approach called Image-to-Text Object Retrieval\n(I2TOR). In I2TOR, we used the textual names of COCO\n9310\n\n=== Page 4 ===\n%DVH\u0003,PDJH\u001db\n7KUHH\u00032EMHFWV\u0003,PDJH\n6LQJOH\u00032EMHFW\u0003,PDJH\u0003ZKLFK\nPDWFKLQJb%DVH\u0003,PDJH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n2WKHUb6LQJOHb2EMHFW\n,PDJHV\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n\u0011\u0011\u0011\n&/,3\u00036FRUH\n\u0013\u0011\u001a\u0013\u0018\n\u0013\u0011\u001b\u0015\u0017\n\u0013\u0011\u0019\u0015\u0014\n\u0013\u0011\u0018\u0015\u001a\n\u0013\u0011\u0018\u001a\u001c\n\u0013\u0011\u0018\u0018\u0017\nE\f\nD\f\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n\u0011\u0011\u0011\n&/,3\u00036FRUH\n\u0013\u0011\u0016\u0017\u0019\n\u0013\u0011\u0019\u0017\u001a\n\u0013\u0011\u0017\u001b\u0014\n\u0013\u0011\u0015\u0014\u001c\n\u0013\u0011\u0015\u0017\u0015\n\u0013\u0011\u0015\u0016\u0019\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n%DVH\u00037H[W\u001db\n7KUHH\u00032EMHFWV\u00037H[W\n6LQJOH\u00032EMHFW\u00037H[W\u0003ZKLFK\nPDWFKLQJb%DVH\u00037H[W\n2WKHUb6LQJOHb2EMHFW\n7H[WV\nSL]]D\u0003DQGbDSSOH\nDQGbGHVN\nDSSOH\nSL]]D\nGHVN\nKDW\nFDU\nD[H\n\u0011\u0011\u0011\n\u0011\u0011\u0011\n725\n,25\nFigure 2. Experimental setup for Text-based Object Retrieval (TOR) and Image-based Object Retrieval (IOR) tasks. a) TOR: The CLIP\ntext encoder generates embeddings for multi-object and single-object texts. Cosine similarity scores are calculated between the base text\nembedding and single-object text embeddings to identify the most similar object. b) IOR: The CLIP image encoder generates embeddings\nfor multi-object and single-object images. Cosine similarity scores are calculated between the base image embedding and single-object\nimage embeddings to identify the most similar object.\nTable 1. Performance on TOR for ComCO datasets\nTask\nModel\nFirst Obj\nSecond Obj\nThird Obj\nFourth Obj\nTOR\nCLIP LAION\n63.96\n21.59\n10.68\n3.76\nCLIP Datacomp\n71.13\n16.26\n8.74\n3.87\nCLIP Roberta\n44.03\n23.73\n18.07\n14.18\nSIGLIP\n58.11\n21.16\n10.99\n9.73\nCLIP openAI\n50.31\n20.74\n14.45\n6.79\nNegCLIP\n51.63\n28.92\n14.86\n4.59\nSugarCrepe\n44.29\n30.32\n18.73\n6.66\nTable 2. Performance on IOR for ComCO datasets\nTask\nModel\nLarge Object\nSmall Obj 1\nSmall Obj 2\nSmall Obj 3\nIOR\nCLIP LAION\n85.45\n6.36\n5.45\n2.73\nCLIP Datacomp\n85.16\n5.65\n4.95\n4.24\nCLIP Roberta\n87.40\n8.66\n2.36\n1.57\nSIGLIP\n77.66\n10.11\n6.38\n5.85\nCLIP openAI\n65.22\n17.39\n8.70\n8.70\nNegCLIP\n61.67\n15.00\n13.33\n10.00\nSugarCrepe\n60.0\n18.38\n16.85\n4.7\nobjects instead of single-object images. These object names\nwere embedded using CLIP\u2019s text encoder, allowing us to\nperform a retrieval task consistent with the IOR methodol-\nogy while adapting to the constraints of the COCO dataset.\nTables 3 and 4 present the results of our COCO dataset\nexperiments. In TOR, the \ufb01rst-mentioned object in COCO\ncaptions was retrieved with higher accuracy, which aligns\nwith our earlier \ufb01ndings of bias in the text encoder. Simi-\nlarly, in IOR, larger objects in COCO images were retrieved\nmore accurately, consistent with the trends observed in our\nsynthetic dataset experiments. The I2TOR results further\ncon\ufb01rmed this bias, demonstrating that even when using\ntextual object representations, the bias towards larger ob-\nTable 3. Performance on TOR for coco dataset\nTask\nModel\nFirst Obj\nSecond Obj\nThird Obj\nFourth Obj\nTOR\nCLIP openAI\n35.24\n21.90\n20.48\n22.38\nCLIP LAION\n67.89\n13.76\n8.26\n10.09\nCLIP Datacomp\n57.68\n17.68\n12.75\n11.88\nCLIP Roberta\n40.78\n23.30\n20.39\n15.53\nSIGLIP\n49.47\n26.84\n12.11\n11.58\nNegCLIP\n38.69\n22.11\n17.09\n22.11\nTable 4. Performance on IOR for coco dataset\nTask\nModel\nLarge Object\nSmall Obj 1\nSmall Obj 2\nSmall Obj 3\nIOR\nCLIP openAI\n43.02\n28.82\n17.13\n11.03\nCLIP LAION\n39.44\n28.45\n17.70\n14.41\nCLIP Datacomp\n36.71\n29.55\n19.13\n14.61\nCLIP Roberta\n36.71\n28.61\n19.82\n14.86\nSIGLIP\n36.63\n28.29\n20.02\n15.06\nNegCLIP\n44.04\n28.86\n16.48\n10.62\nI2TOR\nCLIP openAI\n51.49\n24.87\n13.68\n9.97\nCLIP LAION\n45.50\n27.02\n15.91\n11.56\nCLIP Datacomp\n46.64\n26.82\n14.53\n12.01\nCLIP Roberta\n44.69\n26.98\n16.04\n12.29\nSIGLIP\n47.09\n27.07\n15.10\n10.74\nNegCLIP\n49.04\n27.07\n14.08\n9.81\njects persists.\nOur experiments reveal two signi\ufb01cant biases in the\nCLIP model: the text encoder shows a strong preference for\nthe \ufb01rst mentioned object in textual descriptions, while the\nimage encoder exhibits greater sensitivity to larger objects\nin images. These biases can signi\ufb01cantly impact the overall\nsystem performance in various vision-language tasks, par-\nticularly in multi-object scenarios.\n9311\n\n=== Page 5 ===\n4. Origin of Bias in CLIP Models\nIn this section, we investigate the potential origins of the\nbiases observed in CLIP models and provide evidence sup-\nporting our hypotheses.\n4.1. Bias in the Image Encoder\nThe observed bias favoring larger objects within the image\ndomain can be attributed to the architectural characteristics\nof Vision Transformers (ViT) [2] utilized in CLIP\u2019s image\nencoder. Our hypothesis is that larger objects, which occupy\na greater number of patches in the ViT\u2019s patch-based image\nrepresentation, exert a more signi\ufb01cant in\ufb02uence on the \ufb01-\nnal class (CLS) token representation. This bias is not exclu-\nsive to CLIP; it appears to be a consistent feature across ViT\nmodels, as demonstrated by our experiments detailed in the\nappendix.\nTo substantiate this hypothesis, we designed an experi-\nment to quantify the attention allocated by the CLS token\nto each image patch. By calculating the cumulative atten-\ntion received by each object from the CLS token, we could\nassess the in\ufb02uence of object size on attention allocation.\nWe applied this analysis to our three-object ComCO dataset,\nand the results are illustrated in Figure 3. The \ufb01ndings con-\n\ufb01rm our hypothesis: larger objects indeed receive more at-\ntention from the CLS token.\n4.2. Bias in the Text Encoder\nWe explore the bias present in the text encoder from two\nperspectives: the attention mechanism in the model struc-\nture and the model\u2019s training method.\n4.2.1. Impact of Attention Mechanism\nText encoder models can be categorized based on their at-\ntention mechanisms: uni-directional (causal) attention and\nbi-directional attention. In models with causal attention,\neach token attends only to preceding tokens, whereas in bi-\ndirectional models, each token attends to all tokens in the\nsequence.\nWhen OpenAI introduced the CLIP model, its text en-\ncoder employed causal attention, meaning each token could\nonly attend to tokens before it and itself.\nThis differs\nfrom typical self-attention mechanisms, where tokens at-\ntend to all other tokens. Most CLIP models use causal self-\nattention, with the exception of the variant using the XLM-\nRoberta text encoder, which also employs self-attention.\nHowever, as shown in Table 1, even this model exhibits the\nmentioned bias. This indicates that the bias does not origi-\nnate from the attention mechanism itself.\n4.2.2. Role of Training Method\nTo determine whether the observed bias is speci\ufb01c to CLIP\nmodels, we compared CLIP\u2019s text encoder with two other\nTable 5. Performance on TOC and TOR for ComCO datasets\nTask\nModel\nFirst Obj\nSecond Obj\nThird Obj\nFourth Obj\nTOR\nCLIP\n56.28\n22.71\n13.17\n7.48\nSBERT\n29.02\n19.80\n17.50\n33.57\nSimCSE [7]\n27.59\n19.07\n17.76\n34.83\nmodels designed to embed sentences into a meaningful se-\nmantic space: Sentence-BERT (SBERT) [14] and SimCSE\n[7]. The primary distinction is that CLIP\u2019s embedding space\nis shared between images and text, whereas SBERT and\nSimCSE operate solely in the text domain.\nWe conducted the TOR experiment on our dataset using\nthese models. As presented in Table 5, the bias observed in\nCLIP differs from that in the other models. This suggests\nthat CLIP\u2019s unique training method, which aligns images\nand text in a shared embedding space through contrastive\nlearning, contributes to the bias. Therefore, to uncover the\nroot cause of the bias, we focus on the speci\ufb01cs of CLIP\u2019s\ntraining procedure.\n4.3. Hypothesized Origin of Text-Side Bias in CLIP\nWe hypothesize that the text-side bias in CLIP, which fa-\nvors objects mentioned earlier in text descriptions, origi-\nnates from the image-side bias toward larger objects and is\ntransferred to the text encoder during contrastive training.\nWe present evidence supporting this hypothesis through two\nkey claims and an analysis of the training progression.\nClaim 1: Larger Objects Have More In\ufb02uence on Text\nEmbeddings.\nBuilding upon the established image-side\nbias discussed earlier, we posit that objects with larger\nphysical sizes exert more in\ufb02uence on CLIP\u2019s text em-\nbeddings due to the alignment enforced during contrastive\ntraining. To test this, we categorized objects in the Domain-\nNet dataset into large, medium, and small groups based on\ntheir relative physical sizes in real-world (with the full list of\nobjects provided in the appendix 7.10). Speci\ufb01cally, objects\nsmaller than a school bag were categorized as small, objects\nsized between a school bag and a medium-sized car were\nclassi\ufb01ed as medium, and objects larger than a car\u2014up to\nsigni\ufb01cantly larger items\u2014were considered large. We then\nconstructed two sets of sentences, each containing four ob-\njects: one set with a large object mentioned \ufb01rst followed by\nthree medium-sized objects, and another with a small object\nmentioned \ufb01rst followed by three medium-sized objects.\nFigure 4.a compares the TOR accuracy for the \ufb01rst ob-\nject in these two groups. The higher TOR accuracy for sen-\ntences beginning with large objects supports our hypothe-\nsis that larger objects, when mentioned \ufb01rst, have a more\nsigni\ufb01cant impact on the text embeddings due to the cross-\nmodal alignment with their prominent representation in im-\nages.\n9312\n\n=== Page 6 ===\na)\nb)\nFigure 3. Attention allocation from the CLS token to objects of different sizes in the ComCO dataset. a) Qualitative results showing the\nCLS token\u2019s attention to each object. b) Quantitative analysis of attention distribution across 8,000 images, with each image containing one\nlarge and two small objects. The bar chart shows the average attention allocated to the large object versus the smaller ones, demonstrating\na bias towards larger objects.\na)\nb)\nc)\nFigure 4. a) Top-1 Object Retrieval accuracy comparison for sentences where the \ufb01rst object is either large or small. The higher TOR\naccuracy for sentences beginning with large objects supports the hypothesis that larger objects, when mentioned \ufb01rst, exert a stronger\nin\ufb02uence on text embeddings due to cross-modal alignment with their prominent visual representation in images. b) Distribution of the\nposition of the largest object within image captions from the LAION datasets. The results show a consistent bias where larger objects\ntend to be mentioned earlier in text descriptions. c) Progression of TOR rates across different training stages, indicating that text-side bias\nstrengthens as the model is exposed to more data, suggesting the cumulative effect of image-side bias being transferred to the text encoder\nthrough contrastive learning.\nClaim 2: Caption Bias in Training Datasets.\nTo inves-\ntigate potential biases in CLIP\u2019s training data, we analyzed\nboth the LAION [19] and COCO datasets. Due to limited\ncomputational resources and the large size of the LAION\ndataset, which contains over 2 billion image-text pairs, we\nrandomly selected a subset of 200,000 samples for our anal-\nysis. Using the Llama3 model, we extracted objects from\nthe image captions and employed the Language Segment-\nAnything tool to generate object masks in the correspond-\ning images, calculating their areas based on these masks. A\ndetailed description of our LAION dataset analysis method-\nology can be found in Appendix 7.8.\nFigure4.b shows the position of the largest object within\neach caption. The results indicate that, in the majority of\ncases, the largest object in an image is mentioned earlier\nin its caption. The same experiment was conducted on the\nCOCO dataset, with detailed results and the distribution for\ntwo to \ufb01ve object scenarios provided in Appendix 7.9. This\ndemonstrates a consistent bias in the training data, where\nlarger objects are not only more visually prominent but are\nalso described earlier in text annotations.\nAnalysis of Bias Development During Training.\nTo fur-\nther validate our hypothesis, we examined the progression\nof text-side bias during CLIP\u2019s training. We utilized model\ncheckpoints from the LAION dataset at \ufb01ve training stages,\ncorresponding to exposure to 2, 4, 6, 8, and 10 billion sam-\nples. We conducted TOR experiments at each stage, focus-\n9313\n\n=== Page 7 ===\ning on the retrieval accuracy for the \ufb01rst object mentioned\nin text descriptions.\nFigure4.c depicts the evolution of the TOR rate across\ndifferent training stages for scenarios with varying numbers\nof objects (from 3 to 8). The consistent upward trend in\nthe TOR rate as the model is exposed to more training data\nsuggests that the text-side bias strengthens over time, likely\ndue to the cumulative effect of the image-side bias being\ntransferred to the text encoder through contrastive learning.\nIncomplete Text Representation of CLIP\nHere we want\nto theoretically highlight why the CLIP text encoder could\nlearn an incomplete representation of the text. Let z and\nw represent a latent representation of an image content de-\nscribed in the caption, and such visual content not men-\ntioned in the text, respectively. For example, z represents\nthe fact that an image contains \u201ca horse that is eating the\ngrass.\u201d In this case, w might represent other details in the\nimage, like the \u201chorse color,\u201d \u201cwhere the horse is located,\u201d\netc. We assume a data generative process as follows:\nI := g(z,w)\nT := h(z),\nwhere I is the image, and T is its corresponding caption.\nNow we want to learn a joint embedding of the image\nand text through the CLIP. Here, we assume that f\u03b8(.) and\ni\u03c9(.) as learnable functions that map the image and text into\nthe joint embedding space, respectively.\nTheorem 1 Let elements of z be independent, zero-mean,\nand unit-variance. The contrastive loss for the ideal text en-\ncoder, i\u03c9(T) = z converges to that of a non-ideal incomplete\none, i.e. i\u03c9\u2032(T) = zs, where zs is the \ufb01rst d \u2212k dimensions\nof z, with k being a constant, and d \u2192\u221e.\nProof: The contrastive loss in making this learning hap-\npen can be written as:\nEz,z\u2032,w\n\u0002\nexp(sim(z,z))\nexp(sim(z,z))+\u2211k exp(sim(z,z\u2032\nk))\n\u0003\n(1)\nwith\nsim(z,z\u2032) = S(f\u03b8(g(z,w),i\u03c9(h(z\u2032)))),\nand z and {z\u2032\nk|1 \u2264k \u2264b} are b + 1 i.i.d. samples of the\ncontent in the representation space, and S is some normal-\nized similarity metric, e.g. cosine similarity, and b+1 is the\nbatch size. We assume that elements of z are independent,\nunit-variance, and zero mean. We further assume that the\ndimensionality of z, denoted as d, goes to in\ufb01nity.\nUnder such conditions, and based on Law of Large\nNumbers, \u2225z\u2225\np\u2212\u2192\n\u221a\nd, when d is large.\nTherefore, for\nany two independent copies of z, z\u2032\nk, we have sim(z,z\u2032\nk) =\nz\u22a4z\u2032\nk/(\u2225z\u2225\u2225z\u2032\nk\u2225)\np\u2212\u21920.\nIt is evident that in the ideal case, f\u03b8(g(z,w)) = z and\nalso i\u03c9(h(z)) = z, so the contrastive loss would converge\nto e/(e + b), as the numerator is e, and the second term in\nthe denominator converges to exp(0) = 1, according to the\nMann-Wald\u2019s theorem.\nHowever, we show that other learning of this representa-\ntion could achieve the same amount of loss. For instance, let\nzs be the \ufb01rst d \u2212k elements of z, with k being a constant.\nWe show that if f\u03b8\u2032(I) = zs and i\u03c9\u2032(T) = zs, the same loss\nwould be achieved in the limit of large d. To see this, note\nthat the numerator stays the same, i.e. e, while the second\nterm in the denominator still converges to bexp(0) = b.\nThis means that even if the image and text encoder of\nthe CLIP only partially recover the content embedding, they\nreach an excellent loss. But such possible incomplete rep-\nresentations of z are combinatorially large, making conver-\ngence of the CLIP to such local minima pretty likely. This\nmakes the text encoding of CLIP be far from ideal. Fur-\nthermore, the text encoder would become biased, depend-\ning on which of such local minima it converges to. Based\non this explanation, we would expect a text encoder that has\nlearned a complete representation to exhibit such biases to a\nlesser degree. As mentioned earlier, the subject of learning\ntext representations in VLMs that are discriminative of hard\nnegatives (e.g. NegCLIP) has been around for few years.\nWe tested one of strongest such models, [8], in our bench-\nmark to validate the hypothesis that an incomplete text rep-\nresentation is one of the causes of the bias in the VLMs.\nWe noticed that this model shows lower bias based on our\nbenchmark (see the SugarCrepe model in tables 1 and 2).\nWe have developed an initial approach to address the\nidenti\ufb01ed bias in the CLIP model, which is presented in\nAppendix 7.12. While this method is speci\ufb01c to our cur-\nrent dataset, it represents a promising step toward address-\ning these challenges and can inspire further advancements.\nThis work demonstrates our commitment to exploring prac-\ntical solutions while maintaining the primary focus of this\nstudy on the analysis of bias and its implications.\n5. Practical Impacts of Encoder Biases\nThe biases observed in CLIP\u2019s image and text encoders sig-\nni\ufb01cantly impact model performance in real-world appli-\ncations. This section explores how these biases manifest in\nimage-text matching tasks, while further analyses of text-to-\nimage generation impacts are presented in Appendix 7.11.\nOur analysis in this section serves two primary purposes.\nFirst, it provides concrete evidence of how these theoretical\nbiases can translate into practical limitations. Second, it of-\nfers insights into potential areas for improvement in vision-\nlanguage models, particularly in handling complex, multi-\n9314\n\n=== Page 8 ===\nFigure 5. An example of the correct and incorrect caption structures in the \ufb01rst and second scenarios.\nobject scenarios. Through a series of carefully designed ex-\nperiments, we illustrate how the biases in both text and im-\nage encoders can lead to unexpected or suboptimal results\nin tasks that are crucial for many downstream applications.\n5.1. Image-Text Matching\nBuilding upon our \ufb01ndings of biases in CLIP\u2019s image and\ntext encoders, we now demonstrate how these biases tangi-\nbly affect the model\u2019s performance in image-caption match-\ning tasks. We designed two experimental scenarios, con-\nducted on both the ComCO and COCO datasets, to evaluate\nthese biases. The results of these experiments are summa-\nrized in Table 6. To better illustrate the differences between\nthese two scenarios, an example of the caption structures is\nshown in Figure 5. In each scenario, we created incorrect\ncaptions by switching one object in the caption with an ob-\nject that is not present in the image. Additionally, GPT-4O\n[1] was used to rewrite the captions in the COCO dataset.\nFirst Scenario\nIn the \ufb01rst scenario, biases assist the\nmodel in distinguishing between the correct and incorrect\ncaptions. In the correct captions, the largest object in the\nimage is placed at the beginning, aligning with the model\u2019s\nbias towards prioritizing \ufb01rst-mentioned objects and larger\nobjects. For the incorrect captions, the non-existent object is\ndeliberately placed at the beginning, which helps the model\nrecognize the difference between the correct and incorrect\ncaptions more effectively. This positioning emphasizes the\ndiscrepancy early on, allowing the model to better detect the\nmismatch between the caption and the image. The perfor-\nmance of different models in this scenario can be seen in\nTable 6 under the \u201dFirst Scenario\u201d column.\nSecond Scenario\nIn the second scenario, biases lead the\nmodel to make errors. The correct captions place the largest\nobject at the end of the sentence, disrupting the model\u2019s\nbias towards objects mentioned earlier and its preference\nfor larger objects. In the incorrect captions, the non-existent\nobject is placed at the end, making it more dif\ufb01cult for the\nmodel to differentiate between correct and incorrect cap-\ntions as its attention is drawn away from the critical discrep-\nancies. The performance of different models in this scenario\nis shown in Table 6 under the \u201dSecond Scenario\u201d column.\nTable 6. Performance Comparison on Image-Text Matching for\nComCO and COCO Datasets\nDataset\nModel\nFirst Scenario\nSecond Scenario\nComCO\nCLIP Datacomp [6]\n99.99\n67.50\nCLIP Roberta\n99.98\n64.75\nSIGLIP [22]\n99.49\n72.36\nCLIP openAI\n99.59\n52.23\nNegCLIP\n96.82\n46.94\nSugarCrepe\n98.55\n60.43\nCOCO\nCLIP Datacomp [6]\n71.2\n54.2\nCLIP Roberta\n72.2\n54.1\nSIGLIP [22]\n64.8\n39.5\nCLIP openAI\n63.5\n26.4\nNegCLIP\n72\n28.7\nSugarCrepe\n80.0\n40.9\nBy comparing these two scenarios, we demonstrate that\nbiases in CLIP can either help or hinder the model\u2019s perfor-\nmance depending on how captions are structured. The ex-\nperimental results, particularly with the use of GPT-4O for\ncaption rephrasing in the COCO dataset, reveal how such\nbiases can in\ufb02uence the accuracy of image-text matching\ntasks. These biases must be addressed to improve CLIP\u2019s\nrobustness in real-world multi-object scenarios.\nFor further insights on how these biases affect text-to-\nimage generation, refer to our extended experiments in Ap-\npendix 7.11.\n6. Conclusion\nThis study uncovers biases in CLIP\u2019s encoders, with the\ntext encoder favoring \ufb01rst-mentioned objects and the im-\nage encoder emphasizing larger ones, which impacts per-\nformance in multi-object tasks. Using the ComCO dataset,\nwe highlighted these biases\u2019 effects on object representation\nand positioning, underscoring the need for balanced train-\ning. We attribute these biases to CLIP\u2019s contrastive frame-\nwork, where alignment issues propagate across modalities.\nAddressing these biases is essential for vision-language ad-\nvancements, as seen with models like Stable Diffusion.\n9315\n\n=== Page 9 ===\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 8\n[2] Dosovitskiy Alexey. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint arXiv:\n2010.11929, 2020. 5\n[3] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev.\nReproducible\nscaling laws for contrastive language-image learning.\nIn\n2023 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR). IEEE, 2023. 1\n[4] Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sas-\ntry, Evangelos Milios, Sageev Oore, and Hassan Saj-\njad.\nSugarcrepe++ dataset: Vision-language model sensi-\ntivity to semantic and lexical alterations.\narXiv preprint\narXiv:2406.11171, 2024. 1\n[5] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Or-\ngad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek\nRamanujan, Yonatan Bitton, Kalyani Marathe, Stephen\nMussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna,\nPang Wei Koh, Olga Saukh, Alexander Ratner, Shuran\nSong, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,\nSewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon,\nVaishaal Shankar, and Ludwig Schmidt.\nDatacomp: In\nsearch of the next generation of multimodal datasets, 2023.\n1\n[6] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-\nacomp:\nIn search of the next generation of multimodal\ndatasets. Advances in Neural Information Processing Sys-\ntems, 36, 2024. 8, 17\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen.\nSimcse:\nSimple contrastive learning of sentence embeddings. arXiv\npreprint arXiv:2104.08821, 2021. 5\n[8] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kem-\nbhavi, and Ranjay Krishna.\nSugarcrepe: Fixing hackable\nbenchmarks for vision-language compositionality. Advances\nin neural information processing systems, 36, 2024. 1, 7\n[9] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona, Deva\nRamanan, C. Lawrence Zitnick, and Piotr Doll\u00b4ar. Microsoft\ncoco: Common objects in context, 2015. 2\n[10] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi,\nIrena Gao, and Ranjay Krishna. Crepe: Can vision-language\nfoundation models reason compositionally? In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 10910\u201310921, 2023. 1\n[11] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\nSaenko, and Bo Wang. Moment matching for multi-source\ndomain adaptation. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 1406\u20131415,\n2019. 3\n[12] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 17\n[13] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision, 2021. 1\n[14] N Reimers.\nSentence-bert:\nSentence embeddings using\nsiamese bert-networks.\narXiv preprint arXiv:1908.10084,\n2019. 5\n[15] Dillon Reis, Jordan Kupec, Jacqueline Hong, and Ahmad\nDaoudi. Real-time \ufb02ying object detection with yolov8. arXiv\npreprint arXiv:2305.09972, 2023. 17\n[16] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, 2022. 17\n[17] Ugur Sahin, Hang Li, Qadeer Khan, Daniel Cremers, and\nVolker Tresp. Enhancing multimodal compositional reason-\ning of visual language models with generative negative min-\ning. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pages 5563\u20135573, 2024. 1\n[18] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-\ufb01ltered 400 million image-text pairs,\n2021. 1\n[19] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 6\n[20] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace Ross.\nWinoground: Probing vision and language models for visio-\nlinguistic compositionality. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5238\u20135248, 2022. 1\n[21] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,\nDan Jurafsky, and James Zou.\nWhen and why vision-\nlanguage models behave like bags-of-words, and what to\ndo about it?\nIn The Eleventh International Conference on\nLearning Representations, 2023. 1\n[22] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 11975\u201311986, 2023. 8, 17\n[23] Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan\nShen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin.\nVl-\nchecklist:\nEvaluating pre-trained vision-language models\n9316\n\n=== Page 10 ===\nwith objects, attributes and relations.\narXiv preprint\narXiv:2207.00221, 2022. 1\n9317\n",
      "searchable_content": "page 1 clip under the microscope a fine-grained analysis of multi-object representation reza abbasi ali nazari aminreza se\ufb01d mohammadali banayeeanzade mohammad hossein rohban mahdieh soleymani baghshah sharif university of technology tehran iran reza.abbasi ali.nazari02 aminreza.sefid a.banayeean rohban soleymani sharif.edu abstract contrastive language-image pre-training clip mod- els excel in zero-shot classi\ufb01cation yet face challenges in complex multi-object scenarios. this study offers a com- prehensive analysis of clip s limitations in these contexts using a specialized dataset comco designed to evaluate clip s encoders in diverse multi-object scenarios. our \ufb01ndings reveal signi\ufb01cant biases the text encoder prior- itizes \ufb01rst-mentioned objects and the image encoder fa- vors larger objects. through retrieval and classi\ufb01cation tasks we quantify these biases across multiple clip vari- ants and trace their origins to clip s training process sup- ported by analyses of the laion dataset and training pro- gression. our image-text matching experiments show sub- stantial performance drops when object size or token order changes underscoring clip s instability with rephrased but semantically similar captions. extending this to longer captions and text-to-image models like stable diffusion we demonstrate how prompt order in\ufb02uences object promi- nence in generated images. for more details and access to our dataset and analysis code visit our project repository https clip-oscope.github.io . 1. introduction the convergence of vision and language in arti\ufb01cial in- telligence has led to the development of vision-language models vlms that can interpret and generate multimodal content. among these openai s contrastive language- image pre-training clip model 13 has been particu- larly in\ufb02uential demonstrating remarkable capabilities in zero-shot image classi\ufb01cation and setting new standards for multimodal understanding 3 5 18 20 . the success of clip has catalyzed a wide array of applications from im- age retrieval and visual question answering to text-to-image generation signifying a paradigm shift in how models per- ceive and relate visual and linguistic information. visual language models like clip face signi\ufb01cant challenges in understanding and reasoning about complex scenes with multiple objects and intricate relationships. clip struggles to identify distinct objects and model their relationships accurately especially when captions contain the same objects but differ in their relationships. this re- sults in dif\ufb01culty distinguishing between similar captions with different object relationships. several benchmark datasets have been introduced to elucidate the limitations of existing models in capturing subtle relational nuances. no- tably winoground 20 vl-checklist 23 aro 21 and crepe 10 have been instrumental in evaluating models capacities to accurately match images with semantically ap- propriate captions. numerous studies have addressed compositionality chal- lenges in multi-object scenarios often through end-to-end methods like \ufb01ne-tuning with hard-negative samples 21 to improve model performance. however these approaches have faced criticism and subsequent re\ufb01nement as seen in methods like sugarcrepe 8 and 17 which generate negative captions with minor structural changes or llms to highlight semantic distinctions. while most focus on clip s ability to distinguish structurally similar yet concep- tually different captions few studies such as dumpala et al. 4 explore clip s performance on semantically equivalent but structurally distinct captions revealing a gap in under- standing clip s inconsistency with such prompts. while previous studies have advanced our understanding of clip s limitations our work uniquely focuses on clip s performance with semantically equivalent but structurally varied captions rather than simply distinguishing conceptu- ally different captions. this shift enables a deeper exam- ination of the model s grasp of language and visual con- tent where systematic errors reveal potential biases. un- like prior works that primarily propose benchmarks or end- to-end solutions we investigate the root causes of clip s behavior delving into the mechanisms of both image and text encoders to uncover why the model displays biases and lacks robustness to certain linguistic and visual varia- this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 9308 page 2 figure 1. overview of our key contributions. step 1 we create comco dataset for controlled multi-object experiments. step 2 we identify biases in clip s image encoder favoring larger objects and text encoder prioritizing \ufb01rst-mentioned objects . step 3 we investigate the origin of these biases \ufb01nding a connection to training data characteristics. step 4 we demonstrate the practical impacts of these biases on image-text matching task showing how they affect model performance in multi-object scenarios. tions. to support this analysis we introduce the comco dataset purpose-built for examining clip s performance under controlled multi-object scenarios. our study spans multiple versions of clip trained on diverse datasets and ar- chitectures ensuring the broad applicability of our \ufb01ndings. this comprehensive approach aims to deepen our under- standing of clip s limitations and pave the way for more adaptable vision-language models. beyond clip our in- sights have signi\ufb01cant implications for text-to-image t2i generative models and multimodal large language models mllms where decoding clip s encoding intricacies can inform advancements in arti\ufb01cial intelligence across do- mains. as shown in figure 1 our key contributions are as follows development of novel dataset we introduce comco a specialized dataset for creating controlled multi-object scenarios. unlike previous benchmarks comco al- lows control over object size and caption order enabling precise analysis of model performance across composi- tional challenges and enhancing understanding of vlms strengths and weaknesses. encoder analysis we conduct an in-depth examination of clip s image and text encoders in multi-object scenes revealing weaknesses in preserving information for object distinction and identifying where compositional informa- tion is lost. bias identi\ufb01cation our study reveals that clip s im- age encoder prefers larger objects while the text encoder favors \ufb01rst-mentioned and visually larger objects high- lighting biases in clip s handling of visual and linguistic information. investigation of bias origins we explore the origins of these biases showing that larger objects are often men- tioned earlier in clip s training captions and are favored in embeddings due to the abundance of their visual to- kens. we substantiate this with analyses of the laion dataset and clip s training progression. practical impact we show how these biases affect per- formance in multi-object tasks with signi\ufb01cant drops in image-text matching accuracy in comco and coco 9 . these biases also extend to text-to-image models in\ufb02u- encing object prominence based on prompt order. these \ufb01ndings reveal how biases in clip s text and im- age encoders signi\ufb01cantly reduce its performance in multi- object scenarios emphasizing the need to address these bi- ases to enhance vision-language models robustness. our work offers key insights into clip s behavior and lays groundwork for improving model performance in real- world applications. 2. methodology 2.1. dataset design to thoroughly evaluate the performance of clip models in multi-object scenarios under controlled conditions we constructed the comco complex coco objects dataset. utilizing blender software allowed us precise control over the number location and dimensions of objects in the im- ages see appendix 7.1 . the comco dataset comprises 72 objects derived from the coco dataset. we generated 9309 page 3 images containing 2 3 4 and 5 objects. each image is paired with a speci\ufb01c caption that accurately describes the objects present. this approach ensures high control over the dataset and minimizes confounding factors providing a robust platform for evaluating the clip models. we deliberately chose not to use text-to-image models for generating these datasets due to two main reasons. first these models often lack the capability to produce high- quality fully controlled multi-object images. second since clip is used in many of these models utilizing them could introduce unwanted biases into our evaluations. 2.2. experimental framework for encoder analy- sis the main goal of this study is to evaluate the performance of clip s text and image encoders separately in multi-object scenarios. we aim to analyze the impact and contribution of each object in the \ufb01nal output of the encoders. to achieve this we conducted experiments using our designed comco dataset with images and captions containing two to \ufb01ve ob- jects. to ensure the generalizability of our \ufb01ndings we also validated our results on the widely-used coco dataset. we designed two sets of experiments retrieval-based experi- ments and classi\ufb01cation-based experiments. given the con- sistency of the results in both types of experiments we have included the classi\ufb01cation results in the appendix 7.2 and 7.4 and explain the retrieval-based experiments bellow. 2.2.1. text-based object retrieval tor the text-based object retrieval task evaluates how well clip s text encoder can identify individual objects within multi-object captions. as illustrated in figure 2a this ex- periment involves several steps first we use clip s text encoder to create embeddings for both multi-object captions and single-object captions. we then measure the similar- ity between each multi-object caption embedding and all single-object caption embeddings. the single-object cap- tion with the highest similarity score is considered the re- trieved object. to assess performance we calculate re- trieval accuracy for each object position in the multi-object captions. this helps us identify any biases related to an object s position within a caption such as favoring objects mentioned \ufb01rst or last. 2.2.2. image-based object retrieval ior the image-based object retrieval task is similar to tor but focuses on clip s image encoder. as shown in fig- ure 2b this experiment involves several steps we begin by using clip s image encoder to generate embeddings for multi-object images and single-object images. we then compute similarity scores between each multi-object image embedding and all single-object image embeddings. the single-object image with the highest similarity score is con- sidered the retrieved object. to evaluate performance we calculate retrieval accuracy for different object size cate- gories e.g. large small within the multi-object images. this allows us to determine if the image encoder shows any preference for objects of a particular size. we also experimented with a variation of comco called simco where objects were replaced with simple geometric shapes from the clevr dataset. this was done to con\ufb01rm that bias persists even with non-natural geometric objects. further details are provided in appendix 7.1. 3. results and analysis our experiments revealed signi\ufb01cant biases in both the text and image encoders of the clip model. this section presents our \ufb01ndings organized by encoder type and focus- ing on retrieval tasks. 3.1. text encoder biases we observed a consistent bias in the text encoder towards the \ufb01rst object mentioned in descriptions. in the tor ex- periment the retrieval accuracy as shown in table 1 was highest for the \ufb01rst object indicating its dominant in\ufb02uence on the overall text representation. this suggests that the text encoder prioritizes the initial object leading to its more accurate retrieval compared to subsequent objects. the de- tailed results for the scenarios involving 2 3 and 5 objects can be found in the appendix 7.3 and experiments on longer caption templates are in appendix 7.6 and 7.7. 3.2. image encoder biases in multi-object images the image encoder exhibited a strong bias towards larger objects. the image-based ob- ject retrieval ior experiment detailed in table 2 shows that larger objects were more frequently and accurately re- trieved during single-object image searches. this \ufb01nding highlights the image encoder s bias towards larger objects which receive disproportionate emphasis in the \ufb01nal image representation. further detailed results speci\ufb01cally for sce- narios with 2 3 and 5 objects are provided in the appendix 7.5. 3.3. coco dataset experiments to validate the generalizability of our \ufb01ndings from the synthetic dataset we conducted similar experiments on the coco dataset which comprises real images with accom- panying captions. this real-world dataset allowed us to in- vestigate whether the previously observed biases persist in more naturalistic settings. due to the absence of single-object images for coco objects we approached the ior experiment in two ways. first we used single-object images from the domainnet dataset 11 as retrieval targets. second we introduced an alternative approach called image-to-text object retrieval i2tor . in i2tor we used the textual names of coco 9310 page 4 dvh pdjh b 7kuhh 2emhfwv pdjh 6lqjoh 2emhfw pdjh zklfk pdwfklqjb dvh pdjh 3 6fruh 3 6fruh 3 6fruh 2wkhub6lqjohb2emhfw pdjhv 3 6fruh 3 6fruh 3 6fruh e d 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3 6fruh 3 6fruh 3 6fruh 3 6fruh 3 6fruh 3 6fruh 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu dvh 7h w b 7kuhh 2emhfwv 7h w 6lqjoh 2emhfw 7h w zklfk pdwfklqjb dvh 7h w 2wkhub6lqjohb2emhfw 7h wv sl d dqgbdssoh dqgbghvn dssoh sl d ghvn kdw fdu d h 725 25 figure 2. experimental setup for text-based object retrieval tor and image-based object retrieval ior tasks. a tor the clip text encoder generates embeddings for multi-object and single-object texts. cosine similarity scores are calculated between the base text embedding and single-object text embeddings to identify the most similar object. b ior the clip image encoder generates embeddings for multi-object and single-object images. cosine similarity scores are calculated between the base image embedding and single-object image embeddings to identify the most similar object. table 1. performance on tor for comco datasets task model first obj second obj third obj fourth obj tor clip laion 63.96 21.59 10.68 3.76 clip datacomp 71.13 16.26 8.74 3.87 clip roberta 44.03 23.73 18.07 14.18 siglip 58.11 21.16 10.99 9.73 clip openai 50.31 20.74 14.45 6.79 negclip 51.63 28.92 14.86 4.59 sugarcrepe 44.29 30.32 18.73 6.66 table 2. performance on ior for comco datasets task model large object small obj 1 small obj 2 small obj 3 ior clip laion 85.45 6.36 5.45 2.73 clip datacomp 85.16 5.65 4.95 4.24 clip roberta 87.40 8.66 2.36 1.57 siglip 77.66 10.11 6.38 5.85 clip openai 65.22 17.39 8.70 8.70 negclip 61.67 15.00 13.33 10.00 sugarcrepe 60.0 18.38 16.85 4.7 objects instead of single-object images. these object names were embedded using clip s text encoder allowing us to perform a retrieval task consistent with the ior methodol- ogy while adapting to the constraints of the coco dataset. tables 3 and 4 present the results of our coco dataset experiments. in tor the \ufb01rst-mentioned object in coco captions was retrieved with higher accuracy which aligns with our earlier \ufb01ndings of bias in the text encoder. simi- larly in ior larger objects in coco images were retrieved more accurately consistent with the trends observed in our synthetic dataset experiments. the i2tor results further con\ufb01rmed this bias demonstrating that even when using textual object representations the bias towards larger ob- table 3. performance on tor for coco dataset task model first obj second obj third obj fourth obj tor clip openai 35.24 21.90 20.48 22.38 clip laion 67.89 13.76 8.26 10.09 clip datacomp 57.68 17.68 12.75 11.88 clip roberta 40.78 23.30 20.39 15.53 siglip 49.47 26.84 12.11 11.58 negclip 38.69 22.11 17.09 22.11 table 4. performance on ior for coco dataset task model large object small obj 1 small obj 2 small obj 3 ior clip openai 43.02 28.82 17.13 11.03 clip laion 39.44 28.45 17.70 14.41 clip datacomp 36.71 29.55 19.13 14.61 clip roberta 36.71 28.61 19.82 14.86 siglip 36.63 28.29 20.02 15.06 negclip 44.04 28.86 16.48 10.62 i2tor clip openai 51.49 24.87 13.68 9.97 clip laion 45.50 27.02 15.91 11.56 clip datacomp 46.64 26.82 14.53 12.01 clip roberta 44.69 26.98 16.04 12.29 siglip 47.09 27.07 15.10 10.74 negclip 49.04 27.07 14.08 9.81 jects persists. our experiments reveal two signi\ufb01cant biases in the clip model the text encoder shows a strong preference for the \ufb01rst mentioned object in textual descriptions while the image encoder exhibits greater sensitivity to larger objects in images. these biases can signi\ufb01cantly impact the overall system performance in various vision-language tasks par- ticularly in multi-object scenarios. 9311 page 5 4. origin of bias in clip models in this section we investigate the potential origins of the biases observed in clip models and provide evidence sup- porting our hypotheses. 4.1. bias in the image encoder the observed bias favoring larger objects within the image domain can be attributed to the architectural characteristics of vision transformers vit 2 utilized in clip s image encoder. our hypothesis is that larger objects which occupy a greater number of patches in the vit s patch-based image representation exert a more signi\ufb01cant in\ufb02uence on the \ufb01- nal class cls token representation. this bias is not exclu- sive to clip it appears to be a consistent feature across vit models as demonstrated by our experiments detailed in the appendix. to substantiate this hypothesis we designed an experi- ment to quantify the attention allocated by the cls token to each image patch. by calculating the cumulative atten- tion received by each object from the cls token we could assess the in\ufb02uence of object size on attention allocation. we applied this analysis to our three-object comco dataset and the results are illustrated in figure 3. the \ufb01ndings con- \ufb01rm our hypothesis larger objects indeed receive more at- tention from the cls token. 4.2. bias in the text encoder we explore the bias present in the text encoder from two perspectives the attention mechanism in the model struc- ture and the model s training method. 4.2.1. impact of attention mechanism text encoder models can be categorized based on their at- tention mechanisms uni-directional causal attention and bi-directional attention. in models with causal attention each token attends only to preceding tokens whereas in bi- directional models each token attends to all tokens in the sequence. when openai introduced the clip model its text en- coder employed causal attention meaning each token could only attend to tokens before it and itself. this differs from typical self-attention mechanisms where tokens at- tend to all other tokens. most clip models use causal self- attention with the exception of the variant using the xlm- roberta text encoder which also employs self-attention. however as shown in table 1 even this model exhibits the mentioned bias. this indicates that the bias does not origi- nate from the attention mechanism itself. 4.2.2. role of training method to determine whether the observed bias is speci\ufb01c to clip models we compared clip s text encoder with two other table 5. performance on toc and tor for comco datasets task model first obj second obj third obj fourth obj tor clip 56.28 22.71 13.17 7.48 sbert 29.02 19.80 17.50 33.57 simcse 7 27.59 19.07 17.76 34.83 models designed to embed sentences into a meaningful se- mantic space sentence-bert sbert 14 and simcse 7 . the primary distinction is that clip s embedding space is shared between images and text whereas sbert and simcse operate solely in the text domain. we conducted the tor experiment on our dataset using these models. as presented in table 5 the bias observed in clip differs from that in the other models. this suggests that clip s unique training method which aligns images and text in a shared embedding space through contrastive learning contributes to the bias. therefore to uncover the root cause of the bias we focus on the speci\ufb01cs of clip s training procedure. 4.3. hypothesized origin of text-side bias in clip we hypothesize that the text-side bias in clip which fa- vors objects mentioned earlier in text descriptions origi- nates from the image-side bias toward larger objects and is transferred to the text encoder during contrastive training. we present evidence supporting this hypothesis through two key claims and an analysis of the training progression. claim 1 larger objects have more in\ufb02uence on text embeddings. building upon the established image-side bias discussed earlier we posit that objects with larger physical sizes exert more in\ufb02uence on clip s text em- beddings due to the alignment enforced during contrastive training. to test this we categorized objects in the domain- net dataset into large medium and small groups based on their relative physical sizes in real-world with the full list of objects provided in the appendix 7.10 . speci\ufb01cally objects smaller than a school bag were categorized as small objects sized between a school bag and a medium-sized car were classi\ufb01ed as medium and objects larger than a car up to signi\ufb01cantly larger items were considered large. we then constructed two sets of sentences each containing four ob- jects one set with a large object mentioned \ufb01rst followed by three medium-sized objects and another with a small object mentioned \ufb01rst followed by three medium-sized objects. figure 4.a compares the tor accuracy for the \ufb01rst ob- ject in these two groups. the higher tor accuracy for sen- tences beginning with large objects supports our hypothe- sis that larger objects when mentioned \ufb01rst have a more signi\ufb01cant impact on the text embeddings due to the cross- modal alignment with their prominent representation in im- ages. 9312 page 6 a b figure 3. attention allocation from the cls token to objects of different sizes in the comco dataset. a qualitative results showing the cls token s attention to each object. b quantitative analysis of attention distribution across 8 000 images with each image containing one large and two small objects. the bar chart shows the average attention allocated to the large object versus the smaller ones demonstrating a bias towards larger objects. a b c figure 4. a top-1 object retrieval accuracy comparison for sentences where the \ufb01rst object is either large or small. the higher tor accuracy for sentences beginning with large objects supports the hypothesis that larger objects when mentioned \ufb01rst exert a stronger in\ufb02uence on text embeddings due to cross-modal alignment with their prominent visual representation in images. b distribution of the position of the largest object within image captions from the laion datasets. the results show a consistent bias where larger objects tend to be mentioned earlier in text descriptions. c progression of tor rates across different training stages indicating that text-side bias strengthens as the model is exposed to more data suggesting the cumulative effect of image-side bias being transferred to the text encoder through contrastive learning. claim 2 caption bias in training datasets. to inves- tigate potential biases in clip s training data we analyzed both the laion 19 and coco datasets. due to limited computational resources and the large size of the laion dataset which contains over 2 billion image-text pairs we randomly selected a subset of 200 000 samples for our anal- ysis. using the llama3 model we extracted objects from the image captions and employed the language segment- anything tool to generate object masks in the correspond- ing images calculating their areas based on these masks. a detailed description of our laion dataset analysis method- ology can be found in appendix 7.8. figure4.b shows the position of the largest object within each caption. the results indicate that in the majority of cases the largest object in an image is mentioned earlier in its caption. the same experiment was conducted on the coco dataset with detailed results and the distribution for two to \ufb01ve object scenarios provided in appendix 7.9. this demonstrates a consistent bias in the training data where larger objects are not only more visually prominent but are also described earlier in text annotations. analysis of bias development during training. to fur- ther validate our hypothesis we examined the progression of text-side bias during clip s training. we utilized model checkpoints from the laion dataset at \ufb01ve training stages corresponding to exposure to 2 4 6 8 and 10 billion sam- ples. we conducted tor experiments at each stage focus- 9313 page 7 ing on the retrieval accuracy for the \ufb01rst object mentioned in text descriptions. figure4.c depicts the evolution of the tor rate across different training stages for scenarios with varying numbers of objects from 3 to 8 . the consistent upward trend in the tor rate as the model is exposed to more training data suggests that the text-side bias strengthens over time likely due to the cumulative effect of the image-side bias being transferred to the text encoder through contrastive learning. incomplete text representation of clip here we want to theoretically highlight why the clip text encoder could learn an incomplete representation of the text. let z and w represent a latent representation of an image content de- scribed in the caption and such visual content not men- tioned in the text respectively. for example z represents the fact that an image contains a horse that is eating the grass. in this case w might represent other details in the image like the horse color where the horse is located etc. we assume a data generative process as follows i g z w t h z where i is the image and t is its corresponding caption. now we want to learn a joint embedding of the image and text through the clip. here we assume that f\u03b8 . and i\u03c9 . as learnable functions that map the image and text into the joint embedding space respectively. theorem 1 let elements of z be independent zero-mean and unit-variance. the contrastive loss for the ideal text en- coder i\u03c9 t z converges to that of a non-ideal incomplete one i.e. i\u03c9 t zs where zs is the \ufb01rst d k dimensions of z with k being a constant and d . proof the contrastive loss in making this learning hap- pen can be written as ez z w exp sim z z exp sim z z k exp sim z z k 1 with sim z z s f\u03b8 g z w i\u03c9 h z and z and z k 1 k b are b 1 i.i.d. samples of the content in the representation space and s is some normal- ized similarity metric e.g. cosine similarity and b 1 is the batch size. we assume that elements of z are independent unit-variance and zero mean. we further assume that the dimensionality of z denoted as d goes to in\ufb01nity. under such conditions and based on law of large numbers z p d when d is large. therefore for any two independent copies of z z k we have sim z z k z z k z z k p 0. it is evident that in the ideal case f\u03b8 g z w z and also i\u03c9 h z z so the contrastive loss would converge to e e b as the numerator is e and the second term in the denominator converges to exp 0 1 according to the mann-wald s theorem. however we show that other learning of this representa- tion could achieve the same amount of loss. for instance let zs be the \ufb01rst d k elements of z with k being a constant. we show that if f\u03b8 i zs and i\u03c9 t zs the same loss would be achieved in the limit of large d. to see this note that the numerator stays the same i.e. e while the second term in the denominator still converges to bexp 0 b. this means that even if the image and text encoder of the clip only partially recover the content embedding they reach an excellent loss. but such possible incomplete rep- resentations of z are combinatorially large making conver- gence of the clip to such local minima pretty likely. this makes the text encoding of clip be far from ideal. fur- thermore the text encoder would become biased depend- ing on which of such local minima it converges to. based on this explanation we would expect a text encoder that has learned a complete representation to exhibit such biases to a lesser degree. as mentioned earlier the subject of learning text representations in vlms that are discriminative of hard negatives e.g. negclip has been around for few years. we tested one of strongest such models 8 in our bench- mark to validate the hypothesis that an incomplete text rep- resentation is one of the causes of the bias in the vlms. we noticed that this model shows lower bias based on our benchmark see the sugarcrepe model in tables 1 and 2 . we have developed an initial approach to address the identi\ufb01ed bias in the clip model which is presented in appendix 7.12. while this method is speci\ufb01c to our cur- rent dataset it represents a promising step toward address- ing these challenges and can inspire further advancements. this work demonstrates our commitment to exploring prac- tical solutions while maintaining the primary focus of this study on the analysis of bias and its implications. 5. practical impacts of encoder biases the biases observed in clip s image and text encoders sig- ni\ufb01cantly impact model performance in real-world appli- cations. this section explores how these biases manifest in image-text matching tasks while further analyses of text-to- image generation impacts are presented in appendix 7.11. our analysis in this section serves two primary purposes. first it provides concrete evidence of how these theoretical biases can translate into practical limitations. second it of- fers insights into potential areas for improvement in vision- language models particularly in handling complex multi- 9314 page 8 figure 5. an example of the correct and incorrect caption structures in the \ufb01rst and second scenarios. object scenarios. through a series of carefully designed ex- periments we illustrate how the biases in both text and im- age encoders can lead to unexpected or suboptimal results in tasks that are crucial for many downstream applications. 5.1. image-text matching building upon our \ufb01ndings of biases in clip s image and text encoders we now demonstrate how these biases tangi- bly affect the model s performance in image-caption match- ing tasks. we designed two experimental scenarios con- ducted on both the comco and coco datasets to evaluate these biases. the results of these experiments are summa- rized in table 6. to better illustrate the differences between these two scenarios an example of the caption structures is shown in figure 5. in each scenario we created incorrect captions by switching one object in the caption with an ob- ject that is not present in the image. additionally gpt-4o 1 was used to rewrite the captions in the coco dataset. first scenario in the \ufb01rst scenario biases assist the model in distinguishing between the correct and incorrect captions. in the correct captions the largest object in the image is placed at the beginning aligning with the model s bias towards prioritizing \ufb01rst-mentioned objects and larger objects. for the incorrect captions the non-existent object is deliberately placed at the beginning which helps the model recognize the difference between the correct and incorrect captions more effectively. this positioning emphasizes the discrepancy early on allowing the model to better detect the mismatch between the caption and the image. the perfor- mance of different models in this scenario can be seen in table 6 under the first scenario column. second scenario in the second scenario biases lead the model to make errors. the correct captions place the largest object at the end of the sentence disrupting the model s bias towards objects mentioned earlier and its preference for larger objects. in the incorrect captions the non-existent object is placed at the end making it more dif\ufb01cult for the model to differentiate between correct and incorrect cap- tions as its attention is drawn away from the critical discrep- ancies. the performance of different models in this scenario is shown in table 6 under the second scenario column. table 6. performance comparison on image-text matching for comco and coco datasets dataset model first scenario second scenario comco clip datacomp 6 99.99 67.50 clip roberta 99.98 64.75 siglip 22 99.49 72.36 clip openai 99.59 52.23 negclip 96.82 46.94 sugarcrepe 98.55 60.43 coco clip datacomp 6 71.2 54.2 clip roberta 72.2 54.1 siglip 22 64.8 39.5 clip openai 63.5 26.4 negclip 72 28.7 sugarcrepe 80.0 40.9 by comparing these two scenarios we demonstrate that biases in clip can either help or hinder the model s perfor- mance depending on how captions are structured. the ex- perimental results particularly with the use of gpt-4o for caption rephrasing in the coco dataset reveal how such biases can in\ufb02uence the accuracy of image-text matching tasks. these biases must be addressed to improve clip s robustness in real-world multi-object scenarios. for further insights on how these biases affect text-to- image generation refer to our extended experiments in ap- pendix 7.11. 6. conclusion this study uncovers biases in clip s encoders with the text encoder favoring \ufb01rst-mentioned objects and the im- age encoder emphasizing larger ones which impacts per- formance in multi-object tasks. using the comco dataset we highlighted these biases effects on object representation and positioning underscoring the need for balanced train- ing. we attribute these biases to clip s contrastive frame- work where alignment issues propagate across modalities. addressing these biases is essential for vision-language ad- vancements as seen with models like stable diffusion. 9315 page 9 references 1 josh achiam steven adler sandhini agarwal lama ah- mad ilge akkaya florencia leoni aleman diogo almeida janko altenschmidt sam altman shyamal anadkat et al. gpt-4 technical report. arxiv preprint arxiv 2303.08774 2023. 8 2 dosovitskiy alexey. an image is worth 16x16 words trans- formers for image recognition at scale. arxiv preprint arxiv 2010.11929 2020. 5 3 mehdi cherti romain beaumont ross wightman mitchell wortsman gabriel ilharco cade gordon christoph schuh- mann ludwig schmidt and jenia jitsev. reproducible scaling laws for contrastive language-image learning. in 2023 ieee cvf conference on computer vision and pat- tern recognition cvpr . ieee 2023. 1 4 sri harsha dumpala aman jaiswal chandramouli sas- try evangelos milios sageev oore and hassan saj- jad. sugarcrepe dataset vision-language model sensi- tivity to semantic and lexical alterations. arxiv preprint arxiv 2406.11171 2024. 1 5 samir yitzhak gadre gabriel ilharco alex fang jonathan hayase georgios smyrnis thao nguyen ryan marten mitchell wortsman dhruba ghosh jieyu zhang eyal or- gad rahim entezari giannis daras sarah pratt vivek ramanujan yonatan bitton kalyani marathe stephen mussmann richard vencu mehdi cherti ranjay krishna pang wei koh olga saukh alexander ratner shuran song hannaneh hajishirzi ali farhadi romain beaumont sewoong oh alex dimakis jenia jitsev yair carmon vaishaal shankar and ludwig schmidt. datacomp in search of the next generation of multimodal datasets 2023. 1 6 samir yitzhak gadre gabriel ilharco alex fang jonathan hayase georgios smyrnis thao nguyen ryan marten mitchell wortsman dhruba ghosh jieyu zhang et al. dat- acomp in search of the next generation of multimodal datasets. advances in neural information processing sys- tems 36 2024. 8 17 7 tianyu gao xingcheng yao and danqi chen. simcse simple contrastive learning of sentence embeddings. arxiv preprint arxiv 2104.08821 2021. 5 8 cheng-yu hsieh jieyu zhang zixian ma aniruddha kem- bhavi and ranjay krishna. sugarcrepe fixing hackable benchmarks for vision-language compositionality. advances in neural information processing systems 36 2024. 1 7 9 tsung-yi lin michael maire serge belongie lubomir bourdev ross girshick james hays pietro perona deva ramanan c. lawrence zitnick and piotr doll ar. microsoft coco common objects in context 2015. 2 10 zixian ma jerry hong mustafa omer gul mona gandhi irena gao and ranjay krishna. crepe can vision-language foundation models reason compositionally in proceedings of the ieee cvf conference on computer vision and pat- tern recognition pages 10910 10921 2023. 1 11 xingchao peng qinxun bai xide xia zijun huang kate saenko and bo wang. moment matching for multi-source domain adaptation. in proceedings of the ieee cvf inter- national conference on computer vision pages 1406 1415 2019. 3 12 dustin podell zion english kyle lacey andreas blattmann tim dockhorn jonas m uller joe penna and robin rombach. sdxl improving latent diffusion mod- els for high-resolution image synthesis. arxiv preprint arxiv 2307.01952 2023. 17 13 alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sastry amanda askell pamela mishkin jack clark gretchen krueger and ilya sutskever. learning transferable visual models from natural language supervision 2021. 1 14 n reimers. sentence-bert sentence embeddings using siamese bert-networks. arxiv preprint arxiv 1908.10084 2019. 5 15 dillon reis jordan kupec jacqueline hong and ahmad daoudi. real-time \ufb02ying object detection with yolov8. arxiv preprint arxiv 2305.09972 2023. 17 16 robin rombach andreas blattmann dominik lorenz patrick esser and bj orn ommer. high-resolution image synthesis with latent diffusion models. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr pages 10684 10695 2022. 17 17 ugur sahin hang li qadeer khan daniel cremers and volker tresp. enhancing multimodal compositional reason- ing of visual language models with generative negative min- ing. in proceedings of the ieee cvf winter conference on applications of computer vision pages 5563 5573 2024. 1 18 christoph schuhmann richard vencu romain beaumont robert kaczmarczyk clayton mullis aarush katta theo coombes jenia jitsev and aran komatsuzaki. laion-400m open dataset of clip-\ufb01ltered 400 million image-text pairs 2021. 1 19 christoph schuhmann romain beaumont richard vencu cade gordon ross wightman mehdi cherti theo coombes aarush katta clayton mullis mitchell worts- man et al. laion-5b an open large-scale dataset for training next generation image-text models. advances in neural in- formation processing systems 35 25278 25294 2022. 6 20 tristan thrush ryan jiang max bartolo amanpreet singh adina williams douwe kiela and candace ross. winoground probing vision and language models for visio- linguistic compositionality. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 5238 5248 2022. 1 21 mert yuksekgonul federico bianchi pratyusha kalluri dan jurafsky and james zou. when and why vision- language models behave like bags-of-words and what to do about it in the eleventh international conference on learning representations 2023. 1 22 xiaohua zhai basil mustafa alexander kolesnikov and lucas beyer. sigmoid loss for language image pre-training. in proceedings of the ieee cvf international conference on computer vision pages 11975 11986 2023. 8 17 23 tiancheng zhao tianqi zhang mingwei zhu haozhan shen kyusong lee xiaopeng lu and jianwei yin. vl- checklist evaluating pre-trained vision-language models 9316 page 10 with objects attributes and relations. arxiv preprint arxiv 2207.00221 2022. 1 9317",
      "keywords": [
        "page",
        "clip",
        "under",
        "microscope",
        "fine-grained",
        "analysis",
        "multi-object",
        "representation",
        "reza",
        "abbasi",
        "ali",
        "nazari",
        "aminreza",
        "se\ufb01d",
        "mohammadali",
        "banayeeanzade",
        "mohammad",
        "hossein",
        "rohban",
        "mahdieh",
        "soleymani",
        "baghshah",
        "sharif",
        "university",
        "technology",
        "tehran",
        "iran",
        "reza.abbasi",
        "ali.nazari02",
        "aminreza.sefid",
        "a.banayeean",
        "sharif.edu",
        "abstract",
        "contrastive",
        "language-image",
        "pre-training",
        "mod-",
        "els",
        "excel",
        "zero-shot",
        "classi\ufb01cation",
        "yet",
        "face",
        "challenges",
        "complex",
        "scenarios.",
        "study",
        "offers",
        "com-",
        "prehensive",
        "limitations",
        "contexts",
        "using",
        "specialized",
        "dataset",
        "comco",
        "designed",
        "evaluate",
        "encoders",
        "diverse",
        "\ufb01ndings",
        "reveal",
        "signi\ufb01cant",
        "biases",
        "text",
        "encoder",
        "prior-",
        "itizes",
        "\ufb01rst-mentioned",
        "objects",
        "image",
        "fa-",
        "vors",
        "larger",
        "objects.",
        "through",
        "retrieval",
        "tasks",
        "quantify",
        "across",
        "multiple",
        "vari-",
        "ants",
        "trace",
        "origins",
        "training",
        "process",
        "sup-",
        "ported",
        "analyses",
        "laion",
        "pro-",
        "gression.",
        "image-text",
        "matching",
        "experiments",
        "show",
        "sub-",
        "stantial",
        "performance"
      ],
      "metadata": {
        "filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf",
        "original_filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 1583559,
        "upload_file_id": "upload_1752161152679_uae4hw8qf",
        "upload_timestamp": "2025-07-10T15:25:52.748749",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:25:52.919373+00:00",
      "content_length": 40912,
      "content_type": ".pdf",
      "filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf"
    },
    "3": {
      "file_id": "3",
      "content": "\n=== Page 1 ===\nKEXUAN ZHANG\nACCOUNT SUMMARY | ACCOUNTS\nFOR HOME\n CUSTOMER SERVICE\n|\nSet up a new account\nOpen up a new Toronto Hydro account.\nYour reference number is: C2RVR2\nDate/time submitted: 04/29/2025 07:42 PM\nThank you for submitting your request to set up a new account. Your request will be processed within two business days and you'll receive an\nemail confirmation once complete. If additional information is required, we'll be in touch.\nInformation you provided to us\nNew service address\n72 ESTHER SHINER BLVD SUITE 1210, Toronto, Ontario, M2K 0C4\nPrimary account holder\nFull name: Zhang, Kexuan\nPossession date: 5/1/2025\nPrimary phone number: (437) 361-2016\nEmail address: zhangkexuan0514@outlook.com\nReceive paperless statement: Yes\nDate of birth: 5/14/2002\nCanadian Driver's Licence Number: Z3187-43400-20514\nPricing plan: Time-of-Use (TOU)\nMailing address\n72 ESTHER SHINER BLVD SUITE 1210, Toronto, Ontario, M2K 0C4\nLandlord/property manager information\nFull name: Merve Bekaroglu\nPhone number: (416) 568-6988\nPrint this page\nEasy ways to help you manage your account\nWant an easy way to manage your Toronto Hydro account? Learn more about our My TorontoHydro\u2122 online self-service portal today!\nLooking for ways to better manage your bill? See our conservation programs and find ways to save!\nLearn about our payment options .\nStarting on May 1, summer Time-of-Use (TOU) hours and Tier thresholds will be in effect. Customers on the Ultra-Low Overnight price plan will not be affected,\nand electricity rates will remain unchanged.\n\u00d7\nHome\nBusiness\nContractors & Developers\nLog Out\n",
      "searchable_content": "page 1 kexuan zhang account summary accounts for home customer service set up a new account open up a new toronto hydro account. your reference number is c2rvr2 date time submitted 04 29 2025 07 42 pm thank you for submitting your request to set up a new account. your request will be processed within two business days and you ll receive an email confirmation once complete. if additional information is required we ll be in touch. information you provided to us new service address 72 esther shiner blvd suite 1210 toronto ontario m2k 0c4 primary account holder full name zhang kexuan possession date 5 1 2025 primary phone number 437 361-2016 email address zhangkexuan0514 outlook.com receive paperless statement yes date of birth 5 14 2002 canadian driver s licence number z3187-43400-20514 pricing plan time-of-use tou mailing address 72 esther shiner blvd suite 1210 toronto ontario m2k 0c4 landlord property manager information full name merve bekaroglu phone number 416 568-6988 print this page easy ways to help you manage your account want an easy way to manage your toronto hydro account learn more about our my torontohydro online self-service portal today looking for ways to better manage your bill see our conservation programs and find ways to save learn about our payment options . starting on may 1 summer time-of-use tou hours and tier thresholds will be in effect. customers on the ultra-low overnight price plan will not be affected and electricity rates will remain unchanged. home business contractors developers log out",
      "keywords": [
        "page",
        "kexuan",
        "zhang",
        "account",
        "summary",
        "accounts",
        "home",
        "customer",
        "service",
        "set",
        "new",
        "open",
        "toronto",
        "hydro",
        "account.",
        "reference",
        "number",
        "c2rvr2",
        "date",
        "time",
        "submitted",
        "2025",
        "thank",
        "submitting",
        "request",
        "processed",
        "within",
        "two",
        "business",
        "days",
        "receive",
        "email",
        "confirmation",
        "once",
        "complete.",
        "additional",
        "information",
        "required",
        "touch.",
        "provided",
        "address",
        "esther",
        "shiner",
        "blvd",
        "suite",
        "1210",
        "ontario",
        "m2k",
        "0c4",
        "primary",
        "holder",
        "full",
        "name",
        "possession",
        "phone",
        "437",
        "361-2016",
        "zhangkexuan0514",
        "outlook.com",
        "paperless",
        "statement",
        "yes",
        "birth",
        "2002",
        "canadian",
        "driver",
        "licence",
        "z3187-43400-20514",
        "pricing",
        "plan",
        "time-of-use",
        "tou",
        "mailing",
        "landlord",
        "property",
        "manager",
        "merve",
        "bekaroglu",
        "416",
        "568-6988",
        "print",
        "easy",
        "ways",
        "help",
        "manage",
        "want",
        "way",
        "learn",
        "more",
        "about",
        "torontohydro",
        "online",
        "self-service",
        "portal",
        "today",
        "looking",
        "better",
        "bill",
        "see",
        "conservation"
      ],
      "metadata": {
        "filename": "New Account - Toronto Hydro.pdf",
        "original_filename": "New Account - Toronto Hydro.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 106280,
        "upload_file_id": "upload_1752159887036_brch06orq",
        "upload_timestamp": "2025-07-10T15:04:47.062990",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:04:47.211730+00:00",
      "content_length": 1597,
      "content_type": ".pdf",
      "filename": "New Account - Toronto Hydro.pdf"
    },
    "14": {
      "file_id": "14",
      "content": "\n=== Page 1 ===\nV2Dial\n: Unification of Video and Visual Dialog via Multimodal Experts\nAdnen Abdessaied\nAnna Rohrbach\nMarcus Rohrbach\nAndreas Bulling\nUniversity of Stuttgart, Germany\nTU Darmstadt, Germany\nhessian.AI, Germany\nhttps://www.collaborative-ai.org/publications/abdessaied25_cvpr/\nAbstract\nWe present V2Dial \u2013 a novel expert-based model specif-\nically geared towards simultaneously handling image and\nvideo input data for multimodal conversational tasks. Cur-\nrent multimodal models primarily focus on simpler tasks\n(e.g., VQA, VideoQA, video-text retrieval) and often neglect\nthe more challenging conversational counterparts, such as\nvideo and visual/image dialog. Moreover, works on both\nconversational tasks evolved separately from each other de-\nspite their apparent similarities, limiting their applicability\npotential. To this end, we propose to unify both tasks using a\nsingle model that for the first time jointly learns the spatial\nand temporal features of images and videos by routing them\nthrough dedicated experts and aligns them using matching\nand contrastive learning techniques. Furthermore, we sys-\ntemically study the domain shift between the two tasks by\ninvestigating whether and to what extent these seemingly re-\nlated tasks can mutually benefit from their respective train-\ning data. Extensive evaluations on the widely used video\nand visual dialog datasets of AVSD and VisDial show that\nour model achieves new state-of-the-art results across four\nbenchmarks both in zero-shot and fine-tuning settings.\n1. Introduction\nEnabled by the availability of large-scale training data\n[10, 14, 41] and advances in model design [12, 24, 37, 48,\n52], the field of vision-and-language learning saw unprece-\ndented success in recent years.\nHowever, current multi-\nmodal foundational models [7, 32, 37, 50, 54] still mainly\nfocus on single-round tasks (e.g., VQA [8], VideoQA [58],\nvideo-text and text-video retrieval [59]). In contrast, the sig-\nnificantly more challenging conversational tasks, such as vi-\nsual [1, 22] and video dialog [5], received considerably less\nattention. Furthermore, methods for these different tasks\nhave advanced independently of each other despite the ap-\nparent structural similarities between them. They both op-\nerate on a visual input (i.e. an image or video), a short vi-\nsual description (caption), and a dialog history composed of\nFigure 1. V2Dial\nuses multimodal experts and outperforms\nstate-of-the-art methods on both video and visual dialog in zero-\nshot and fine-tuning evaluation settings.\nprevious question-answer pairs. On the one hand, visual di-\nalog models [4, 15, 16, 44, 56] have been primarily trained\nto rank a list of candidate answers using a Next Sentence\nPrediction (NSP) head similar to BERT [23] and negative\nsampling. Thus, they are benchmarked using retrieval met-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8637\n\n=== Page 2 ===\nrics such as recall (R@k) and normalized discounted cu-\nmulative gain (NDCG). In contrast, video dialog models\n[2, 3, 6, 18, 20, 29, 46, 60] are trained to auto-regressively\npredict the next answer token using teacher forcing [57] and\nare evaluated using language generation metrics.\nIn this work, we mitigate the shortcomings of current\ndialog systems by proposing V2Dial \u2013 a novel multimodal\nexpert-based model capable of unifying video and visual di-\nalog tasks without any architectural changes. Specifically,\nwe train dedicated multimodal expert layers that separately\nprocess the features of each input modality and learn how\nto align them using matching and contrastive learning tech-\nniques. A key novelty of our approach is that we use ded-\nicated experts to jointly learn the spatial and temporal fea-\ntures of images and videos by routing them through the ap-\npropriate experts. Then, we couple these layers with a pre-\ntrained LLM to align their hidden states. Thanks to its mod-\nularity, our model can efficiently tackle image and video in-\nput data simultaneously and seamlessly train on both data\ntypes. In summary, our contributions are three-fold:\n\u2022 We propose V2Dial \u2013 a multimodal expert-based model\nthat unifies visual and video dialog by simultaneously\nlearning from image and video data.\nAs a core nov-\nelty, it employs two experts to separately learn the\nspatial and temporal features of images and videos.\nV2Dial outperforms state-of-the-art models in both zero-\nshot and fine-tuning settings (see Figure 1).\n\u2022 We are the first to systematically quantify the effect of\ndomain shift between video and visual dialog tasks based\non evaluations on the two widely used datasets of AVSD\n[5] and VisDial [22]. To this end, we propose an alter-\nnative ranking scheme that allows computing the VisDial\nretrieval metric for fully generative models and enables a\nfair comparison with previous works.\n\u2022 We are the first to evaluate AVSD in a zero-shot setting,\nwhich provides a more solid generalization evaluation of\nvideo dialog models compared to the fine-tuned setting.\nFor this, we establish the first benchmark comparison of\nrecent state-of-the-art multimodal models.\n2. Related Work\nVisual & Video Dialog. Modeled after human-human com-\nmunication, visual and video dialog involve reasoning about\na visual scene in the form of a video or an image through\nmultiple question-answering rounds in natural language. In\ncomparison to their single-round counterparts, VQA [8] and\nVideoQA [58], dialog models need to additionally reason\nabout the previous dialog history together with the visual\ngrounding and the current question to be able to answer\nit efficiently.\nThe best performing visual dialog models\n[4, 42, 56, 61] leverage pre-trained VLMs and are trained\nusing an NSP head, negative sampling, and binary clas-\nsification loss.\nAt test time, for each question, the can-\ndidate answers are ranked based on their respective NSP\nscores to compute the retrieval metrics.\nAlthough some\nwork [15, 16, 55] claim to train generative visual dialog\nmodels, they do so by providing a generative mask where\neach token can only attend to its left tokens. However, they\nare trained using the NSP head like the discriminative mod-\nels. However, this training approach is limiting and subop-\ntimal for a unifying model. Thus, we advocate for a fully\ngenerative training paradigm and adapt the ranking scheme\nof VisDial answers to cater to modern generative models.\nIn contrast, works on video dialog follow a purely-\ngenerative training paradigm and achieved great success\nbuilding on top of powerful pre-trained LLMs [30, 47].\nFor example, [28, 34] fine-tuned a LLM on AVSD and\nobtained performance boosts. More recent works [3, 29]\ncombined LLMs with GNNs and pushed the state-of-\nthe-art results even further.\nOthers [60] introduced a\nregularization loss to mitigate hallucination.\nAlthough\nvideo dialog emerged as a natural extension to visual\ndialog with apparent data structure similarities, research\non both tasks evolved separately. To this end, we propose\na unifying model that can simultaneously learn both\ntasks without any architectural modifications and for the\nfirst time; systemically study the effect of domain shift be-\ntween both tasks using the AVSD and VisDial v1.0 datasets.\nMultimodal Expert-based Training. Enhancing models\nwith expert-based training has shown promising potential\nin boosting performance while maintaining computational\nefficiency [25, 63, 64]. Some works [12, 54] explored using\nsingle modality specific experts within a multimodal trans-\nformer architecture. Specifically, they used one vision and\none language specific FFN after a shared multi-head self-\nattention block. Other works [35, 43] explored using multi-\nple sparse modality-agnostic experts and trained them using\nsoft-routers. Our work is positioned at the middle ground of\nthe previously mentioned research directions: We propose\nto use multiple hard-routed experts per modality to be able\nto capture more fine-grained features compared to a single\nexpert or multiple modality agnostic experts. Specifically,\nto the best of our knowledge, V2Dial is the first model that\nlearns disentangled spatial and temporal features using two\ndedicated experts that jointly learn from image and video\ndata. In addition, we propose to deploy two separate lan-\nguage experts (for caption and context) in order to tackle\nthe unique challenges of multimodal conversational tasks.\n3. V2Dial\n3.1. Joint Problem Formulation\nWe use a fully generative formulation to unify both video\nand visual dialog tasks. Specifically, given visual input V\n(video/image), a corresponding visual description (caption\n8638\n\n=== Page 3 ===\nImage\nEncoder\n+\nTemporal masking \nSpatial masking \n+\nLLM\nSelf Att.\nSelf Att.\nVisual Expert \n...\n...\nSpatial Att.\nTemporal Att.\nSpatial \nExpert\nTemporal\nExpert\nCaption\nExpert\nContext\nExpert\nFusion Expert\nMultimodal Expert Layers\nxL\nx(N-L)\nTr. Stage 1 \nEmbedding\n[Caption]\n[Context]\nInput Video / Image\nTr. Stage 2 \nLLM\nTr. Stage 3 \n[Answerout]\n[Answerin]\nTrainable parameters\nFrozen parameters\nInvisible for att.\nVisible for temporal att.\nVisible for spatial att.\nQuery token\ni\nActivated for video / image data\nActivated / Available at stage i\n/\nLinear\nLinear\nLLM\n[Answerout]\n[Answerin]\nLinear\nFigure 2. Architectural overview of V2Dial\n. We adopt a training strategy composed of three stages. First, we only train the multi-\nmodal expert layers using spatial-temporal and video/image text matching losses (Lstm, Lvtm), spatial-temporal and video/image contrastive\nlearning losses (Lstc, Lvtc), and masked language modeling loss (Lmlm). Second, we couple the expert layers with a frozen pre-trained LLM\nend-to-end, using a generative loss Lgen to align their hidden representations. Finally, we additionally fine-tune the LLM weights on the\ndownstream benchmarks. Each expert is a feed-forward network (FFN) composed of two fully connected layers.\nC), a dialog history Hr = {(Q1, A1), ..., (Qr\u22121, Ar\u22121)} com-\nprised of the previous question-answer pairs {(Qi,Ai)}r\u22121\ni=1\nand the current question Qr, a model is trained to auto-\nregressively predict a free-form answer Ar at round r.\nSpecifically, each answer token ai\nr satisfies\nai\nr = arg max\na\u2208V\n[p (a\u2223V, C, HrQr, A<i\nr )] ,\n(1)\nwhere A<i\nr denotes the previously predicted answer tokens\nand V the vocabulary. In the rest, we use \u201ccontext\u201d to refer\nto the concatenation of the history Hr and the question Qr.\n3.2. Architecture\nOverview.\nAs can be seen from Figure 2, our model takes\nan image/video V \u2208RF \u00d73\u00d7H\u00d7W as input, where F is the\nnumber of frames and is set to one for images, and (H, W)\nis the re-sized resolution. Then it processes every frame us-\ning a pre-trained EVA-CLIP [51] Image Encoder and con-\ncatenates every four spatially adjacent visual patches into a\nsingle one. Then, a linear layer maps each visual token into\na lower dimensional vector v of dimension D to obtain\nV =\n\u23a1\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3\nv1\n1\nv2\n1\n\u22ef\nvF\n1\n\u22ee\n\u22ee\n\u22f1\n\u22ee\nv1\nP\nv2\nP\n\u22ef\nvF\nP\n\u23a4\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6\n\u2208RF \u00d7P \u00d7D,\n(2)\nwhere P =\n1\n4\nH\u00d7W\n142\nand D denote the visual input length\nand the joint hidden dimension, respectively. Thereafter, in\nstark contrast to previous works [13, 19] that performed spa-\ntial and temporal attention in series, our model separately\nperforms these operations using the masks Mspa and Mtmp\nas shown in Figure 2 on the visual features V to obtain\nVspa = SA(V, Mspa) \u2208R(F P )\u00d7D\n(3)\nVtmp = SA(V, Mtmp) \u2208R(F P )\u00d7D\n(4)\nMspa\nm,n(v\nj\ni ) = \u03b4nj, Mtmp\nm,n(v\nj\ni ) = \u03b4mi\n(5)\nwhere SA and \u03b4 denote self-attention and Kronecker delta.\nSubsequently, the textual input in the form of a caption\nand a context is processed by an embedding layer to obtain\nTcap/ctx \u2208RNcap/ctx\u00d7D, where Ncap and Nctx are the respective\nlengths of the caption and context. These visual and textual\nfeatures form the initial input to the multimodal expert lay-\ners which are pre-trained using a combination of matching,\ncontrastive learning, and masked language modeling losses.\nFinally, they are coupled with a pre-trained LLM and are\nfine-tuned end-to-end using a generative loss.\nMultimodal Expert Layers.\nThese consist of N layers\nof stacked multi-head self-attention with layer normaliza-\ntion (SA), and several modality-specific and one modality-\nagnostic feed-forward networks that we refer to as experts.\nAs shown in Figure 2, we propose to use a set of six ex-\nperts denoted as {E\u2217}: three of which are vision-specific\nand two are language-specific and are activated in the first\nL layers. The remaining expert Efus is the fusion expert and\nis only activated in the last (N \u2212L) layers and operates on\nthe concatenation of all available modalities (Equation 9).\nTo the best of our knowledge, we propose for the first time\nto learn the spatial and temporal features using dedicated\n8639\n\n=== Page 4 ===\nTime\n Zero-shot Eval.\nTr. Stage 1\nTr. Stage 2\n Fine-tuning Eval.\nTr. Stage 3\nWebVid-2M\nCC-3M\nChampagne\nVisDial\nAVSD\nFigure 3. Overview of the training and evaluation pipeline of V2Dial\n. We show the different datasets used to train our model at each\nstage. Evaluations are conducted on the most popular video and visual dialog datasets of AVSD and VisDial, respectively. (\n= video\ndata,\n= image data,\n= closed / visual captioning data,\n= dialog data).\nexperts (i.e., the spatial Espa and temporal Etmp experts, re-\nspectively) as shown in Equation 11. This allows our model\nto unify video and visual dialog by jointly learning from\nimage and video data. The visual expert Evis operates on\ntop of the concatenation of Espa and Etmp to learn a joint\nspatial-temporal video representation (Equation 10). Simi-\nlarly, the textual experts Ecap and Ectx operate on the caption\nand context embeddings Tcap and Tctx (Equation 12). As\nseen in Table 1, the availability of the multimodal features\ndepends on the visual input type (i.e., videos vs images) and\nthe training stage. However, without the loss of generality,\nwe can formulate the multimodal expert layers as follows:\nX0 = [V\nspa, V\ntmp, T\ncap, T\nctx],\n(6)\n\u02dcXl = [ \u02dcV\nspa\nl , \u02dcV\ntmp\nl\n, \u02dcT\ncap\nl , \u02dcT\nctx\nl ]\n(7)\n= SA(Xl\u22121) + Xl\u22121\n(8)\nXl = {[Vvis\nl , T\ncap\nl , Tctx\nl ]\nif 1 \u2264l \u2264L\nEfus( \u02dcXl) + \u02dcXl\nif L < l \u2264N ,\n(9)\nV\nvis\nl\n= Evis( \u02dcV\nvis\nl ) + \u02dcV\nvis\nl , \u02dcV\nvis\nl\n\u2254[V\nspa\nl , V\ntmp\nl\n],\n(10)\nV\nspa\nl\n= Espa( \u02dcV\nspa\nl ) + \u02dcV\nspa\nl , V\ntmp\nl\n= Etmp( \u02dcV\ntmp\nl\n) + \u02dcV\ntmp\nl\n,\n(11)\nT\ncap\nl\n= Ecap(\u02dcT\ncap\nl ) + \u02dcT\ncap\nl , T\nctx\nl\n= Ectx(\u02dcT\nctx\nl ) + \u02dcT\nctx\nl .\n(12)\nWhen dealing with images and non-dialog data, we drop\nV\ntmp\nl\nand T\ncap\nl\nfrom the previous equations and deactivate\nthe respective expert.\n3.3. Training\n3.3.1\nStage 1\nIn the first stage, we only pre-train the multimodal ex-\npert layers, the vision encoder linear layer, and the spatial-\ntemporal attention modules. Since we are the first to sug-\ngest learning the spatial and temporal features of videos and\nimages using dedicated experts, we propose to train our\nmodel using spatial-temporal contrastive learning (STC)\nand spatial-temporal matching (STM). In addition, we use\nthe established masked language modeling (MLM), vision-\ntext1 contrastive learning (VTC), and vision-text matching\n(VTM) similar to [19, 31, 32].\n1Vision can either be video or image depending on the dataset.\nTr. Stage\nTr. Stage\nTr. Stage\nVideos\nVspa, Vtmp, Tcap\nVspa, Vtmp, Tcap, Tctx\nVspa, Vtmp, Tcap, Tctx\nImages\nVspa, Tcap\n-\nVspa, Tcap, Tctx\nTable 1. Overview of the available features for each training stage\nand visual input type.\nSpatial-Temporal Contrastive Learning\naims to better\nalign the spatial and temporal features of video data. To\nthis end, we use output features of the last multi-modal exert\nlayer2 and learn a cosine similarity function\ns(Vspa, Vtmp) = \u0398spa(Vspa)\u22a4\u0398tmp(Vtmp),\n(13)\nso that aligned spatial-temporal features result in higher\nsimilarity scores, where \u0398\u2217are linear layers that map the\nfeatures to a normalized lower dimensional vector space.\nThen, given spatial and temporal feature pairs, we compute\nthe softmax normalized spatial-to-temporal and temporal-\nto-spatial similarities as\np\ns2t\ni (V\nspa) =\nexp(\u02dcs(Vspa, V\ntmp\ni\n)/\u03c4)\n\u2211\nK\nk=1 exp(\u02dcs(Vspa, V\ntmp\nk )/\u03c4)\n,\n(14)\np\nt2s\ni (V\ntmp) =\nexp(\u02dcs(Vtmp, V\nspa\ni )/\u03c4)\n\u2211\nK\nk=1 exp(\u02dcs(Vtmp, V\nspa\nk )/\u03c4)\n,\n(15)\nwhere \u03c4 is learnable temperature parameters, and \u02dcs is the\nmaximum value of s as in [32]. Finally, we can compute the\nloss as the cross-entropy H between p and y:\nLstc = 1\n2E(Vspa,Vtmp) [H (y\ns2t, p\ns2t) + H (y\nt2s, p\nt2s)] ,\n(16)\nwhere ys2t and yt2s are the golden one-hot similarities.\nSpatial-Temporal\nMatching\ncomplements\nSTC\nand\nteaches the model to distinguish between positive and neg-\native spatial-temporal feature pairs. Specifically, a matched\nfeature pair originates from the same video, whereas an un-\nmatched pair is constructed using negative sampling from a\n2Index dropped for clarity.\n8640\n\n=== Page 5 ===\ndifferent video. We use a classification token as a proxy of\nthe joint spatial-temporal representations to learn a binary\nclassification problem using the STM loss\nLstm = E(Vspa,Vtmp) [H(ystm, pstm)] ,\n(17)\nwhere pstm and ystm are the predicted and the ground-truth\ntwo-class probabilities, respectively.\nWe provide more details about the remaining established\nobjectives (i.e., MLM, VTC, VTM) in the supplementary.\n3.3.2\nStages 2 & 3\nIn the subsequent stages, we couple the multimodal ex-\npert layers with a pre-trained Flan-T5large [21] via a linear\nlayer. Specifically, Stage 2 aims to align the hidden states\nof the proposed layers with those of the pre-trained LLM.\nTo this end, we keep the LLM weights frozen and train the\nwhole architecture end-to-end using the generative loss (i.e.,\nnext token prediction) on large scale video dialog data 3 ,\ni.e.,\nLgen = EXgen [H(ygen\n\u2192, pgen)] ,\n(18)\nXgen = \u0398gen (LLMdec([Xenc, Tans])) ,\n(19)\nwhere Xenc, Tans and \u0398gen are the LLM encoder output, the\nanswer token embeddings, and a linear layer that maps the\nfeatures to the vocabulary space, respectively. ygen\n\u2192and pgen\ndenote the right-shifted ground-truth answer tokens and the\npredicted text token probabilities. Finally, in Stage 3, we\nunfreeze the LLM weights and fine-tune our model end-to-\nend on the downstream tasks of video and visual dialog us-\ning the same generative loss.\n4. Experiments\n4.1. Datasets\nAs shown in Figure 3, we simultaneously use the video and\nimage captioning datasets of WebVid-2M [10] and CC-3M\n[49] to pre-train the multimodal expert layers in Stage 1.\nThen in the second stage, we use 25% of the recent large-\nscale video dialog dataset Champagne [26] before perform-\ning zero-shot evaluation on the widely used video and visual\ndialog datasets of AVSD [5] and VisDial [22], respectively.\nFinally, in the third stage, we perform a domain shift evalu-\nation based on different combinations of AVSD and VisDial\nto quantify whether and to what extent these seemingly sim-\nilar benchmarks benefit from each other in both zero-shot\nand fine-tuning evaluation settings.\n3The weights of Ectx are initialized with those of Ecap from Stage 1.\nNSP\n[IMG] [Caption] [Context]\nNSP\nVLM\nNSP\n+\n[A2]\n[AN]\n[A1]\nCandidate \nAnswers {Ai}\nRanked\nCandidate\nAnswers {Ai}\n[Agen]\nCandidate \nAnswers {Ai}\nRanked\nCandidate\nAnswers {Ai}\n[IMG] [Caption] [Context]\nAuto-regressive\nGeneration\nSentence\nTansformer\nVLM\nFigure 4. Instead of training a dedicated NSP head, we propose\na ranking scheme based on the cosine similarity of the candidate\nanswers\u2019 embeddings with the respect to those of the generated\nones. We used RoBERTalarge [38] and OpenAI Text Embedding-\n3large to generate these embeddings.\n4.2. Evaluation Metrics\nWe use the established official metrics for each dataset to\nfairly benchmark V2Dial with previous works. Specifi-\ncally, for all three AVSD datasets, we use BLEU (B-n) [45],\nROUGE-L (R) [36], METEOR (M) [11], and CIDEr (C)\n[53]. Whereas for VisDial, we use the retrieval metrics of\nrecall (R@k), mean reciprocal rank (MRR), and normal-\nized discounted cumulative gain (NDCG). However, since\nwe are jointly tackling both tasks with a fully generative\nmodel, we propose to rank the VisDial candidate answers by\nmeans of cosine similarity with respect to the generated an-\nswer using the embdeddings of a pre-trained sentence trans-\nformer (i.e. RoBERTa [38] and OpenAI Text Embedding-\n3). We posit that this approach is more natural, caters to\nthe current advances in generative models, and appropri-\nately captures the semantic similarities between the gener-\nated and the candidate answers. In addition, it allows for\na seamless unification of AVSD and VisDial without any\ntraining or architectural modifications. As shown in Fig-\nure 4, our proposed adaptation does not alter the computa-\ntion of the sparse metrics itself and only rethinks the rank-\ning of the candidate answers allowing for a fair comparison\nwith previous works.\n4.3. Experimental Setup\nIn the first stage, we trained our model for a maximum of\nten epochs and applied early stopping based on a validation\nsplit to select the best checkpoint. In the subsequent stages,\nwe trained it for up to three and twelve epochs, respectively.\nIn all stages, we used the AdamW [39] optimizer with the\ndefault parameters and a weight decay value of 0.01. Fur-\nthermore, we applied a linear learning rate schedule with\nwarm-up and minimum and base values of 5e\u22125 and 1e\u22124,\nrespectively. We conducted our experiments on a cluster\nconsisting of eight A100 GPUs.\n4.4. Zero-shot Evaluation\nAVSD.\nWe first assessed V2Dial in a zero-shot setting on\nAVSD. This is in stark contrast to previous models that were\nexclusively evaluated in a fine-tuning setting. We instead\nadvocate for complementing the fine-tuning evaluation set-\nting with a zero-shot one, as it results in a more rigorous\n8641\n\n=== Page 6 ===\nModel\nAVSD-DSTC10\nAVSD-DSTC8\nAVSD-DSTC7\nB-1\nB-2\nB-3\nB-4\nM\nR\nC\nB-1\nB-2\nB-3\nB-4\nM\nR\nC\nB-1\nB-2\nB-3\nB-4\nM\nR\nC\n\u2666MoE-LLAVAarXiv\u201924 [35]\n35.8\n18.9\n10.1\n5.9\n15.4\n27.1\n12.8\n39.8\n23.9\n15.2\n10.1\n18.7\n32.2\n23.7\n44.7\n29.1\n19.6\n13.8\n21.8\n37.3\n33.2\n\u2666MiniGPT4-videoCVPR\u201924 [9]\n37.9\n19.9\n11.3\n6.8\n16.2\n28.7\n17.7\n34.8\n17.6\n9.7\n5.8\n15.8\n26.3\n13.3\n37.8\n21.2\n12.7\n8.2\n18.4\n30.2\n17.7\n\u2666Video-ChatGPTACL\u201924 [40]\n24.5\n14.7\n8.8\n5.4\n16.7\n25.2\n3.9\n25.5\n16.0\n10.1\n6.4\n18.4\n27.1\n9.1\n28.5\n18.5\n11.8\n7.6\n20.4\n32.1\n19.1\n\u2666MST-MIXERECCV\u201924 [3]\n0.1\n0.0\n0.0\n0.0\n3.1\n6.8\n3.0\n0.2\n0.1\n0.1\n0.0\n3.3\n7.1\n4.3\n0.2\n0.1\n0.0\n0.0\n3.4\n6.9\n4.6\n\u2666VideoChat2CVPR\u201924 [33]\n42.5\n25.9\n16.0\n10.3\n18.7\n33.1\n25.4\n43.9\n28.1\n18.5\n12.6\n20.8\n34.5\n29.2\n46.7\n31.1\n20.9\n14.4\n22.9\n37.6\n31.4\nV2Dial\n54.6\n34.8\n24.0\n17.2\n19.7\n38.3\n53.8\n53.2\n33.8\n23.5\n16.7\n18.8\n37.7\n49.7\n55.5\n36.7\n26.2\n18.7\n20.0\n39.2\n50.8\nTable 2. Zero-shot performance comparison on AVSD-DSTC10, AVSD-DSTC8 and AVSD-DSTC7. Best and second-best performances\nare in bold and underlined. \u2666indicates that we evaluated the model. (B-n = BLEU-n, M = METEOR, R = ROUGE-L, C = CIDEr).\nModel\nSent. Trans.\nR@1\nR@5\nR@10\nMRR\nNDCG\nFROMAGeICML\u201923 [27]\n17.6\n20.1\n25.1\n22.0\n16.5\nESPERCVPR\u201923 [62]\nn/a\n14.6\n\u2212\n\u2212\n25.7\n22.3\nChampagneICCV\u201923 [26]\n\u2212\n\u2212\n\u2212\n\u2212\n25.5\n\u2666MoE-LLAVAarXiv\u201924 [35]\n10.6\n25.4\n36.4\n19.6\n26.7\n\u2666MiniGPT-videoCVPR\u201924 [9]\n7.4\n17.4\n26.5\n14.6\n23.2\n\u2666Video-ChatGPTACL\u201924 [40]\nRoBERTa\n10.0\n22.5\n31.5\n18.1\n24.8\n\u2666MST-MIXERECCV\u201924 [3]\n18.2\n22.1\n25.7\n21.9\n24.6\n\u2666VideoChat2CVPR\u201924 [33]\n12.7\n29.0\n39.9\n22.3\n30.9\nRoBERTa\n20.0\n30.2\n39.3\n26.9\n33.3\nV2Dial\nOpenAI TE-3\n22.1\n41.2\n48.1\n32.7\n32.0\nTable 3. Zero-shot performance comparison on the VisDial v1.0\nval split. OpenAI TE-3 = OpenAI Text Embedding-3large.\nand challenging testbed for the proposed models. To this\nend, we establish; to the best of our knowledge; the first\nzero-shot benchmark comparison on AVSD comprised of\nrecent capable multimodal models. As can be seen from Ta-\nble 2, our model outperforms all baselines by a considerable\nmargin across 6/7 metrics of AVSD-DSTC8 and AVSD-\nDSTC7. On the more recent and challenging version of the\nbenchmark (i.e. AVSD-DSTC10), V2Dial ranks first across\nall metrics. For instance, it more than doubles the CIDEr\nscore compared to VideoChat2 [33].\nVisDial.\nAdditionally,\nwe assessed the same model\ncheckpoint on VisDial v1.0. As can be seen from Table 3,\nV2Dial managed to outperform previous models such as\nFROMAGe [27] by a considerable margin across all met-\nrics of the dataset. In addition, it outperformed Champagne\n[26] that was trained on x4 more dialog data by 7.8 abso-\nlute NDCG points. Furthermore, our model outperformed\nthe more recent baselines of the previous section on 4/5\nmetrics, underlining it capability of dealing with both video\nand image input data types. Finally, replacing the sentence\nembeddings generated by RoBERTalarge with those from\nOpenAI Text Emedding-3 improved the external ranking of\nthe candidate answers and resulted in higher scores across\nall metrics, as can be seen in the last row of Table 3.\n4.5. Fine-tuning Evaluation\nAVSD.\nSimilar to almost all previous works on AVSD, we\nassessed V2Dial in a fine-tuning setting on all three bench-\nmarks of the dataset. As can be seen from Table 4, our\nmodel managed to maintain it competitiveness ahead of re-\ncent models and outperformed them on the latest and most\nchallenging AVSD-DSTC10 benchmarks across all evalua-\ntion metrics. For instance, it lifted CIDEr by over 6 absolute\npoints compared to the second-best model. Furthermore,\nour model managed to maintain an on par performance with\nthe state of the art on AVSD-DSTC8 and AVSD-DSTC7.\nAs shown in Table 4, V2Dial increased their respective\nCIDEr scores by over 2 and 3 absolute points compared to\nthe second-best model.\nVisDial.\nFinally, we fine-tuned our model and MST-\nMIXER [3] that had the closest AVSD performance on Vis-\ndial v1.0 using the same fully-generative approach.\nAs\ncan be seen from Table 5, V2Dial managed to outperform\nall previous models on the strictest metric of the dataset\nby achieving a R@1 score of 44.2. However, when using\nOpenAI Text Embedding-3 our model managed to increase\nthe R@1 and MRR scores to 44.9 and 52.4, respectively,\nthereby setting new state-of-the-art results.\nAs expected\nand due to the more challenging aspect of a tackling Vis-\nDial as a fully generative task, our model performed slightly\nworse than the previous fine-tuned models on the remain-\ning metrics of the dataset. However, when comparing our\nmodel with MST-MIXER that was trained using the same\nparadigm (i.e. the last two rows of Table 5), we can see that\nour model outperformed it across 4/5 metrics of the task and\nscored almost equally on NDCG.\n4.6. Domain Shift Evaluation\nZero-shot setting.\nFirst,\nwe fine-tuned our model\u2019s\ncheckpoint from Stage 2 on AVSD and zero-shot evaluated\nit on VisDial. As can be seen from the second section of\nTable 6, our model\u2019s performance was lifted by a consider-\nable margin across most metrics. Notably, the NDCG score\nimproved by 9 absolute points compared to the results of Ta-\nble 3. Then, we replicated the same experiment on AVSD\nafter having fine-tuned the model on VisDial. Interestingly,\nour model\u2019s performance deteriorated across all metrics of\nthe benchmark. This behavior could be explained by the na-\nture of both datasets. Whereas AVSD encourages the model\n8642\n\n=== Page 7 ===\nAVSD \nA man sits at a computer desk and fixes the drawer. [...] He gets up \nand picks up laundry and leaves.\nDoes the man walk into the room?\nNo the man begins seated in the room.\nIt appears to be an office.\nIs it a office or a bedroom?\n[...]\nDoes he go on his computer at all?\nNo he does not go on his computer at all.\nNo.\n(Before VisDial)\n(After VisDial)\nVisDial\nA boy is holding his skateboard by a tree.\nAbout how old is the boy in the picture?\nAround 8 years old.\nIt is black and white.\nWhat color is the skateboard?\n[...]\nIs the boy sitting or standing?\nStanding.\nHe is standing the whole time.\n(Before AVSD)\n(After AVSD)\nFigure 5. Zero-shot qualitative examples of V2Dial before and after fine-tuning on VisDial and AVSD. The former teaches the model to\nanswer question with brief responses whereas the latter teaches it to produce longer and more elaborate answers.\nModel\nAVSD-DSTC10\nAVSD-DSTC8\nAVSD-DSTC7\nB-1\nB-2\nB-3\nB-4\nM\nR\nC\nB-1\nB-2\nB-3\nB-4\nM\nR\nC\nB-1\nB-2\nB-3\nB-4\nM\nR\nC\nPDCICLR\u201921 [29]\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n74.9\n62.9\n52.8\n43.9\n28.5\n59.2\n120.1\n77.0\n65.3\n53.9\n44.9\n29.2\n60.6\n129.5\nTHAMEMNLP\u201922 [60]\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n76.4\n64.1\n53.8\n45.5\n30.1\n61.0\n130.4\n77.8\n65.4\n54.9\n46.8\n30.8\n61.9\n133.5\nDialogMCFTASLP\u201923 [18]\n69.3\n55.6\n45.0\n36.9\n24.9\n53.6\n91.2\n75.6\n63.3\n53.2\n44.9\n29.3\n60.1\n125.3\n77.7\n65.3\n54.7\n45.7\n30.6\n61.3\n135.2\n\u2666VideoLLAMA 2arXiv\u201924 [20]\n50.2\n35.0\n24.9\n18.1\n21.8\n42.8\n57.5\n53.3\n39.0\n29.1\n22.2\n24.8\n46.3\n74.0\n56.2\n41.1\n30.7\n23.2\n26.4\n48.5\n79.2\nMST-MIXERECCV\u201924 [3]\n69.7\n57.1\n47.2\n39.5\n25.1\n54.0\n96.9\n77.1\n65.6\n55.7\n47.1\n30.2\n61.8\n133.6\n78.4\n66.0\n55.8\n47.1\n31.0\n62.0\n136.5\nV2Dial\n70.7\n58.2\n48.2\n40.3\n26.0\n55.4\n103.3\n76.8\n65.5\n55.8\n47.5\n30.4\n62.1\n135.7\n78.9\n66.5\n56.1\n47.4\n31.2\n62.3\n139.8\nTable 4. Fine-tuning performance comparison on AVSD-DSTC10, AVSD-DSTC8 and AVSD-DSTC7. VideoLLAMA 2 [20] was trained\non AVSD amongst other datasets. Additional model comparisons can be found in the supplementary material.\nModel\nSent. Trans.\nR@1\nR@5\nR@10\nMRR\nNDCG\nLTMIECCV\u201920 [44]\n40.4\n61.6\n69.7\n50.7\n63.5\nLTMI-LGEMNLP\u201921 [17]\n41.3\n61.6\n69.0\n51.3\n63.2\nGoGACL\u201921 [16]\nn/a\n41.2\n61.8\n69.4\n51.3\n62.6\nUTCCVPR\u201922 [15]\n41.3\n59.8\n66.3\n50.6\n61.0\nChampagneICCV\u201923 [26]\n\u2212\n\u2212\n\u2212\n\u2212\n62.5\n\u2663MST-MIXERECCV\u201924 [3]\nRoBERTa\n42.2\n51.6\n57.8\n47.7\n52.5\nRoBERTa\n45.4\n54.7\n61.1\n50.9\n54.0\nV2Dial\nOpenAI TE-3\n46.1\n59.3\n65.7\n53.2\n53.1\nTable 5. Fine-tuning performance comparison on the VisDial v1.0\nval split. \u2663indicates that we trained and evaluated the model.\nto produce long and elaborate responses, VisDial teaches it\nto produce brief answers instead, which diminishes its per-\nformance on the language generation metrics. The qualita-\ntive examples of Figure 5 clearly illustrate this phenomenon\non both datasets.\nFine-tuning Setting.\nWe first experimented with a cur-\nriculum learning strategy where we used one dataset for pre-\ntraining before finally fine-tuning on the other. As can be\nseen from the last section of Table 6, this training paradigm\nresulted in performance drops on both datasets compared to\nTable 4 and Table 5 where the model was only trained on\nthe data of the respective benchmark. This indicates that\nthe converged model\u2019s weights on one dataset do not offer\na good initialization for training on the remaining one. Al-\nlowed by our model design that can jointly handle video and\nimage input data, we finally fine-tuned one single model on\nboth datasets simultaneously. As seen from the last row\nof Table 6, this resulted in the best joint performance of\nour model across the two datasets. Although the results\non AVSD slightly dropped compared to Table 4, our model\nlifted its performance on VisDial by a considerable margin.\nThis could largely be attributed to the same previous ob-\nservation, as training on VisDial incentivizes our model to\nshorten its responses on AVSD.\n4.7. Expert Swapping\nIn order to validate the specialization of each expert, we\nconducted a swapping experiment where we routed some\nfeatures through inadequate experts. We first swapped ex-\nperts of the same modality (i.e., experts operating on vision\n8643\n\n=== Page 8 ===\nFine-tuning data\nAVSD-DSTC10\nAVSD-DSTC8\nAVSD-DSTC7\nVisDial\nAVSD\nVisDial\nB-1\nM\nR\nC\nB-1\nM\nR\nC\nB-1\nM\nR\nC\nR@1\nR@5\nR@10\nNDCG\n\u2717\n\u2717\nZero-shot (from Table 2 and Table 3)\n54.6\n19.7\n38.3\n53.8\n53.2\n18.8\n37.7\n49.7\n55.5\n20.0\n39.2\n50.8\n20.0\n30.2\n39.3\n33.3\n\u2713\n\u2717\nFine-tuning (from Table 4)\nZero-shot\n70.7\n26.0\n55.4\n103.3\n76.8\n30.4\n62.1\n135.7\n78.9\n31.2\n62.3\n139.8\n12.8\n36.7\n50.8\n42.3\n\u2717\n\u2713\nZero-shot\nFine-tuning (from Table 5)\n11.5\n6.8\n20.1\n14.6\n11.5\n7.3\n20.7\n20.9\n7.9\n6.2\n17.4\n18.2\n44.2\n53.3\n59.5\n52.3\nFine-tuning\n\u2713\n\u2192\n\u2713\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n42.2\n50.1\n56.3\n51.3\n\u2713\n\u2190\n\u2713\n69.6\n25.7\n55.0\n100.5\n75.9\n29.8\n61.4\n132.1\n77.6\n30.4\n61.5\n134.5\n\u2212\n\u2212\n\u2212\n\u2212\n\u2713\n&\n\u2713\n69.3\n25.4\n54.8\n99.9\n75.1\n29.3\n61.1\n130.0\n77.3\n30.0\n61.7\n134.5\n45.4\n54.7\n61.1\n54.0\nTable 6. Domain shift evaluation between the respective most prominent video and visual dialog datasets of AVSD and VisDial. \u25a1\u2192\u25b3\nmeans that the model was pre-trained on dataset \u25a1before fine-tuning on dataset \u25b3.\nExpert\nAVSD-DSTC7\nVisDial\nSwapping\nB-1\nM\nR\nC\nR@1\nNDCG\nOriginal\n78.9\n31.2\n62.3\n139.8\n44.2\n52.3\nSwapping experts of the same modality (vision / language)\nEspa\n\u2194\nEtmp\n77.0\n29.5\n61.2\n133.7\n\u2212\n\u2212\nEcap\n\u2194\nEctx\n76.1\n29.6\n60.3\n131.1\n42.8\n51.9\nSwapping experts of different modalities\nEspa\n\u2194\nEcap\n28.5\n10.7\n22.0\n10.1\n35.7\n45.3\nEtmp\n\u2194\nEctx\n\u2212\n\u2212\nEspa\n\u2194\nEctx\n34.4\n12\n25.4\n11.7\n32.4\n42.9\nEtmp\n\u2194\nEcap\n\u2212\n\u2212\nTable 7. Expert swapping results. E\u25a1\u2194E\u25b3means that the \u25a1\nfeatures are inadequately routed at test time through E\u25b3and vice\nversa. The other experts remain unchanged.\nor language data). As shown in Table 7, this resulted in\nperformance drops across all metrics of both datasets, indi-\ncating that experts of the same modality are able to capture\nthe semantic nuances of the data they specialize on. More\ninterestingly, the performance of our model dropped more\nsignificantly when swapping experts of different modalities,\nas seen from the last section of Table 7. This showcases\ntheir ability to adjust to the nature of the data they process\nand to capture its modality specific features.\n4.8. Ablation Study\nEffect of Pre-training Data. We trained two versions of\nour model, where one was only pre-trained on Stage 1 using\nWebVid-2M & CC-3M and the other only on Stage 2 with\na subset of Champagne. As can be seen from the middle\nsection of Table 8, our model witnessed a comparable drop\nin performance compared to the full model. This underlines\nthe equal importance of these proposed training stages to\nthe joint down-stream performance on AVSD and VisDial.\nWe did not conduct ablations using either WebVid-2M or\nCC-3M in Stage 1 as this was sufficiently explored by other\nrecent works [19] that showed the benefit of pre-training on\nboth image and video data.\nEffect of Pre-training Objectives & Model Design. Then,\nModel Ablations\nAVSD-DSTC7\nVisDial\nB-1\nM\nR\nC\nR@1\nNDCG\nFull\n78.9\n31.2\n62.3\n139.8\n44.2\n52.3\nw/o Tr. Stage 1\n76.9\n30.0\n61.4\n134.0\n34.5\n44.6\nw/o Tr. Stage 2\n77.8\n30.7\n61.7\n134.7\n32.6\n44.1\nw/o Lstc & Lstm\n77.2\n29.9\n61.1\n133.2\n33.1\n44.6\nw/o separate Espa & Etmp\n77.0\n30.1\n61.1\n133.8\n32.9\n43.8\nw/o experts {E\u2217}\n77.5\n30.0\n61.4\n134.8\n30.6\n42.2\nTable 8. Ablation results.\nwe trained a version of our model without Lstc and Lstm in\nStage 1 using the same schedule and training data as our\nfull model. As shown in the fourth row of Table 8, this ab-\nlated version suffered a performance drop not only in AVSD\nbut also in VisDial. This indicates that these losses improve\nnot only the temporal capabilities of our model but also its\nspatial ones. Then, we trained a version that sequentially\napplies spatial and temporal attention, as in [13, 19]. Since\nthis version does not have separate spatial-temporal experts,\nwe also omitted the previous two objectives. As seen in the\npenultimate row of Table 8, this version underperformed\nour full model on both datasets, showcasing the effective-\nness of our approach. Finally, we trained a version without\nall the expert layers. As shown in the last row, its perfor-\nmance dropped compared to our full model and performed\nthe worst on VisDial.\n5. Conclusion\nWe presented V2Dial \u2013 a model that can jointly tackle video\nand visual conversational tasks using a multimodal expert-\nbased approach that; for the first time, disentangles the\nlearning of the spatial and temporal features of images and\nvideos using two separate experts. Extensive evaluation on\nthe respective widely used video and visual dialog datasets\nof AVSD and VisDial show that our model achieves new\nstate-of-the-art zero-shot and fine-tuning performance. Fi-\nnally, we conducted the first domain shift evaluation of\nAVSD and VisDial and provided insights on how to opti-\nmally leverage their respective training data.\n8644\n\n=== Page 9 ===\nAcknowledgment\nThis work was partially funded by a LOEWE-Start-\nProfessur\n(LOEWE/4b//519/05.01.002-(0006)/94),\nLOEWE-Spitzen-Professur\n(LOEWE/4a//519/05.00.002-\n(0010)/93), and an Alexander von Humboldt Professorship\nin Multimodal Reliable AI, sponsored by Germany\u2019s\nFederal Ministry for Education and Research.\nReferences\n[1] Adnen Abdessaied, Mihai B\u02c6ace, and Andreas Bulling.\nNeuro-Symbolic Visual Dialog. In Proceedings of the In-\nternational Conference on Computational Linguistics (COL-\nING), 2022. 1\n[2] Adnen Abdessaied, Manuel Hochmeister, and Andreas\nBulling. OLViT: Multi-Modal State Tracking via Attention-\nBased Embeddings for Video-Grounded Dialog. In Proceed-\nings of the Joint International Conference on Computational\nLinguistics, Language Resources and Evaluation (LREC-\nCOLING), 2024. 2\n[3] Adnen Abdessaied, Lei Shi, and Andreas Bulling. Multi-\nModal Video Dialog State Tracking in the Wild.\nIn Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), 2024. 2, 6, 7\n[4] Adnen Abdessaied, Lei Shi, and Andreas Bulling. VD-GR:\nBoosting Visual Dialog With Cascaded Spatial-Temporal\nMulti-Modal Graphs. In Proceedings of the IEEE/CVF Win-\nter Conference on Applications of Computer Vision (WACV),\n2024. 1, 2\n[5] Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang,\nAnoop Cherian, Irfan Essa, Dhruv Batra, Tim K Marks,\nChiori Hori, Peter Anderson, et al.\nAudio visual scene-\naware dialog. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2019.\n1, 2, 5\n[6] Huda Alamri, Anthony Bilic, Michael Hu, Apoorva Beedu,\nand Irfan Essa. End-to-end multimodal representation learn-\ning for video dialog. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS), 2022. 2\n[7] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\nsch, Katherine Millican, Malcolm Reynolds, Roman Ring,\nEliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,\nSina Samangooei, Marianne Monteiro, Jacob Menick, Se-\nbastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-\nhand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Karen Simonyan.\nFlamingo:\na visual language model for few-shot learn-\ning. In Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 1\n[8] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh. VQA: Visual Question Answering. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 2015. 1, 2\n[9] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Es-\nsam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elho-\nseiny.\nMiniGPT4-video: Advancing multimodal llms for\nvideo understanding with interleaved visual-textual tokens.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) Workshops, 2024. 6\n[10] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 2021. 1,\n5\n[11] Satanjeev Banerjee and Alon Lavie. METEOR: An auto-\nmatic metric for MT evaluation with improved correlation\nwith human judgments. In Proceedings of the ACL Workshop\non Intrinsic and Extrinsic Evaluation Measures for Machine\nTranslation and/or Summarization, 2005. 5\n[12] Hangbo Bao,\nWenhui\nWang,\nLi\nDong,\nQiang Liu,\nOwais Khan Mohammed, Kriti Aggarwal, Subhojit Som,\nSonghao Piao, and Furu Wei.\nVLMo:\nUnified vision-\nlanguage pre-training with mixture-of-modality-experts.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 1, 2\n[13] Gedas Bertasius, Heng Wang, and Lorenzo Torresani.\nIs\nSpace-Time Attention All You Need for Video Understand-\ning? In Proceedings of the International Conference on Ma-\nchine Learning (ICML), 2021. 3, 8\n[14] Soravit Changpinyo, Piyush Sharma, and Nan Ding andRadu\nSoricut. Conceptual 12M: Pushing Web-Scale Image-Text\nPre-Training To Recognize Long-Tail Visual Concepts. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 2021. 1\n[15] Cheng Chen, Yudong Zhu, Zhenshan Tan, Qingrong Cheng,\nXin Jiang, Qun Liu, and Xiaodong Gu.\nUTC: A Unified\nTransformer with Inter-Task Contrastive Learning for Visual\nDialog.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2022. 1,\n2, 7\n[16] Feilong Chen, Xiuyi Chen, Fandong Meng, Peng Li, and Jie\nZhou. GoG: Relation-aware graph-over-graph network for\nvisual dialog. In Findings of the Association for Computa-\ntional Linguistics: ACL, 2021. 1, 2, 7\n[17] Feilong Chen, Xiuyi Chen, Can Xu, and Daxin Jiang. Learn-\ning to ground visual objects for visual dialog. In Findings\nof the Association for Computational Linguistics: EMNLP,\n2021. 7\n[18] Zhe Chen, Hongcheng Liu, and Yu Wang. DialogMCF: Mul-\ntimodal Context Flow for Audio Visual Scene-Aware Dialog.\nIEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 2023. 2, 7\n[19] Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit\nBansal, and Gedas Bertasius.\nVindLU: A Recipe for Ef-\nfective Video-and-Language Pretraining. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. 3, 4, 8\n[20] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li,\nGuanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo,\nDeli Zhao, and Lidong Bing.\nVideoLLaMA 2: Advanc-\ning Spatial-Temporal Modeling and Audio Understanding in\nVideo-LLMs. arXiv preprint arXiv:2406.07476, 2024. 2, 7\n8645\n\n=== Page 10 ===\n[21] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph,\nYi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa\nDehghani, Siddhartha Brahma, et al.\nScaling instruction-\nfinetuned language models. Journal of Machine Learning\nResearch (JMLR), 2024. 5\n[22] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\nDeshraj Yadav, Jos\u00b4e M.F. Moura, Devi Parikh, and Dhruv\nBatra.\nVisual Dialog.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2017. 1, 2, 5\n[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics (NAACL), 2019. 1\n[24] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In Proceedings of\nthe International Conference on Learning Representations\n(ICLR), 2020. 1\n[25] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong,\nDmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi\nZhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Efficient\nscaling of language models with mixture-of-experts. In Pro-\nceedings of the International Conference on Machine Learn-\ning (ICML), 2022. 2\n[26] Seungju Han, Jack Hessel, Nouha Dziri, Yejin Choi, and\nYoungjae Yu. Champagne: Learning real-world conversa-\ntion from large-scale web videos.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 2023. 5, 6, 7\n[27] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\nGrounding language models to images for multimodal inputs\nand outputs. In Proceedings of the International Conference\non Machine Learning (ICML), 2023. 6\n[28] Hung Le and Steven C.H. Hoi. Video-Grounded Dialogues\nwith Pretrained Generation Language Models. In Proceed-\nings of the Annual Meeting of the Association for Computa-\ntional Linguistics (ACL), 2020. 2\n[29] Hung Le, Nancy F. Chen, and Steven Hoi. Learning rea-\nsoning paths over semantic graphs for video-grounded dia-\nlogues. In Proceedings of the International Conference on\nLearning Representations (ICLR), 2021. 2, 7\n[30] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-\njad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and\nLuke Zettlemoyer. BART: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and\ncomprehension. In Proceedings of the Annual Meeting of the\nAssociation for Computational Linguistics (ACL), 2020. 2\n[31] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.\nAlign before fuse: Vision and language representation learn-\ning with momentum distillation. In Advances in Neural In-\nformation Processing Systems (NeurIPS), 2021. 4\n[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In Pro-\nceedings of the International Conference on Learning Rep-\nresentations (ICLR), 2023. 1, 4\n[33] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,\nYi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.\nMVBench: A Comprehensive Multi-modal Video Under-\nstanding Benchmark.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2024. 6\n[34] Zekang Li, Zongjia Li, Jinchao Zhang, Yang Feng, and Jie\nZhou.\nBridging text and video: A universal multimodal\ntransformer for audio-visual scene-aware dialog. Transac-\ntions on Audio, Speech, and Language Processing, 2021. 2\n[35] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng\nJin, Jinfa Huang, Junwu Zhang, Yatian Pang, Munan Ning,\nand Li Yuan. MoE-LLaVA: Mixture of Experts for Large\nVision-Language Models. arXiv preprint arXiv:2401.15947,\n2024. 2, 6\n[36] Chin-Yew Lin. ROUGE: A package for automatic evaluation\nof summaries. In Text Summarization Branches Out, 2004. 5\n[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2023. 1\n[38] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. RoBERTa: A Robustly Op-\ntimized BERT Pretraining Approach. arXiv preprint arXiv\n1907.11692, 2019. 5\n[39] Ilya Loshchilov and Frank Hutter. Fixing weight decay regu-\nlarization in adam. In Proceedings of the International Con-\nference on Learning Representations (ICLR), 2019. 5\n[40] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and\nFahad Shahbaz Khan.\nVideo-ChatGPT: Towards Detailed\nVideo Understanding via Large Vision and Language Mod-\nels. In Proceedings of the Annual Meeting of the Association\nfor Computational Linguistics (ACL), 2024. 6\n[41] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand\nTapaswi,\nIvan\nLaptev,\nand\nJosef\nSivic.\nHowto100m: Learning a text-video embedding by watching\nhundred million narrated video clips. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 2019. 1\n[42] Vishvak Murahari, Dhruv Batra, Devi Parikh, and Abhishek\nDas.\nLarge-scale pretraining for visual dialog: A simple\nstate-of-the-art baseline.\nIn Proceedings of the European\nConference on Computer Vision (ECCV), 2020. 2\n[43] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe\nJenatton, and Neil Houlsby. Multimodal Contrastive Learn-\ning with LIMoE: the Language-Image Mixture of Experts.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 2\n[44] Van Quang Nguyen, Masanori Suganuma, and Takayuki\nOkatani. Efficient Attention Mechanism for Visual Dialog\nthat Can Handle All the Interactions Between Multiple In-\nputs. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), 2020. 1, 7\n[45] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\n8646\n\n=== Page 11 ===\ntranslation. In Proceedings of the Annual Meeting of the As-\nsociation for Computational Linguistics (ACL), 2002. 5\n[46] Hoang-Anh Pham, Thao Minh Le, Vuong Le, Tu Minh\nPhuong, and Truyen Tran.\nVideo Dialog as Conversation\nabout Objects Living in Space-Time. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV), 2022. 2\n[47] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 2019. 2\n[48] Ankit P. Shah, Shijie Geng, Peng Gao, Anoop Cherian,\nTakaaki Hori, Tim K. Marks, Jonathan Le Roux, and Chiori\nHori. Audio-Visual Scene-Aware Dialog and Reasoning us-\ning Audio-Visual Transformers with Joint Student-Teacher\nLearning. In Proceedings of the IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP),\n2022. 1\n[49] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In Pro-\nceedings of the Annual Meeting of the Association for Com-\nputational Linguistics (ACL), 2018. 5\n[50] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guil-\nlaume Couairon, Wojciech Galuba, Marcus Rohrbach, and\nDouwe Kiela. FLAVA: A foundational language and vision\nalignment model. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2022. 1\n[51] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao. EVA-CLIP: Improved Training Techniques for CLIP at\nScale. arXiv preprint arXiv:2303.15389, 2023. 3\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2017. 1\n[53] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi\nParikh. CIDEr: Consensus-based Image Description Evalu-\nation. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2015. 5\n[54] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-\niang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-\nhammed, Saksham Singhal, Subhojit Som, et al. Image as\na foreign language: Beit pretraining for vision and vision-\nlanguage tasks. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2023.\n1, 2\n[55] Yue Wang, Shafiq R. Joty, Michael R. Lyu, Irwin King,\nCaiming Xiong, and Steven C. H. Hoi. VD-BERT: A Unified\nVision and Dialog Transformer with BERT. In Proceedings\nof the Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), 2020. 2\n[56] Zihao Wang, Junli Wang, and Changjun Jiang. Unified multi-\nmodal model with unlikelihood training for visual dialog. In\nProceedings of the ACM International Conference on Multi-\nmedia (ACM MM), 2022. 1, 2\n[57] Ronald J Williams and David Zipser. A learning algorithm\nfor continually running fully recurrent neural networks. Neu-\nral Comput., 1989. 2\n[58] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,\nXiangnan He, and Yueting Zhuang. Video question answer-\ning via gradually refined attention over appearance and mo-\ntion. In Proceedings of the ACM International Conference\non Multimedia (ACM MM), 2017. 1, 2\n[59] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2016. 1\n[60] Sunjae Yoon, Eunseop Yoon, Hee Suk Yoon, Junyeong Kim,\nand Chang Yoo. Information-theoretic text hallucination re-\nduction for video-grounded dialogue. In Proceedings of the\nConference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), 2022. 2, 7\n[61] Xintong Yu, Hongming Zhang, Ruixin Hong, Yangqiu Song,\nand Changshui Zhang. VD-PCR: Improving visual dialog\nwith pronoun coreference resolution. Pattern Recognition,\n2022. 2\n[62] Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel,\nJae Sung Park, Ximing Lu, Rowan Zellers, Prithviraj Am-\nmanabrolu, Ronan Le Bras, Gunhee Kim, et al. Fusing pre-\ntrained language models with multimodal prompts through\nreinforcement learning.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 6\n[63] Zexuan Zhong, Mengzhou Xia, Danqi Chen, and Mike\nLewis.\nLory: Fully Differentiable Mixture-of-Experts for\nAutoregressive Language Model Pre-training. In Proceed-\nings of the Conference on Language Modeling (COLM),\n2024. 2\n[64] Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi\nTong, Conghui He, and Yu Cheng. Llama-moe: Building\nmixture-of-experts from llama with continual pre-training.\nIn Proceedings of the Conference on Empirical Methods in\nNatural Language Processing (EMNLP), 2024. 2\n8647\n",
      "searchable_content": "page 1 v2dial unification of video and visual dialog via multimodal experts adnen abdessaied anna rohrbach marcus rohrbach andreas bulling university of stuttgart germany tu darmstadt germany hessian.ai germany https www.collaborative-ai.org publications abdessaied25_cvpr abstract we present v2dial a novel expert-based model specif- ically geared towards simultaneously handling image and video input data for multimodal conversational tasks. cur- rent multimodal models primarily focus on simpler tasks e.g. vqa videoqa video-text retrieval and often neglect the more challenging conversational counterparts such as video and visual image dialog. moreover works on both conversational tasks evolved separately from each other de- spite their apparent similarities limiting their applicability potential. to this end we propose to unify both tasks using a single model that for the first time jointly learns the spatial and temporal features of images and videos by routing them through dedicated experts and aligns them using matching and contrastive learning techniques. furthermore we sys- temically study the domain shift between the two tasks by investigating whether and to what extent these seemingly re- lated tasks can mutually benefit from their respective train- ing data. extensive evaluations on the widely used video and visual dialog datasets of avsd and visdial show that our model achieves new state-of-the-art results across four benchmarks both in zero-shot and fine-tuning settings. 1. introduction enabled by the availability of large-scale training data 10 14 41 and advances in model design 12 24 37 48 52 the field of vision-and-language learning saw unprece- dented success in recent years. however current multi- modal foundational models 7 32 37 50 54 still mainly focus on single-round tasks e.g. vqa 8 videoqa 58 video-text and text-video retrieval 59 . in contrast the sig- nificantly more challenging conversational tasks such as vi- sual 1 22 and video dialog 5 received considerably less attention. furthermore methods for these different tasks have advanced independently of each other despite the ap- parent structural similarities between them. they both op- erate on a visual input i.e. an image or video a short vi- sual description caption and a dialog history composed of figure 1. v2dial uses multimodal experts and outperforms state-of-the-art methods on both video and visual dialog in zero- shot and fine-tuning evaluation settings. previous question-answer pairs. on the one hand visual di- alog models 4 15 16 44 56 have been primarily trained to rank a list of candidate answers using a next sentence prediction nsp head similar to bert 23 and negative sampling. thus they are benchmarked using retrieval met- this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 8637 page 2 rics such as recall r k and normalized discounted cu- mulative gain ndcg . in contrast video dialog models 2 3 6 18 20 29 46 60 are trained to auto-regressively predict the next answer token using teacher forcing 57 and are evaluated using language generation metrics. in this work we mitigate the shortcomings of current dialog systems by proposing v2dial a novel multimodal expert-based model capable of unifying video and visual di- alog tasks without any architectural changes. specifically we train dedicated multimodal expert layers that separately process the features of each input modality and learn how to align them using matching and contrastive learning tech- niques. a key novelty of our approach is that we use ded- icated experts to jointly learn the spatial and temporal fea- tures of images and videos by routing them through the ap- propriate experts. then we couple these layers with a pre- trained llm to align their hidden states. thanks to its mod- ularity our model can efficiently tackle image and video in- put data simultaneously and seamlessly train on both data types. in summary our contributions are three-fold we propose v2dial a multimodal expert-based model that unifies visual and video dialog by simultaneously learning from image and video data. as a core nov- elty it employs two experts to separately learn the spatial and temporal features of images and videos. v2dial outperforms state-of-the-art models in both zero- shot and fine-tuning settings see figure 1 . we are the first to systematically quantify the effect of domain shift between video and visual dialog tasks based on evaluations on the two widely used datasets of avsd 5 and visdial 22 . to this end we propose an alter- native ranking scheme that allows computing the visdial retrieval metric for fully generative models and enables a fair comparison with previous works. we are the first to evaluate avsd in a zero-shot setting which provides a more solid generalization evaluation of video dialog models compared to the fine-tuned setting. for this we establish the first benchmark comparison of recent state-of-the-art multimodal models. 2. related work visual video dialog. modeled after human-human com- munication visual and video dialog involve reasoning about a visual scene in the form of a video or an image through multiple question-answering rounds in natural language. in comparison to their single-round counterparts vqa 8 and videoqa 58 dialog models need to additionally reason about the previous dialog history together with the visual grounding and the current question to be able to answer it efficiently. the best performing visual dialog models 4 42 56 61 leverage pre-trained vlms and are trained using an nsp head negative sampling and binary clas- sification loss. at test time for each question the can- didate answers are ranked based on their respective nsp scores to compute the retrieval metrics. although some work 15 16 55 claim to train generative visual dialog models they do so by providing a generative mask where each token can only attend to its left tokens. however they are trained using the nsp head like the discriminative mod- els. however this training approach is limiting and subop- timal for a unifying model. thus we advocate for a fully generative training paradigm and adapt the ranking scheme of visdial answers to cater to modern generative models. in contrast works on video dialog follow a purely- generative training paradigm and achieved great success building on top of powerful pre-trained llms 30 47 . for example 28 34 fine-tuned a llm on avsd and obtained performance boosts. more recent works 3 29 combined llms with gnns and pushed the state-of- the-art results even further. others 60 introduced a regularization loss to mitigate hallucination. although video dialog emerged as a natural extension to visual dialog with apparent data structure similarities research on both tasks evolved separately. to this end we propose a unifying model that can simultaneously learn both tasks without any architectural modifications and for the first time systemically study the effect of domain shift be- tween both tasks using the avsd and visdial v1.0 datasets. multimodal expert-based training. enhancing models with expert-based training has shown promising potential in boosting performance while maintaining computational efficiency 25 63 64 . some works 12 54 explored using single modality specific experts within a multimodal trans- former architecture. specifically they used one vision and one language specific ffn after a shared multi-head self- attention block. other works 35 43 explored using multi- ple sparse modality-agnostic experts and trained them using soft-routers. our work is positioned at the middle ground of the previously mentioned research directions we propose to use multiple hard-routed experts per modality to be able to capture more fine-grained features compared to a single expert or multiple modality agnostic experts. specifically to the best of our knowledge v2dial is the first model that learns disentangled spatial and temporal features using two dedicated experts that jointly learn from image and video data. in addition we propose to deploy two separate lan- guage experts for caption and context in order to tackle the unique challenges of multimodal conversational tasks. 3. v2dial 3.1. joint problem formulation we use a fully generative formulation to unify both video and visual dialog tasks. specifically given visual input v video image a corresponding visual description caption 8638 page 3 image encoder temporal masking spatial masking llm self att. self att. visual expert ... ... spatial att. temporal att. spatial expert temporal expert caption expert context expert fusion expert multimodal expert layers xl x n-l tr. stage 1 embedding caption context input video image tr. stage 2 llm tr. stage 3 answerout answerin trainable parameters frozen parameters invisible for att. visible for temporal att. visible for spatial att. query token i activated for video image data activated available at stage i linear linear llm answerout answerin linear figure 2. architectural overview of v2dial . we adopt a training strategy composed of three stages. first we only train the multi- modal expert layers using spatial-temporal and video image text matching losses lstm lvtm spatial-temporal and video image contrastive learning losses lstc lvtc and masked language modeling loss lmlm . second we couple the expert layers with a frozen pre-trained llm end-to-end using a generative loss lgen to align their hidden representations. finally we additionally fine-tune the llm weights on the downstream benchmarks. each expert is a feed-forward network ffn composed of two fully connected layers. c a dialog history hr q1 a1 ... qr 1 ar 1 com- prised of the previous question-answer pairs qi ai r 1 i 1 and the current question qr a model is trained to auto- regressively predict a free-form answer ar at round r. specifically each answer token ai r satisfies ai r arg max a v p a v c hrqr a i r 1 where a i r denotes the previously predicted answer tokens and v the vocabulary. in the rest we use context to refer to the concatenation of the history hr and the question qr. 3.2. architecture overview. as can be seen from figure 2 our model takes an image video v rf 3 h w as input where f is the number of frames and is set to one for images and h w is the re-sized resolution. then it processes every frame us- ing a pre-trained eva-clip 51 image encoder and con- catenates every four spatially adjacent visual patches into a single one. then a linear layer maps each visual token into a lower dimensional vector v of dimension d to obtain v v1 1 v2 1 vf 1 v1 p v2 p vf p rf p d 2 where p 1 4 h w 142 and d denote the visual input length and the joint hidden dimension respectively. thereafter in stark contrast to previous works 13 19 that performed spa- tial and temporal attention in series our model separately performs these operations using the masks mspa and mtmp as shown in figure 2 on the visual features v to obtain vspa sa v mspa r f p d 3 vtmp sa v mtmp r f p d 4 mspa m n v j i \u03b4nj mtmp m n v j i \u03b4mi 5 where sa and \u03b4 denote self-attention and kronecker delta. subsequently the textual input in the form of a caption and a context is processed by an embedding layer to obtain tcap ctx rncap ctx d where ncap and nctx are the respective lengths of the caption and context. these visual and textual features form the initial input to the multimodal expert lay- ers which are pre-trained using a combination of matching contrastive learning and masked language modeling losses. finally they are coupled with a pre-trained llm and are fine-tuned end-to-end using a generative loss. multimodal expert layers. these consist of n layers of stacked multi-head self-attention with layer normaliza- tion sa and several modality-specific and one modality- agnostic feed-forward networks that we refer to as experts. as shown in figure 2 we propose to use a set of six ex- perts denoted as e three of which are vision-specific and two are language-specific and are activated in the first l layers. the remaining expert efus is the fusion expert and is only activated in the last n l layers and operates on the concatenation of all available modalities equation 9 . to the best of our knowledge we propose for the first time to learn the spatial and temporal features using dedicated 8639 page 4 time zero-shot eval. tr. stage 1 tr. stage 2 fine-tuning eval. tr. stage 3 webvid-2m cc-3m champagne visdial avsd figure 3. overview of the training and evaluation pipeline of v2dial . we show the different datasets used to train our model at each stage. evaluations are conducted on the most popular video and visual dialog datasets of avsd and visdial respectively. video data image data closed visual captioning data dialog data . experts i.e. the spatial espa and temporal etmp experts re- spectively as shown in equation 11. this allows our model to unify video and visual dialog by jointly learning from image and video data. the visual expert evis operates on top of the concatenation of espa and etmp to learn a joint spatial-temporal video representation equation 10 . simi- larly the textual experts ecap and ectx operate on the caption and context embeddings tcap and tctx equation 12 . as seen in table 1 the availability of the multimodal features depends on the visual input type i.e. videos vs images and the training stage. however without the loss of generality we can formulate the multimodal expert layers as follows x0 v spa v tmp t cap t ctx 6 xl v spa l v tmp l t cap l t ctx l 7 sa xl 1 xl 1 8 xl vvis l t cap l tctx l if 1 l l efus xl xl if l l n 9 v vis l evis v vis l v vis l v vis l v spa l v tmp l 10 v spa l espa v spa l v spa l v tmp l etmp v tmp l v tmp l 11 t cap l ecap t cap l t cap l t ctx l ectx t ctx l t ctx l . 12 when dealing with images and non-dialog data we drop v tmp l and t cap l from the previous equations and deactivate the respective expert. 3.3. training 3.3.1 stage 1 in the first stage we only pre-train the multimodal ex- pert layers the vision encoder linear layer and the spatial- temporal attention modules. since we are the first to sug- gest learning the spatial and temporal features of videos and images using dedicated experts we propose to train our model using spatial-temporal contrastive learning stc and spatial-temporal matching stm . in addition we use the established masked language modeling mlm vision- text1 contrastive learning vtc and vision-text matching vtm similar to 19 31 32 . 1vision can either be video or image depending on the dataset. tr. stage tr. stage tr. stage videos vspa vtmp tcap vspa vtmp tcap tctx vspa vtmp tcap tctx images vspa tcap - vspa tcap tctx table 1. overview of the available features for each training stage and visual input type. spatial-temporal contrastive learning aims to better align the spatial and temporal features of video data. to this end we use output features of the last multi-modal exert layer2 and learn a cosine similarity function s vspa vtmp \u03b8spa vspa \u03b8tmp vtmp 13 so that aligned spatial-temporal features result in higher similarity scores where \u03b8 are linear layers that map the features to a normalized lower dimensional vector space. then given spatial and temporal feature pairs we compute the softmax normalized spatial-to-temporal and temporal- to-spatial similarities as p s2t i v spa exp s vspa v tmp i \u03c4 k k 1 exp s vspa v tmp k \u03c4 14 p t2s i v tmp exp s vtmp v spa i \u03c4 k k 1 exp s vtmp v spa k \u03c4 15 where \u03c4 is learnable temperature parameters and s is the maximum value of s as in 32 . finally we can compute the loss as the cross-entropy h between p and y lstc 1 2e vspa vtmp h y s2t p s2t h y t2s p t2s 16 where ys2t and yt2s are the golden one-hot similarities. spatial-temporal matching complements stc and teaches the model to distinguish between positive and neg- ative spatial-temporal feature pairs. specifically a matched feature pair originates from the same video whereas an un- matched pair is constructed using negative sampling from a 2index dropped for clarity. 8640 page 5 different video. we use a classification token as a proxy of the joint spatial-temporal representations to learn a binary classification problem using the stm loss lstm e vspa vtmp h ystm pstm 17 where pstm and ystm are the predicted and the ground-truth two-class probabilities respectively. we provide more details about the remaining established objectives i.e. mlm vtc vtm in the supplementary. 3.3.2 stages 2 3 in the subsequent stages we couple the multimodal ex- pert layers with a pre-trained flan-t5large 21 via a linear layer. specifically stage 2 aims to align the hidden states of the proposed layers with those of the pre-trained llm. to this end we keep the llm weights frozen and train the whole architecture end-to-end using the generative loss i.e. next token prediction on large scale video dialog data 3 i.e. lgen exgen h ygen pgen 18 xgen \u03b8gen llmdec xenc tans 19 where xenc tans and \u03b8gen are the llm encoder output the answer token embeddings and a linear layer that maps the features to the vocabulary space respectively. ygen and pgen denote the right-shifted ground-truth answer tokens and the predicted text token probabilities. finally in stage 3 we unfreeze the llm weights and fine-tune our model end-to- end on the downstream tasks of video and visual dialog us- ing the same generative loss. 4. experiments 4.1. datasets as shown in figure 3 we simultaneously use the video and image captioning datasets of webvid-2m 10 and cc-3m 49 to pre-train the multimodal expert layers in stage 1. then in the second stage we use 25 of the recent large- scale video dialog dataset champagne 26 before perform- ing zero-shot evaluation on the widely used video and visual dialog datasets of avsd 5 and visdial 22 respectively. finally in the third stage we perform a domain shift evalu- ation based on different combinations of avsd and visdial to quantify whether and to what extent these seemingly sim- ilar benchmarks benefit from each other in both zero-shot and fine-tuning evaluation settings. 3the weights of ectx are initialized with those of ecap from stage 1. nsp img caption context nsp vlm nsp a2 an a1 candidate answers ai ranked candidate answers ai agen candidate answers ai ranked candidate answers ai img caption context auto-regressive generation sentence tansformer vlm figure 4. instead of training a dedicated nsp head we propose a ranking scheme based on the cosine similarity of the candidate answers embeddings with the respect to those of the generated ones. we used robertalarge 38 and openai text embedding- 3large to generate these embeddings. 4.2. evaluation metrics we use the established official metrics for each dataset to fairly benchmark v2dial with previous works. specifi- cally for all three avsd datasets we use bleu b-n 45 rouge-l r 36 meteor m 11 and cider c 53 . whereas for visdial we use the retrieval metrics of recall r k mean reciprocal rank mrr and normal- ized discounted cumulative gain ndcg . however since we are jointly tackling both tasks with a fully generative model we propose to rank the visdial candidate answers by means of cosine similarity with respect to the generated an- swer using the embdeddings of a pre-trained sentence trans- former i.e. roberta 38 and openai text embedding- 3 . we posit that this approach is more natural caters to the current advances in generative models and appropri- ately captures the semantic similarities between the gener- ated and the candidate answers. in addition it allows for a seamless unification of avsd and visdial without any training or architectural modifications. as shown in fig- ure 4 our proposed adaptation does not alter the computa- tion of the sparse metrics itself and only rethinks the rank- ing of the candidate answers allowing for a fair comparison with previous works. 4.3. experimental setup in the first stage we trained our model for a maximum of ten epochs and applied early stopping based on a validation split to select the best checkpoint. in the subsequent stages we trained it for up to three and twelve epochs respectively. in all stages we used the adamw 39 optimizer with the default parameters and a weight decay value of 0.01. fur- thermore we applied a linear learning rate schedule with warm-up and minimum and base values of 5e 5 and 1e 4 respectively. we conducted our experiments on a cluster consisting of eight a100 gpus. 4.4. zero-shot evaluation avsd. we first assessed v2dial in a zero-shot setting on avsd. this is in stark contrast to previous models that were exclusively evaluated in a fine-tuning setting. we instead advocate for complementing the fine-tuning evaluation set- ting with a zero-shot one as it results in a more rigorous 8641 page 6 model avsd-dstc10 avsd-dstc8 avsd-dstc7 b-1 b-2 b-3 b-4 m r c b-1 b-2 b-3 b-4 m r c b-1 b-2 b-3 b-4 m r c moe-llavaarxiv 24 35 35.8 18.9 10.1 5.9 15.4 27.1 12.8 39.8 23.9 15.2 10.1 18.7 32.2 23.7 44.7 29.1 19.6 13.8 21.8 37.3 33.2 minigpt4-videocvpr 24 9 37.9 19.9 11.3 6.8 16.2 28.7 17.7 34.8 17.6 9.7 5.8 15.8 26.3 13.3 37.8 21.2 12.7 8.2 18.4 30.2 17.7 video-chatgptacl 24 40 24.5 14.7 8.8 5.4 16.7 25.2 3.9 25.5 16.0 10.1 6.4 18.4 27.1 9.1 28.5 18.5 11.8 7.6 20.4 32.1 19.1 mst-mixereccv 24 3 0.1 0.0 0.0 0.0 3.1 6.8 3.0 0.2 0.1 0.1 0.0 3.3 7.1 4.3 0.2 0.1 0.0 0.0 3.4 6.9 4.6 videochat2cvpr 24 33 42.5 25.9 16.0 10.3 18.7 33.1 25.4 43.9 28.1 18.5 12.6 20.8 34.5 29.2 46.7 31.1 20.9 14.4 22.9 37.6 31.4 v2dial 54.6 34.8 24.0 17.2 19.7 38.3 53.8 53.2 33.8 23.5 16.7 18.8 37.7 49.7 55.5 36.7 26.2 18.7 20.0 39.2 50.8 table 2. zero-shot performance comparison on avsd-dstc10 avsd-dstc8 and avsd-dstc7. best and second-best performances are in bold and underlined. indicates that we evaluated the model. b-n bleu-n m meteor r rouge-l c cider . model sent. trans. r 1 r 5 r 10 mrr ndcg fromageicml 23 27 17.6 20.1 25.1 22.0 16.5 espercvpr 23 62 n a 14.6 25.7 22.3 champagneiccv 23 26 25.5 moe-llavaarxiv 24 35 10.6 25.4 36.4 19.6 26.7 minigpt-videocvpr 24 9 7.4 17.4 26.5 14.6 23.2 video-chatgptacl 24 40 roberta 10.0 22.5 31.5 18.1 24.8 mst-mixereccv 24 3 18.2 22.1 25.7 21.9 24.6 videochat2cvpr 24 33 12.7 29.0 39.9 22.3 30.9 roberta 20.0 30.2 39.3 26.9 33.3 v2dial openai te-3 22.1 41.2 48.1 32.7 32.0 table 3. zero-shot performance comparison on the visdial v1.0 val split. openai te-3 openai text embedding-3large. and challenging testbed for the proposed models. to this end we establish to the best of our knowledge the first zero-shot benchmark comparison on avsd comprised of recent capable multimodal models. as can be seen from ta- ble 2 our model outperforms all baselines by a considerable margin across 6 7 metrics of avsd-dstc8 and avsd- dstc7. on the more recent and challenging version of the benchmark i.e. avsd-dstc10 v2dial ranks first across all metrics. for instance it more than doubles the cider score compared to videochat2 33 . visdial. additionally we assessed the same model checkpoint on visdial v1.0. as can be seen from table 3 v2dial managed to outperform previous models such as fromage 27 by a considerable margin across all met- rics of the dataset. in addition it outperformed champagne 26 that was trained on x4 more dialog data by 7.8 abso- lute ndcg points. furthermore our model outperformed the more recent baselines of the previous section on 4 5 metrics underlining it capability of dealing with both video and image input data types. finally replacing the sentence embeddings generated by robertalarge with those from openai text emedding-3 improved the external ranking of the candidate answers and resulted in higher scores across all metrics as can be seen in the last row of table 3. 4.5. fine-tuning evaluation avsd. similar to almost all previous works on avsd we assessed v2dial in a fine-tuning setting on all three bench- marks of the dataset. as can be seen from table 4 our model managed to maintain it competitiveness ahead of re- cent models and outperformed them on the latest and most challenging avsd-dstc10 benchmarks across all evalua- tion metrics. for instance it lifted cider by over 6 absolute points compared to the second-best model. furthermore our model managed to maintain an on par performance with the state of the art on avsd-dstc8 and avsd-dstc7. as shown in table 4 v2dial increased their respective cider scores by over 2 and 3 absolute points compared to the second-best model. visdial. finally we fine-tuned our model and mst- mixer 3 that had the closest avsd performance on vis- dial v1.0 using the same fully-generative approach. as can be seen from table 5 v2dial managed to outperform all previous models on the strictest metric of the dataset by achieving a r 1 score of 44.2. however when using openai text embedding-3 our model managed to increase the r 1 and mrr scores to 44.9 and 52.4 respectively thereby setting new state-of-the-art results. as expected and due to the more challenging aspect of a tackling vis- dial as a fully generative task our model performed slightly worse than the previous fine-tuned models on the remain- ing metrics of the dataset. however when comparing our model with mst-mixer that was trained using the same paradigm i.e. the last two rows of table 5 we can see that our model outperformed it across 4 5 metrics of the task and scored almost equally on ndcg. 4.6. domain shift evaluation zero-shot setting. first we fine-tuned our model s checkpoint from stage 2 on avsd and zero-shot evaluated it on visdial. as can be seen from the second section of table 6 our model s performance was lifted by a consider- able margin across most metrics. notably the ndcg score improved by 9 absolute points compared to the results of ta- ble 3. then we replicated the same experiment on avsd after having fine-tuned the model on visdial. interestingly our model s performance deteriorated across all metrics of the benchmark. this behavior could be explained by the na- ture of both datasets. whereas avsd encourages the model 8642 page 7 avsd a man sits at a computer desk and fixes the drawer. ... he gets up and picks up laundry and leaves. does the man walk into the room no the man begins seated in the room. it appears to be an office. is it a office or a bedroom ... does he go on his computer at all no he does not go on his computer at all. no. before visdial after visdial visdial a boy is holding his skateboard by a tree. about how old is the boy in the picture around 8 years old. it is black and white. what color is the skateboard ... is the boy sitting or standing standing. he is standing the whole time. before avsd after avsd figure 5. zero-shot qualitative examples of v2dial before and after fine-tuning on visdial and avsd. the former teaches the model to answer question with brief responses whereas the latter teaches it to produce longer and more elaborate answers. model avsd-dstc10 avsd-dstc8 avsd-dstc7 b-1 b-2 b-3 b-4 m r c b-1 b-2 b-3 b-4 m r c b-1 b-2 b-3 b-4 m r c pdciclr 21 29 74.9 62.9 52.8 43.9 28.5 59.2 120.1 77.0 65.3 53.9 44.9 29.2 60.6 129.5 thamemnlp 22 60 76.4 64.1 53.8 45.5 30.1 61.0 130.4 77.8 65.4 54.9 46.8 30.8 61.9 133.5 dialogmcftaslp 23 18 69.3 55.6 45.0 36.9 24.9 53.6 91.2 75.6 63.3 53.2 44.9 29.3 60.1 125.3 77.7 65.3 54.7 45.7 30.6 61.3 135.2 videollama 2arxiv 24 20 50.2 35.0 24.9 18.1 21.8 42.8 57.5 53.3 39.0 29.1 22.2 24.8 46.3 74.0 56.2 41.1 30.7 23.2 26.4 48.5 79.2 mst-mixereccv 24 3 69.7 57.1 47.2 39.5 25.1 54.0 96.9 77.1 65.6 55.7 47.1 30.2 61.8 133.6 78.4 66.0 55.8 47.1 31.0 62.0 136.5 v2dial 70.7 58.2 48.2 40.3 26.0 55.4 103.3 76.8 65.5 55.8 47.5 30.4 62.1 135.7 78.9 66.5 56.1 47.4 31.2 62.3 139.8 table 4. fine-tuning performance comparison on avsd-dstc10 avsd-dstc8 and avsd-dstc7. videollama 2 20 was trained on avsd amongst other datasets. additional model comparisons can be found in the supplementary material. model sent. trans. r 1 r 5 r 10 mrr ndcg ltmieccv 20 44 40.4 61.6 69.7 50.7 63.5 ltmi-lgemnlp 21 17 41.3 61.6 69.0 51.3 63.2 gogacl 21 16 n a 41.2 61.8 69.4 51.3 62.6 utccvpr 22 15 41.3 59.8 66.3 50.6 61.0 champagneiccv 23 26 62.5 mst-mixereccv 24 3 roberta 42.2 51.6 57.8 47.7 52.5 roberta 45.4 54.7 61.1 50.9 54.0 v2dial openai te-3 46.1 59.3 65.7 53.2 53.1 table 5. fine-tuning performance comparison on the visdial v1.0 val split. indicates that we trained and evaluated the model. to produce long and elaborate responses visdial teaches it to produce brief answers instead which diminishes its per- formance on the language generation metrics. the qualita- tive examples of figure 5 clearly illustrate this phenomenon on both datasets. fine-tuning setting. we first experimented with a cur- riculum learning strategy where we used one dataset for pre- training before finally fine-tuning on the other. as can be seen from the last section of table 6 this training paradigm resulted in performance drops on both datasets compared to table 4 and table 5 where the model was only trained on the data of the respective benchmark. this indicates that the converged model s weights on one dataset do not offer a good initialization for training on the remaining one. al- lowed by our model design that can jointly handle video and image input data we finally fine-tuned one single model on both datasets simultaneously. as seen from the last row of table 6 this resulted in the best joint performance of our model across the two datasets. although the results on avsd slightly dropped compared to table 4 our model lifted its performance on visdial by a considerable margin. this could largely be attributed to the same previous ob- servation as training on visdial incentivizes our model to shorten its responses on avsd. 4.7. expert swapping in order to validate the specialization of each expert we conducted a swapping experiment where we routed some features through inadequate experts. we first swapped ex- perts of the same modality i.e. experts operating on vision 8643 page 8 fine-tuning data avsd-dstc10 avsd-dstc8 avsd-dstc7 visdial avsd visdial b-1 m r c b-1 m r c b-1 m r c r 1 r 5 r 10 ndcg zero-shot from table 2 and table 3 54.6 19.7 38.3 53.8 53.2 18.8 37.7 49.7 55.5 20.0 39.2 50.8 20.0 30.2 39.3 33.3 fine-tuning from table 4 zero-shot 70.7 26.0 55.4 103.3 76.8 30.4 62.1 135.7 78.9 31.2 62.3 139.8 12.8 36.7 50.8 42.3 zero-shot fine-tuning from table 5 11.5 6.8 20.1 14.6 11.5 7.3 20.7 20.9 7.9 6.2 17.4 18.2 44.2 53.3 59.5 52.3 fine-tuning 42.2 50.1 56.3 51.3 69.6 25.7 55.0 100.5 75.9 29.8 61.4 132.1 77.6 30.4 61.5 134.5 69.3 25.4 54.8 99.9 75.1 29.3 61.1 130.0 77.3 30.0 61.7 134.5 45.4 54.7 61.1 54.0 table 6. domain shift evaluation between the respective most prominent video and visual dialog datasets of avsd and visdial. means that the model was pre-trained on dataset before fine-tuning on dataset . expert avsd-dstc7 visdial swapping b-1 m r c r 1 ndcg original 78.9 31.2 62.3 139.8 44.2 52.3 swapping experts of the same modality vision language espa etmp 77.0 29.5 61.2 133.7 ecap ectx 76.1 29.6 60.3 131.1 42.8 51.9 swapping experts of different modalities espa ecap 28.5 10.7 22.0 10.1 35.7 45.3 etmp ectx espa ectx 34.4 12 25.4 11.7 32.4 42.9 etmp ecap table 7. expert swapping results. e e means that the features are inadequately routed at test time through e and vice versa. the other experts remain unchanged. or language data . as shown in table 7 this resulted in performance drops across all metrics of both datasets indi- cating that experts of the same modality are able to capture the semantic nuances of the data they specialize on. more interestingly the performance of our model dropped more significantly when swapping experts of different modalities as seen from the last section of table 7. this showcases their ability to adjust to the nature of the data they process and to capture its modality specific features. 4.8. ablation study effect of pre-training data. we trained two versions of our model where one was only pre-trained on stage 1 using webvid-2m cc-3m and the other only on stage 2 with a subset of champagne. as can be seen from the middle section of table 8 our model witnessed a comparable drop in performance compared to the full model. this underlines the equal importance of these proposed training stages to the joint down-stream performance on avsd and visdial. we did not conduct ablations using either webvid-2m or cc-3m in stage 1 as this was sufficiently explored by other recent works 19 that showed the benefit of pre-training on both image and video data. effect of pre-training objectives model design. then model ablations avsd-dstc7 visdial b-1 m r c r 1 ndcg full 78.9 31.2 62.3 139.8 44.2 52.3 w o tr. stage 1 76.9 30.0 61.4 134.0 34.5 44.6 w o tr. stage 2 77.8 30.7 61.7 134.7 32.6 44.1 w o lstc lstm 77.2 29.9 61.1 133.2 33.1 44.6 w o separate espa etmp 77.0 30.1 61.1 133.8 32.9 43.8 w o experts e 77.5 30.0 61.4 134.8 30.6 42.2 table 8. ablation results. we trained a version of our model without lstc and lstm in stage 1 using the same schedule and training data as our full model. as shown in the fourth row of table 8 this ab- lated version suffered a performance drop not only in avsd but also in visdial. this indicates that these losses improve not only the temporal capabilities of our model but also its spatial ones. then we trained a version that sequentially applies spatial and temporal attention as in 13 19 . since this version does not have separate spatial-temporal experts we also omitted the previous two objectives. as seen in the penultimate row of table 8 this version underperformed our full model on both datasets showcasing the effective- ness of our approach. finally we trained a version without all the expert layers. as shown in the last row its perfor- mance dropped compared to our full model and performed the worst on visdial. 5. conclusion we presented v2dial a model that can jointly tackle video and visual conversational tasks using a multimodal expert- based approach that for the first time disentangles the learning of the spatial and temporal features of images and videos using two separate experts. extensive evaluation on the respective widely used video and visual dialog datasets of avsd and visdial show that our model achieves new state-of-the-art zero-shot and fine-tuning performance. fi- nally we conducted the first domain shift evaluation of avsd and visdial and provided insights on how to opti- mally leverage their respective training data. 8644 page 9 acknowledgment this work was partially funded by a loewe-start- professur loewe 4b 519 05.01.002- 0006 94 loewe-spitzen-professur loewe 4a 519 05.00.002- 0010 93 and an alexander von humboldt professorship in multimodal reliable ai sponsored by germany s federal ministry for education and research. references 1 adnen abdessaied mihai b\u02c6ace and andreas bulling. neuro-symbolic visual dialog. in proceedings of the in- ternational conference on computational linguistics col- ing 2022. 1 2 adnen abdessaied manuel hochmeister and andreas bulling. olvit multi-modal state tracking via attention- based embeddings for video-grounded dialog. in proceed- ings of the joint international conference on computational linguistics language resources and evaluation lrec- coling 2024. 2 3 adnen abdessaied lei shi and andreas bulling. multi- modal video dialog state tracking in the wild. in pro- ceedings of the european conference on computer vision eccv 2024. 2 6 7 4 adnen abdessaied lei shi and andreas bulling. vd-gr boosting visual dialog with cascaded spatial-temporal multi-modal graphs. in proceedings of the ieee cvf win- ter conference on applications of computer vision wacv 2024. 1 2 5 huda alamri vincent cartillier abhishek das jue wang anoop cherian irfan essa dhruv batra tim k marks chiori hori peter anderson et al. audio visual scene- aware dialog. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr 2019. 1 2 5 6 huda alamri anthony bilic michael hu apoorva beedu and irfan essa. end-to-end multimodal representation learn- ing for video dialog. in advances in neural information pro- cessing systems neurips 2022. 2 7 jean-baptiste alayrac jeff donahue pauline luc antoine miech iain barr yana hasson karel lenc arthur men- sch katherine millican malcolm reynolds roman ring eliza rutherford serkan cabi tengda han zhitao gong sina samangooei marianne monteiro jacob menick se- bastian borgeaud andrew brock aida nematzadeh sa- hand sharifzadeh mikolaj binkowski ricardo barreira oriol vinyals andrew zisserman and karen simonyan. flamingo a visual language model for few-shot learn- ing. in advances in neural information processing systems neurips 2022. 1 8 stanislaw antol aishwarya agrawal jiasen lu margaret mitchell dhruv batra c. lawrence zitnick and devi parikh. vqa visual question answering. in proceedings of the ieee cvf international conference on computer vi- sion iccv 2015. 1 2 9 kirolos ataallah xiaoqian shen eslam abdelrahman es- sam sleiman deyao zhu jian ding and mohamed elho- seiny. minigpt4-video advancing multimodal llms for video understanding with interleaved visual-textual tokens. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr workshops 2024. 6 10 max bain arsha nagrani g ul varol and andrew zisser- man. frozen in time a joint video and image encoder for end-to-end retrieval. in proceedings of the ieee cvf inter- national conference on computer vision iccv 2021. 1 5 11 satanjeev banerjee and alon lavie. meteor an auto- matic metric for mt evaluation with improved correlation with human judgments. in proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and or summarization 2005. 5 12 hangbo bao wenhui wang li dong qiang liu owais khan mohammed kriti aggarwal subhojit som songhao piao and furu wei. vlmo unified vision- language pre-training with mixture-of-modality-experts. in advances in neural information processing systems neurips 2022. 1 2 13 gedas bertasius heng wang and lorenzo torresani. is space-time attention all you need for video understand- ing in proceedings of the international conference on ma- chine learning icml 2021. 3 8 14 soravit changpinyo piyush sharma and nan ding andradu soricut. conceptual 12m pushing web-scale image-text pre-training to recognize long-tail visual concepts. in proceedings of the ieee cvf conference on computer vi- sion and pattern recognition cvpr 2021. 1 15 cheng chen yudong zhu zhenshan tan qingrong cheng xin jiang qun liu and xiaodong gu. utc a unified transformer with inter-task contrastive learning for visual dialog. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr 2022. 1 2 7 16 feilong chen xiuyi chen fandong meng peng li and jie zhou. gog relation-aware graph-over-graph network for visual dialog. in findings of the association for computa- tional linguistics acl 2021. 1 2 7 17 feilong chen xiuyi chen can xu and daxin jiang. learn- ing to ground visual objects for visual dialog. in findings of the association for computational linguistics emnlp 2021. 7 18 zhe chen hongcheng liu and yu wang. dialogmcf mul- timodal context flow for audio visual scene-aware dialog. ieee acm transactions on audio speech and language processing 2023. 2 7 19 feng cheng xizi wang jie lei david crandall mohit bansal and gedas bertasius. vindlu a recipe for ef- fective video-and-language pretraining. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr 2023. 3 4 8 20 zesen cheng sicong leng hang zhang yifei xin xin li guanzheng chen yongxin zhu wenqi zhang ziyang luo deli zhao and lidong bing. videollama 2 advanc- ing spatial-temporal modeling and audio understanding in video-llms. arxiv preprint arxiv 2406.07476 2024. 2 7 8645 page 10 21 hyung won chung le hou shayne longpre barret zoph yi tay william fedus yunxuan li xuezhi wang mostafa dehghani siddhartha brahma et al. scaling instruction- finetuned language models. journal of machine learning research jmlr 2024. 5 22 abhishek das satwik kottur khushi gupta avi singh deshraj yadav jos e m.f. moura devi parikh and dhruv batra. visual dialog. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr 2017. 1 2 5 23 jacob devlin ming-wei chang kenton lee and kristina toutanova. bert pre-training of deep bidirectional trans- formers for language understanding. in proceedings of the conference of the north american chapter of the associa- tion for computational linguistics naacl 2019. 1 24 alexey dosovitskiy lucas beyer alexander kolesnikov dirk weissenborn xiaohua zhai thomas unterthiner mostafa dehghani matthias minderer georg heigold syl- vain gelly et al. an image is worth 16x16 words trans- formers for image recognition at scale. in proceedings of the international conference on learning representations iclr 2020. 1 25 nan du yanping huang andrew m dai simon tong dmitry lepikhin yuanzhong xu maxim krikun yanqi zhou adams wei yu orhan firat et al. glam efficient scaling of language models with mixture-of-experts. in pro- ceedings of the international conference on machine learn- ing icml 2022. 2 26 seungju han jack hessel nouha dziri yejin choi and youngjae yu. champagne learning real-world conversa- tion from large-scale web videos. in proceedings of the ieee cvf international conference on computer vision iccv 2023. 5 6 7 27 jing yu koh ruslan salakhutdinov and daniel fried. grounding language models to images for multimodal inputs and outputs. in proceedings of the international conference on machine learning icml 2023. 6 28 hung le and steven c.h. hoi. video-grounded dialogues with pretrained generation language models. in proceed- ings of the annual meeting of the association for computa- tional linguistics acl 2020. 2 29 hung le nancy f. chen and steven hoi. learning rea- soning paths over semantic graphs for video-grounded dia- logues. in proceedings of the international conference on learning representations iclr 2021. 2 7 30 mike lewis yinhan liu naman goyal marjan ghazvinine- jad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer. bart denoising sequence-to-sequence pre-training for natural language generation translation and comprehension. in proceedings of the annual meeting of the association for computational linguistics acl 2020. 2 31 junnan li ramprasaath selvaraju akhilesh gotmare shafiq joty caiming xiong and steven chu hong hoi. align before fuse vision and language representation learn- ing with momentum distillation. in advances in neural in- formation processing systems neurips 2021. 4 32 junnan li dongxu li silvio savarese and steven hoi. blip-2 bootstrapping language-image pre-training with frozen image encoders and large language models. in pro- ceedings of the international conference on learning rep- resentations iclr 2023. 1 4 33 kunchang li yali wang yinan he yizhuo li yi wang yi liu zun wang jilan xu guo chen ping luo et al. mvbench a comprehensive multi-modal video under- standing benchmark. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr 2024. 6 34 zekang li zongjia li jinchao zhang yang feng and jie zhou. bridging text and video a universal multimodal transformer for audio-visual scene-aware dialog. transac- tions on audio speech and language processing 2021. 2 35 bin lin zhenyu tang yang ye jiaxi cui bin zhu peng jin jinfa huang junwu zhang yatian pang munan ning and li yuan. moe-llava mixture of experts for large vision-language models. arxiv preprint arxiv 2401.15947 2024. 2 6 36 chin-yew lin. rouge a package for automatic evaluation of summaries. in text summarization branches out 2004. 5 37 haotian liu chunyuan li qingyang wu and yong jae lee. visual instruction tuning. in advances in neural information processing systems neurips 2023. 1 38 yinhan liu myle ott naman goyal jingfei du mandar joshi danqi chen omer levy mike lewis luke zettle- moyer and veselin stoyanov. roberta a robustly op- timized bert pretraining approach. arxiv preprint arxiv 1907.11692 2019. 5 39 ilya loshchilov and frank hutter. fixing weight decay regu- larization in adam. in proceedings of the international con- ference on learning representations iclr 2019. 5 40 muhammad maaz hanoona rasheed salman khan and fahad shahbaz khan. video-chatgpt towards detailed video understanding via large vision and language mod- els. in proceedings of the annual meeting of the association for computational linguistics acl 2024. 6 41 antoine miech dimitri zhukov jean-baptiste alayrac makarand tapaswi ivan laptev and josef sivic. howto100m learning a text-video embedding by watching hundred million narrated video clips. in proceedings of the ieee cvf international conference on computer vision iccv 2019. 1 42 vishvak murahari dhruv batra devi parikh and abhishek das. large-scale pretraining for visual dialog a simple state-of-the-art baseline. in proceedings of the european conference on computer vision eccv 2020. 2 43 basil mustafa carlos riquelme joan puigcerver rodolphe jenatton and neil houlsby. multimodal contrastive learn- ing with limoe the language-image mixture of experts. in advances in neural information processing systems neurips 2022. 2 44 van quang nguyen masanori suganuma and takayuki okatani. efficient attention mechanism for visual dialog that can handle all the interactions between multiple in- puts. in proceedings of the european conference on com- puter vision eccv 2020. 1 7 45 kishore papineni salim roukos todd ward and wei-jing zhu. bleu a method for automatic evaluation of machine 8646 page 11 translation. in proceedings of the annual meeting of the as- sociation for computational linguistics acl 2002. 5 46 hoang-anh pham thao minh le vuong le tu minh phuong and truyen tran. video dialog as conversation about objects living in space-time. in proceedings of the european conference on computer vision eccv 2022. 2 47 alec radford jeffrey wu rewon child david luan dario amodei ilya sutskever et al. language models are unsu- pervised multitask learners. openai blog 2019. 2 48 ankit p. shah shijie geng peng gao anoop cherian takaaki hori tim k. marks jonathan le roux and chiori hori. audio-visual scene-aware dialog and reasoning us- ing audio-visual transformers with joint student-teacher learning. in proceedings of the ieee international confer- ence on acoustics speech and signal processing icassp 2022. 1 49 piyush sharma nan ding sebastian goodman and radu soricut. conceptual captions a cleaned hypernymed im- age alt-text dataset for automatic image captioning. in pro- ceedings of the annual meeting of the association for com- putational linguistics acl 2018. 5 50 amanpreet singh ronghang hu vedanuj goswami guil- laume couairon wojciech galuba marcus rohrbach and douwe kiela. flava a foundational language and vision alignment model. in proceedings of the ieee cvf confer- ence on computer vision and pattern recognition cvpr 2022. 1 51 quan sun yuxin fang ledell wu xinlong wang and yue cao. eva-clip improved training techniques for clip at scale. arxiv preprint arxiv 2303.15389 2023. 3 52 ashish vaswani noam shazeer niki parmar jakob uszko- reit llion jones aidan n gomez \u0142ukasz kaiser and illia polosukhin. attention is all you need. in advances in neural information processing systems neurips 2017. 1 53 ramakrishna vedantam c. lawrence zitnick and devi parikh. cider consensus-based image description evalu- ation. in proceedings of the ieee cvf conference on com- puter vision and pattern recognition cvpr 2015. 5 54 wenhui wang hangbo bao li dong johan bjorck zhil- iang peng qiang liu kriti aggarwal owais khan mo- hammed saksham singhal subhojit som et al. image as a foreign language beit pretraining for vision and vision- language tasks. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr 2023. 1 2 55 yue wang shafiq r. joty michael r. lyu irwin king caiming xiong and steven c. h. hoi. vd-bert a unified vision and dialog transformer with bert. in proceedings of the conference on empirical methods in natural lan- guage processing emnlp 2020. 2 56 zihao wang junli wang and changjun jiang. unified multi- modal model with unlikelihood training for visual dialog. in proceedings of the acm international conference on multi- media acm mm 2022. 1 2 57 ronald j williams and david zipser. a learning algorithm for continually running fully recurrent neural networks. neu- ral comput. 1989. 2 58 dejing xu zhou zhao jun xiao fei wu hanwang zhang xiangnan he and yueting zhuang. video question answer- ing via gradually refined attention over appearance and mo- tion. in proceedings of the acm international conference on multimedia acm mm 2017. 1 2 59 jun xu tao mei ting yao and yong rui. msr-vtt a large video description dataset for bridging video and language. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr 2016. 1 60 sunjae yoon eunseop yoon hee suk yoon junyeong kim and chang yoo. information-theoretic text hallucination re- duction for video-grounded dialogue. in proceedings of the conference on empirical methods in natural language pro- cessing emnlp 2022. 2 7 61 xintong yu hongming zhang ruixin hong yangqiu song and changshui zhang. vd-pcr improving visual dialog with pronoun coreference resolution. pattern recognition 2022. 2 62 youngjae yu jiwan chung heeseung yun jack hessel jae sung park ximing lu rowan zellers prithviraj am- manabrolu ronan le bras gunhee kim et al. fusing pre- trained language models with multimodal prompts through reinforcement learning. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr 2023. 6 63 zexuan zhong mengzhou xia danqi chen and mike lewis. lory fully differentiable mixture-of-experts for autoregressive language model pre-training. in proceed- ings of the conference on language modeling colm 2024. 2 64 tong zhu xiaoye qu daize dong jiacheng ruan jingqi tong conghui he and yu cheng. llama-moe building mixture-of-experts from llama with continual pre-training. in proceedings of the conference on empirical methods in natural language processing emnlp 2024. 2 8647",
      "keywords": [
        "page",
        "v2dial",
        "unification",
        "video",
        "visual",
        "dialog",
        "via",
        "multimodal",
        "experts",
        "adnen",
        "abdessaied",
        "anna",
        "rohrbach",
        "marcus",
        "andreas",
        "bulling",
        "university",
        "stuttgart",
        "germany",
        "darmstadt",
        "hessian.ai",
        "https",
        "www.collaborative-ai.org",
        "publications",
        "abdessaied25_cvpr",
        "abstract",
        "present",
        "novel",
        "expert-based",
        "model",
        "specif-",
        "ically",
        "geared",
        "towards",
        "simultaneously",
        "handling",
        "image",
        "input",
        "data",
        "conversational",
        "tasks.",
        "cur-",
        "rent",
        "models",
        "primarily",
        "focus",
        "simpler",
        "tasks",
        "e.g.",
        "vqa",
        "videoqa",
        "video-text",
        "retrieval",
        "often",
        "neglect",
        "more",
        "challenging",
        "counterparts",
        "such",
        "dialog.",
        "moreover",
        "works",
        "both",
        "evolved",
        "separately",
        "from",
        "each",
        "other",
        "de-",
        "spite",
        "apparent",
        "similarities",
        "limiting",
        "applicability",
        "potential.",
        "end",
        "propose",
        "unify",
        "using",
        "single",
        "first",
        "time",
        "jointly",
        "learns",
        "spatial",
        "temporal",
        "features",
        "images",
        "videos",
        "routing",
        "through",
        "dedicated",
        "aligns",
        "matching",
        "contrastive",
        "learning",
        "techniques.",
        "furthermore",
        "sys-",
        "temically"
      ],
      "metadata": {
        "filename": "Abdessaied_V2Dial_Unification_of_Video_and_Visual_Dialog_via_Multimodal_Experts_CVPR_2025_paper.pdf",
        "original_filename": "Abdessaied_V2Dial_Unification_of_Video_and_Visual_Dialog_via_Multimodal_Experts_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 2398504,
        "upload_file_id": "upload_1752161814523_u20tfp96y",
        "upload_timestamp": "2025-07-10T15:36:54.592204",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:36:55.145728+00:00",
      "content_length": 52361,
      "content_type": ".pdf",
      "filename": "Abdessaied_V2Dial_Unification_of_Video_and_Visual_Dialog_via_Multimodal_Experts_CVPR_2025_paper.pdf"
    },
    "15": {
      "file_id": "15",
      "content": "\n=== Page 1 ===\nMultiMorph: On-demand Atlas Construction\nS. Mazdak Abulnaga1,2\nAndrew Hoopes1,2\nNeel Dey1\nMalte Hoffmann2\nBruce Fischl2\nJohn Guttag1\nAdrian Dalca1,2\n1MIT Computer Science and Arti\ufb01cial Intelligence Laboratory\n2Massachusetts General Hospital, Harvard Medical School\nabulnaga@csail.mit.edu\nAbstract\nWe present MultiMorph, a fast and ef\ufb01cient method for con-\nstructing anatomical atlases on the \ufb02y. Atlases capture the\ncanonical structure of a collection of images and are es-\nsential for quantifying anatomical variability across popu-\nlations. However, current atlas construction methods often\nrequire days to weeks of computation, thereby discouraging\nrapid experimentation. As a result, many scienti\ufb01c studies\nrely on suboptimal, precomputed atlases from mismatched\npopulations, negatively impacting downstream analyses.\nMultiMorph addresses these challenges with a feedfor-\nward model that rapidly produces high-quality, population-\nspeci\ufb01c atlases in a single forward pass for any 3D brain\ndataset, without any \ufb01ne-tuning or optimization.\nMulti-\nMorph is based on a linear group-interaction layer that ag-\ngregates and shares features within the group of input im-\nages. Further, by leveraging auxiliary synthetic data, Mul-\ntiMorph generalizes to new imaging modalities and popu-\nlation groups at test-time. Experimentally, MultiMorph out-\nperforms state-of-the-art optimization-based and learning-\nbased atlas construction methods in both small and large\npopulation settings, with a 100-fold reduction in time. This\nmakes MultiMorph an accessible framework for biomedical\nresearchers without machine learning expertise, enabling\nrapid, high-quality atlas generation for diverse studies.\n1. Introduction\nWe present MultiMorph, a rapid and \ufb02exible method for\nconstructing anatomical atlases. An atlas, or deformable\ntemplate, is a reference image that represents the typical\nstructure within a collection of related images. In biomed-\nical imaging studies, atlases facilitate studying anatomical\nvariability within and across population groups by serving\nas as a common coordinate system for key image analy-\nsis tasks such as segmentation [5, 26, 30, 82], shape analy-\nsis [1, 25, 52, 62], and longitudinal modeling [36, 66, 67].\nTraditional unbiased atlas construction for a population\ninvolves solving a computationally intensive iterative opti-\nmization problem that often requires several days or weeks\nof computation. The optimization alternates between align-\ning (registering) all images to the estimated atlas and updat-\ning the atlas in both shape and appearance by averaging the\nimages mapped to the intermediate atlas space [7, 48]. Re-\ncent learning-based methods employ a target dataset to ex-\nplicitly learn an atlas jointly with a registration model [18,\n22], yet still require days of training.\nThis necessitates\ncomputational infrastructure and machine learning exper-\ntise that is unavailable to many biomedical researchers.\nRegardless of strategy, an atlas produced from one popu-\nlation of images may not be appropriate for populations that\ndiffer from the group used to build the atlas. Re-estimating\nthe atlas is often required for each new experiment. These\ncomputational challenges are further compounded by the\nneed to construct atlases for speci\ufb01c image types as many\nbiomedical studies acquire several imaging modalities to\nhighlight different biomedical properties of interest. The\nrepeated, prohibitive computational cost of producing a\nnew atlas leads most scientists to use existing atlases that\nmight not be appropriate for their population group or\nmodality, thereby negatively impacting the analyses in\nthese studies [56].\nTo meet these challenges, we introduce MultiMorph, a\nmachine learning model that constructs atlases in a sin-\ngle forward pass, requiring only seconds to minutes of\ncomputation on a CPU, and no machine learning exper-\ntise to use. MultiMorph ef\ufb01ciently generates population-\nand subgroup-speci\ufb01c atlases, enabling accurate and \ufb01ne-\ngrained anatomical analyses. We employ a convolutional\narchitecture that processes an arbitrary number of images\nand computes a set of regularized deformation \ufb01elds that\nalign the group of images to an atlas space central to that\ngroup. The proposed method uses a nonparametric convo-\nlutional operation that interacts the intermediate represen-\ntations of the input images with each other, summarizing\nand aggregating shared features. Further, by training on\ndiverse imaging modalities alongside supplementary syn-\nthetic neuroimaging volumes [28], MultiMorph generalizes\nto arbitrary imaging modalities at test time. We also intro-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n30906\n\n=== Page 2 ===\nduce a centrality layer that ensures that the estimated atlases\nare unbiased [48]. As a result, MultiMorph rapidly pro-\nduces high quality atlases for new populations and imaging\nmodalities unseen during training. It further yields more ac-\ncurate segmentation transfer across population groups than\nboth the most widely used optimization-based approach [7]\nand recent machine learning approaches [18, 24]. To sum-\nmarize our contributions:\n\u2022 We frame atlas construction as a learning-based group\nregistration problem to a central space.\n\u2022 We present a novel neural network architecture that en-\nables communication between the intermediate represen-\ntations of a group of images, and show how this can be\nused to construct accurate group-speci\ufb01c atlases.\n\u2022 We develop a centrality layer that encourages predicted\ndeformations and atlases to be central and unbiased.\n\u2022 Experimentally, MultiMorph produces atlases that are as\ngood, and often better, than those produced by other\nmethods\u2013and it does it up to 100 times faster.\n\u2022 We demonstrate the generalizability of the proposed\nmethod by constructing atlases for unseen imaging\nmodalities and population groups. These atlases condi-\ntioned on age and disease state capture population trends\nwithin the data, enabling cross-group analyses.\nOur model weights and code are available at https://\ngithub.com/mabulnaga/multimorph.\n2. Related work\nDeformable Registration.\nDeformable registration esti-\nmates a dense spatial mapping between image pairs. Tradi-\ntional methods [4, 6, 47, 60, 69, 71, 75] solve an optimiza-\ntion problem balancing image-similarity and regularization\nterms to ensure smooth, invertible deformations.\nLearning-based methods improve test-time ef\ufb01ciency by\ntraining models to directly predict transformations between\nimage pairs, generally enabling faster predictions on new\nimage pairs as compared to traditional methods. Supervised\napproaches [68, 78, 86, 87] are trained to regress simulated\ndeformations or the outputs of registration solvers, whereas\nunsupervised methods [9, 15, 19, 21, 29, 32, 37, 38, 40,\n54, 59, 61, 65, 80, 84, 89] optimize an unsupervised\nimage-similarity loss and a regularization term in training.\nSynthetic Data in Neuroimage Analysis. Recent machine\nlearning-based neuroimage analysis methods have bene\ufb01ted\nfrom synthetic training data that extend far beyond real-\nworld variations [10\u201312, 28, 35, 37, 38, 40, 41, 43, 49, 74].\nThis domain-randomization strategy trains neural networks\non simulated intensity images, synthesized on the \ufb02y from\na training set of anatomical segmentation maps. As part of\nthe generative model, the images undergo corruption steps\nsimulating common acquisition-related artifacts like distor-\ntion [2], low-frequency intensity modulation [77], global\nintensity exponentiation [42], resolution reduction, partial\nvoluming [83], among many others. The large variety of\ndata yields shape-biased networks agnostic to the imaging\nmodality. As a result, these models generalize to arbitrary\nmedical images that have the same anatomy as the synthetic\ntraining data \u2013 largely eliminating the need for retraining to\nmaintain peak performance [23, 39].\nAtlas Construction. Deformable atlas construction seeks\nto \ufb01nd an image that optimally represents a given popula-\ntion, for example, to facilitate atlas-based brain segmenta-\ntion [5, 26, 30, 82] or to initialize longitudinal morphomet-\nric analyses in an unbiased fashion [36, 66, 67].\nIterative atlas construction alternates between registering\neach image of the population to a current estimate of the at-\nlas and updating the atlas with the average of the moved\nimages until convergence [7, 48, 57, 64, 72]. Another ap-\nproach computes a spanning tree of pair-wise transforms\nbetween subjects to estimate an atlas [44, 73].\nIterative\nmethods on 3D data incur prohibitively long runtimes due to\nthe cost of optimization. Therefore, many studies have used\npublicly available atlases [27], although these are often not\nrepresentative of the population being studied.\nRecent learning-based atlas construction techniques\njointly learn an atlas and a registration network that maps\nimages from the training population to the atlas [16, 18,\n20, 22, 24, 31, 76, 79]. These approaches naturally extend\nto constructing conditional atlases, for example condition-\ning on age [18, 22, 79], or incorporating tasks like segmen-\ntation [76]. However, obtaining an atlas for a new popula-\ntion requires machine learning expertise and computational\nresources for re-training from scratch or \ufb01ne-tuning a net-\nwork.\nTest-time\nadaptation\nfor\ngroupwise\nregistration\n(TAG) [33, 34] maps a group of images to a latent\nspace using a VAE, computes an average of latent vectors,\nthen decodes to estimate an atlas.\nWhile this rapidly\nproduces atlases at inference, linearly averaging vec-\ntors in a VAE latent space most often does not yield\na representation that can be decoded into an unbiased\ndeformable atlas. Further, this model must still be retrained\nfor new imaging modalities or populations.\nIn contrast,\nMultiMorph\ndirectly constructs group-speci\ufb01c atlases\nfrom warped images, ensuring \ufb01delity to the data without\ndistortions introduced by latent space aggregation. A single\nMultiMorph model can generate atlases for a wide variety\nof imaging modalities and population groups.\nFlexible-size Inputs. Recent methods have employed a va-\nriety of mechanisms that are \ufb02exible to input size, in other\napplications. For example, in-context learning methods use\na \ufb02exible-sized input set of input-output example pairs to\nguide a new image-processing task at inference [14, 17].\nOther methods use attention mechanisms across different\n30907\n\n=== Page 3 ===\ninputs to aggregate information among volume slices [85]\nor tabular data [53]. While cross-attention and variants have\nbeen effective for many tasks in vision, they have quadratic\nmemory complexity. At each iteration, our model requires\na large set of 3D volumes. Using cross-attention would lead\nto infeasible memory requirements. In contrast, we propose\na \ufb02exible feature sharing mechanism with linear complexity\nto produce central atlases for large groups of 3D images.\n3. Methods\n3.1. Background\nGiven two images x1, x2 \u2192Rd, deformable registration\nseeks a nonlinear mapping \u03c9 : x1 \u2191x2 that warps one\nimage into the space of the other. The deformation \u03c9 at-\ntempts to align the underlying anatomy captured by the im-\nages while maintaining a well-behaved map, and it is tradi-\ntionally computed by optimization:\narg min\n\u03c9\nLsim (x2, x1 \u2193\u03c9) + \u03b5Lreg (\u03c9) ,\n(1)\nwhere Lsim (x2, x1 \u2193\u03c9) measures similarity between im-\nage x2 and the warped image x1 \u2193\u03c9, Lreg(\u00b7) regularizes\nthe map \u03c9, and \u03b5 is a hyperparameter that balances the two.\nMany population-based studies involve groupwise anal-\nyses. Group registration aligns a collection of m images\nXm = {xi}m\ni=1 to an explicit image template t,\narg min\n\u03c91,...,\u03c9m\nm\n!\ni=1\nLsim (t, xi \u2193\u03c9i) + \u03b5Lreg (\u03c9i) .\n(2)\nIn many scenarios, an explicit template is not available. One\ncan be constructed by iterating a template estimation step,\n\u00aft =\n1\nm\n\" xi \u2193\u03c9i, and the groupwise optimization (2) until\nconvergence. However, this is computationally expensive\nand does not scale well to large populations.\nMachine learning approaches for pairwise registration\nuse a neural network to predict \u03c9 as a function of the in-\nput images: f\u03b5(x1, x2) = \u03c9, where f is a neural network\nparameterized by \u03d1. Pairwise registration is rapidly com-\nputed by a single forward pass of a trained network. Recent\nmethods [18, 22, 24, 76] also estimate a common template\nt together with parameters \u03d1 in a network, f\u03b5(t, x) = \u03c9.\nIn this work, we develop a model to directly predict a\ngroup-speci\ufb01c set of deformation \ufb01elds to a central template\nspace. We formulate template construction as a group reg-\nistration problem given a variable number of inputs.\n3.2. Flexible Group Registration\nGiven a set Xm of m images from a dataset X = {xi}n\ni=1,\nwe seek to map the images to a space central to Xm. The set\nXm could be an entire population or a subgroup of patients\nrepresenting an underlying demographic or condition.\nLet \u03c9i\n:\nRd\n\u2191\nRd represent the map from xi\nto a central latent image space.\nWe model a function\nf\u03b5({x1, . . . , xm}) = {\u03c91, . . . , \u03c9m} using a convolutional\nneural network with learnable parameters \u03d1. The number of\nparameters of \u03d1 is independent of the group size m.\nTo achieve desirable group registration, we construct f\nto satisfy the following desiderata:\n\u2022 Flexible to group size: f takes as input a variable num-\nber of m images, and computes m maps \u03c9 to a group-\nspeci\ufb01c central space.\n\u2022 Fast: Computation of {\u03c9i}m\ni=1 can be done in a single\nforward pass of an ef\ufb01cient network.\n\u2022 Generalizable: generalize to unseen datasets Ym.\n\u2022 Unbiased: the images in Xm map to a space central to\nthat set: mean({\u03c9i}m\ni=1) = 0.\n\u2022 Aligned: Images {xi \u2193\u03c9i}m\ni=1 mapped to the template\nspace are anatomically aligned.\nSatisfying the desiderata leads to a model that can produce\n\ufb02exible templates for user-de\ufb01ned groups on demand. We\nintroduce new methods to achieve these properties below.\n3.3. Model\nFigure 1 gives an overview of the network architecture of\nMultiMorph. The network takes a group of a variable num-\nber of images and predicts diffeomorphic transformations\nto a central template space speci\ufb01c to the group. At each\nnetwork layer, we share features across the inputs using the\nproposed GroupBlock layer. The network outputs sta-\ntionary velocity \ufb01elds, which are then adjusted by a central-\nity layer to produce an unbiased atlas.\nConvolution Layer for Variable Group Size. We propose\nGroupBlock, a convolutional layer that combines image\nfeatures across a group. As group registration seeks to align\nimages to a central space, feature communication is helpful\nto produce a subgroup alignment.\nWe use a summary statistic to aggregate group features,\nand communicate the statistic back to each individual group\nelement. Let c(l)\ni\nrepresent the feature map for input image\ni at network layer l. The GroupBlock layer aggregates\ninformation as follows:\n\u00afc(l) = s({c(l)\n1 , . . . , c(l)\nm })\nc(l+1)\ni\n= Conv\n#$\nc(l)\ni\n%%\u00afc(l)&\n; \u03d1(l)'\n,\nwhere s(\u00b7) is the summary statistic across the group dimen-\nsion, [\u00b7\u2194\u00b7] is the concatenation operation along the channel\ndimension, and Conv is a convolutional layer with parame-\nters \u03d1(l). We use the mean as our summary statistic.\nNetwork. We modify the popular UNet architecture [70],\nemploying a multi-scale structure with residual connec-\ntions. We replace the standard Conv layers with the pro-\nposed GroupBlock feature sharing layer (\u00a73.3). The net-\n30908\n\n=== Page 4 ===\nm input\nimages\nMultiscale\nGroupBlocks\nm warp\nm warped\nimages\noutput atlas\ninputs\noutputs\nGroupBlock\nCentrality Layer\nSVF Integration\nConv Layer\nConv Layer\nConv Layer\n= channel-wise concatenation\nLsim\nLsim\nLsim\nLreg\nmean\nmean\nFigure 1. MultiMorph architecture diagram. The model takes in a variable group of m images and constructs an atlas speci\ufb01c to that\ngroup. At each layer of the UNet, the proposed GroupBlock mechanism replaces standard convolution kernels. Speci\ufb01cally, it computes\nthe elementwise mean of the intermediate features across the group, and concatenates the resulting features with the individual features.\nThe mechanism enables group interaction by sharing summarized input features across the group. The network outputs m velocity \ufb01elds\nmapping images to the group-speci\ufb01c template space. A centrality layer removes any global bias in the average velocity \ufb01eld, before\nintegration and warping the images. The output is a central template representing the shared anatomy of the input group.\nwork takes as input a group of m images and outputs m\nd-dimensional stationary velocity \ufb01elds (SVF).\nWe use the standard SVF representation of a diffeomor-\nphism [4, 18]. The deformation \ufb01eld is de\ufb01ned through the\nordinary differential equation:\n\u03d1\u03c9(t)\nv\n\u03d1t\n= v \u2193\u03c9(t)\nv , where v\nis the velocity \ufb01eld.\nThe \ufb01eld is de\ufb01ned for t \u2197[0, 1]\nwith \u03c9(0)\nv\n= Id, the identity map. The deformation \ufb01eld\nis obtained by integrating v using the scaling and squaring\nmethod [4, 19].\nCentrality Layer. Constructing an atlas central to the pop-\nulation group is key to performing unbiased downstream\nanalyses, such as in quantifying anatomical shape differ-\nences without bias to any particular structure or subject. A\ncentral atlas is one that is \u201cclose\u201d to the target population.\nMany learning approaches use a regularization term to min-\nimize the mean displacement \ufb01eld [18, 22].\nWe construct a layer that produces a group-speci\ufb01c cen-\ntral template by construction. We subtract the groupwise\nmean from the output velocity \ufb01elds: vi = v(L)\ni\n\u2198\u00afv(L),\nwhere v(L)\ni\nis the \ufb01nal output velocity \ufb01eld for image i, and\n\u00afv(L) is the group mean. This centers the velocity \ufb01elds in\nthe zero-mean Lie subspace.\nTemplate Construction. Given a trained network f\u03b5, we\ncan construct a template t by aggregating the warped im-\nages of the group Xm: t = g (x1 \u2193\u03c91, . . . , xm \u2193\u03c9m). We\nuse the mean operation for g.\nTo apply the map to the group of images, we integrate the\nSVF to obtain a diffeomorphic displacement \ufb01eld [4, 19].\nWe then use a spatial transformation function [46] to warp\nthe images to the central space. The spatial transformer per-\nforms a pullback operation with linear interpolation.\n3.4. Auxiliary Structural Information\nThe use of anatomical labelmaps during training of\nlearning-based registration often improves substructure\nalignment [9]. When segmentation maps are available for\nsome of the images in a set, we use this information to form\nan atlas segmentation map. Let seg[xi] indicate the proba-\nbilistic segmentation labelmap of the K structures for image\nxi. We construct the labelmap of the template, seg[t], by\ntaking the set-wise average of the warped probability maps\nseg[t] = meanm{seg[x1] \u2193\u03c91, . . . , seg[xm] \u2193\u03c9m}.\n3.5. Synthetic Training\nTo aid generalization to unseen modalities, we also train on\nimages synthesized from brain tissue segmentations. For\neach synthetic training group, we sample K random val-\nues uniformly corresponding to K structures. We then use\na domain randomization procedure [28] to randomly sam-\nple intensity values for each structure, along with a variety\nof noise patterns and artifacts. This yields groups of syn-\nthetic images, where each group exhibits random intensity\ndistributions and tissue contrasts. Supplementary Fig. 11\npresents a representative set of example synthetic images.\n3.6. Loss\nWe maximize alignment between the images and anatom-\nical structures of the group and the constructed template,\nwhile maintaining a smooth map. For a single image, the\n30909\n\n=== Page 5 ===\nloss is computed as:\nL (\u03c9i) =Lsim (t, xi \u2193\u03c9i) + \u03b5Lreg (\u03c9i)\n+ \u03d6Lstruc (seg [t] , seg [xi] \u2193\u03c9i) .\n(3)\nThe \ufb01rst term Lsim measures pairwise similarity between\nimage xi and the template t. We use the normalized cross-\ncorrelation objective. The second term regularizes the de-\nformation \ufb01eld to be smooth, Lreg(\u03c9i) = \u2194\u2243\u03c9i\u21942. When\nlabel maps are available during training, we use the third\n(auxiliary) loss term to align the structures of the training\nset with the constructed template, using soft-Dice.\nOur\ncomplete\ngroup\nloss\nis\nL(\u03c91, . . . , \u03c9m)\n=\n1\nm\n\"\n{i:xi\u2192Xm} L(\u03c9i). Since the template t is constructed\nby averaging warped images of the group, the loss is depen-\ndent on all images of the group.\n4. Experiments\nWe evaluate MultiMorph using 3D brain MRI brain scans, a\ncommon setting for atlas construction. We compare Multi-\nMorph against iterative and learning-based approaches in\nterms of speed, centrality, and accuracy.\nWe also test\nwhether MultiMorph generalizes to new datasets, imaging\nmodalities, and populations that are unseen during training.\n4.1. Experimental Setup\nData.\nWe use four public 3D brain MRI datasets. Three\ndatasets\u2014OASIS-1, OASIS-3, and MBB\u2014are used for\ntraining, validation, and testing, while IXI serves as an un-\nseen test set. OASIS-1 [58] includes T1-weighted (T1-w)\nscans of 416 subjects aged 18-96. A hundred OASIS-1 sub-\njects of ages 60 years and older were diagnosed with mild\nto moderate Alzheimer\u2019s disease (AD), which is correlated\nwith brain atrophy. OASIS-3 [55] contains T1-w and T2-w\nMRI scans of subjects aged 42-95 years old. We use a sub-\nset of 1043 subjects, with 210 diagnosed with mild to severe\ncognitive dementia. The Mind Brain Body dataset [8] in-\ncludes T2-w and T2-FLAIR scans of 226 healthy subjects.\nFor each training dataset, we randomly hold out 20% of the\nsubjects for testing, and split the rest into 85% for training\nand 15% for validation. Each split includes an equal mix of\nhealthy and abnormal subjects of all age ranges. We use the\nsame model for all experiments.\nLastly, to evaluate generalization, we hold out the IXI\ndataset [45]. We arbitrarily select the Guys Hospital site\nwithin IXI and retrieve T1-w, T2-w, and PD-w MRI scans\nof 319 adult subjects. Importantly, the PD-w MRI modality\nis not included in any of the training datasets used by our\nmodel. These datasets span a large age range and include\na mix of disease states and imaging modalities, simulating\nreal-world population studies.\nImplementation details. During training, all images within\na sampled group have the same acquisition modality. We\napply augmentations, including random exponential scal-\ning, intensity inhomogeneities, and per-voxel Gaussian\nnoise. Additionally, 50% of the sampled training groups\ncontain synthetic images instead of real acquisitions. For\npreprocessing, using ANTs [81], we af\ufb01nely align each 3D\nscan to a common 1-mm isotropic af\ufb01ne reference used\nin [37, 39].\nWe extract brain tissue signal using Synth-\nStrip [41] and generate segmentation maps of 30 unique\nanatomical brain structures using SynthSeg [11].\nWe train using the Adam optimizer [51] with a learning\nrate of 10\u21914. The \ufb01eld regularization hyperparameter is set\nto \u03b5 = 1.0 and the segmentation-loss weight is \u03d6 = 0.5,\nboth chosen via grid search (see Supplemental Sec. 7). At\neach training iteration, we randomly sample m = [2, 12]\nimages to form a group and train for 80, 000 iterations,\nusing the \ufb01nal saved model. All models are trained on a\nsingle RTX8000 GPU. All runtime evaluation results were\ndone on an Intel(R) Xeon(R) Gold 5218 CPU.\nBaselines. We evaluate SyGN [7], a widely-used iterative\natlas construction method from the ANTs library [81].\nAdditionally, we compare against AtlasMorph, a learning-\nbased atlas constructor [18] that explicitly learns an atlas\nto best \ufb01t the training data. For AtlasMorph, we set the\ndeformation \ufb01eld regularization hyperparmeter to \u03b5 = 0.1,\nas determined via cross-validation. Both MultiMorph and\nAtlasMorph use the same core registration network.\nWe also evaluate Aladdin [24], a learning-based method\nthat constructs an average reference atlas during training by\nlearning pairwise registrations. At test time, this atlas serves\nas the registration target, enabling the generation of new at-\nlases for different population groups. Since Aladdin con-\nstructs modality-speci\ufb01c atlases, we train a separate model\n(with the same capacity as our network) for each modality\nin our dataset using an optimal regularization loss weight\nof 10,000, a similarity loss weight of 10, and an image pair\nloss weight of 0.2, all determined using a grid search. Both\nAtlasMorph and Aladdin models are trained for 50,000 it-\nerations, followed by 1,500 \ufb01netuning iterations per popu-\nlation subgroup to estimate a group atlas at test-time.\nEvaluation. We assess the effectiveness of atlas construc-\ntion techniques in rapidly generating central atlases for new\npopulations. To evaluate registration quality, we compute\nthe Dice score to assess how well the atlas aligns with\nwarped subject scans. We assess \ufb01eld regularity and topol-\nogy by computing the determinant of the Jacobian of the\nmap, det J\u03c9(p) = det (\u2243\u03c9 (p)) at each voxel p. Locations\nwhere det J\u03c9(p) < 0 represent folded regions breaking lo-\ncal injectivity. Additionally, we measure atlas centrality by\nreporting the mean displacement \ufb01eld \u2194\u00afu\u21942. Statistical sig-\nni\ufb01cance is determined using a paired t-test with p < 0.01.\nSegmentation transfer.\nAs atlases are commonly used\nfor segmentation by warping atlas labels to new target\n30910\n\n=== Page 6 ===\nTable 1. Atlas construction evaluation on 319 brain volumes from IXI. While all baselines were trained or optimized on the full dataset,\nMultiMorph was not, demonstrating its ability to generalize to entirely new datasets. MultiMorph also generalizes to the PD-w modality\nnot seen during training, demonstrating its capabilities on unseen imaging modalities. \u2192indicates statistical signi\ufb01cance (p < 0.01).\nModality\nMethod\nConstruction time (min.) (\u2192)\nDice (\u2191)\nFolds (\u2192)\nCentrality \u219310\u21912 (\u2192)\nT1-w\nANTs[7]\n4345.20\n0.863 \u00b1 0.075\n524.2 \u00b1 580.04\n10.4 \u00b1 30.67\nAtlasMorph[18]\n1141.50\n0.894 \u00b1 0.015\n47.9 \u00b1 29.22\n7.8 \u00b1 19.09\nAladdin[24]\n325.20\n0.885 \u00b1 0.01\n0.0 \u00b1 0.0\u2192\n106.8 \u00b1 97.6\nOurs\n10.50\n0.913 \u00b1 0.006\u2192\n1.1 \u00b1 1.55\n1.4 \u00b1 4.32\u2192\nT2-w\nANTs[7]\n4380.60\n0.862 \u00b1 0.071\n522.6 \u00b1 476.86\n18.6 \u00b1 44.286\nAtlasMorph[18]\n831.60\n0.882 \u00b1 0.018\n57.5 \u00b1 31.935\n7.8 \u00b1 19.34\nAladdin[24]\n261.00\n0.875 \u00b1 0.012\n0.0 \u00b1 0.125\u2192\n771.5 \u00b1 744.309\nOurs\n10.40\n0.906 \u00b1 0.007\u2192\n2.0 \u00b1 2.49\n1.5 \u00b1 4.683\u2192\nPD-w\nANTs[7]\n4320.20\n0.856 \u00b1 0.069\n313.1 \u00b1 359.9\n12.4 \u00b1 32.805\nAtlasMorph[18]\n959.00\n0.884 \u00b1 0.018\n40.5 \u00b1 26.10\n7.4 \u00b1 19.483\nAladdin[24]\n163.80\n0.849 \u00b1 0.029\n0.0 \u00b1 0.0\u2192\n1175.7 \u00b1 1731.773\nOurs\n7.80\n0.900 \u00b1 0.009\u2192\n1.601 \u00b1 0.205\n0.9 \u00b1 3.02\u2192\nimages, we evaluate each method\u2019s segmentation perfor-\nmance.\nEach atlas is estimated using half the subgroup\n( m\n2 images). We randomly sample m\n2 segmentation label\nmaps to generate the atlas segmentation mask, which is then\ntransferred to the remaining m\n2 images. Segmentation qual-\nity is assessed using the Dice score.\n4.2. Results\n4.2.1\nGeneralizing to Unseen Datasets and Modalities\nTable 1 presents results for all methods on the IXI dataset,\nwhich was entirely held-out for MultiMorph\u2019s training and\nvalidation. MultiMorph produces atlases over a 100\u21d0faster\nthan ANTs and AtlasMorph, and 30\u21d0faster than Aladdin.\nIt consistently achieves the highest Dice score, indicating\nbetter anatomical alignment even when constructed on un-\nseen data at test time in a single forward pass. Additionally,\nMultiMorph yields regular deformation \ufb01elds with negligi-\nble folding and signi\ufb01cantly lower bias in the displacement\n\ufb01elds, indicating that the constructed atlases are central.\nFig. 2 visualizes sample registration predictions for each\nmodality in IXI and Fig. 3 illustrates example atlases for IXI\nT1-w and PD-w. Despite never having been trained on this\ndataset nor having seen the PD-w imaging modality during\ntraining, MultiMorph estimates atlases that yield high group\nalignment in only minutes, demonstrating its potential for\nTable 2. Sub-group atlas construction results. Reported scores\nare averaged across atlases constructed using subgroups of\n[5, 10, 20, . . . , 60]. \u2192indicates statistical signi\ufb01cance (p < 0.01).\nMethod\nRun time\n(min.) (\u2193)\nDice\nTransfer\n(\u2194)\nFolds (\u2193)\nCentrality\n\u219710\u21922 (\u2193)\nANTs [7]\n436 \u00b1 0.4\n0.875 \u00b1 0.009\n447 \u00b1 110\n8.7 \u00b1 0.1\nAtlasMorph [18]\n17 \u00b1 1.4\n0.893 \u00b1 0.005\n50.0 \u00b1 8.7\n9.7 \u00b1 0.1\nAladdin [24]\n12 \u00b1 0.1\n0.877 \u00b1 0.004\n0.0 \u00b1 0.0\u2191\n173 \u00b1 3.7\nOurs\n1.5 \u00b1 0.0\n0.904 \u00b1 0.002\u2191\n1.3 \u00b1 0.4\n1.4 \u00b1 0.04\u2191\nscienti\ufb01c studies requiring speci\ufb01c atlases. We provide ad-\nditional examples in Supplemental Fig. 10.\n4.2.2\nStandard Atlas Construction\nWe now evaluate the ability of MultiMorph to construct\npopulation atlases across different age groups and disease\nstates. Speci\ufb01cally, we construct an atlas on the OASIS-\n3 T1-w test dataset. All baseline models were trained and\nvalidated on the test set. Table 3 shows that MultiMorph\nachieves the highest Dice score while producing atlases\n30 \u2198400 times faster than the baseline methods.\n4.2.3\nSubgroup Atlas Construction\nMultiMorph enables the rapid construction of subgroup at-\nlases for granular population analyses. We evaluate atlases\nAtlas\nMoving image\nWarp\nMoved image\nT1-w\nT2-w\nPD-w\nFigure 2. Example images and warps to the constructed atlas on\nthe IXI dataset. Three subjects and three modalities are shown.\n30911\n\n=== Page 7 ===\nANTs\nAladdin\nAtlasMorph\nOurs\nANTs\nAladdin\nAtlasMorph\nOurs\nFigure 3. Atlases constructed on the IXI T1-w (left) and IXI PD-w (right) image modality. All baseline methods used the dataset for\ntraining or optimization, while our method was never trained on the IXI data. Further, our method was never trained on PD-w images, yet\ngeneralizes to this modality.\nconditioned on age, age and disease state, as well as random\nsubgroupings of the population.\nRandom Subgroup Analysis. We quantify the effect of\nsubgroup size on atlas quality using the held-out IXI T1-w\ndataset. Subgroups of [5, 10, 20, . . . , 60] images are ran-\ndomly sampled, with half used to construct the atlas seg-\nmentation and the other half used for evaluation. As in Sec-\ntion 4.2.1, the baselines were trained or optimized on this\ndataset, whereas MultiMorph was not exposed to any IXI\nT1-w data during training or validation.\nFig. 4 shows that MultiMorph consistently outperforms\nbaselines, with performance improving as the subgroup\nFigure 4. Segmentation transfer performance when varying the\nnumber of images used to construct an atlas. Data is taken from the\nIXI T1-w dataset, which our model did not have access to during\ntraining. Our method consistently outperforms the baselines.\nsize increases. Table 2 reports mean performance across\nsubgroups, with MultiMorph showing better segmentation\ntransfer while maintaining well-behaved deformation \ufb01elds.\nImportantly, MultiMorph only requires 1.5 minutes of infer-\nence time on a CPU, whereas baselines require \ufb01ne-tuning\nor re-optimization, which is both time consuming and re-\nquires tens or hundreds of minutes.\nAge. We \ufb01rst demonstrate MultiMorph\u2019s ability to create\nappropriate atlases for user-de\ufb01ned subgroups by grouping\nhealthy OASIS-1 subjects into age bins. We take normal\nsubjects in the validation and test set, and bin them into age\nranges [0 \u219819, 20 \u219829, . . . , 80 \u219889]. Fig. 5 presents qual-\nitative results, showing anatomical changes consistent with\nnormal aging, such as ventricular enlargement due to brain\natrophy [3]. All atlases were generated in under a minute\nwithout any \ufb01ne-tuning.\nDiagnosis.\nLastly, we examine the effect of dementia\non brain aging in the OASIS-3 (T1-w) dataset. We con-\nTable 3. Atlas estimation results on 212 subjects from the OASIS-\n3 T1-w test set. \u2192indicates statistical signi\ufb01cance (p < 0.01).\nMethod\nRun time\n(min.) (\u2193)\nDice\nTransfer\n(\u2194)\nFolds (\u2193)\nCentrality\n\u219710\u21922 (\u2193)\nANTs [7]\n2858\n0.886 \u00b1 0.017\n765 \u00b1 877\n9.5 \u00b1 25.6\nAtlasMorph [18]\n688\n0.881 \u00b1 0.024\n50.2 \u00b1 31.9\n8.0 \u00b1 0.2\nAladdin [24]\n277\n0.878 \u00b1 0.016\n0.0 \u00b1 0.07\u2191\n175.9 \u00b1 1.8\nOurs\n5.9\n0.910 \u00b1 0.014\u2191\n1.2 \u00b1 2.3\n1.5 \u00b1 0.05\u2191\n30912\n\n=== Page 8 ===\n0-19\n20-29\n30-39\n40-49\n50-59\n60-69\n70-79\n80-89\nFigure 5. Atlases conditioned on age for healthy subjects in OASIS-1. Ventricle enlargement (red boxes) is observed across time, consistent\nwith neurodegeneration with aging.\nstruct age-conditioned atlases separately for normal and\ndementia-diagnosed subjects. Fig. 6 compares brain atro-\nphy across matched age groups.\nWe observe substantial\nenlargement of the ventricles (outlined in red boxes) and\ndeterioration of the white matter in the dementia group\nas compared to the controls, consistent with the litera-\nture [3, 50, 63, 88].\n4.3. Ablation studies\nWe quantify the effect of several key model components, in-\ncluding the centrality layer (CL), the Group Block (GB)\nmechanism with varying summary statistics (mean, vari-\nance, max), and training without the Dice Loss. Using the\nOASIS-1 dataset [58], we train our model for 50, 000 itera-\ntions and assess performance on the test set.\nTable 4 summarizes the results. The CL signi\ufb01cantly\nreduced the centrality measure by 1000\u21d0, ensuring un-\nbiased atlas construction, though it came along with a 1\npoint decrease in Dice. The GB mechanism improved Dice\nby 1.4 points with negligible degradation of \ufb01eld regular-\nity. We observe no signi\ufb01cant performance variation across\nthe various summary statistics tested.\nFinally, the Dice\nloss improved performance by over 2 Dice points. Taken\nholistically, each component strongly contributed to Multi-\nMorph\u2019s performance. We further quantify the impact on\nsubgroup atlas construction in Supplemental Section 6.2\nand observe similar trends. Additionally, we assess the im-\npact of training with synthetic data in Supplemental Sec-\ntion 6.1, which improved IXI dataset performance by up to\nNormal\nDementia\n60-69\n80-89\n75-79\n70-74\nFigure 6. Atlases conditioned on age for normal subjects (top)\nand subjects with dementia (bottom) from OASIS-3. Visual differ-\nences indicate considerable enlargement of ventricles (red boxes)\nand atrophy of white matter when compared to normal subjects.\n1.8 Dice points while maintaining \ufb01eld regularity, demon-\nstrating better generalization.\n5. Discussion\nLimitations and future work.\nMultiMorph has several\navenues for extensions.\nFor example, as it assumes dif-\nfeomorphic transformations, MultiMorph cannot accurately\nconstruct atlases for neuroimages with topology-changing\npathologies.\nHowever, this can be addressed by using\npathology masks when calculating losses in training [13].\nAdditionally, MultiMorph is currently only trained for neu-\nroimages, but can be trained on anatomy-agnostic synthetic\ndata [23, 37] to estimate atlases for arbitrary applications.\nLastly, our implementation stores all activations in mem-\nory at inference, potentially limiting higher group sizes with\nlarge 3D volumes in memory-constrained settings.\nConclusion. We presented MultiMorph, a test-time atlas\nconstruction framework that works with unseen imaging\nmodalities and any number of input images\u2013without retrain-\ning. At its core, MultiMorph leverages a novel convolu-\ntional layer for groups of images, independent of the num-\nber of input samples, enabling ef\ufb01cient and scalable atlas\ngeneration. MultiMorph produces unbiased atlases for ar-\nbitrary inputs with comparable (and often better) perfor-\nmance, while also being over 100 times faster than previ-\nous approaches that require either solving an optimization\nproblem or retraining a model. By making high-quality at-\nlas construction fast, accessible, and adaptable, MultiMorph\npotentially unlocks new avenues for biomedical research,\nenabling computational anatomy studies that were previ-\nously impractical due to computational constraints.\nTable 4. Model ablations on the Centrality Layer, Group Block\nmechanism, and Dice loss on the OASIS-1 test set. All proposed\ncomponents improved atlas construction performance.\nAblation\nDice\nTransfer (\u2194)\nFolds (\u2193)\nCentrality\n\u219710\u21923 (\u2193)\nno CL, GB(mean)\n0.892 \u00b1 0.018\n0.0 \u00b1 0.0\n16125 \u00b1 11494\nCL, no GB\n0.870 \u00b1 0.021\n0.1 \u00b1 0.3\n9.9 \u00b1 27.4\nCL, GB(var)\n0.883 \u00b1 0.020\n1.5 \u00b1 2.8\n12.8 \u00b1 59.27\nCL, GB(max)\n0.880 \u00b1 0.019\n1.5 \u00b1 2.7\n12.6 \u00b1 46.69\nCL, GB(mean)\n0.884 \u00b1 0.020\n1.1 \u00b1 1.9\n12.0 \u00b1 39.48\nCL, GB(mean), Dice\n0.919 \u00b1 0.011\n5.4 \u00b1 7.5\n18.6 \u00b1 61.31\n30913\n\n=== Page 9 ===\nAcknowledgements\nWe thank Marianne Rakic for her help in the AtlasMorph\nexperiments.\nWe thank Zack Berger for help in proof-\nreading. Support for this research was provided in part by\nQuanta Computer Inc. project AIR, the NIH BICCN grants\nU01 MH117023 and UM1 MH130981, NIH BRAIN CON-\nNECTS U01 NS132181, UM1 NS132358, NIH NIBIB\nR01 EB023281, R21 EB018907, R01 EB019956, P41\nEB030006, NIH NIA R21 AG082082, R01 AG064027,\nR01 AG016495, R01 AG070988, the NIH NIMH UM1\nMH130981,\nR01 MH123195,\nR01 MH121885,\nRF1\nMH123195, NIH NINDS U24 NS135561, R01 NS070963,\nR01 NS083534, R01 NS105820, R25 NS125599, NIH\nNICHD R00 HD101553, NIH R01 EB033773, and was\nmade possible by the resources provided by NIH Shared In-\nstrumentation Grants S10 RR023401, S10 RR019307, and\nS10 RR023043. Additional support was provided by the\nNIH Blueprint for Neuroscience Research U01 MH093765,\npart of the multi-institutional Human Connectome Project.\nMuch of the computation resources was performed on hard-\nware provided by the Massachusetts Life Sciences Center.\nReferences\n[1] Riza Alp Guler, George Trigeorgis, Epameinondas Anton-\nakos, Patrick Snape, Stefanos Zafeiriou, and Iasonas Kokki-\nnos.\nDensereg: Fully convolutional dense shape regres-\nsion in-the-wild. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 6799\u20136808,\n2017. 1\n[2] Jesper LR Andersson, Stefan Skare, and John Ashburner.\nHow to correct susceptibility distortions in spin-echo echo-\nplanar images: application to diffusion tensor imaging. Neu-\nroimage, 20(2):870\u2013888, 2003. 2\n[3] Liana G Apostolova, Amity E Green, Sona Babakchanian,\nKristy S Hwang, Yi-Yu Chou, Arthur W Toga, and Paul M\nThompson. Hippocampal atrophy and ventricular enlarge-\nment in normal aging, mild cognitive impairment (mci), and\nalzheimer disease. Alzheimer Disease & Associated Disor-\nders, 26(1):17\u201327, 2012. 7, 8\n[4] John Ashburner. A fast diffeomorphic image registration al-\ngorithm. Neuroimage, 38(1):95\u2013113, 2007. 2, 4\n[5] John Ashburner and Karl J Friston. Uni\ufb01ed segmentation.\nneuroimage, 26(3):839\u2013851, 2005. 1, 2\n[6] Brian B Avants, Charles L Epstein, Murray Grossman, and\nJames C Gee. Symmetric diffeomorphic image registration\nwith cross-correlation: evaluating automated labeling of el-\nderly and neurodegenerative brain. Medical image analysis,\n12(1):26\u201341, 2008. 2\n[7] Brian B Avants, Paul Yushkevich, John Pluta, David\nMinkoff, Marc Korczykowski, John Detre, and James C Gee.\nThe optimal template effect in hippocampus studies of dis-\neased populations. Neuroimage, 49(3):2457\u20132466, 2010. 1,\n2, 5, 6, 7\n[8] A Babayan, M Erbey, D Kumral, JD Reinelt, AMF Reiter, J\nR\u00a8obbig, HL Schaare, M Uhlig, A Anwander, PL Bazin, et al.\nA mind-brain-body dataset of mri, eeg, cognition, emotion,\nand peripheral physiology in young and old adults. sci. data\n6, 180308, 2018. 5\n[9] Guha Balakrishnan, Amy Zhao, Mert R Sabuncu, John Gut-\ntag, and Adrian V Dalca. Voxelmorph: a learning framework\nfor deformable medical image registration. IEEE transac-\ntions on medical imaging, 38(8):1788\u20131800, 2019. 2, 4\n[10] Benjamin Billot, Eleanor Robinson, Adrian V Dalca, and\nJuan Eugenio Iglesias. Partial volume segmentation of brain\nmri scans of any resolution and contrast. In Medical Image\nComputing and Computer Assisted Intervention\u2013MICCAI\n2020: 23rd International Conference, Lima, Peru, Octo-\nber 4\u20138, 2020, Proceedings, Part VII 23, pages 177\u2013187.\nSpringer, 2020. 2\n[11] Benjamin Billot, Douglas N Greve, Oula Puonti, Axel\nThielscher, Koen Van Leemput, Bruce Fischl, Adrian V\nDalca, Juan Eugenio Iglesias, et al. Synthseg: Segmenta-\ntion of brain mri scans of any contrast and resolution without\nretraining. Medical image analysis, 86:102789, 2023. 5\n[12] Benjamin Billot, Colin Magdamo, You Cheng, Steven E\nArnold, Sudeshna Das, and Juan Eugenio Iglesias. Robust\nmachine learning segmentation for large-scale analysis of\nheterogeneous clinical brain mri datasets. Proceedings of the\nNational Academy of Sciences, 120(9):e2216399120, 2023.\n2\n[13] Matthew Brett, Alexander P Leff, Chris Rorden, and John\nAshburner. Spatial normalization of brain images with fo-\ncal lesions using cost function masking. Neuroimage, 14(2):\n486\u2013500, 2001. 8\n[14] Victor Ion Butoi, Jose Javier Gonzalez Ortiz, Tianyu Ma,\nMert R Sabuncu, John Guttag, and Adrian V Dalca. Uni-\nverseg: Universal medical image segmentation. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 21438\u201321451, 2023. 2\n[15] Qing Chang, Chenhao Lu, and Mengke Li. Cascading af\ufb01ne\nand B-spline registration method for large deformation reg-\nistration of lung X-rays. Journal of Digital Imaging, 36(3):\n1262\u20131278, 2023. 2\n[16] Zeen Chi, Zhongxiao Cong, Clinton J Wang, Yingcheng\nLiu, Esra Abaci Turk, P Ellen Grant, S Mazdak Abulnaga,\nPolina Golland, and Neel Dey. Dynamic neural \ufb01elds for\nlearning atlases of 4d fetal mri time-series. arXiv preprint\narXiv:2311.02874, 2023. 2\n[17] Steffen Czolbe and Adrian V Dalca. Neuralizer: General\nneuroimage analysis without re-training. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6217\u20136230, 2023. 2\n[18] Adrian Dalca, Marianne Rakic, John Guttag, and Mert\nSabuncu. Learning conditional deformable templates with\nconvolutional networks.\nAdvances in neural information\nprocessing systems, 32, 2019. 1, 2, 3, 4, 5, 6, 7\n[19] Adrian V Dalca, Guha Balakrishnan, John Guttag, and\nMert R Sabuncu. Unsupervised learning of probabilistic dif-\nfeomorphic registration for images and surfaces.\nMedical\nimage analysis, 57:226\u2013236, 2019. 2, 4\n[20] Maik Dannecker, Vanessa Kyriakopoulou, Lucilio Cordero-\nGrande, Anthony N Price, Joseph V Hajnal, and Daniel\n30914\n\n=== Page 10 ===\nRueckert. Cina: Conditional implicit neural atlas for spatio-\ntemporal representation of fetal brains.\nIn International\nConference on Medical Image Computing and Computer-\nAssisted Intervention, pages 181\u2013191. Springer, 2024. 2\n[21] Bob D De Vos, Floris F Berendsen, Max A Viergever, Mar-\nius Staring, and Ivana I\u02c7sgum. End-to-end unsupervised de-\nformable image registration with a convolutional neural net-\nwork. In Deep Learning in Medical Image Analysis and Mul-\ntimodal Learning for Clinical Decision Support: Third In-\nternational Workshop, DLMIA 2017, and 7th International\nWorkshop, ML-CDS 2017, Held in Conjunction with MIC-\nCAI 2017, Qu\u00b4ebec City, QC, Canada, September 14, Pro-\nceedings 3, pages 204\u2013212. Springer, 2017. 2\n[22] Neel Dey, Mengwei Ren, Adrian V Dalca, and Guido Gerig.\nGenerative adversarial registration for improved conditional\ndeformable templates.\nIn Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 3929\u2013\n3941, 2021. 1, 2, 3, 4\n[23] Neel Dey, Benjamin Billot, Hallee E Wong, Clinton J Wang,\nMengwei Ren, P Ellen Grant, Adrian V Dalca, and Polina\nGolland. Learning general-purpose biomedical volume rep-\nresentations using randomized synthesis.\narXiv preprint\narXiv:2411.02372, 2024. 2, 8\n[24] Zhipeng Ding and Marc Niethammer. Aladdin: Joint atlas\nbuilding and diffeomorphic registration learning with pair-\nwise alignment. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 20784\u2013\n20793, 2022. 2, 3, 5, 6, 7\n[25] Pedro F Felzenszwalb. Representation and detection of de-\nformable shapes. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 27(2):208\u2013220, 2005. 1\n[26] Bruce Fischl, David H Salat, Evelina Busa, Marilyn Al-\nbert, Megan Dieterich, Christian Haselgrove, Andre Van\nDer Kouwe, Ron Killiany, David Kennedy, Shuna Klaveness,\net al. Whole brain segmentation: automated labeling of neu-\nroanatomical structures in the human brain. Neuron, 33(3):\n341\u2013355, 2002. 1, 2\n[27] Vladimir S Fonov, Alan C Evans, Robert C McKinstry,\nC Robert Almli, and DL Collins. Unbiased nonlinear aver-\nage age-appropriate brain templates from birth to adulthood.\nNeuroImage, 47:S102, 2009. 2\n[28] Karthik Gopinath, Andrew Hoopes, Daniel C Alexander,\nSteven E Arnold, Yael Balbastre, Adri`a Casamitjana, You\nCheng, Russ Yue Zhi Chua, Brian L Edlow, Bruce Fischl,\net al. Synthetic data in generalizable, learning-based neu-\nroimaging. Imaging Neuroscience, 2024. 1, 2, 4\n[29] Karthik Gopinath, Xiaoling Hu, Malte Hoffmann, Oula\nPuonti, and Juan Eugenio Iglesias. Registration by regres-\nsion (rbr): a framework for interpretable and \ufb02exible atlas\nregistration. arXiv preprint arXiv:2404.16781, 2024. 2\n[30] Vicente Grau, AUJ Mewes, M Alcaniz, Ron Kikinis, and Si-\nmon K War\ufb01eld. Improved watershed transform for medical\nimage segmentation using prior information. IEEE transac-\ntions on medical imaging, 23(4):447\u2013458, 2004. 1, 2\n[31] Christoph Gro\u00dfbr\u00a8ohmer, Ziad Al-Haj Hemidi, Fenja Falta,\nand Mattias P Heinrich. SINA: Sharp implicit neural atlases\nby joint optimisation of representation and deformation. In\nInternational Workshop on Biomedical Image Registration,\npages 165\u2013180. Springer, 2024. 2\n[32] Daniel Grzech, Mohammad Farid Azampour, Ben Glocker,\nJulia Schnabel, Nassir Navab, Bernhard Kainz, and Lo\u00a8\u0131c\nLe Folgoc.\nA variational Bayesian method for similarity\nlearning in non-rigid image registration. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 119\u2013128, 2022. 2\n[33] Ziyi He and Albert C. S. Chung. Instantgroup: Instant tem-\nplate generation for scalable group of brain mri registration,\n2024. 2\n[34] Ziyi He, Tony C. W. Mok, and Albert C. S. Chung. Group-\nwise image registration with atlas of multiple resolutions re-\n\ufb01ned at test phase. In MICCAI 2023 Workshops, 2023. 2\n[35] Timothy J Hendrickson, Paul Reiners, Lucille A Moore, An-\nders J Perrone, Dimitrios Alexopoulos, Erik G Lee, Mar-\ntin Styner, Omid Kardan, Taylor A Chamberlain, Anurima\nMummaneni, et al. Bibsnet: A deep learning baby image\nbrain segmentation network for mri scans. bioRxiv, 2023. 2\n[36] Malte Hoffmann, David Salat, Martin Reuter, and Bruce Fis-\nchl. Longitudinal FreeSurfer with non-linear subject-speci\ufb01c\ntemplate improves sensitivity to cortical thinning. In Pro-\nceedings of the International Society for Magnetic Reso-\nnance in Medicine, page 1050. ISMRM, 2020. 1, 2\n[37] Malte Hoffmann, Benjamin Billot, Douglas N Greve,\nJuan Eugenio Iglesias, Bruce Fischl, and Adrian V Dalca.\nSynthMorph: learning contrast-invariant registration without\nacquired images. IEEE transactions on medical imaging, 41\n(3):543\u2013558, 2021. 2, 5, 8\n[38] Malte Hoffmann, Benjamin Billot, Juan E Iglesias, Bruce\nFischl, and Adrian V Dalca. Learning MRI contrast-agnostic\nregistration. In 2021 IEEE 18th International Symposium on\nBiomedical Imaging (ISBI), pages 899\u2013903. IEEE, 2021. 2\n[39] Malte Hoffmann, Andrew Hoopes, Bruce Fischl, and\nAdrian V Dalca.\nAnatomy-speci\ufb01c acquisition-agnostic\naf\ufb01ne registration learned from \ufb01ctitious images. In Medi-\ncal Imaging 2023: Image Processing, page 1246402. SPIE,\n2023. 2, 5\n[40] Malte Hoffmann, Andrew Hoopes, Douglas N Greve,\nBruce Fischl, and Adrian V Dalca.\nAnatomy-aware and\nacquisition-agnostic joint registration with SynthMorph.\nImaging Neuroscience, 2:1\u201333, 2024. 2\n[41] Andrew Hoopes, Jocelyn S Mora, Adrian V Dalca, Bruce\nFischl, and Malte Hoffmann. SynthStrip: skull-stripping for\nany brain image. NeuroImage, 260:119474, 2022. 2, 5\n[42] Shih-Chia Huang, Fan-Chieh Cheng, and Yi-Sheng Chiu.\nEf\ufb01cient contrast enhancement using adaptive gamma cor-\nrection with weighting distribution. IEEE transactions on\nimage processing, 22(3):1032\u20131041, 2012. 2\n[43] Juan Eugenio Iglesias. A ready-to-use machine learning tool\nfor symmetric multi-modality registration of brain mri. Sci-\nenti\ufb01c Reports, 13(1):6657, 2023. 2\n[44] Juan Eugenio Iglesias, Marco Lorenzi, Sebastiano Ferraris,\nLo\u00a8\u0131c Peter, Marc Modat, Allison Stevens, Bruce Fischl,\nand Tom Vercauteren. Model-based re\ufb01nement of nonlin-\near registrations in 3d histology reconstruction.\nIn Medi-\ncal Image Computing and Computer Assisted Intervention\u2013\nMICCAI 2018: 21st International Conference, Granada,\n30915\n\n=== Page 11 ===\nSpain, September 16-20, 2018, Proceedings, Part II 11,\npages 147\u2013155. Springer, 2018. 2\n[45] IXI Consortium.\nIXI dataset.\nhttps://brain-\ndevelopment.org/ixi-dataset/. 5\n[46] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial transformer networks. Advances in neural informa-\ntion processing systems, 28, 2015. 4\n[47] Rohit Jena, Pratik Chaudhari, and James C Gee. Fireants:\nAdaptive riemannian optimization for multi-scale diffeomor-\nphic matching. arXiv preprint arXiv:2404.01249, 2024. 2\n[48] Sarang Joshi, Brad Davis, Matthieu Jomier, and Guido\nGerig. Unbiased diffeomorphic atlas construction for com-\nputational anatomy. NeuroImage, 23:S151\u2013S160, 2004. 1,\n2\n[49] William Kelley, Nathan Ngo, Adrian V Dalca, Bruce Fischl,\nLilla Z\u00a8ollei, and Malte Hoffmann. Boosting skull-stripping\nperformance for pediatric brain images. ArXiv, 2024. 2\n[50] Matthew J Kempton, Tracy SA Underwood, Simon Brunton,\nFloris Stylios, Anne Schmechtig, Ulrich Ettinger, Marcus S\nSmith, Simon Lovestone, William R Crum, Sophia Frangou,\net al. A comprehensive testing protocol for mri neuroanatom-\nical segmentation techniques: evaluation of a novel lateral\nventricle segmentation method.\nNeuroimage, 58(4):1051\u2013\n1059, 2011. 8\n[51] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 5\n[52] Iasonas Kokkinos, Michael M Bronstein, Roee Litman, and\nAlex M Bronstein. Intrinsic shape context descriptors for\ndeformable shapes. In 2012 IEEE Conference on Computer\nVision and Pattern Recognition, pages 159\u2013166. IEEE, 2012.\n1\n[53] Jannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez,\nThomas Rainforth, and Yarin Gal. Self-attention between\ndatapoints: Going beyond individual input-output pairs in\ndeep learning. Advances in Neural Information Processing\nSystems, 34:28742\u201328756, 2021. 3\n[54] Julian Krebs, Herv\u00b4e Delingette, Boris Mailh\u00b4e, Nicholas Ay-\nache, and Tommaso Mansi. Learning a probabilistic model\nfor diffeomorphic registration. IEEE transactions on medical\nimaging, 38(9):2165\u20132176, 2019. 2\n[55] Pamela J LaMontagne, Tammie LS Benzinger, John C Mor-\nris, Sarah Keefe, Russ Hornbeck, Chengjie Xiong, Eliza-\nbeth Grant, Jason Hassenstab, Krista Moulder, Andrei G\nVlassenko, et al. Oasis-3: longitudinal neuroimaging, clin-\nical, and cognitive dataset for normal aging and alzheimer\ndisease. MedRxiv, pages 2019\u201312, 2019. 5\n[56] Jack L Lancaster, Diana Tordesillas-Guti\u00b4errez, Michael Mar-\ntinez, Felipe Salinas, Alan Evans, Karl Zilles, John C Mazz-\niotta, and Peter T Fox. Bias between mni and talairach coor-\ndinates analyzed using the icbm-152 brain template. Human\nbrain mapping, 28(11):1194\u20131205, 2007. 1\n[57] Jun Ma, Michael I Miller, Alain Trouv\u00b4e, and Laurent Younes.\nBayesian template estimation in computational anatomy.\nNeuroImage, 42(1):252\u2013261, 2008. 2\n[58] Daniel S Marcus, Tracy H Wang, Jamie Parker, John G Cser-\nnansky, John C Morris, and Randy L Buckner. Open access\nseries of imaging studies (oasis): cross-sectional mri data\nin young, middle aged, nondemented, and demented older\nadults. Journal of cognitive neuroscience, 19(9):1498\u20131507,\n2007. 5, 8\n[59] Mingyuan Meng, Lei Bi, Michael Fulham, Dagan Feng, and\nJinman Kim. Non-iterative coarse-to-\ufb01ne transformer net-\nworks for joint af\ufb01ne and deformable image registration. In\nInternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, pages 750\u2013760. Springer,\n2023. 2\n[60] Marc Modat, Gerard R Ridgway, Zeike A Taylor, Manja\nLehmann, Josephine Barnes, David J Hawkes, Nick C Fox,\nand S\u00b4ebastien Ourselin. Fast free-form deformation using\ngraphics processing units. Computer methods and programs\nin biomedicine, 98(3):278\u2013284, 2010. 2\n[61] Tony CW Mok, Zi Li, Yingda Xia, Jiawen Yao, Ling Zhang,\nJingren Zhou, and Le Lu. Deformable medical image reg-\nistration under distribution shifts with neural instance opti-\nmization. In International Workshop on Machine Learning\nin Medical Imaging, pages 126\u2013136. Springer, 2023. 2\n[62] Federico\nMonti,\nDavide\nBoscaini,\nJonathan\nMasci,\nEmanuele Rodola, Jan Svoboda, and Michael M Bronstein.\nGeometric deep learning on graphs and manifolds using\nmixture model cnns.\nIn Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, pages\n5115\u20135124, 2017. 1\n[63] Sean M Nestor, Raul Rupsingh, Michael Borrie, Matthew\nSmith, Vittorio Accomazzi, Jennie L Wells, Jennifer Fog-\narty, Robert Bartha, and Alzheimer\u2019s Disease Neuroimag-\ning Initiative. Ventricular enlargement as a possible mea-\nsure of alzheimer\u2019s disease progression validated using the\nalzheimer\u2019s disease neuroimaging initiative database. Brain,\n131(9):2443\u20132454, 2008. 8\n[64] Wolfgang M Pauli, Amanda N Nili, and J Michael Tyszka. A\nhigh-resolution probabilistic in vivo atlas of human subcor-\ntical brain nuclei. Scienti\ufb01c data, 5(1):1\u201313, 2018. 2\n[65] Wei Qiu, Lianjin Xiong, Ning Li, Zhangrong Luo, Yaobin\nWang, and Yangsong Zhang. AEAU-Net: an unsupervised\nend-to-end registration network by combining af\ufb01ne trans-\nformation and deformable medical image registration. Med-\nical & Biological Engineering & Computing, 61(11):2859\u2013\n2873, 2023. 2\n[66] Martin Reuter and Bruce Fischl.\nAvoiding asymmetry-\ninduced bias in longitudinal image processing. Neuroimage,\n57(1):19\u201321, 2011. 1, 2\n[67] Martin Reuter, Nicholas J Schmansky, H Diana Rosas, and\nBruce Fischl. Within-subject template estimation for unbi-\nased longitudinal image analysis. Neuroimage, 61(4):1402\u2013\n1418, 2012. 1, 2\n[68] Marc-Michel Roh\u00b4e, Manasi Datar, Tobias Heimann, Maxime\nSermesant, and Xavier Pennec. Svf-net: learning deformable\nimage registration using shape matching.\nIn Medical Im-\nage Computing and Computer Assisted Intervention- MIC-\nCAI 2017: 20th International Conference, Quebec City, QC,\nCanada, September 11-13, 2017, Proceedings, Part I 20,\npages 266\u2013274. Springer, 2017. 2\n[69] Karl Rohr, H Siegfried Stiehl, Rainer Sprengel, Thorsten M\nBuzug, J\u00a8urgen Weese, and MH Kuhn. Landmark-based elas-\n30916\n\n=== Page 12 ===\ntic registration using approximating thin-plate splines. IEEE\nTransactions on medical imaging, 20(6):526\u2013534, 2001. 2\n[70] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical image computing and computer-assisted\nintervention\u2013MICCAI 2015: 18th international conference,\nMunich, Germany, October 5-9, 2015, proceedings, part III\n18, pages 234\u2013241. Springer, 2015. 3\n[71] Daniel Rueckert, Luke I Sonoda, Carmel Hayes, Derek LG\nHill, Martin O Leach, and David J Hawkes. Nonrigid reg-\nistration using free-form deformations: application to breast\nmr images. IEEE transactions on medical imaging, 18(8):\n712\u2013721, 1999. 2\n[72] Stephen J Sawiak, Nigel I Wood, Guy B Williams, A Jen-\nnifer Morton, and T Adrian Carpenter.\nVoxel-based mor-\nphometry with templates and validation in a mouse model of\nhuntington\u2019s disease. Magnetic resonance imaging, 31(9):\n1522\u20131531, 2013. 2\n[73] Dieter Seghers, Emiliano D\u2019Agostino, Frederik Maes, Dirk\nVandermeulen, and Paul Suetens. Construction of a brain\ntemplate from mr images using state-of-the-art registration\nand segmentation techniques. In Medical Image Computing\nand Computer-Assisted Intervention\u2013MICCAI 2004: 7th In-\nternational Conference, Saint-Malo, France, September 26-\n29, 2004. Proceedings, Part I 7, pages 696\u2013703. Springer,\n2004. 2\n[74] Ziyao Shang, Md Asadullah Turja, Eric Feczko, Audrey\nHoughton, Amanda Rueter, Lucille A Moore, Kathy Snider,\nTimothy Hendrickson, Paul Reiners, Sally Stoyell, et al.\nLearning strategies for contrast-agnostic segmentation via\nsynthseg for infant mri data.\nIn International Conference\non Medical Imaging with Deep Learning, pages 1075\u20131084.\nPMLR, 2022. 2\n[75] Hanna Siebert, Christoph Gro\u00dfbr\u00a8ohmer, Lasse Hansen, and\nMattias P Heinrich.\nConvexadam: Self-con\ufb01guring dual-\noptimisation-based 3d multitask medical image registration.\nIEEE Transactions on Medical Imaging, 2024. 2\n[76] Matthew Sinclair, Andreas Schuh, Karl Hahn, Kersten Pe-\ntersen, Ying Bai, James Batten, Michiel Schaap, and Ben\nGlocker. Atlas-istn: joint segmentation, registration and atlas\nconstruction with image-and-spatial transformer networks.\nMedical Image Analysis, 78:102383, 2022. 2, 3\n[77] John G Sled, Alex P Zijdenbos, and Alan C Evans.\nA\nnonparametric method for automatic correction of intensity\nnonuniformity in mri data. IEEE transactions on medical\nimaging, 17(1):87\u201397, 1998. 2\n[78] Hessam\nSokooti,\nBob\nDe\nVos,\nFloris\nBerendsen,\nBoudewijn\nPF\nLelieveldt,\nIvana\nI\u02c7sgum,\nand\nMarius\nStaring.\nNonrigid image registration using multi-scale\n3d convolutional neural networks.\nIn Medical Image\nComputing and Computer Assisted Intervention- MICCAI\n2017: 20th International Conference, Quebec City, QC,\nCanada, September 11-13, 2017, Proceedings, Part I 20,\npages 232\u2013239. Springer, 2017. 2\n[79] Sophie Starck, Vasiliki Sideri-Lampretsa, Bernhard Kainz,\nMartin Menten, Tamara Mueller, and Daniel Rueckert. Diff-\ndef: Diffusion-generated deformation \ufb01elds for conditional\natlases. arXiv preprint arXiv:2403.16776, 2024. 2\n[80] Haosheng Su and Xuan Yang. Nonuniformly Spaced Control\nPoints Based on Variational Cardiac Image Registration. In\nInternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, pages 634\u2013644. Springer,\n2023. 2\n[81] Nicholas J Tustison, Philip A Cook, Andrew J Holbrook,\nHans J Johnson, John Muschelli, Gabriel A Devenyi, Jef-\nfrey T Duda, Sandhitsu R Das, Nicholas C Cullen, Daniel L\nGillen, et al. The antsx ecosystem for quantitative biological\nand medical imaging. Scienti\ufb01c reports, 11(1):9068, 2021. 5\n[82] Koen Van Leemput, Frederik Maes, Dirk Vandermeulen, and\nPaul Suetens. Automated model-based tissue classi\ufb01cation\nof mr images of the brain. IEEE transactions on medical\nimaging, 18(10):897\u2013908, 1999. 1, 2\n[83] Koen Van Leemput, Frederik Maes, Dirk Vandermeulen, and\nPaul Suetens. A unifying framework for partial volume seg-\nmentation of brain mr images. IEEE transactions on medical\nimaging, 22(1):105\u2013119, 2003. 2\n[84] Alan Q Wang, M Yu Evan, Adrian V Dalca, and Mert R\nSabuncu. A robust and interpretable deep learning frame-\nwork for multi-modal registration via keypoints.\nMedical\nImage Analysis, 90:102962, 2023. 2\n[85] Junshen Xu, Daniel Moyer, P Ellen Grant, Polina Golland,\nJuan Eugenio Iglesias, and Elfar Adalsteinsson. Svort: it-\nerative transformer for slice-to-volume registration in fetal\nbrain mri. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention, pages 3\u2013\n13. Springer, 2022. 3\n[86] Xiao Yang, Roland Kwitt, Martin Styner, and Marc Nietham-\nmer. Quicksilver: Fast predictive image registration\u2013a deep\nlearning approach. NeuroImage, 158:378\u2013396, 2017. 2\n[87] Sean I Young, Ya\u00a8el Balbastre, Adrian V Dalca, William M\nWells, Juan Eugenio Iglesias, and Bruce Fischl.\nSuper-\nwarp: Supervised learning and warping on u-net for invari-\nant subvoxel-precise registration. In International Workshop\non Biomedical Image Registration, pages 103\u2013115. Springer,\n2022. 2\n[88] Yu Zhang, Norbert Schuff, An-Tao Du, Howard J Rosen,\nJoel H Kramer, Maria Luisa Gorno-Tempini, Bruce L Miller,\nand Michael W Weiner. White matter damage in frontotem-\nporal dementia and alzheimer\u2019s disease measured by diffu-\nsion mri. Brain, 132(9):2579\u20132592, 2009. 8\n[89] Lei Zhao, Shumao Pang, Yangfan Chen, Xiongfeng Zhu,\nZiyue Jiang, Zhihai Su, Hai Lu, Yujia Zhou, and Qianjin\nFeng. SpineRegNet: Spine Registration Network for vol-\numetric MR and CT image by the joint estimation of an\naf\ufb01ne-elastic deformation \ufb01eld. Medical Image Analysis, 86:\n102786, 2023. 2\n30917\n",
      "searchable_content": "page 1 multimorph on-demand atlas construction s. mazdak abulnaga1 2 andrew hoopes1 2 neel dey1 malte hoffmann2 bruce fischl2 john guttag1 adrian dalca1 2 1mit computer science and arti\ufb01cial intelligence laboratory 2massachusetts general hospital harvard medical school abulnaga csail.mit.edu abstract we present multimorph a fast and ef\ufb01cient method for con- structing anatomical atlases on the \ufb02y. atlases capture the canonical structure of a collection of images and are es- sential for quantifying anatomical variability across popu- lations. however current atlas construction methods often require days to weeks of computation thereby discouraging rapid experimentation. as a result many scienti\ufb01c studies rely on suboptimal precomputed atlases from mismatched populations negatively impacting downstream analyses. multimorph addresses these challenges with a feedfor- ward model that rapidly produces high-quality population- speci\ufb01c atlases in a single forward pass for any 3d brain dataset without any \ufb01ne-tuning or optimization. multi- morph is based on a linear group-interaction layer that ag- gregates and shares features within the group of input im- ages. further by leveraging auxiliary synthetic data mul- timorph generalizes to new imaging modalities and popu- lation groups at test-time. experimentally multimorph out- performs state-of-the-art optimization-based and learning- based atlas construction methods in both small and large population settings with a 100-fold reduction in time. this makes multimorph an accessible framework for biomedical researchers without machine learning expertise enabling rapid high-quality atlas generation for diverse studies. 1. introduction we present multimorph a rapid and \ufb02exible method for constructing anatomical atlases. an atlas or deformable template is a reference image that represents the typical structure within a collection of related images. in biomed- ical imaging studies atlases facilitate studying anatomical variability within and across population groups by serving as as a common coordinate system for key image analy- sis tasks such as segmentation 5 26 30 82 shape analy- sis 1 25 52 62 and longitudinal modeling 36 66 67 . traditional unbiased atlas construction for a population involves solving a computationally intensive iterative opti- mization problem that often requires several days or weeks of computation. the optimization alternates between align- ing registering all images to the estimated atlas and updat- ing the atlas in both shape and appearance by averaging the images mapped to the intermediate atlas space 7 48 . re- cent learning-based methods employ a target dataset to ex- plicitly learn an atlas jointly with a registration model 18 22 yet still require days of training. this necessitates computational infrastructure and machine learning exper- tise that is unavailable to many biomedical researchers. regardless of strategy an atlas produced from one popu- lation of images may not be appropriate for populations that differ from the group used to build the atlas. re-estimating the atlas is often required for each new experiment. these computational challenges are further compounded by the need to construct atlases for speci\ufb01c image types as many biomedical studies acquire several imaging modalities to highlight different biomedical properties of interest. the repeated prohibitive computational cost of producing a new atlas leads most scientists to use existing atlases that might not be appropriate for their population group or modality thereby negatively impacting the analyses in these studies 56 . to meet these challenges we introduce multimorph a machine learning model that constructs atlases in a sin- gle forward pass requiring only seconds to minutes of computation on a cpu and no machine learning exper- tise to use. multimorph ef\ufb01ciently generates population- and subgroup-speci\ufb01c atlases enabling accurate and \ufb01ne- grained anatomical analyses. we employ a convolutional architecture that processes an arbitrary number of images and computes a set of regularized deformation \ufb01elds that align the group of images to an atlas space central to that group. the proposed method uses a nonparametric convo- lutional operation that interacts the intermediate represen- tations of the input images with each other summarizing and aggregating shared features. further by training on diverse imaging modalities alongside supplementary syn- thetic neuroimaging volumes 28 multimorph generalizes to arbitrary imaging modalities at test time. we also intro- this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 30906 page 2 duce a centrality layer that ensures that the estimated atlases are unbiased 48 . as a result multimorph rapidly pro- duces high quality atlases for new populations and imaging modalities unseen during training. it further yields more ac- curate segmentation transfer across population groups than both the most widely used optimization-based approach 7 and recent machine learning approaches 18 24 . to sum- marize our contributions we frame atlas construction as a learning-based group registration problem to a central space. we present a novel neural network architecture that en- ables communication between the intermediate represen- tations of a group of images and show how this can be used to construct accurate group-speci\ufb01c atlases. we develop a centrality layer that encourages predicted deformations and atlases to be central and unbiased. experimentally multimorph produces atlases that are as good and often better than those produced by other methods and it does it up to 100 times faster. we demonstrate the generalizability of the proposed method by constructing atlases for unseen imaging modalities and population groups. these atlases condi- tioned on age and disease state capture population trends within the data enabling cross-group analyses. our model weights and code are available at https github.com mabulnaga multimorph. 2. related work deformable registration. deformable registration esti- mates a dense spatial mapping between image pairs. tradi- tional methods 4 6 47 60 69 71 75 solve an optimiza- tion problem balancing image-similarity and regularization terms to ensure smooth invertible deformations. learning-based methods improve test-time ef\ufb01ciency by training models to directly predict transformations between image pairs generally enabling faster predictions on new image pairs as compared to traditional methods. supervised approaches 68 78 86 87 are trained to regress simulated deformations or the outputs of registration solvers whereas unsupervised methods 9 15 19 21 29 32 37 38 40 54 59 61 65 80 84 89 optimize an unsupervised image-similarity loss and a regularization term in training. synthetic data in neuroimage analysis. recent machine learning-based neuroimage analysis methods have bene\ufb01ted from synthetic training data that extend far beyond real- world variations 10 12 28 35 37 38 40 41 43 49 74 . this domain-randomization strategy trains neural networks on simulated intensity images synthesized on the \ufb02y from a training set of anatomical segmentation maps. as part of the generative model the images undergo corruption steps simulating common acquisition-related artifacts like distor- tion 2 low-frequency intensity modulation 77 global intensity exponentiation 42 resolution reduction partial voluming 83 among many others. the large variety of data yields shape-biased networks agnostic to the imaging modality. as a result these models generalize to arbitrary medical images that have the same anatomy as the synthetic training data largely eliminating the need for retraining to maintain peak performance 23 39 . atlas construction. deformable atlas construction seeks to \ufb01nd an image that optimally represents a given popula- tion for example to facilitate atlas-based brain segmenta- tion 5 26 30 82 or to initialize longitudinal morphomet- ric analyses in an unbiased fashion 36 66 67 . iterative atlas construction alternates between registering each image of the population to a current estimate of the at- las and updating the atlas with the average of the moved images until convergence 7 48 57 64 72 . another ap- proach computes a spanning tree of pair-wise transforms between subjects to estimate an atlas 44 73 . iterative methods on 3d data incur prohibitively long runtimes due to the cost of optimization. therefore many studies have used publicly available atlases 27 although these are often not representative of the population being studied. recent learning-based atlas construction techniques jointly learn an atlas and a registration network that maps images from the training population to the atlas 16 18 20 22 24 31 76 79 . these approaches naturally extend to constructing conditional atlases for example condition- ing on age 18 22 79 or incorporating tasks like segmen- tation 76 . however obtaining an atlas for a new popula- tion requires machine learning expertise and computational resources for re-training from scratch or \ufb01ne-tuning a net- work. test-time adaptation for groupwise registration tag 33 34 maps a group of images to a latent space using a vae computes an average of latent vectors then decodes to estimate an atlas. while this rapidly produces atlases at inference linearly averaging vec- tors in a vae latent space most often does not yield a representation that can be decoded into an unbiased deformable atlas. further this model must still be retrained for new imaging modalities or populations. in contrast multimorph directly constructs group-speci\ufb01c atlases from warped images ensuring \ufb01delity to the data without distortions introduced by latent space aggregation. a single multimorph model can generate atlases for a wide variety of imaging modalities and population groups. flexible-size inputs. recent methods have employed a va- riety of mechanisms that are \ufb02exible to input size in other applications. for example in-context learning methods use a \ufb02exible-sized input set of input-output example pairs to guide a new image-processing task at inference 14 17 . other methods use attention mechanisms across different 30907 page 3 inputs to aggregate information among volume slices 85 or tabular data 53 . while cross-attention and variants have been effective for many tasks in vision they have quadratic memory complexity. at each iteration our model requires a large set of 3d volumes. using cross-attention would lead to infeasible memory requirements. in contrast we propose a \ufb02exible feature sharing mechanism with linear complexity to produce central atlases for large groups of 3d images. 3. methods 3.1. background given two images x1 x2 rd deformable registration seeks a nonlinear mapping \u03c9 x1 x2 that warps one image into the space of the other. the deformation \u03c9 at- tempts to align the underlying anatomy captured by the im- ages while maintaining a well-behaved map and it is tradi- tionally computed by optimization arg min \u03c9 lsim x2 x1 \u03c9 \u03b5lreg \u03c9 1 where lsim x2 x1 \u03c9 measures similarity between im- age x2 and the warped image x1 \u03c9 lreg regularizes the map \u03c9 and \u03b5 is a hyperparameter that balances the two. many population-based studies involve groupwise anal- yses. group registration aligns a collection of m images xm xi m i 1 to an explicit image template t arg min \u03c91 ... \u03c9m m i 1 lsim t xi \u03c9i \u03b5lreg \u03c9i . 2 in many scenarios an explicit template is not available. one can be constructed by iterating a template estimation step t 1 m xi \u03c9i and the groupwise optimization 2 until convergence. however this is computationally expensive and does not scale well to large populations. machine learning approaches for pairwise registration use a neural network to predict \u03c9 as a function of the in- put images f\u03b5 x1 x2 \u03c9 where f is a neural network parameterized by \u03d1. pairwise registration is rapidly com- puted by a single forward pass of a trained network. recent methods 18 22 24 76 also estimate a common template t together with parameters \u03d1 in a network f\u03b5 t x \u03c9. in this work we develop a model to directly predict a group-speci\ufb01c set of deformation \ufb01elds to a central template space. we formulate template construction as a group reg- istration problem given a variable number of inputs. 3.2. flexible group registration given a set xm of m images from a dataset x xi n i 1 we seek to map the images to a space central to xm. the set xm could be an entire population or a subgroup of patients representing an underlying demographic or condition. let \u03c9i rd rd represent the map from xi to a central latent image space. we model a function f\u03b5 x1 . . . xm \u03c91 . . . \u03c9m using a convolutional neural network with learnable parameters \u03d1. the number of parameters of \u03d1 is independent of the group size m. to achieve desirable group registration we construct f to satisfy the following desiderata flexible to group size f takes as input a variable num- ber of m images and computes m maps \u03c9 to a group- speci\ufb01c central space. fast computation of \u03c9i m i 1 can be done in a single forward pass of an ef\ufb01cient network. generalizable generalize to unseen datasets ym. unbiased the images in xm map to a space central to that set mean \u03c9i m i 1 0. aligned images xi \u03c9i m i 1 mapped to the template space are anatomically aligned. satisfying the desiderata leads to a model that can produce \ufb02exible templates for user-de\ufb01ned groups on demand. we introduce new methods to achieve these properties below. 3.3. model figure 1 gives an overview of the network architecture of multimorph. the network takes a group of a variable num- ber of images and predicts diffeomorphic transformations to a central template space speci\ufb01c to the group. at each network layer we share features across the inputs using the proposed groupblock layer. the network outputs sta- tionary velocity \ufb01elds which are then adjusted by a central- ity layer to produce an unbiased atlas. convolution layer for variable group size. we propose groupblock a convolutional layer that combines image features across a group. as group registration seeks to align images to a central space feature communication is helpful to produce a subgroup alignment. we use a summary statistic to aggregate group features and communicate the statistic back to each individual group element. let c l i represent the feature map for input image i at network layer l. the groupblock layer aggregates information as follows c l s c l 1 . . . c l m c l 1 i conv c l i c l \u03d1 l where s is the summary statistic across the group dimen- sion is the concatenation operation along the channel dimension and conv is a convolutional layer with parame- ters \u03d1 l . we use the mean as our summary statistic. network. we modify the popular unet architecture 70 employing a multi-scale structure with residual connec- tions. we replace the standard conv layers with the pro- posed groupblock feature sharing layer 3.3 . the net- 30908 page 4 m input images multiscale groupblocks m warp m warped images output atlas inputs outputs groupblock centrality layer svf integration conv layer conv layer conv layer channel-wise concatenation lsim lsim lsim lreg mean mean figure 1. multimorph architecture diagram. the model takes in a variable group of m images and constructs an atlas speci\ufb01c to that group. at each layer of the unet the proposed groupblock mechanism replaces standard convolution kernels. speci\ufb01cally it computes the elementwise mean of the intermediate features across the group and concatenates the resulting features with the individual features. the mechanism enables group interaction by sharing summarized input features across the group. the network outputs m velocity \ufb01elds mapping images to the group-speci\ufb01c template space. a centrality layer removes any global bias in the average velocity \ufb01eld before integration and warping the images. the output is a central template representing the shared anatomy of the input group. work takes as input a group of m images and outputs m d-dimensional stationary velocity \ufb01elds svf . we use the standard svf representation of a diffeomor- phism 4 18 . the deformation \ufb01eld is de\ufb01ned through the ordinary differential equation \u03d1\u03c9 t v \u03d1t v \u03c9 t v where v is the velocity \ufb01eld. the \ufb01eld is de\ufb01ned for t 0 1 with \u03c9 0 v id the identity map. the deformation \ufb01eld is obtained by integrating v using the scaling and squaring method 4 19 . centrality layer. constructing an atlas central to the pop- ulation group is key to performing unbiased downstream analyses such as in quantifying anatomical shape differ- ences without bias to any particular structure or subject. a central atlas is one that is close to the target population. many learning approaches use a regularization term to min- imize the mean displacement \ufb01eld 18 22 . we construct a layer that produces a group-speci\ufb01c cen- tral template by construction. we subtract the groupwise mean from the output velocity \ufb01elds vi v l i v l where v l i is the \ufb01nal output velocity \ufb01eld for image i and v l is the group mean. this centers the velocity \ufb01elds in the zero-mean lie subspace. template construction. given a trained network f\u03b5 we can construct a template t by aggregating the warped im- ages of the group xm t g x1 \u03c91 . . . xm \u03c9m . we use the mean operation for g. to apply the map to the group of images we integrate the svf to obtain a diffeomorphic displacement \ufb01eld 4 19 . we then use a spatial transformation function 46 to warp the images to the central space. the spatial transformer per- forms a pullback operation with linear interpolation. 3.4. auxiliary structural information the use of anatomical labelmaps during training of learning-based registration often improves substructure alignment 9 . when segmentation maps are available for some of the images in a set we use this information to form an atlas segmentation map. let seg xi indicate the proba- bilistic segmentation labelmap of the k structures for image xi. we construct the labelmap of the template seg t by taking the set-wise average of the warped probability maps seg t meanm seg x1 \u03c91 . . . seg xm \u03c9m . 3.5. synthetic training to aid generalization to unseen modalities we also train on images synthesized from brain tissue segmentations. for each synthetic training group we sample k random val- ues uniformly corresponding to k structures. we then use a domain randomization procedure 28 to randomly sam- ple intensity values for each structure along with a variety of noise patterns and artifacts. this yields groups of syn- thetic images where each group exhibits random intensity distributions and tissue contrasts. supplementary fig. 11 presents a representative set of example synthetic images. 3.6. loss we maximize alignment between the images and anatom- ical structures of the group and the constructed template while maintaining a smooth map. for a single image the 30909 page 5 loss is computed as l \u03c9i lsim t xi \u03c9i \u03b5lreg \u03c9i \u03d6lstruc seg t seg xi \u03c9i . 3 the \ufb01rst term lsim measures pairwise similarity between image xi and the template t. we use the normalized cross- correlation objective. the second term regularizes the de- formation \ufb01eld to be smooth lreg \u03c9i \u03c9i 2. when label maps are available during training we use the third auxiliary loss term to align the structures of the training set with the constructed template using soft-dice. our complete group loss is l \u03c91 . . . \u03c9m 1 m i xi xm l \u03c9i . since the template t is constructed by averaging warped images of the group the loss is depen- dent on all images of the group. 4. experiments we evaluate multimorph using 3d brain mri brain scans a common setting for atlas construction. we compare multi- morph against iterative and learning-based approaches in terms of speed centrality and accuracy. we also test whether multimorph generalizes to new datasets imaging modalities and populations that are unseen during training. 4.1. experimental setup data. we use four public 3d brain mri datasets. three datasets oasis-1 oasis-3 and mbb are used for training validation and testing while ixi serves as an un- seen test set. oasis-1 58 includes t1-weighted t1-w scans of 416 subjects aged 18-96. a hundred oasis-1 sub- jects of ages 60 years and older were diagnosed with mild to moderate alzheimer s disease ad which is correlated with brain atrophy. oasis-3 55 contains t1-w and t2-w mri scans of subjects aged 42-95 years old. we use a sub- set of 1043 subjects with 210 diagnosed with mild to severe cognitive dementia. the mind brain body dataset 8 in- cludes t2-w and t2-flair scans of 226 healthy subjects. for each training dataset we randomly hold out 20 of the subjects for testing and split the rest into 85 for training and 15 for validation. each split includes an equal mix of healthy and abnormal subjects of all age ranges. we use the same model for all experiments. lastly to evaluate generalization we hold out the ixi dataset 45 . we arbitrarily select the guys hospital site within ixi and retrieve t1-w t2-w and pd-w mri scans of 319 adult subjects. importantly the pd-w mri modality is not included in any of the training datasets used by our model. these datasets span a large age range and include a mix of disease states and imaging modalities simulating real-world population studies. implementation details. during training all images within a sampled group have the same acquisition modality. we apply augmentations including random exponential scal- ing intensity inhomogeneities and per-voxel gaussian noise. additionally 50 of the sampled training groups contain synthetic images instead of real acquisitions. for preprocessing using ants 81 we af\ufb01nely align each 3d scan to a common 1-mm isotropic af\ufb01ne reference used in 37 39 . we extract brain tissue signal using synth- strip 41 and generate segmentation maps of 30 unique anatomical brain structures using synthseg 11 . we train using the adam optimizer 51 with a learning rate of 10 4. the \ufb01eld regularization hyperparameter is set to \u03b5 1.0 and the segmentation-loss weight is \u03d6 0.5 both chosen via grid search see supplemental sec. 7 . at each training iteration we randomly sample m 2 12 images to form a group and train for 80 000 iterations using the \ufb01nal saved model. all models are trained on a single rtx8000 gpu. all runtime evaluation results were done on an intel r xeon r gold 5218 cpu. baselines. we evaluate sygn 7 a widely-used iterative atlas construction method from the ants library 81 . additionally we compare against atlasmorph a learning- based atlas constructor 18 that explicitly learns an atlas to best \ufb01t the training data. for atlasmorph we set the deformation \ufb01eld regularization hyperparmeter to \u03b5 0.1 as determined via cross-validation. both multimorph and atlasmorph use the same core registration network. we also evaluate aladdin 24 a learning-based method that constructs an average reference atlas during training by learning pairwise registrations. at test time this atlas serves as the registration target enabling the generation of new at- lases for different population groups. since aladdin con- structs modality-speci\ufb01c atlases we train a separate model with the same capacity as our network for each modality in our dataset using an optimal regularization loss weight of 10 000 a similarity loss weight of 10 and an image pair loss weight of 0.2 all determined using a grid search. both atlasmorph and aladdin models are trained for 50 000 it- erations followed by 1 500 \ufb01netuning iterations per popu- lation subgroup to estimate a group atlas at test-time. evaluation. we assess the effectiveness of atlas construc- tion techniques in rapidly generating central atlases for new populations. to evaluate registration quality we compute the dice score to assess how well the atlas aligns with warped subject scans. we assess \ufb01eld regularity and topol- ogy by computing the determinant of the jacobian of the map det j\u03c9 p det \u03c9 p at each voxel p. locations where det j\u03c9 p 0 represent folded regions breaking lo- cal injectivity. additionally we measure atlas centrality by reporting the mean displacement \ufb01eld u 2. statistical sig- ni\ufb01cance is determined using a paired t-test with p 0.01. segmentation transfer. as atlases are commonly used for segmentation by warping atlas labels to new target 30910 page 6 table 1. atlas construction evaluation on 319 brain volumes from ixi. while all baselines were trained or optimized on the full dataset multimorph was not demonstrating its ability to generalize to entirely new datasets. multimorph also generalizes to the pd-w modality not seen during training demonstrating its capabilities on unseen imaging modalities. indicates statistical signi\ufb01cance p 0.01 . modality method construction time min. dice folds centrality 10 2 t1-w ants 7 4345.20 0.863 0.075 524.2 580.04 10.4 30.67 atlasmorph 18 1141.50 0.894 0.015 47.9 29.22 7.8 19.09 aladdin 24 325.20 0.885 0.01 0.0 0.0 106.8 97.6 ours 10.50 0.913 0.006 1.1 1.55 1.4 4.32 t2-w ants 7 4380.60 0.862 0.071 522.6 476.86 18.6 44.286 atlasmorph 18 831.60 0.882 0.018 57.5 31.935 7.8 19.34 aladdin 24 261.00 0.875 0.012 0.0 0.125 771.5 744.309 ours 10.40 0.906 0.007 2.0 2.49 1.5 4.683 pd-w ants 7 4320.20 0.856 0.069 313.1 359.9 12.4 32.805 atlasmorph 18 959.00 0.884 0.018 40.5 26.10 7.4 19.483 aladdin 24 163.80 0.849 0.029 0.0 0.0 1175.7 1731.773 ours 7.80 0.900 0.009 1.601 0.205 0.9 3.02 images we evaluate each method s segmentation perfor- mance. each atlas is estimated using half the subgroup m 2 images . we randomly sample m 2 segmentation label maps to generate the atlas segmentation mask which is then transferred to the remaining m 2 images. segmentation qual- ity is assessed using the dice score. 4.2. results 4.2.1 generalizing to unseen datasets and modalities table 1 presents results for all methods on the ixi dataset which was entirely held-out for multimorph s training and validation. multimorph produces atlases over a 100 faster than ants and atlasmorph and 30 faster than aladdin. it consistently achieves the highest dice score indicating better anatomical alignment even when constructed on un- seen data at test time in a single forward pass. additionally multimorph yields regular deformation \ufb01elds with negligi- ble folding and signi\ufb01cantly lower bias in the displacement \ufb01elds indicating that the constructed atlases are central. fig. 2 visualizes sample registration predictions for each modality in ixi and fig. 3 illustrates example atlases for ixi t1-w and pd-w. despite never having been trained on this dataset nor having seen the pd-w imaging modality during training multimorph estimates atlases that yield high group alignment in only minutes demonstrating its potential for table 2. sub-group atlas construction results. reported scores are averaged across atlases constructed using subgroups of 5 10 20 . . . 60 . indicates statistical signi\ufb01cance p 0.01 . method run time min. dice transfer folds centrality 10 2 ants 7 436 0.4 0.875 0.009 447 110 8.7 0.1 atlasmorph 18 17 1.4 0.893 0.005 50.0 8.7 9.7 0.1 aladdin 24 12 0.1 0.877 0.004 0.0 0.0 173 3.7 ours 1.5 0.0 0.904 0.002 1.3 0.4 1.4 0.04 scienti\ufb01c studies requiring speci\ufb01c atlases. we provide ad- ditional examples in supplemental fig. 10. 4.2.2 standard atlas construction we now evaluate the ability of multimorph to construct population atlases across different age groups and disease states. speci\ufb01cally we construct an atlas on the oasis- 3 t1-w test dataset. all baseline models were trained and validated on the test set. table 3 shows that multimorph achieves the highest dice score while producing atlases 30 400 times faster than the baseline methods. 4.2.3 subgroup atlas construction multimorph enables the rapid construction of subgroup at- lases for granular population analyses. we evaluate atlases atlas moving image warp moved image t1-w t2-w pd-w figure 2. example images and warps to the constructed atlas on the ixi dataset. three subjects and three modalities are shown. 30911 page 7 ants aladdin atlasmorph ours ants aladdin atlasmorph ours figure 3. atlases constructed on the ixi t1-w left and ixi pd-w right image modality. all baseline methods used the dataset for training or optimization while our method was never trained on the ixi data. further our method was never trained on pd-w images yet generalizes to this modality. conditioned on age age and disease state as well as random subgroupings of the population. random subgroup analysis. we quantify the effect of subgroup size on atlas quality using the held-out ixi t1-w dataset. subgroups of 5 10 20 . . . 60 images are ran- domly sampled with half used to construct the atlas seg- mentation and the other half used for evaluation. as in sec- tion 4.2.1 the baselines were trained or optimized on this dataset whereas multimorph was not exposed to any ixi t1-w data during training or validation. fig. 4 shows that multimorph consistently outperforms baselines with performance improving as the subgroup figure 4. segmentation transfer performance when varying the number of images used to construct an atlas. data is taken from the ixi t1-w dataset which our model did not have access to during training. our method consistently outperforms the baselines. size increases. table 2 reports mean performance across subgroups with multimorph showing better segmentation transfer while maintaining well-behaved deformation \ufb01elds. importantly multimorph only requires 1.5 minutes of infer- ence time on a cpu whereas baselines require \ufb01ne-tuning or re-optimization which is both time consuming and re- quires tens or hundreds of minutes. age. we \ufb01rst demonstrate multimorph s ability to create appropriate atlases for user-de\ufb01ned subgroups by grouping healthy oasis-1 subjects into age bins. we take normal subjects in the validation and test set and bin them into age ranges 0 19 20 29 . . . 80 89 . fig. 5 presents qual- itative results showing anatomical changes consistent with normal aging such as ventricular enlargement due to brain atrophy 3 . all atlases were generated in under a minute without any \ufb01ne-tuning. diagnosis. lastly we examine the effect of dementia on brain aging in the oasis-3 t1-w dataset. we con- table 3. atlas estimation results on 212 subjects from the oasis- 3 t1-w test set. indicates statistical signi\ufb01cance p 0.01 . method run time min. dice transfer folds centrality 10 2 ants 7 2858 0.886 0.017 765 877 9.5 25.6 atlasmorph 18 688 0.881 0.024 50.2 31.9 8.0 0.2 aladdin 24 277 0.878 0.016 0.0 0.07 175.9 1.8 ours 5.9 0.910 0.014 1.2 2.3 1.5 0.05 30912 page 8 0-19 20-29 30-39 40-49 50-59 60-69 70-79 80-89 figure 5. atlases conditioned on age for healthy subjects in oasis-1. ventricle enlargement red boxes is observed across time consistent with neurodegeneration with aging. struct age-conditioned atlases separately for normal and dementia-diagnosed subjects. fig. 6 compares brain atro- phy across matched age groups. we observe substantial enlargement of the ventricles outlined in red boxes and deterioration of the white matter in the dementia group as compared to the controls consistent with the litera- ture 3 50 63 88 . 4.3. ablation studies we quantify the effect of several key model components in- cluding the centrality layer cl the group block gb mechanism with varying summary statistics mean vari- ance max and training without the dice loss. using the oasis-1 dataset 58 we train our model for 50 000 itera- tions and assess performance on the test set. table 4 summarizes the results. the cl signi\ufb01cantly reduced the centrality measure by 1000 ensuring un- biased atlas construction though it came along with a 1 point decrease in dice. the gb mechanism improved dice by 1.4 points with negligible degradation of \ufb01eld regular- ity. we observe no signi\ufb01cant performance variation across the various summary statistics tested. finally the dice loss improved performance by over 2 dice points. taken holistically each component strongly contributed to multi- morph s performance. we further quantify the impact on subgroup atlas construction in supplemental section 6.2 and observe similar trends. additionally we assess the im- pact of training with synthetic data in supplemental sec- tion 6.1 which improved ixi dataset performance by up to normal dementia 60-69 80-89 75-79 70-74 figure 6. atlases conditioned on age for normal subjects top and subjects with dementia bottom from oasis-3. visual differ- ences indicate considerable enlargement of ventricles red boxes and atrophy of white matter when compared to normal subjects. 1.8 dice points while maintaining \ufb01eld regularity demon- strating better generalization. 5. discussion limitations and future work. multimorph has several avenues for extensions. for example as it assumes dif- feomorphic transformations multimorph cannot accurately construct atlases for neuroimages with topology-changing pathologies. however this can be addressed by using pathology masks when calculating losses in training 13 . additionally multimorph is currently only trained for neu- roimages but can be trained on anatomy-agnostic synthetic data 23 37 to estimate atlases for arbitrary applications. lastly our implementation stores all activations in mem- ory at inference potentially limiting higher group sizes with large 3d volumes in memory-constrained settings. conclusion. we presented multimorph a test-time atlas construction framework that works with unseen imaging modalities and any number of input images without retrain- ing. at its core multimorph leverages a novel convolu- tional layer for groups of images independent of the num- ber of input samples enabling ef\ufb01cient and scalable atlas generation. multimorph produces unbiased atlases for ar- bitrary inputs with comparable and often better perfor- mance while also being over 100 times faster than previ- ous approaches that require either solving an optimization problem or retraining a model. by making high-quality at- las construction fast accessible and adaptable multimorph potentially unlocks new avenues for biomedical research enabling computational anatomy studies that were previ- ously impractical due to computational constraints. table 4. model ablations on the centrality layer group block mechanism and dice loss on the oasis-1 test set. all proposed components improved atlas construction performance. ablation dice transfer folds centrality 10 3 no cl gb mean 0.892 0.018 0.0 0.0 16125 11494 cl no gb 0.870 0.021 0.1 0.3 9.9 27.4 cl gb var 0.883 0.020 1.5 2.8 12.8 59.27 cl gb max 0.880 0.019 1.5 2.7 12.6 46.69 cl gb mean 0.884 0.020 1.1 1.9 12.0 39.48 cl gb mean dice 0.919 0.011 5.4 7.5 18.6 61.31 30913 page 9 acknowledgements we thank marianne rakic for her help in the atlasmorph experiments. we thank zack berger for help in proof- reading. support for this research was provided in part by quanta computer inc. project air the nih biccn grants u01 mh117023 and um1 mh130981 nih brain con- nects u01 ns132181 um1 ns132358 nih nibib r01 eb023281 r21 eb018907 r01 eb019956 p41 eb030006 nih nia r21 ag082082 r01 ag064027 r01 ag016495 r01 ag070988 the nih nimh um1 mh130981 r01 mh123195 r01 mh121885 rf1 mh123195 nih ninds u24 ns135561 r01 ns070963 r01 ns083534 r01 ns105820 r25 ns125599 nih nichd r00 hd101553 nih r01 eb033773 and was made possible by the resources provided by nih shared in- strumentation grants s10 rr023401 s10 rr019307 and s10 rr023043. additional support was provided by the nih blueprint for neuroscience research u01 mh093765 part of the multi-institutional human connectome project. much of the computation resources was performed on hard- ware provided by the massachusetts life sciences center. references 1 riza alp guler george trigeorgis epameinondas anton- akos patrick snape stefanos zafeiriou and iasonas kokki- nos. densereg fully convolutional dense shape regres- sion in-the-wild. in proceedings of the ieee conference on computer vision and pattern recognition pages 6799 6808 2017. 1 2 jesper lr andersson stefan skare and john ashburner. how to correct susceptibility distortions in spin-echo echo- planar images application to diffusion tensor imaging. neu- roimage 20 2 870 888 2003. 2 3 liana g apostolova amity e green sona babakchanian kristy s hwang yi-yu chou arthur w toga and paul m thompson. hippocampal atrophy and ventricular enlarge- ment in normal aging mild cognitive impairment mci and alzheimer disease. alzheimer disease associated disor- ders 26 1 17 27 2012. 7 8 4 john ashburner. a fast diffeomorphic image registration al- gorithm. neuroimage 38 1 95 113 2007. 2 4 5 john ashburner and karl j friston. uni\ufb01ed segmentation. neuroimage 26 3 839 851 2005. 1 2 6 brian b avants charles l epstein murray grossman and james c gee. symmetric diffeomorphic image registration with cross-correlation evaluating automated labeling of el- derly and neurodegenerative brain. medical image analysis 12 1 26 41 2008. 2 7 brian b avants paul yushkevich john pluta david minkoff marc korczykowski john detre and james c gee. the optimal template effect in hippocampus studies of dis- eased populations. neuroimage 49 3 2457 2466 2010. 1 2 5 6 7 8 a babayan m erbey d kumral jd reinelt amf reiter j r obbig hl schaare m uhlig a anwander pl bazin et al. a mind-brain-body dataset of mri eeg cognition emotion and peripheral physiology in young and old adults. sci. data 6 180308 2018. 5 9 guha balakrishnan amy zhao mert r sabuncu john gut- tag and adrian v dalca. voxelmorph a learning framework for deformable medical image registration. ieee transac- tions on medical imaging 38 8 1788 1800 2019. 2 4 10 benjamin billot eleanor robinson adrian v dalca and juan eugenio iglesias. partial volume segmentation of brain mri scans of any resolution and contrast. in medical image computing and computer assisted intervention miccai 2020 23rd international conference lima peru octo- ber 4 8 2020 proceedings part vii 23 pages 177 187. springer 2020. 2 11 benjamin billot douglas n greve oula puonti axel thielscher koen van leemput bruce fischl adrian v dalca juan eugenio iglesias et al. synthseg segmenta- tion of brain mri scans of any contrast and resolution without retraining. medical image analysis 86 102789 2023. 5 12 benjamin billot colin magdamo you cheng steven e arnold sudeshna das and juan eugenio iglesias. robust machine learning segmentation for large-scale analysis of heterogeneous clinical brain mri datasets. proceedings of the national academy of sciences 120 9 e2216399120 2023. 2 13 matthew brett alexander p leff chris rorden and john ashburner. spatial normalization of brain images with fo- cal lesions using cost function masking. neuroimage 14 2 486 500 2001. 8 14 victor ion butoi jose javier gonzalez ortiz tianyu ma mert r sabuncu john guttag and adrian v dalca. uni- verseg universal medical image segmentation. in proceed- ings of the ieee cvf international conference on com- puter vision pages 21438 21451 2023. 2 15 qing chang chenhao lu and mengke li. cascading af\ufb01ne and b-spline registration method for large deformation reg- istration of lung x-rays. journal of digital imaging 36 3 1262 1278 2023. 2 16 zeen chi zhongxiao cong clinton j wang yingcheng liu esra abaci turk p ellen grant s mazdak abulnaga polina golland and neel dey. dynamic neural \ufb01elds for learning atlases of 4d fetal mri time-series. arxiv preprint arxiv 2311.02874 2023. 2 17 steffen czolbe and adrian v dalca. neuralizer general neuroimage analysis without re-training. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 6217 6230 2023. 2 18 adrian dalca marianne rakic john guttag and mert sabuncu. learning conditional deformable templates with convolutional networks. advances in neural information processing systems 32 2019. 1 2 3 4 5 6 7 19 adrian v dalca guha balakrishnan john guttag and mert r sabuncu. unsupervised learning of probabilistic dif- feomorphic registration for images and surfaces. medical image analysis 57 226 236 2019. 2 4 20 maik dannecker vanessa kyriakopoulou lucilio cordero- grande anthony n price joseph v hajnal and daniel 30914 page 10 rueckert. cina conditional implicit neural atlas for spatio- temporal representation of fetal brains. in international conference on medical image computing and computer- assisted intervention pages 181 191. springer 2024. 2 21 bob d de vos floris f berendsen max a viergever mar- ius staring and ivana i\u02c7sgum. end-to-end unsupervised de- formable image registration with a convolutional neural net- work. in deep learning in medical image analysis and mul- timodal learning for clinical decision support third in- ternational workshop dlmia 2017 and 7th international workshop ml-cds 2017 held in conjunction with mic- cai 2017 qu ebec city qc canada september 14 pro- ceedings 3 pages 204 212. springer 2017. 2 22 neel dey mengwei ren adrian v dalca and guido gerig. generative adversarial registration for improved conditional deformable templates. in proceedings of the ieee cvf international conference on computer vision pages 3929 3941 2021. 1 2 3 4 23 neel dey benjamin billot hallee e wong clinton j wang mengwei ren p ellen grant adrian v dalca and polina golland. learning general-purpose biomedical volume rep- resentations using randomized synthesis. arxiv preprint arxiv 2411.02372 2024. 2 8 24 zhipeng ding and marc niethammer. aladdin joint atlas building and diffeomorphic registration learning with pair- wise alignment. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 20784 20793 2022. 2 3 5 6 7 25 pedro f felzenszwalb. representation and detection of de- formable shapes. ieee transactions on pattern analysis and machine intelligence 27 2 208 220 2005. 1 26 bruce fischl david h salat evelina busa marilyn al- bert megan dieterich christian haselgrove andre van der kouwe ron killiany david kennedy shuna klaveness et al. whole brain segmentation automated labeling of neu- roanatomical structures in the human brain. neuron 33 3 341 355 2002. 1 2 27 vladimir s fonov alan c evans robert c mckinstry c robert almli and dl collins. unbiased nonlinear aver- age age-appropriate brain templates from birth to adulthood. neuroimage 47 s102 2009. 2 28 karthik gopinath andrew hoopes daniel c alexander steven e arnold yael balbastre adri a casamitjana you cheng russ yue zhi chua brian l edlow bruce fischl et al. synthetic data in generalizable learning-based neu- roimaging. imaging neuroscience 2024. 1 2 4 29 karthik gopinath xiaoling hu malte hoffmann oula puonti and juan eugenio iglesias. registration by regres- sion rbr a framework for interpretable and \ufb02exible atlas registration. arxiv preprint arxiv 2404.16781 2024. 2 30 vicente grau auj mewes m alcaniz ron kikinis and si- mon k war\ufb01eld. improved watershed transform for medical image segmentation using prior information. ieee transac- tions on medical imaging 23 4 447 458 2004. 1 2 31 christoph gro\u00dfbr ohmer ziad al-haj hemidi fenja falta and mattias p heinrich. sina sharp implicit neural atlases by joint optimisation of representation and deformation. in international workshop on biomedical image registration pages 165 180. springer 2024. 2 32 daniel grzech mohammad farid azampour ben glocker julia schnabel nassir navab bernhard kainz and lo \u0131c le folgoc. a variational bayesian method for similarity learning in non-rigid image registration. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 119 128 2022. 2 33 ziyi he and albert c. s. chung. instantgroup instant tem- plate generation for scalable group of brain mri registration 2024. 2 34 ziyi he tony c. w. mok and albert c. s. chung. group- wise image registration with atlas of multiple resolutions re- \ufb01ned at test phase. in miccai 2023 workshops 2023. 2 35 timothy j hendrickson paul reiners lucille a moore an- ders j perrone dimitrios alexopoulos erik g lee mar- tin styner omid kardan taylor a chamberlain anurima mummaneni et al. bibsnet a deep learning baby image brain segmentation network for mri scans. biorxiv 2023. 2 36 malte hoffmann david salat martin reuter and bruce fis- chl. longitudinal freesurfer with non-linear subject-speci\ufb01c template improves sensitivity to cortical thinning. in pro- ceedings of the international society for magnetic reso- nance in medicine page 1050. ismrm 2020. 1 2 37 malte hoffmann benjamin billot douglas n greve juan eugenio iglesias bruce fischl and adrian v dalca. synthmorph learning contrast-invariant registration without acquired images. ieee transactions on medical imaging 41 3 543 558 2021. 2 5 8 38 malte hoffmann benjamin billot juan e iglesias bruce fischl and adrian v dalca. learning mri contrast-agnostic registration. in 2021 ieee 18th international symposium on biomedical imaging isbi pages 899 903. ieee 2021. 2 39 malte hoffmann andrew hoopes bruce fischl and adrian v dalca. anatomy-speci\ufb01c acquisition-agnostic af\ufb01ne registration learned from \ufb01ctitious images. in medi- cal imaging 2023 image processing page 1246402. spie 2023. 2 5 40 malte hoffmann andrew hoopes douglas n greve bruce fischl and adrian v dalca. anatomy-aware and acquisition-agnostic joint registration with synthmorph. imaging neuroscience 2 1 33 2024. 2 41 andrew hoopes jocelyn s mora adrian v dalca bruce fischl and malte hoffmann. synthstrip skull-stripping for any brain image. neuroimage 260 119474 2022. 2 5 42 shih-chia huang fan-chieh cheng and yi-sheng chiu. ef\ufb01cient contrast enhancement using adaptive gamma cor- rection with weighting distribution. ieee transactions on image processing 22 3 1032 1041 2012. 2 43 juan eugenio iglesias. a ready-to-use machine learning tool for symmetric multi-modality registration of brain mri. sci- enti\ufb01c reports 13 1 6657 2023. 2 44 juan eugenio iglesias marco lorenzi sebastiano ferraris lo \u0131c peter marc modat allison stevens bruce fischl and tom vercauteren. model-based re\ufb01nement of nonlin- ear registrations in 3d histology reconstruction. in medi- cal image computing and computer assisted intervention miccai 2018 21st international conference granada 30915 page 11 spain september 16-20 2018 proceedings part ii 11 pages 147 155. springer 2018. 2 45 ixi consortium. ixi dataset. https brain- development.org ixi-dataset . 5 46 max jaderberg karen simonyan andrew zisserman et al. spatial transformer networks. advances in neural informa- tion processing systems 28 2015. 4 47 rohit jena pratik chaudhari and james c gee. fireants adaptive riemannian optimization for multi-scale diffeomor- phic matching. arxiv preprint arxiv 2404.01249 2024. 2 48 sarang joshi brad davis matthieu jomier and guido gerig. unbiased diffeomorphic atlas construction for com- putational anatomy. neuroimage 23 s151 s160 2004. 1 2 49 william kelley nathan ngo adrian v dalca bruce fischl lilla z ollei and malte hoffmann. boosting skull-stripping performance for pediatric brain images. arxiv 2024. 2 50 matthew j kempton tracy sa underwood simon brunton floris stylios anne schmechtig ulrich ettinger marcus s smith simon lovestone william r crum sophia frangou et al. a comprehensive testing protocol for mri neuroanatom- ical segmentation techniques evaluation of a novel lateral ventricle segmentation method. neuroimage 58 4 1051 1059 2011. 8 51 diederik p kingma and jimmy ba. adam a method for stochastic optimization. arxiv preprint arxiv 1412.6980 2014. 5 52 iasonas kokkinos michael m bronstein roee litman and alex m bronstein. intrinsic shape context descriptors for deformable shapes. in 2012 ieee conference on computer vision and pattern recognition pages 159 166. ieee 2012. 1 53 jannik kossen neil band clare lyle aidan n gomez thomas rainforth and yarin gal. self-attention between datapoints going beyond individual input-output pairs in deep learning. advances in neural information processing systems 34 28742 28756 2021. 3 54 julian krebs herv e delingette boris mailh e nicholas ay- ache and tommaso mansi. learning a probabilistic model for diffeomorphic registration. ieee transactions on medical imaging 38 9 2165 2176 2019. 2 55 pamela j lamontagne tammie ls benzinger john c mor- ris sarah keefe russ hornbeck chengjie xiong eliza- beth grant jason hassenstab krista moulder andrei g vlassenko et al. oasis-3 longitudinal neuroimaging clin- ical and cognitive dataset for normal aging and alzheimer disease. medrxiv pages 2019 12 2019. 5 56 jack l lancaster diana tordesillas-guti errez michael mar- tinez felipe salinas alan evans karl zilles john c mazz- iotta and peter t fox. bias between mni and talairach coor- dinates analyzed using the icbm-152 brain template. human brain mapping 28 11 1194 1205 2007. 1 57 jun ma michael i miller alain trouv e and laurent younes. bayesian template estimation in computational anatomy. neuroimage 42 1 252 261 2008. 2 58 daniel s marcus tracy h wang jamie parker john g cser- nansky john c morris and randy l buckner. open access series of imaging studies oasis cross-sectional mri data in young middle aged nondemented and demented older adults. journal of cognitive neuroscience 19 9 1498 1507 2007. 5 8 59 mingyuan meng lei bi michael fulham dagan feng and jinman kim. non-iterative coarse-to-\ufb01ne transformer net- works for joint af\ufb01ne and deformable image registration. in international conference on medical image computing and computer-assisted intervention pages 750 760. springer 2023. 2 60 marc modat gerard r ridgway zeike a taylor manja lehmann josephine barnes david j hawkes nick c fox and s ebastien ourselin. fast free-form deformation using graphics processing units. computer methods and programs in biomedicine 98 3 278 284 2010. 2 61 tony cw mok zi li yingda xia jiawen yao ling zhang jingren zhou and le lu. deformable medical image reg- istration under distribution shifts with neural instance opti- mization. in international workshop on machine learning in medical imaging pages 126 136. springer 2023. 2 62 federico monti davide boscaini jonathan masci emanuele rodola jan svoboda and michael m bronstein. geometric deep learning on graphs and manifolds using mixture model cnns. in proceedings of the ieee confer- ence on computer vision and pattern recognition pages 5115 5124 2017. 1 63 sean m nestor raul rupsingh michael borrie matthew smith vittorio accomazzi jennie l wells jennifer fog- arty robert bartha and alzheimer s disease neuroimag- ing initiative. ventricular enlargement as a possible mea- sure of alzheimer s disease progression validated using the alzheimer s disease neuroimaging initiative database. brain 131 9 2443 2454 2008. 8 64 wolfgang m pauli amanda n nili and j michael tyszka. a high-resolution probabilistic in vivo atlas of human subcor- tical brain nuclei. scienti\ufb01c data 5 1 1 13 2018. 2 65 wei qiu lianjin xiong ning li zhangrong luo yaobin wang and yangsong zhang. aeau-net an unsupervised end-to-end registration network by combining af\ufb01ne trans- formation and deformable medical image registration. med- ical biological engineering computing 61 11 2859 2873 2023. 2 66 martin reuter and bruce fischl. avoiding asymmetry- induced bias in longitudinal image processing. neuroimage 57 1 19 21 2011. 1 2 67 martin reuter nicholas j schmansky h diana rosas and bruce fischl. within-subject template estimation for unbi- ased longitudinal image analysis. neuroimage 61 4 1402 1418 2012. 1 2 68 marc-michel roh e manasi datar tobias heimann maxime sermesant and xavier pennec. svf-net learning deformable image registration using shape matching. in medical im- age computing and computer assisted intervention- mic- cai 2017 20th international conference quebec city qc canada september 11-13 2017 proceedings part i 20 pages 266 274. springer 2017. 2 69 karl rohr h siegfried stiehl rainer sprengel thorsten m buzug j urgen weese and mh kuhn. landmark-based elas- 30916 page 12 tic registration using approximating thin-plate splines. ieee transactions on medical imaging 20 6 526 534 2001. 2 70 olaf ronneberger philipp fischer and thomas brox. u- net convolutional networks for biomedical image segmen- tation. in medical image computing and computer-assisted intervention miccai 2015 18th international conference munich germany october 5-9 2015 proceedings part iii 18 pages 234 241. springer 2015. 3 71 daniel rueckert luke i sonoda carmel hayes derek lg hill martin o leach and david j hawkes. nonrigid reg- istration using free-form deformations application to breast mr images. ieee transactions on medical imaging 18 8 712 721 1999. 2 72 stephen j sawiak nigel i wood guy b williams a jen- nifer morton and t adrian carpenter. voxel-based mor- phometry with templates and validation in a mouse model of huntington s disease. magnetic resonance imaging 31 9 1522 1531 2013. 2 73 dieter seghers emiliano d agostino frederik maes dirk vandermeulen and paul suetens. construction of a brain template from mr images using state-of-the-art registration and segmentation techniques. in medical image computing and computer-assisted intervention miccai 2004 7th in- ternational conference saint-malo france september 26- 29 2004. proceedings part i 7 pages 696 703. springer 2004. 2 74 ziyao shang md asadullah turja eric feczko audrey houghton amanda rueter lucille a moore kathy snider timothy hendrickson paul reiners sally stoyell et al. learning strategies for contrast-agnostic segmentation via synthseg for infant mri data. in international conference on medical imaging with deep learning pages 1075 1084. pmlr 2022. 2 75 hanna siebert christoph gro\u00dfbr ohmer lasse hansen and mattias p heinrich. convexadam self-con\ufb01guring dual- optimisation-based 3d multitask medical image registration. ieee transactions on medical imaging 2024. 2 76 matthew sinclair andreas schuh karl hahn kersten pe- tersen ying bai james batten michiel schaap and ben glocker. atlas-istn joint segmentation registration and atlas construction with image-and-spatial transformer networks. medical image analysis 78 102383 2022. 2 3 77 john g sled alex p zijdenbos and alan c evans. a nonparametric method for automatic correction of intensity nonuniformity in mri data. ieee transactions on medical imaging 17 1 87 97 1998. 2 78 hessam sokooti bob de vos floris berendsen boudewijn pf lelieveldt ivana i\u02c7sgum and marius staring. nonrigid image registration using multi-scale 3d convolutional neural networks. in medical image computing and computer assisted intervention- miccai 2017 20th international conference quebec city qc canada september 11-13 2017 proceedings part i 20 pages 232 239. springer 2017. 2 79 sophie starck vasiliki sideri-lampretsa bernhard kainz martin menten tamara mueller and daniel rueckert. diff- def diffusion-generated deformation \ufb01elds for conditional atlases. arxiv preprint arxiv 2403.16776 2024. 2 80 haosheng su and xuan yang. nonuniformly spaced control points based on variational cardiac image registration. in international conference on medical image computing and computer-assisted intervention pages 634 644. springer 2023. 2 81 nicholas j tustison philip a cook andrew j holbrook hans j johnson john muschelli gabriel a devenyi jef- frey t duda sandhitsu r das nicholas c cullen daniel l gillen et al. the antsx ecosystem for quantitative biological and medical imaging. scienti\ufb01c reports 11 1 9068 2021. 5 82 koen van leemput frederik maes dirk vandermeulen and paul suetens. automated model-based tissue classi\ufb01cation of mr images of the brain. ieee transactions on medical imaging 18 10 897 908 1999. 1 2 83 koen van leemput frederik maes dirk vandermeulen and paul suetens. a unifying framework for partial volume seg- mentation of brain mr images. ieee transactions on medical imaging 22 1 105 119 2003. 2 84 alan q wang m yu evan adrian v dalca and mert r sabuncu. a robust and interpretable deep learning frame- work for multi-modal registration via keypoints. medical image analysis 90 102962 2023. 2 85 junshen xu daniel moyer p ellen grant polina golland juan eugenio iglesias and elfar adalsteinsson. svort it- erative transformer for slice-to-volume registration in fetal brain mri. in international conference on medical image computing and computer-assisted intervention pages 3 13. springer 2022. 3 86 xiao yang roland kwitt martin styner and marc nietham- mer. quicksilver fast predictive image registration a deep learning approach. neuroimage 158 378 396 2017. 2 87 sean i young ya el balbastre adrian v dalca william m wells juan eugenio iglesias and bruce fischl. super- warp supervised learning and warping on u-net for invari- ant subvoxel-precise registration. in international workshop on biomedical image registration pages 103 115. springer 2022. 2 88 yu zhang norbert schuff an-tao du howard j rosen joel h kramer maria luisa gorno-tempini bruce l miller and michael w weiner. white matter damage in frontotem- poral dementia and alzheimer s disease measured by diffu- sion mri. brain 132 9 2579 2592 2009. 8 89 lei zhao shumao pang yangfan chen xiongfeng zhu ziyue jiang zhihai su hai lu yujia zhou and qianjin feng. spineregnet spine registration network for vol- umetric mr and ct image by the joint estimation of an af\ufb01ne-elastic deformation \ufb01eld. medical image analysis 86 102786 2023. 2 30917",
      "keywords": [
        "page",
        "multimorph",
        "on-demand",
        "atlas",
        "construction",
        "mazdak",
        "abulnaga1",
        "andrew",
        "hoopes1",
        "neel",
        "dey1",
        "malte",
        "hoffmann2",
        "bruce",
        "fischl2",
        "john",
        "guttag1",
        "adrian",
        "dalca1",
        "1mit",
        "computer",
        "science",
        "arti\ufb01cial",
        "intelligence",
        "laboratory",
        "2massachusetts",
        "general",
        "hospital",
        "harvard",
        "medical",
        "school",
        "abulnaga",
        "csail.mit.edu",
        "abstract",
        "present",
        "fast",
        "ef\ufb01cient",
        "method",
        "con-",
        "structing",
        "anatomical",
        "atlases",
        "\ufb02y.",
        "capture",
        "canonical",
        "structure",
        "collection",
        "images",
        "es-",
        "sential",
        "quantifying",
        "variability",
        "across",
        "popu-",
        "lations.",
        "however",
        "current",
        "methods",
        "often",
        "require",
        "days",
        "weeks",
        "computation",
        "thereby",
        "discouraging",
        "rapid",
        "experimentation.",
        "result",
        "many",
        "scienti\ufb01c",
        "studies",
        "rely",
        "suboptimal",
        "precomputed",
        "from",
        "mismatched",
        "populations",
        "negatively",
        "impacting",
        "downstream",
        "analyses.",
        "addresses",
        "challenges",
        "feedfor-",
        "ward",
        "model",
        "rapidly",
        "produces",
        "high-quality",
        "population-",
        "speci\ufb01c",
        "single",
        "forward",
        "pass",
        "any",
        "brain",
        "dataset",
        "without",
        "\ufb01ne-tuning",
        "optimization."
      ],
      "metadata": {
        "filename": "Abulnaga_MultiMorph_On-demand_Atlas_Construction_CVPR_2025_paper.pdf",
        "original_filename": "Abulnaga_MultiMorph_On-demand_Atlas_Construction_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 3287022,
        "upload_file_id": "upload_1752161978249_cerqkxr5e",
        "upload_timestamp": "2025-07-10T15:39:38.306023",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:39:38.752232+00:00",
      "content_length": 59513,
      "content_type": ".pdf",
      "filename": "Abulnaga_MultiMorph_On-demand_Atlas_Construction_CVPR_2025_paper.pdf"
    },
    "1": {
      "file_id": "1",
      "content": "\n=== Page 1 ===\nSEESAW: HIGH-THROUGHPUT LLM INFERENCE VIA MODEL RE-SHARDING\nQidong Su123 Wei Zhao34 Xin Li3 Muralidhar Andoorveedu3 Chenhao Jiang12 Zhanda Zhu123 Kevin Song12\nChristina Giannoula123 Gennady Pekhimenko123\nABSTRACT\nTo improve the efficiency of distributed large language model (LLM) inference, various parallelization strategies,\nsuch as tensor and pipeline parallelism, have been proposed. However, the distinct computational characteristics\ninherent in the two stages of LLM inference\u2014prefilling and decoding\u2014render a single static parallelization\nstrategy insufficient for the effective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind Seesaw is dynamic model re-\nsharding, a technique that facilitates the dynamic reconfiguration of parallelization strategies across stages, thereby\nmaximizing throughput at both phases. To mitigate re-sharding overhead and optimize computational efficiency,\nwe employ tiered KV cache buffering and transition-minimizing scheduling. These approaches work synergistically\nto reduce the overhead caused by frequent stage transitions while ensuring maximum batching efficiency. Our\nevaluation demonstrates that Seesaw achieves a throughput increase of up to 1.78\u00d7 (1.36\u00d7 on average) compared\nto vLLM, the most widely used state-of-the-art LLM inference engine.\n1\nINTRODUCTION\nLarge language models (LLMs), such as the LLaMA (Tou-\nvron et al., 2023a) and GPT (Achiam et al., 2023) families,\nhave demonstrated exceptional performance across a wide\nrange of tasks. Beyond their prevalent use in interactive\napplications like chatbots (OpenAI, 2024), LLMs are also\ngaining high interest in throughput-oriented offline inference\nworkloads such as information extraction (Narayan et al.,\n2022), database querying (Liu et al., 2024), and knowledge\ngraph processing (Edge et al., 2024). Unlike interactive\napplications where low latency is crucial, these offline in-\nference tasks prioritize high throughput over response time.\nThese offline inference workloads are widely adopted in in-\ndustry (Kamsetty et al., 2023; Yu et al., 2024; Dell Technolo-\ngies, 2024; Chan et al., 2024), leading MLPerf to develop\nbenchmarks specifically for them (MLCommons, 2024). In\nthis work, we focus on improving inference efficiency for\noffline, throughput-oriented LLM inference workloads.\nAs LLMs often exceed the memory capacity of individual\nGPUs, parallelization is essential for their deployment (Ben-\nNun & Hoefler, 2019; Shoeybi et al., 2019). Several paral-\nlelization strategies, including tensor parallelism (Shoeybi\net al., 2019) and pipeline parallelism (Narayanan et al., 2019;\nHuang et al., 2019), have been proposed, each presenting\ndistinct trade-offs in memory efficiency, inter-device com-\nmunication, and computational efficiency. Tensor paral-\nlelism distributes model weights across devices but suffers\n1 University of Toronto\n2 Vector Institute\n3 CentML\n4 Stan-\nford University\nTP1PP8\nTP2PP4\nTP4PP2\nTP8PP1\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized Time\ncommunication\ncompute\nweight transfer\n(a) Prefill\nTP1PP8\nTP2PP4\nTP4PP2\nTP8PP1\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized Time\ncommunication\ncompute\nweight transfer\n(b) Decode\nFigure 1. Breakdown of execution time for the prefill and decode\nstages for LLaMA2-13B inference on 8 L4 GPUs (The global\nbatch size is 16. Pipeline parallelism further divides the data into\nmicro-batches of size 16/PP to fully utilize pipelining).\nfrom high communication costs due to frequent all-reduce\noperations at each layer (Pope et al., 2023; Chang et al.,\n2024). The communication cost becomes particularly severe\nin systems connected via PCIe (Dell Technologies, 2023) or\nwith partial high-speed connections (NVIDIA Corporation,\n2020). In contrast, pipeline parallelism partitions the model\ninto sequential stages, reducing inter-device communica-\ntion by passing only activations between them. However,\nto enable pipelining, each data batch needs to be divided\ninto micro-batches, leading to extra execution overheads,\nsince every micro-batch repeatedly loads weights into the\ncompute units (see Section 3.1 for details).\nWhile numerous studies have proposed methods to optimize\nparallelization strategies for LLMs (Miao et al., 2023; Kwon\net al., 2023; Li et al., 2023; Pope et al., 2023), prior works\ntypically rely on a single, static configuration throughout\narXiv:2503.06433v1  [cs.DC]  9 Mar 2025\n\n=== Page 2 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nthe entire generation process. However, our findings indi-\ncate that this one-size-fits-all approach is often inefficient\nfor throughput-oriented LLM inference because it fails to\nleverage the distinct patterns between the two stages in LLM\ngeneration: the prefill stage, where the input sequence is pro-\ncessed at once to produce the initial token, and the decode\nstage, where subsequent tokens are generated sequentially\nbased on prior tokens. These two stages exhibit fundamen-\ntally different computational characteristics (Yuan et al.,\n2024). During the prefill stage, multiple tokens from the\ninput prompt are processed simultaneously, making com-\nputation and communication the dominant contributors to\nruntime. In contrast, the decode stage processes one token at\na time for each sequence, increasing the relative time spent\non weight transfer. This difference indicates that the optimal\nparallelization strategy for each stage may also vary.\nTo illustrate the performance limitations of applying a uni-\nform parallelization strategy for both prefill and decode,\nwe measure the execution time of each stage under various\ncombinations of tensor and pipeline parallelism, as shown\nin Figure 1. In the prefill stage, as the degree of tensor par-\nallelism increases, the communication overhead increases\nsignificantly due to additional GPUs participating in all-\nreduce operations. As a result, tensor parallelism performs\nsignificantly worse than pipeline parallelism. In contrast,\nduring the decode stage, pipeline parallelism is slower than\ntensor parallelism, largely due to increased weight trans-\nferring overhead caused by micro-batching required for\npipelining (see Section 3.1 for more details). Therefore,\nwe need stage-specific parallelization strategies to provide\nbetter LLM inference throughput.\nAn existing approach is disaggregated prefill-decode (Zhong\net al., 2024; Qin et al., 2024), which assigns prefill and de-\ncode computation to different GPU instances. The prefill\ninstances and decode instances form a two-stage pipeline\nto serve inference requests. Therefore, the overall through-\nput of disaggregated prefill-decode is constrained by the\nslower of the two stages, and balancing throughput between\nthese two stages is essential. The key drawback of disag-\ngregated prefill-decode is that it can cause large amounts of\npipeline bubbles under resource-constrained environments.\nFor example, when deploying a 70B model on 8\u00d740GB\nGPUs, even the most balanced configuration results in a\n6\u00d7 difference in throughput between the prefill and decode\nstages. In this setup, the decode stage operates at one-sixth\nthe throughput of the prefill stage, resulting in a significant\nbottleneck at the prefill stage that slows down the entire\nsystem (see Section 3.2 for details).\nTo address these challenges, we present Seesaw, a high-\nthroughput LLM inference engine that dynamically recon-\nfigures parallelization strategies between the prefill and de-\ncode stages. The key idea behind Seesaw is model re-\ntime\nGPU max #seqs\ntime\nGPU max #seqs\ntime\nCPU max #seqs\n#seqs\n#seqs\n#seqs\nunder-utilize\ntoo frequent transitions\n(a) Prefill-prioritizing\n(b) Decode-prioritizing\n(c) Tiered KV cache buffering\u00a0\n\u00a0\u00a0\u00a0 + transition-minimizing scheduling\np\nd\np\nd\np\nd\np\nd\np\nd\np\nd\np\nd\np\nd\nGPU max #seqs\np\nd\np\nd\np\nFigure 2. Different scheduling policies considering transition over-\nhead. Decoding throughput is positively correlated with the num-\nber of sequences in GPU memory (the maximal batch size), which\nis highlighted as light green area.\nsharding, a novel technique that dynamically re-partitions\nmodel weights and KV cache 1 between prefill and decode\nstages. By tailoring parallelization strategies to the dis-\ntinct computational demands of each stage, Seesaw reduces\ncommunication overhead during the prefill stage, while en-\nhancing memory efficiency in the decode stage, resulting in\na substantial increase in overall throughput.\nHowever, the overhead associated with model re-sharding\ncan be high due to frequent transitions between prefill and\ndecode. To maximize throughput, existing systems typi-\ncally adopt prefill-prioritized scheduling (Yu et al., 2022;\nKwon et al., 2023), which interleaves prefill and decode\nstages across batches to achieve continuous batching. Yet,\nas illustrated in Figure 2(a), integrating this approach with\nmodel re-sharding can result in significant overhead due\nto frequent transitions between prefill and decode. On the\nother hand, decode-prioritized scheduling (NVIDIA, 2024a)\ncompletes all decode steps for a batch before proceeding to\nthe next, resulting in lower re-sharding overhead. However,\nas depicted in Figure 2(b), this method suffers from low\nresource utilization due to smaller batch sizes.\nTo overcome this constraint and achieve both minimal\nre-sharding overhead and large batch size, we propose\ntwo synergetic techniques: tiered KV cache buffering\nand transition-minimizing scheduling. Tiered KV cache\nbuffering leverages CPU memory as auxiliary storage for\nthe KV cache, enabling Seesaw to store the KV cache for\na large number of prefill requests. Transition-minimizing\nscheduling reduces re-sharding overhead by minimizing the\nnumber of transitions to the decode stage. Seesaw transi-\ntions from prefill to decode only after the CPU KV cache is\nfull. During decoding, the large number of KV cache in the\nCPU buffer enables Seesaw to perform decode with large\nbatch sizes, and thus enabling high throughput. As depicted\n1 The tensors cached for each sequence\u2019s decoding steps.\n\n=== Page 3 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nin Figure 2(c), this approach maintains the maximal batch\nsize during the decode stage, while significantly reducing\nthe frequency of stage transitions, thereby minimizing re-\nsharding overhead. Additionally, to mitigate the overhead\nof KV cache transfers between CPU and GPU, Seesaw em-\nploys asynchronous pipelining to overlap data transfers with\ncomputation.\nIn summary, we make the following contributions.\n\u2022 We identify and quantitatively analyze the different pref-\nerences for parallelisms in the prefill and decode stages of\nthroughput-oriented LLM inference tasks. Our analysis\ncomprehensively accounts for data movement, computa-\ntion, and communication costs.\n\u2022 We propose dynamic model re-sharding, a novel technique\nthat dynamically reconfigures the parallelization strategies\nfor prefill and decode stages. We address the challenge of\ntransition overhead in model re-sharding with continuous\nbatching by introducing tiered KV cache buffering and\ntransition-minimizing scheduling. Based on these tech-\nniques, we implement Seesaw, a high-throughput offline\ninference system that optimizes parallelization strategies\nfor each LLM inference stage.\n\u2022 We conduct a comprehensive evaluation of Seesaw across\na variety of workloads and hardware configurations. Our\nresults show Seesaw achieves an average speedup of\n1.36\u00d7 and a throughput improvement of up to 1.78\u00d7\ncompared to the state-of-the-art LLM inference engines.\n2\nBACKGROUND\n2.1\nLLM Inference\nTransformer Architecture.\nModern large language mod-\nels are based on the transformer architecture (Vaswani et al.,\n2017), which typically consists of multiple identical decoder\nlayers (OpenAI, 2024). Each layer includes several linear\nlayers and an attention layer. The weights of the linear\nlayers account for the majority of the model\u2019s parameters.\nAuto-regressive Generation.\nLLM inference follows an\nauto-regressive paradigm (Bengio et al., 2000), which takes\nan input prompt and generates a sequence of output tokens.\nThis process is divided into two stages: prefilling, which\nprocesses the input tokens, and decoding, which generates a\ntoken per step. These stages exhibit distinct computational\nproperties (Zhong et al., 2024; Yuan et al., 2024). Prefilling\nprocesses the prompt that are typically hundreds to thou-\nsands of tokens long. The computation and communication\ncosts, both of which scale with the number of tokens, domi-\nnate the runtime during this stage. Since the cost of loading\nweights is amortized over a larger set of tokens, the overall\nperformance is primarily bound by compute and/or commu-\nnication. In contrast, Decoding processes only the newly\ngenerated tokens in each auto-regressive step and has com-\nparatively smaller computation in each step. Therefore the\ncost for loading the weight data from off-chip memory to\ncomputation units has a relatively higher percentage. In each\ngeneration step, the intermediate tensors K and V in each\nattention operator can be cached for reuse in the future gen-\neration, which is called Key-value cache (KV cache) (Pope\net al., 2023). While being able to accelerate computation,\nit occupies a substantial amount of GPU memory, which is\nproportional to the total number of tokens.\n2.2\nLLM Inference Optimization\nParallelism.\nAs the size of LLMs grows, the memory\ncapacity on a single GPU becomes insufficient. Conse-\nquently, various techniques are developed to partition mod-\nels onto multiple GPUs (Zheng et al., 2022). These paral-\nlelization strategies can be classified as (1) inter-operator,\nwhich places different operators or layers across multiple\nGPUs, overlapping them with pipelining (known as Pipeline\nparallelism, PP) (Huang et al., 2019; Narayanan et al., 2019;\nLi et al., 2023), and (2) intra-operator, which partitions\ndifferent dimensions of tensors involved in computation,\nincluding data parallelism (Srivatsa et al., 2024), tensor\nparallelism (Shoeybi et al., 2019), etc. Data parallelism du-\nplicates models on different devices and dispatches requests\namong them. Tensor parallelism shards model weights and\neach device performs a portion of the computation, then\naggregates these partial results to produce the final output.\nBatching.\nBatching more tokens in a single forward pass\nincreases inference efficiency by, for example, amortizing\nthe time required to load model weights (Sheng et al., 2023;\nFang et al., 2021). However, its effectiveness differs be-\ntween the prefilling and decoding stages (Yuan et al., 2024;\nHe & Zhai, 2024; Agrawal et al., 2023). In decoding, where\nweight-loading overhead occupies a larger portion of the\nruntime, batching significantly boosts throughput by effec-\ntively amortizing this overhead. Conversely, in the prefilling\nstage, batching has a less pronounced impact since the token\ncount in input prompts is generally sufficient to keep the\nprocess compute-bound. Overall, larger batch sizes yield\nhigher throughput, though the maximum batch size is lim-\nited by available GPU memory, as it requires additional\nspace for activations and the KV cache.\nContinuous Batching and Scheduling.\nContinuous\nbatching is an essential optimization for throughput-oriented\nLLM inference (Yu et al., 2022; Kwon et al., 2023). By\nbatching multiple sequences at the token level, it allows the\nsystem to onboard new sequences and clear the KV cache of\ncompleted sequences at any generation step. This approach\nenables prefill-prioritizing scheduling, which removes se-\nquences as they finish, frees up their KV cache, and eagerly\n\n=== Page 4 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nTensor Parallel\nPipeline Parallel\nL1\nL1\nGPU1\nGPU2\nGPU1\nGPU2\nGPU1\nGPU2\nGPU1\nGPU2\nL1\n1/2\nL1\n2/2\nL1\nL1\nPrefill\nDecode\nS1\nS2\nS1\nS2\nS2\nS3\nL1\nS1\nL2\nS1\nL2\nS2\n0.5x load weight\n0.5x compute\n+ allreduce\nper-sequence time\ntime\n1x load weight\n0.5x compute\nS2\nS1\nS1 S2\nS1\nS1 S2\nS1\nS2\nS2\nL2\nS1\nL2\nS2\nL1\nS1\ntime\n0.5x load weight\n0.5x compute\n+ allreduce\nper-sequence time\n1x load weight\n0.5x compute\nlayer\ntoken\nsequence\nbatch\nallreduce\nPP is better because of\nlower allreduce overhead\nTP is better because of more\neffective batching\nProportion of allreduce is\nlarger in prefilling\nProportion of loading weights is\nhigher in decoding\nTP shards weights, so load\nweights is parallelized\nL2\n1/2\nL2\n2/2\nS2\nS1\nS2\nS1\nL1\n1/2\nL1\n2/2\nL2\n1/2\nL2\n2/2\nPP has smaller\nbatch sizes\nFigure 3. Different effects of tensor and pipeline parallelisms on prefilling and decoding. Tensor parallelism incurs all-reduce overhead,\nwhich has a higher percentage in prefilling, therefore pipeline parallelism is better for prefilling. Conversely, pipeline parallelism splits\nbatches into smaller micro-batches, which leads to more forward passes and repetitive loading weights, which is insufficient in decoding.\nschedules the prefilling of new sequences whenever GPU\nmemory becomes available. This strategy maximizes the\nnumber of concurrent sequences being processed, resulting\nin higher throughput. Another alternative is to use decode-\nprioritizing scheduling, which minimizes the frequency of\ntransitions. Instead of scheduling to prefilling eagerly, this\napproach waits until all sequences in a batch have finished\ndecoding before initiating the next round of prefilling. How-\never, this scheduling policy results in suboptimal decoding\nthroughput (Agrawal et al., 2024).\n3\nMOTIVATION AND ANALYSIS\nIn this section, we provide an in-depth analysis of two key\nobservations we identify from Figure 1 in Section 1: (1)\nTensor parallelism often exhibits significantly worse per-\nformance than pipeline parallelism during the prefill stage\ndue to its substantial communication overhead; (2) Pipeline\nparallelism tends to fall short in the decode stage owing to\nthe considerable weight loading overhead it incurs. We then\nargue that a dynamic parallelization strategy is essential to\nattain optimal performance across both stages.\nGiven the importance of batching in throughput-oriented\ntasks, it can be useful to consider how different paralleliza-\ntion strategies impact the maximum batch size, rather than\nassuming batch size as a tunable parameter, as is often done\nin online-serving contexts such as DistServe (Zhong et al.,\n2024) and Sarathi-serve (Agrawal et al., 2024).\n3.1\nParallelism Analysis\nObservation 1: Tensor parallelism incurs substantial\ncommunication overhead during the prefill stage.\nIn\nTensor parallelism, each device performs a part of computa-\ntion and aggregate the partial result. The activations at each\nlayer are synchronized across all GPUs using all-reduce\noperations. The overhead associated with this operation can\nbe quantified as:\n#tokens \u00d7 activation size\nall-reduce bandwidth\n,\nwhere all-reduce bandwidth refers to the rate of data transfer\nduring all-reduce operations, calculated as the size of the\ntensor being all-reduced divided by the all-reduce runtime.\nAs the degree of tensor parallelism increases, the proportion\nof execution time of all-reduce operations grows substan-\ntially. This growth is attributed to two main factors. First,\nwhile model weights are partitioned, activations in tensor\nparallelism remain fully replicated across GPUs, leading to\na constant activation size regardless of the degree of tensor\nparallelism. Second, all-reduce bandwidth decreases as the\nnumber of GPUs grows, due to more complex communi-\ncation schemes. Therefore, increasing the degree of tensor\nparallelism not only fails to reduce the traffic of all-reduce\noperations but further limits the communication bandwidth,\nresulting in escalated communication overhead. This is-\nsue is particularly pronounced in the prefill stage, where a\nlarge number of tokens are processed simultaneously, mak-\ning communication overhead the primary bottleneck. Thus,\ntensor parallelism tends to perform worse than pipeline par-\nallelism due to its large communication overhead.\nObservation 2: Pipeline parallelism suffers from signifi-\ncant weight transferring overhead in the decode stage.\nPipeline parallelism distributes model layers sequentially\nacross devices, with each device responsible for processing\na set of consecutive layers before passing the output to the\nnext device. Due to the auto-regressive nature of LLM infer-\nence, a sequence cannot enter the pipeline until its preceding\ntoken is generated. As a result, at any given time step, a\nsequence can appear in only one stage of the pipeline, mak-\ning the batches processed by each device mutually exclusive.\nHowever, the total number of sequences that the pipeline\ncan handle at a time, referred to as the global batch size,\n\n=== Page 5 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nis constrained by the size of KV cache. Given the mutual\nexclusion of batches at each device, pipeline parallelism can\nprocess only approximately 1/PP of the global batch per\nforward pass. We denote this reduced batch size in pipeline\nparallelism as the micro-batch size.\nDividing batches into micro-batches increases the number\nof LLM forward passes required to process the same amount\nof requests. Specifically, a pipeline parallelism degree of\nPP necessitates PP times more forward passes for a given\nglobal batch. This repeated execution degrades inference\nperformance, as model weight matrices must be loaded from\nglobal memory repeatedly. This inefficiency is especially\nsignificant in the decode stage, where weight-loading over-\nhead accounts for a substantial portion of total execution\ntime. As a result, pipeline parallelism generally underper-\nforms relative to tensor parallelism in the decode stage due\nto the amplified weight loading overhead.\nDiscussion on Data Parallelism.\nUnlike tensor and\npipeline parallelism, which distribute the model across de-\nvices, data parallelism distributes the data while duplicating\nthe model. While data parallelism has minimal commu-\nnication overhead, it has two key disadvantages: (1) the\nvolume of weight transferring is higher by the number of\nduplicates compared to tensor parallelism; and (2) it occu-\npies more GPU memory, reducing the available space for\nthe KV cache and thus limiting the maximum batch size re-\nsulting in lower throughput. Data parallelism can be applied\northogonally alongside both tensor and pipeline parallelism.\nWe do not dynamically adjust data parallelism, which will\nbe explained in Section 4.1.\nConclusion: No one-size-fits-all\nWhen comparing these\nthree parallelism strategies for high-throughput LLM infer-\nence, a key observation is that prefilling and decoding stages\nbenefit from different parallelism approaches. This differ-\nence arises from the distinct characteristics of each stage, as\nillustrated in Figure 3. Tensor parallelism is preferred for\ndecoding due to its ability to efficiently accelerate weight\nmatrix loading. However, it incurs significant communica-\ntion overhead, as it requires all-reduce operations at each\nlayer. In contrast, pipeline and data parallelism have much\nlower communication overhead, making them preferable for\nprefilling. However, their decoding throughput is limited by\ninefficient batching and additional weight-loading overhead.\nTo quantitatively analyze the trade-offs across different par-\nallelisms, we model the average runtime per sequence (the\ninverse of throughput) as follows. Derivations and further\ndetails are provided in the Appendix A.\nT \u221dT linear\ndm\nTP\n+ T attn\ndm + Tcomp\nDP \u00b7 TP \u00b7 PP + Tcomm(TP)\nPP \u00b7 DP\nHere T linear\ndm\nrepresents data movement for linear layers (pri-\nGPU0\nGPU1\nGPU2\nGPU3\nGPU4\nGPU5\nGPU6\nGPU7\nprefill worker\ndecode worker\nprefill\nthroughput\ndecode\nthroughput\n0.0\n0.5\n1.0\n1.5\n2.0\nThroughput (reqs/sec)\nDecode (8 GPUs)\nDecode (4 GPUs)\nPrefill (4 GPUs)\nThroughput Mismatch\nFigure 4. An example of spatially disaggregating prefilling and\ndecoding has a restricted search space. Deploying a 70B model on\neight 40GiB GPUs allows only one disaggregation strategy: four\nGPUs for prefilling and four for decoding. However, this causes\nsevere throughput mismatch between the two stages.\nmarily model weights), T attn\ndm represents data movement for\nattention layers (primarily KV cache) , Tcomp represents\ncomputation time, Tcomm represents communication time.\nNote that Tcomm is a monotonically increasing function\nwith respect to TP, as all-reduce operations require more\ntime as TP increases.\nTensor parallelism can effectively accelerate loading model\nweights, which is T linear\ndm , while pipeline and data parallelism\ncannot. On the other hand, pipeline and data parallelism\neffectively reduce the overhead of communication, while\ntensor parallelism contrarily increases the communication\noverhead. In prefilling, T linear\ndm\nis negligible, and Tcomm be-\ncomes larger, so pipeline and data parallelisms are more\npreferred, while in decoding, T linear\ndm\noccupies a larger pro-\nportion so tensor parallelism is more advantageous.\n3.2\nWhy not Disaggregate Prefilling and Decoding?\nSpatially disaggregating prefilling and decoding with sepa-\nrate hardware resources, as done in online serving systems\nsuch as DistServe (Zhong et al., 2024) and MoonCake (Qin\net al., 2024), is one approach to separately select paralleliza-\ntion strategies for prefilling and decoding. Sequences are\nfirst processed by the devices dedicated for prefilling before\nbeing transferred to decoding devices.\nHowever, there are two obstacles when applying prefill-\ndecode disaggregation to purely throughput-oriented sce-\nnarios.\nFirst, since the overall throughput is bound by\nthe slower stage, the throughput of prefilling and decod-\ning needs to be matched by adjusting the devices allocated\nfor each stage. However, it can be impractical in resource-\nconstrained scenarios. As shown in Figure 4, to deploy\na 70B model (which takes 140GiB memory for model\nweights) on eight 40GiB GPUs, there is only one disag-\ngregation strategy, that is four GPUs for prefilling and four\n\n=== Page 6 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nL1\nKV1\nL2\nKV2\nL1\n1/2\nKV1\n1/2\nL2\n1/2\nKV2\n1/2\nL1\n2/2\nKV1\n2/2\nL2\n2/2\nKV2\n2/2\nmodel\nresharding\nGPU 1\nGPU 2\nprefill\n\u00a0(pipeline parallelism)\ndecode\u00a0\n(tensor parallelism)\nFigure 5. Model weights and KV cache need to be re-sharded when\nswitching between different parallelism.\nfor decoding2. However, it causes severe throughput mis-\nmatch where prefilling has more than 6\u00d7 higher throughput\nthan decoding. Second, disaggregation duplicates the model\nweights similarly to data parallelism, bringing similar draw-\nbacks, such as limited KV cache space and increased weight\ntransfer. As a result, decoding throughput with four GPUs\nis only 15% of that with eight GPUs.\nIn conclusion, although disaggregation allows for select-\ning different parallelization strategies for each stage, the\nthroughput mismatch between stages and limited resources\nallocated to each can lead to suboptimal performance. This\ncalls for a method that offers flexibility in parallelization\nwhile maximizing hardware resource utilization.\n4\nSEESAW: KEY IDEAS\n4.1\nDynamic Model Re-sharding\nObserving that prefilling and decoding have distinct pref-\nerences for parallelism, we propose a technique called dy-\nnamic model re-sharding. This technique enables the se-\nlection of different parallelism strategies for each stage and\nautomatically transitions between them. This approach ex-\npands the configuration space, allowing for separate opti-\nmization of the two stages, potentially improving overall\nthroughput compared to using a single configuration. In the\nfollowing paragraphs, we denote the parallelization strategy\nused in prefilling as cp and that in decoding as cd.\nTo support transitions between different parallelization con-\nfigurations, the cluster must rearrange the data stored on\neach device to align with the new parallelism which involves\nboth model weights and KV cache, as illustrated in Figure 5.\nIn Seesaw, model weights are re-sharded by reloading the re-\nquired shards from CPU memory, and KV cache re-sharding\nis performed through CPU shared memory.\nThe inter-device movement of tensors incurs overhead. To\nmitigate this re-sharding cost, we design an asynchronous\npipeline to overlap data transfer with computation, as de-\ntailed in Section 5.2.\nDiscussion on data parallelism.\nUnlike switching be-\ntween tensor and pipeline parallelism, adjusting the degree\n2 At least four GPUs (160 GiB memory) are needed to fit the\nmodel weights.\nasynchronous\u00a0\nswap in\nCPU\nGPUs\nprefill\nCPU\nGPUs\nCPU memory is\u00a0\nempty\nCPU\nGPUs\ndecoding\nCPU\nGPUs\nCPU memory is\nfilled\nCPU\nGPUs\nprefill\n(warm up)\nFigure 6. Tiered KV cache buffering and transition-minimizing\nscheduling, and the change of KV cache occupancy.\nrequest\ncp\ncd\nGPU 1 (worker 1)\nGPU 2 (worker 2)\nCPU\nwrite back kv\nafter prefill\nload kv before decode\ncp\ncp\n(prefill)\ncd\ncd\n(decode)\nCPU KV cache is empty\nCPU KV cache is full\nscheduler\nFigure 7. KV cache re-sharding is completed during swapping,\nleveraging CPU shared memory.\nof data parallelism alters the proportion of GPU memory al-\nlocated to model weights versus KV cache. This adjustment\nincreases system complexity or necessitates additional data\nmovement between the CPU and GPU. Therefore, we only\ndynamically adjust tensor and pipeline parallelism.\n4.2\nTiered KV Cache Buffering and\nTransition-minimizing Scheduling\nChallenge: Transition Overhead.\nIn practice, dynamic\nmodel resharding encounters an obstacle of transition\noverhead, which is amplified by the widely-used contin-\nuous batching and prefill-prioritizing scheduling. Prefill-\nprioritizing scheduling eagerly schedules new prefilling\ntasks, causing frequent transitions between the two stages.\nAs a result, directly applying model re-sharding with this in-\nterleaved prefill-decode scheduling policy would introduce\nsignificant re-sharding overhead. On the other hand, decode-\nprioritizing scheduling minimizes the frequency of transi-\ntions but results in suboptimal decoding throughput. Other\ncompromise solutions involve setting a threshold-based ap-\nproach for managing the prefill-decode transition (Cheng\net al., 2024). However, they still involve a trade-off be-\ntween reducing transition overhead and maximizing decod-\ning throughput.\nTo address this problem, we propose 1) tiered KV cache\nbuffering, which leverages CPU memory offloading 2)\ntransition-minimizing scheduling policy. These two syn-\nergistic techniques prevent frequent stage transitions and\nmaintain a high decoding throughput.\nTiered KV cache buffering uses CPU memory as auxiliary\nstorage for the KV cache, enabling the pre-computation\nof a large batch of prefilling consecutively. During the\n\n=== Page 7 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nprefill stage, the generated KV cache is offloaded to CPU\nKV cache storage, freeing it from the limitations of GPU\nmemory space. During decoding, continuous batching runs\nas normal, except that new sequences are on-boarded by\nswapping in its KV cache from the CPU memory.\nTransition-minimizing scheduling controls the transition to\nonly happen when the CPU KV storage is either full or\nempty. During prefill, once the CPU KV cache storage\nis fully utilized, re-sharding is triggered, and the cluster\ntransitions to decoding. During decoding, GPUs continue\nprocessing requests and loading KV cache from CPU mem-\nory, keeping GPU KV cache fully utilized for high decod-\ning throughput. When the entire CPU KV cache has been\ntransferred to GPU memory, the cluster switches back to\nprefilling. The whole process is illustrated in Figure 6.\nKV cache re-sharding occurs throughout this process. As\nillustrated in Figure 7, in a multi-GPU setup, the CPU KV\ncache storage is shared among all GPUs. During swap-out,\neach GPU pushes its shard (based on cp) of the generated\nKV cache to the shared CPU storage, where these shards\ncollectively form the complete KV cache. During swap-\nin, each GPU retrieves its required KV shard (based on\ncd) from the shared storage. We implement the shared KV\ncache using shared memory of the operating system.\n5\nSYSTEM DESIGN AND IMPLEMENTATION\n5.1\nScheduler-worker Architecture\nIn order to support dynamically switching parallelization\nconfigurations for prefilling and decoding, we build Seesaw,\na new LLM inference engine designed for high-throughput\nLLM inference. The overall architecture of Seesaw fol-\nlows a single-scheduler, multi-worker design. The sched-\nuler manages all generation requests, organizes them into\nbatches, and sends instructions to the workers. To fully\nutilize pipelining, each decoding step processes 1/PP of the\nsequences in GPU KV storage. Once a batch is formed, it is\nsent to workers through shared queues. Each worker is re-\nsponsible for controlling a single GPU and maintains a task\nqueue to receive and execute instructions sequentially. This\narchitecture facilitates the implementation of asynchronous\nfeatures, such as pipeline parallelism and the asynchronous\npipeline for tiered KV cache buffering.\n5.2\nAsynchronous Pipeline\nWhile re-sharding and tiered KV cache buffering offer sub-\nstantial benefits, they also introduce new overhead related\nto moving model weights and KV cache. The overhead of\nreloading model weights remains constant relative to batch\nsize, allowing it to be amortized with larger batches. In con-\ntrast, swapping the KV cache incurs overhead proportional\nto batch size, making it harder to amortize. Fortunately,\nswap out\nqkv_proj\nattn + ffn\n\u00a0\u00a0 swap in\nmain thread\nprefetcher thread\nscheduler\n(prefill)\nscheduler\n(decode)\ndecode\nCPU KV\nGPU KV\nnon-blocking copy\nCPU KV\u00a0 is empty\nCPU KV\u00a0 is full\nFigure 8. Async pipeline of Seesaw: Swap-in overlaps with prefill\ncomputation, while swap-out occurs in a separate asynchronous\nprefetcher thread.\nthese overheads can be mitigated through computation-\ncommunication overlap. We implement an asynchronous\npipeline to overlap KV cache transfer with ongoing compu-\ntation, as illustrated in Figure 8.\nOverlap swap-out with computation.\nThe KV cache\ngenerated during the prefilling stage is not used until decod-\ning begins, allowing the KV cache swap-out to overlap with\nother computations during prefilling. Although CPU-GPU\ndata transfer is relatively slow due to PCIe bandwidth limi-\ntations, it can still be overlapped with computation, given\nthe high FLOPS involved in prefilling.\nIn practice, CPU-GPU data transfer can only overlap with\ncomputation when using pinned memory, but shared mem-\nory cannot be pinned (AlbanD, 2023). To address this, we\nsplit the transfer into two stages: GPU to pinned memory\n(overlapped with computation) and then pinned to shared\nmemory, which is a host-side operation that also runs con-\ncurrently with GPU kernels.\nAsynchronous swap-in.\nWe implement swap-in using a\nbackground thread called the prefetcher on each worker, op-\nerating in a fully asynchronous paradigm. The prefetcher is\ncontrolled directly by the scheduler and runs independently\nof the main thread, whether the main thread is handling pre-\nfilling or decoding. In each iteration, the scheduler creates\nnew prefetching tasks when there are free slots in the GPU\nKV store. Once the prefetcher completes moving the KV\ncache for certain sequences, it notifies the scheduler via a\nshared queue, allowing those sequences to be scheduled for\ndecoding tasks later. As long as the output length is not too\nshort, the swap-in can also be well overlapped.\nBandwidth-aware KV cache layout.\nThe data layout of\nthe KV cache significantly impacts the bandwidth efficiency\nof data movement. There are two common layouts for stor-\ning KV cache: (seq len, num heads, head dim) (NHD) and\n(num heads, seq len, head dim) (HND). NHD is less opti-\nmal for memory access because tensor parallelism shards\nthe KV cache along the H dimension (number of heads),\nwhich is the second-to-last dimension, leading to more non-\ncontiguous memory access. Therefore, we use the HND\n\n=== Page 8 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nlayout for storing the KV cache in CPU memory.\n6\nEVALUATION\nIn this section, we evaluate the performance of Seesaw under\na variety of hardware configurations and workloads.\n6.1\nExperiment Settings\nHardware.\nWe use three types of GPUs: NVIDIA A10,\nL4, and A100. The A10 and L4 are deployed on AWS EC2\ninstances (g5.48xlarge and g6.48xlarge (Amazon\nWeb Services, 2024)), and the A100 is used on GCP (Google\nCloud, 2024). GPU specifications are listed in Table 1. The\nPCIe connection for each GPU is PCIe 4.0 8\u00d7, providing 16\nGiB/s bandwidth (PCI-SIG, 2017), while NVLink (NVIDIA\nCorporation, 2024) offers a bandwidth of 600 GiB/s. Addi-\ntionally, we allocate 80 GiB of CPU memory per GPU.\nModel.\nWe use three different LLMs with different\nsizes: (1) a 15B variety of LLaMA3 (Elinas, 2024); (2)\nCodeLLaMA-34B (Roziere et al., 2023); (3) LLaMA2-\n70B (Touvron et al., 2023b). They all use Grouped Query\nAttention (GQA) (Ainslie et al., 2023). For brevity, we refer\nto them as 15B, 34B, and 70B, respectively, in the following\nsections. We use float16 as the data type.\nWorkload.\nWe use two different datasets in our eval-\nuation,\nnamely\nsharegpt\n(ShareGPT,\n2023)\nand\narxiv-summarization (Cohan et al., 2018). They\ncorrespond\nto\ntwo\ndifferent\ndistributions\nof\nwork-\nload.\nsharegpt is a dataset of chatting history, so\nits input and output have comparable lengths, while\narxiv-summarization dataset is a summarization\ndataset where inputs are much longer than outputs. The\ncharacteristics of these two datasets are shown in Figure 9.\nWe sample 2000 requests from the sharegpt dataset and\n500 requests from arxiv-summarization and also use\nconstant-length workloads in Section 6.5. Since Seesaw\nis purely throughput-oriented, we measure the end-to-end\nthroughput as the metrics.\nBaselines.\nWe use vLLM 0.5.4 (Kwon et al., 2023) as\nthe baseline. It is the most widely used open-source LLM\nserving engine with wide support for different parallelisms.\nWe also directly use the vLLM\u2019s model implementation for\na straightforward comparison. SGLang (Zheng et al., 2023)\nand DeepSpeed-FastGen (Holmes et al., 2024) do not sup-\nport pipeline parallelism. TensorRT-LLM (NVIDIA, 2024b)\nis not included in the comparison because it uses a simi-\nlar scheduling policy as vLLM, and vLLM demonstrates\ncomparable performance (vLLM Team, 2024) in throughput-\noriented tasks. The techniques proposed in Seesaw can also\nbe applied to modifying TensorRT-LLM.\nTable 1. GPU hardware specification\nGPU Model\nMemory Size\nMemory\nBandwidth\nFLOPS\nNVLink\nA10\n24 GiB\n600 GiB/s\n125T\n\u2717\nL4\n24 GiB\n300 GiB/s\n121T\n\u2717\nA100\n40 GiB\n1,555 GiB/s\n312T\n\u2713\n0\n2000\n4000\n#tokens\n0\n2\n4\nDensity\n1e\n3\ninput tokens\noutput tokens\n(a) arxiv-summarization\n0\n2000\n4000\n#tokens\n0.0\n0.5\n1.0\nDensity\n1e\n2\ninput tokens\noutput tokens\n(b) ShareGPT\nFigure 9. Input and output length distributions of the datasets\nWe enable chunked prefill and tune the chunk size for vLLM\nto get the optimal throughput, following the practice of\nSarathi-serve (Agrawal et al., 2024). Otherwise, suboptimal\nchunk sizes would cause severe throughput degradation.\n6.2\nEnd-to-end Throughput on PCIe Systems\nFirst, we measure the end-to-end throughput of Seesaw. We\nsweep over all available single parallelism configurations\nfor vLLM and show the result of the best configuration. We\nuse four GPUs for the 15B model, and eight GPUs for the\n34B and 70B models. The result is shown in Figure 10, with\nthe used parallelism labeled above each bar.\nOn A10, compared with the highest single parallelism base-\nline, Seesaw achieves a geometrically average speedup of\n1.45\u00d7, with up to 1.78\u00d7 speedup. On L4, Seesaw achieves\na geometrically average speedup of 1.29\u00d7, with up to 1.52\u00d7\nspeedup.\nThe overall average speedup is 1.36\u00d7.\nThe\nspeedup is more significant on A10, because A10 has better\nsingle GPU performance than L4, while they have similar\nPCIe inter-connection bandwidth, causing a higher percent-\nage of communication overhead.\n6.3\nSpeedup Breakdown: An Example\nFigure 12 illustrates how Seesaw merges the advantages\nof different parallelisms. Using CodeLLaMA34B on the\narxiv-summarization dataset with four A10 GPUs\nas an example, we measured the runtime of each stage. TP4\nis optimal for decoding but significantly slower for prefilling,\nwhile PP4 excels at prefilling but is slower during decoding.\nSeesaw uses a mixed parallelism strategy, applying PP4\nfor prefilling and TP4 for decoding, achieving performance\ncomparable to the best configuration for each stage.\nCompared to the optimal single parallelism configuration\n(TP2PP2) with chunked prefill, Seesaw is still faster because\n(1) chunked prefill does not piggy-back all decoding steps,\n\n=== Page 9 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\n15b\n34b\n70b\n15b\n34b\n70b\n0\n1\n2\nNormalized\nThroughput\n D2T2\n P4->T4\n D2T2P2\n D2P4->D2T4\n T2P4\n P8->T4P2\n D2P2  P4->T4  D2T2P2\n D2P4->D2T4\n T4P2\n P8->T4P2\n arxiv\n sharegpt\nvllm\nseesaw\n(a) End-to-end Throughput on A10\n15b\n34b\n70b\n15b\n34b\n70b\n0\n1\n2\nNormalized\nThroughput\n T2P2\n P4->T4\n D2T4\n D2P4->D2T4\n T4P2\n P8->T4P2\n P4\n P4->T4\n D2T4\n P8->T4P2\n T4P2 P8->T4P2\n arxiv\n sharegpt\nvllm\nseesaw\n(b) End-to-end Throughput on L4\nFigure 10. End-to-end throughput comparison on PCIe systems.\nThe used parallelization strategies are labelled above each bar.\nLabels such as \u201cP4 \u2192D4\u201d represent the parallelization strategies\nfor prefilling and decoding respectively in Seesaw.\narxiv\nsharegpt\n0.0\n0.5\n1.0\nThroughput\n(Normalized)\n0.61\n0.62\n0.89\n0.82\n1.00\n1.00\n1.00\n1.13\nvllm+pcie\nseesaw+pcie\nvllm+nvlink\nseesaw+nvlink\nFigure 11. Throughput comparison on A100.\nleaving some purely decoding steps, and (2) chunked prefill\nwith TP2PP2 is slower than prefilling with PP4.\n6.4\nEnd-to-end Throughput on A100\nSpeedup on A100 + NVLink\nThe NVLink interconnec-\ntion across A100 GPUs significantly reduces the all-reduce\noverhead and further scales tensor parallelism. Usually,\ntensor parallelism alone is enough to achieve optimal per-\nformance when there are no more than four GPUs. Never-\ntheless, there is still a noticeable percentage of all-reduce\noverhead in prefilling when tensor parallelism scales be-\nyond four GPUs. Seesaw can still provide speedup in this\ncase. As shown in Figure 11, Seesaw still achieves a 13%\nthroughput increase over vLLM for the sharegpt dataset\non LLaMA3-70B on eight A100s.\nSpeedup on A100 + PCIe\nBesides A100 SXM with\nNVLink inter-connection, there is also another version of\nA100 that is inter-connected with PCIe links, where Seesaw\ncan achieve noticeable speedup. As shown in Figure 11, See-\nsaw provides 46% speedup on arxiv-summarization\nand 30% speedup on sharegpt. Seesaw brings the per-\nformance of the A100 PCIe version much closer to the per-\nformance level of the NVLink version. vLLM gets roughly\n60% throughput on A100 PCIe compared with A100 SXM,\nwhile Seesaw boosts it up to 82% \u2013 89%.\ntp4\npp4\np4->t4\ntp2pp2\n+chunked prefill\n0\n500\n1000\n1500\nEnd-to-end time (s)\nprefill\nmix\ndecode\nother\nFigure 12. Speedup breakdown. \u201cmix\u201d represents batches contain-\ning both prefilling and decoding when chunked prefill is enabled.\nWe disable chunked prefill for TP4 and PP4 in order to show the\nreference prefilling and decoding time. TP2PP2 with chunked\nprefill is the optimal parallelism for vLLM.\n0.0\n0.1\n0.2\n0.3\nD:P\n0.5\n1.0\nthroughput\n(normalized)\ntp4pp2\ntp2pp4\npp8\npp8->tp4pp2\nFigure 13. Throughput of various parallelization strategies with\ndifferent ratios between output and input lengths (D : P), mea-\nsured on 70B model and eight A10 GPUs.\n6.5\nSensitivity Study\nRatio between Input and Output Length\nThe speedup\nof Seesaw depends on the ratio between the input and output\nlength, or P : D. Model re-sharding has the opportunity\nto provide speedup when prefilling and decoding have bal-\nanced time. To investigate to what extent model re-sharding\nwould be effective, we measure the throughput of various\nparallelization strategies on synthesized datasets with uni-\nform lengths and different P : D ratios. We fix the input\nlength as 3000 and vary the output length.\nAs shown in Figure 13, PP8 achieves the highest throughput\nduring prefilling, while TP4PP2 excels in decoding. When\nthe output length equals one (prefilling only), Seesaw and\nPP8 show similar throughput, and TP4PP2 performs worse\ndue to high communication overhead. As output length\nincreases, the inefficiency of PP in decoding outweighs its\nadvantage in prefilling, causing PP8\u2019s throughput to drop\nrapidly. There is a range where TP2PP4 becomes optimal\nbefore decoding dominates the runtime and TP4PP2 takes\nover as the fastest. Nonetheless, Seesaw achieves the highest\noverall throughput across all data points. In real scenarios\nwith variable input and output lengths, Seesaw is even more\nadvantageous due to its adaptive capabilities.\nInter-connection Bandwidth\nThe effectiveness of See-\nsaw also depends on the inter-connection bandwidth. We\ninvestigate this by measuring the runtime and tracing all-\nreduce operations of running arxiv-summarization\nand 34B model on eight A10s. We then mutate the all-\nreduce time to project the end-to-end throughput with dif-\n\n=== Page 10 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\n10\n1\n100\n101\nBandwidth scale (\u00d7 all_reduce through PCIe)\n0.0\n0.5\n1.0\nThroughput\n(Normalized)\nd2t1p4\nd2t2p2\nd2t4p1\nd1t1p8\nd1t2p4\nd1t4p2\nd1t8p1\nd2p4->d2t4\nFigure 14. Projected throughput of various parallelization strate-\ngies with different inter-connection bandwidth, measured and\ntraced on 34B model and eight A10 GPUs.\nferent inter-connection bandwidths. As shown in Figure 14,\nwhen the inter-connection bandwidth is slow (for example,\namong geographically distributed devices (Borzunov et al.,\n2022)), pipeline parallelism is optimal; when the bandwidth\nis very high, tensor parallelism is optimal. The throughput\nof Seesaw is superior to fixed parallelization strategies on a\nwide range from 0.1\u00d7 to 50\u00d7 of PCIe bandwidth.\n7\nRELATED WORK\n7.1\nHeterogenity between Prefilling and Decoding\nDue to the different computational characteristics between\nprefilling and decoding leading to under-utilization of hard-\nware resources, prior research has investigated two direc-\ntions to address this problem, namely disaggregating or\nmerging the two stages. Disaggregation places prefilling\nand decoding onto different devices to avoid their interfer-\nence while merging processes prefilling and decoding in\none batch.\nDisaggregate Prefill and Decoding\nDistServe (Zhong\net al., 2024) proposed placing prefilling and decoding on\ndifferent devices to prevent interference and leverage dif-\nferent characteristics of the two stages. Mooncake (Qin\net al., 2024) uses similar through a distributed KV cache\npool. P/D-Serve (Jin et al., 2024) uses the device-to-device\nnetwork to transfer the KV cache between prefill and decode\ndevices. Splitwise (Patel et al., 2024) proposes using dif-\nferent GPU models for the two stages. TetriInfer (Hu et al.,\n2024) further disaggregates different downstream tasks to\navoid interference. These works are designed for online\nserving while Seesaw focuses on offline inference. More-\nover, they are usually designed for large clusters.\nMerge Prefill and Decode\nChunked prefill, as proposed\nby SplitFuse (Holmes et al., 2024), Sarathi (Agrawal et al.,\n2023), and Sarathi-serve (Agrawal et al., 2024), splits long\nprompts in the prefilling stage into smaller chunks, combin-\ning them with decoding steps to strike a balance between\ndata movement and computation and reduce pipeline bub-\nbles in pipeline parallelism. However, determining the opti-\nmal chunk size is challenging. A chunk size that\u2019s too large\nresults in excessive decode-only steps, closely resembling\ntraditional prefill-decode scheduling. Conversely, a chunk\nsize that\u2019s too small reduces kernel efficiency.\n7.2\nParallel and Distributed LLM Inference\nAside from tensor parallelism, pipeline parallelism, and data\nparallelism discussed in Section 2.2, there are also other\ntypes of parallelisms, such as sequence parallelism (SP) (Li\net al., 2021; Liu et al., 2023; Lin et al., 2024; Brandon et al.,\n2023; Xue et al., 2024) and fully sharded data parallelism\n(FSDP) (Zhao et al., 2023; Rajbhandari et al., 2020). Se-\nquence parallelism is especially designed for long sequence\nlengths, and is orthogonal with our work. FSDP requires\nfrequently transferring weight matrices across GPUs, thus\nmainly used in training.\nHexGen (Jiang et al., 2023), LLM-PQ (Zhao et al., 2024),\nHelix (Mei et al., 2024) investigate parallelisms in hetero-\ngeneous clusters. Intra-device parallelism leverages over-\nlapping functions using different resources within each de-\nvice, including NanoFlow (Zhu et al., 2024) and Liger (Du\net al., 2024).\nPetals (Borzunov et al., 2022) explores\nLLM inference in geographically distributed setups, em-\nploying pipeline parallelism to minimize communication\ncosts. SpotServe (Miao et al., 2024) runs LLM inference on\npreemptible instances.\n7.3\nOffloading in LLM Inference\nOffloading is a widely used technique to run LLM applica-\ntions in resource-constrained scenarios (Ren et al., 2021).\nFlexGen (Sheng et al., 2023) swaps tensors across GPU\nmemory, CPU memory, and disks. Fiddler (Kamahori et al.,\n2024), HeteGen (Xuanlei et al., 2024), PowerInfer (Song\net al., 2023) and FastDecoder (He & Zhai, 2024) perform\npart of computation in CPU, which require CPUs with\nstrong compute capability or external CPU nodes connected\nwith high-bandwidth networking. Instinfer (Pan et al., 2024)\noffloads computation to Computational Storage Drives.\n8\nCONCLUSION\nThis paper proposes Seesaw, a high-throughput LLM infer-\nence engine, to address the inefficiencies of fixed paralleliza-\ntion by selecting different parallelization strategies for the\nprefilling and decoding stages and switching between them\nusing model re-sharding. It uses tiered KV cache buffering\nto minimize re-sharding overheads. Our experiments show\nthat Seesaw outperforms widely-used open-source inference\nengines, with a throughput increase of 1.06-1.78\u00d7 and an\naverage throughput improvement of 1.36\u00d7. These results\nhighlight Seesaw\u2019s effectiveness and adaptability.\n\n=== Page 11 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nREFERENCES\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\nAgrawal, A., Panwar, A., Mohan, J., Kwatra, N., Gulavani,\nB. S., and Ramjee, R. Sarathi: Efficient llm inference\nby piggybacking decodes with chunked prefills. arXiv\npreprint arXiv:2308.16369, 2023.\nAgrawal, A., Kedia, N., Panwar, A., Mohan, J., Kwatra, N.,\nGulavani, B. S., Tumanov, A., and Ramjee, R. Taming\nthroughput-latency tradeoff in llm inference with sarathi-\nserve. arXiv preprint arXiv:2403.02310, 2024.\nAinslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y.,\nLebr\u00b4on, F., and Sanghai, S. Gqa: Training generalized\nmulti-query transformer models from multi-head check-\npoints. arXiv preprint arXiv:2305.13245, 2023.\nAlbanD.\nWhy not multiprocess pin memory in\ndata loader?\nhttps://discuss.pytorch.org/t/why-not-\nmultiprocess-pin-memory-in-data-loader/197345/2, 2023.\nAccessed: 2024-10-14.\nAmazon Web Services.\nAmazon EC2 Instance Types,\n2024.\nURL https://aws.amazon.com/ec2/\ninstance-types/. Accessed: 2024-10-26.\nBen-Nun, T. and Hoefler, T. Demystifying parallel and dis-\ntributed deep learning: An in-depth concurrency analysis.\nACM Computing Surveys (CSUR), 52(4):1\u201343, 2019.\nBengio, Y., Ducharme, R., and Vincent, P. A neural proba-\nbilistic language model. Advances in neural information\nprocessing systems, 13, 2000.\nBorzunov, A., Baranchuk, D., Dettmers, T., Ryabinin, M.,\nBelkada, Y., Chumachenko, A., Samygin, P., and Raffel,\nC. Petals: Collaborative inference and fine-tuning of\nlarge models. arXiv preprint arXiv:2209.01188, 2022.\nBrandon, W., Nrusimha, A., Qian, K., Ankner, Z., Jin, T.,\nSong, Z., and Ragan-Kelley, J. Striped attention: Faster\nring attention for causal transformers. arXiv preprint\narXiv:2311.09431, 2023.\nChan, V., Zhang, H., and Wang, F.\nSnowflake llm\ninference:\nOptimizing gpu capacity for interactive\nworkloads.\nhttps://www.snowflake.com/engineering-\nblog/snowflake-llm-inference-interactive-workloads/,\nSeptember 2024. Accessed: 2024-10-30.\nChang, L., Bao, W., Hou, Q., Jiang, C., Zheng, N., Zhong,\nY., Zhang, X., Song, Z., Jiang, Z., Lin, H., et al. Flux: Fast\nsoftware-based communication overlap on gpus through\nkernel fusion. arXiv preprint arXiv:2406.06858, 2024.\nCheng, K., Hu, W., Wang, Z., Peng, H., Li, J., and Zhang,\nS. Slice-level scheduling for high throughput and load\nbalanced llm serving. arXiv preprint arXiv:2406.13511,\n2024.\nCohan, A., Dernoncourt, F., Kim, D. S., Bui, T., Kim, S.,\nChang, W., and Goharian, N. A discourse-aware attention\nmodel for abstractive summarization of long documents.\narXiv preprint arXiv:1804.05685, 2018.\nDell\nTechnologies.\nPoweredge\nserver\ngpu\nmatrix,\n2023.\nURL\nhttps://www.\ndelltechnologies.com/asset/en-ca/\nproducts/servers/briefs-summaries/\npoweredge-server-gpu-matrix.pdf.\nAc-\ncessed: 2024-10-24.\nDell Technologies.\nInferencing performance for gen-\nerative ai in the enterprise with amd accelerators.\nhttps://infohub.delltechnologies.com/en-au/l/generative-\nai-in-the-enterprise-with-amd-accelerators/inferencing-\nperformance/, 2024. Accessed: 2024-10-30.\nDu, J., Wei, J., Jiang, J., Cheng, S., Huang, D., Chen, Z.,\nand Lu, Y. Liger: Interleaving intra-and inter-operator\nparallelism for distributed large model inference. In Pro-\nceedings of the 29th ACM SIGPLAN Annual Symposium\non Principles and Practice of Parallel Programming, pp.\n42\u201354, 2024.\nEdge, D., Trinh, H., Cheng, N., Bradley, J., Chao, A., Mody,\nA., Truitt, S., and Larson, J. From local to global: A graph\nrag approach to query-focused summarization. arXiv\npreprint arXiv:2404.16130, 2024.\nElinas.\nLlama-3-15b\ninstruct-zeroed.\nhttps://huggingface.co/elinas/\nLlama-3-15B-Instruct-zeroed, 2024.\nFang, J., Yu, Y., Zhao, C., and Zhou, J. Turbotransformers:\nan efficient gpu serving system for transformer models.\nIn Proceedings of the 26th ACM SIGPLAN Symposium\non Principles and Practice of Parallel Programming, pp.\n389\u2013402, 2021.\nGoogle Cloud.\nGPU platforms:\nA100 GPUs, 2024.\nURL https://cloud.google.com/compute/\ndocs/gpus#a100-gpus. Accessed: 2024-10-26.\nHe, J. and Zhai, J.\nFastdecode: High-throughput gpu-\nefficient llm serving using heterogeneous pipelines. arXiv\npreprint arXiv:2403.11421, 2024.\nHolmes, C., Tanaka, M., Wyatt, M., Awan, A. A., Rasley, J.,\nRajbhandari, S., Aminabadi, R. Y., Qin, H., Bakhtiari, A.,\nKurilenko, L., et al. Deepspeed-fastgen: High-throughput\ntext generation for llms via mii and deepspeed-inference.\narXiv preprint arXiv:2401.08671, 2024.\n\n=== Page 12 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nHu, C., Huang, H., Xu, L., Chen, X., Xu, J., Chen, S., Feng,\nH., Wang, C., Wang, S., Bao, Y., et al. Inference without\ninterference: Disaggregate llm inference for mixed down-\nstream workloads.\narXiv preprint arXiv:2401.11181,\n2024.\nHuang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen,\nM., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe:\nEfficient training of giant neural networks using pipeline\nparallelism. Advances in neural information processing\nsystems, 32, 2019.\nJiang, Y., Yan, R., Yao, X., Zhou, Y., Chen, B., and Yuan, B.\nHexgen: Generative inference of large language model\nover heterogeneous environment. In Forty-first Interna-\ntional Conference on Machine Learning, 2023.\nJin, Y., Wang, T., Lin, H., Song, M., Li, P., Ma, Y., Shan, Y.,\nYuan, Z., Li, C., Sun, Y., et al. P/d-serve: Serving disag-\ngregated large language model at scale. arXiv preprint\narXiv:2408.08147, 2024.\nKamahori, K., Gu, Y., Zhu, K., and Kasikci, B. Fiddler:\nCpu-gpu orchestration for fast inference of mixture-of-\nexperts models. arXiv preprint arXiv:2402.07033, 2024.\nKamsetty, A., Chen, H., and Xie, L. How bytedance scales\noffline inference with multi-modal llms to 200tb data.\nhttps://www.anyscale.com/blog/how-bytedance-scales-\noffline-inference-with-multi-modal-llms-to-200TB-data,\nAugust 2023. Accessed: 2024-10-30.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention. In Proceedings of the 29th Sym-\nposium on Operating Systems Principles, pp. 611\u2013626,\n2023.\nLi, S., Xue, F., Baranwal, C., Li, Y., and You, Y. Sequence\nparallelism: Long sequence training from system perspec-\ntive. arXiv preprint arXiv:2105.13120, 2021.\nLi, Z., Zheng, L., Zhong, Y., Liu, V., Sheng, Y., Jin, X.,\nHuang, Y., Chen, Z., Zhang, H., Gonzalez, J. E., et al.\n{AlpaServe}: Statistical multiplexing with model paral-\nlelism for deep learning serving. In 17th USENIX Sympo-\nsium on Operating Systems Design and Implementation\n(OSDI 23), pp. 663\u2013679, 2023.\nLin, B., Peng, T., Zhang, C., Sun, M., Li, L., Zhao, H., Xiao,\nW., Xu, Q., Qiu, X., Li, S., et al. Infinite-llm: Efficient llm\nservice for long context with distattention and distributed\nkvcache. arXiv preprint arXiv:2401.02669, 2024.\nLiu, H., Zaharia, M., and Abbeel, P. Ring attention with\nblockwise transformers for near-infinite context. arXiv\npreprint arXiv:2310.01889, 2023.\nLiu, S., Biswal, A., Cheng, A., Mo, X., Cao, S., Gonzalez,\nJ. E., Stoica, I., and Zaharia, M. Optimizing llm queries in\nrelational workloads. arXiv preprint arXiv:2403.05821,\n2024.\nMei, Y., Zhuang, Y., Miao, X., Yang, J., Jia, Z., and Vinayak,\nR. Helix: Distributed serving of large language models\nvia max-flow on heterogeneous gpus. arXiv preprint\narXiv:2406.01566, 2024.\nMiao, X., Oliaro, G., Zhang, Z., Cheng, X., Jin, H., Chen,\nT., and Jia, Z. Towards efficient generative large language\nmodel serving: A survey from algorithms to systems.\narXiv preprint arXiv:2312.15234, 2023.\nMiao, X., Shi, C., Duan, J., Xi, X., Lin, D., Cui, B., and Jia,\nZ. Spotserve: Serving generative large language models\non preemptible instances. In Proceedings of the 29th ACM\nInternational Conference on Architectural Support for\nProgramming Languages and Operating Systems, Volume\n2, pp. 1112\u20131127, 2024.\nMLCommons. Mlperf inference: Datacenter benchmark\nsuite. https://mlcommons.org/benchmarks/\ninference-datacenter/, 2024. Accessed: 2024-\n10-30.\nNarayan, A., Chami, I., Orr, L., Arora, S., and R\u00b4e, C. Can\nfoundation models wrangle your data? arXiv preprint\narXiv:2205.09911, 2022.\nNarayanan, D., Harlap, A., Phanishayee, A., Seshadri, V.,\nDevanur, N. R., Ganger, G. R., Gibbons, P. B., and Za-\nharia, M. Pipedream: Generalized pipeline parallelism for\ndnn training. In Proceedings of the 27th ACM symposium\non operating systems principles, pp. 1\u201315, 2019.\nNVIDIA. Fastertransformer: Transformer related optimiza-\ntion, including bert, gpt.\nhttps://github.com/\nNVIDIA/FasterTransformer, 2024a.\nNVIDIA. Tensorrt-llm: Optimized inference for large lan-\nguage models.\nhttps://github.com/NVIDIA/\nTensorRT-LLM, 2024b.\nNVIDIA Corporation. Nvidia a100 pcie product brief, 2020.\nURL\nhttps://www.nvidia.com/content/\ndam/en-zz/Solutions/Data-Center/a100/\npdf/A100-PCIE-Prduct-Brief.pdf. Accessed:\n2024-10-24.\nNVIDIA Corporation. NVIDIA NVLink: High-Speed GPU\nInterconnect, 2024.\nURL https://www.nvidia.\ncom/en-us/data-center/nvlink/. Accessed:\n2024-10-26.\nOpenAI. Chatgpt (gpt-4), 2024. URL https://www.\nopenai.com/research/gpt-4. Accessed: 2024-\n08-02.\n\n=== Page 13 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nPan, X., Li, E., Li, Q., Liang, S., Shan, Y., Zhou, K., Luo,\nY., Wang, X., and Zhang, J. Instinfer: In-storage attention\noffloading for cost-effective long-context llm inference.\narXiv preprint arXiv:2409.04992, 2024.\nPatel, P., Choukse, E., Zhang, C., Shah, A., Goiri, \u00b4I., Maleki,\nS., and Bianchini, R. Splitwise: Efficient generative llm\ninference using phase splitting. In 2024 ACM/IEEE 51st\nAnnual International Symposium on Computer Architec-\nture (ISCA), pp. 118\u2013132. IEEE, 2024.\nPCI-SIG.\nPCI-SIG\nReleases\nPCIe\n4.0,\nVer-\nsion\n1.0,\n2017.\nURL\nhttps://pcisig.\ncom/pci-sig-releases-pcie%C2%\nAE-40-version-10.\nPope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury,\nJ., Heek, J., Xiao, K., Agrawal, S., and Dean, J. Efficiently\nscaling transformer inference. Proceedings of Machine\nLearning and Systems, 5:606\u2013624, 2023.\nQin, R., Li, Z., He, W., Zhang, M., Wu, Y., Zheng, W., and\nXu, X. Mooncake: Kimi\u2019s kvcache-centric architecture\nfor llm serving. arXiv preprint arXiv:2407.00079, 2024.\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero:\nMemory optimizations toward training trillion parameter\nmodels. In SC20: International Conference for High Per-\nformance Computing, Networking, Storage and Analysis,\npp. 1\u201316. IEEE, 2020.\nRen, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O.,\nYang, S., Zhang, M., Li, D., and He, Y. {Zero-offload}:\nDemocratizing {billion-scale} model training. In 2021\nUSENIX Annual Technical Conference (USENIX ATC\n21), pp. 551\u2013564, 2021.\nRoziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I.,\nTan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., et al.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950, 2023.\nShareGPT.\nSharegpt\nvicuna\nunfiltered\ndataset.\nhttps://huggingface.co/datasets/\nanon8231489123/ShareGPT_Vicuna_\nunfiltered, 2023. Apache 2.0 License.\nSheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen,\nB., Liang, P., R\u00b4e, C., Stoica, I., and Zhang, C. Flexgen:\nHigh-throughput generative inference of large language\nmodels with a single gpu. In International Conference\non Machine Learning, pp. 31094\u201331116. PMLR, 2023.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,\nJ., and Catanzaro, B.\nMegatron-lm: Training multi-\nbillion parameter language models using model paral-\nlelism. arXiv preprint arXiv:1909.08053, 2019.\nSong, Y., Mi, Z., Xie, H., and Chen, H. Powerinfer: Fast\nlarge language model serving with a consumer-grade gpu.\narXiv preprint arXiv:2312.12456, 2023.\nSrivatsa, V., He, Z., Abhyankar, R., Li, D., and Zhang, Y.\nPreble: Efficient distributed prompt scheduling for llm\nserving. 2024.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023b.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\nvLLM Team. Performance update: Bringing vllm to the\nnext level.\nhttps://blog.vllm.ai/2024/09/\n05/perf-update.html, 2024. Accessed: 2024-10-\n14.\nXuanlei, Z., Jia, B., Zhou, H., Liu, Z., Cheng, S., and You,\nY. Hetegen: Efficient heterogeneous parallel inference for\nlarge language models on resource-constrained devices.\nProceedings of Machine Learning and Systems, 6:162\u2013\n172, 2024.\nXue, F., Chen, Y., Li, D., Hu, Q., Zhu, L., Li, X., Fang, Y.,\nTang, H., Yang, S., Liu, Z., et al. Longvila: Scaling long-\ncontext visual language models for long videos. arXiv\npreprint arXiv:2408.10188, 2024.\nYu,\nC.,\nLee,\nS.,\nXu,\nR.,\nLin,\nW.,\nGorthy,\nP.,\nand Liaw, R.\nBatch llm inference on anyscale\nslashes aws bedrock costs by up to 6x, October\n2024. URL https://www.anyscale.com/blog/\nbatch-llm-inference-announcement.\nAc-\ncessed: 2024-10-30.\nYu, G.-I., Jeong, J. S., Kim, G.-W., Kim, S., and Chun, B.-\nG. Orca: A distributed serving system for {Transformer-\nBased} generative models. In 16th USENIX Symposium\non Operating Systems Design and Implementation (OSDI\n22), pp. 521\u2013538, 2022.\nYuan, Z., Shang, Y., Zhou, Y., Dong, Z., Xue, C., Wu, B.,\nLi, Z., Gu, Q., Lee, Y. J., Yan, Y., et al. Llm inference\nunveiled: Survey and roofline model insights.\narXiv\npreprint arXiv:2402.16363, 2024.\n\n=== Page 14 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nZhao, J., Wan, B., Peng, Y., Lin, H., and Wu, C. Llm-\npq: Serving llm on heterogeneous clusters with phase-\naware partition and adaptive quantization. arXiv preprint\narXiv:2403.01136, 2024.\nZhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M.,\nWright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al.\nPytorch fsdp: experiences on scaling fully sharded data\nparallel. arXiv preprint arXiv:2304.11277, 2023.\nZheng, L., Li, Z., Zhang, H., Zhuang, Y., Chen, Z., Huang,\nY., Wang, Y., Xu, Y., Zhuo, D., Xing, E. P., et al. Alpa:\nAutomating inter-and {Intra-Operator} parallelism for\ndistributed deep learning. In 16th USENIX Symposium\non Operating Systems Design and Implementation (OSDI\n22), pp. 559\u2013578, 2022.\nZheng, L., Yin, L., Xie, Z., Huang, J., Sun, C., Yu, C. H.,\nCao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., et al.\nEfficiently programming large language models using\nsglang. arXiv preprint arXiv:2312.07104, 2023.\nZhong, Y., Liu, S., Chen, J., Hu, J., Zhu, Y., Liu, X., Jin,\nX., and Zhang, H. Distserve: Disaggregating prefill and\ndecoding for goodput-optimized large language model\nserving. arXiv preprint arXiv:2401.09670, 2024.\nZhu, K., Zhao, Y., Zhao, L., Zuo, G., Gu, Y., Xie, D., Gao,\nY., Xu, Q., Tang, T., Ye, Z., et al. Nanoflow: Towards\noptimal large language model serving throughput. arXiv\npreprint arXiv:2408.12757, 2024.\nA\nPERFORMANCE MODEL\nIn this section, we examine the trade-offs of various paral-\nlelism strategies by developing an analytical performance\nmodel. We break down the model\u2019s inference time into\nmultiple components and analyze the impact of each paral-\nlelism type on these components. The results reveal that the\nproportion of these components differs across workloads,\nresulting in distinct scaling behaviors for each parallelism\nstrategy. Table 2 lists the notations used in our analysis. We\nassume the data type is float16.\nTable 2. Notations\nN\nnumber of output tokens\nT linear\ndm\ntime of moving weights\nr\nnumber of requests\nT attn\ndm\ntime of moving kvcache\nb\n(global) batch size\nT linear\nc\ntime of computation\ns\naverage sequence length\nT attn\nc\ncomputation of attention\nhq\nnumber of heads\nTnw\ntime of communication\nd\nhead dimension\nhkv\nnumber of KV heads\nL\nnumber of layers\nW\n#parameters of one layer\nPP\npipeline parallel degree\nDP\ndata parallel degree\nTP\ntensor parallel degree\nTable 3. Different components of the runtime of a forward pass.\nThe batch size b representing the batching effect is emphasized.\nT linear\ndm\nT linear\ncomp\nT attn\ndm\nT attn\ncomp\nTnw(TP)\nPrefill\n2W\nBHBM\n2bW s\nFLOPS\n2bs(hq+2hkv)d\nBHBM\nbhqs2d2\nFLOPS\n4bshqd\nBar(T P )\nDecode\n2W\nBHBM\n2bW\nFLOPS\n4bshkvd\nBHBM\n2bhqsd2\nFLOPS\n4bhqd\nBar(T P )\nA.1\nRuntime Break-Down\nThe runtime of each decoding layer can be divided into three\ncomponents: 1) data movement (Tdm) from GPU global\nmemory (HBM) to compute units, which includes transfer-\nring weights (T linear\ndm ) and KV cache (T attn\ndm), 2) computation\nTcomp, including T linear\ncomp and T attn\ncomp, and 3) communication\ncost Tnw (nw stands for network), primarily arising from the\nall-reduce operation in tensor parallelism. Based on the roof-\nline model, the runtime of each layer can be approximated\nas TL = max(T linear\ndm , T linear\ncomp) + max(T attn\ndm, T attn\ncomp) + Tnw.\nData Movement.\nThe runtime of data movement can be\napproximated as transferred data volume divided by the\nbandwidth, which is the HBM bandwidth for GPUs. For lin-\near layers, the transferred data is mostly weight matrices, of\nwhich the size is 2W bytes, which is constant. For attention\nlayers, the transferred data is most the Q, K, and V matrices,\nwhich is 2bs(hq + 2hkv)d bytes in prefilling and 4bshkvd\nin decoding.\nCompute.\nThe computation time can be approximated as\nthe number of floating operations (FLOPs) divided by the\nnumber of floating operations per second of the hardware\n(FLOP/s). For linear layers, the FLOPs is proportional to\nthe weight parameters times the number of tokens, which\nis 2Wbs in prefilling and 2Wb in decoding. For attention\nlayers, most operations come from computing the attention\nscore, which is approximated as bhqs2d2 in prefilling and\n2bhqsd2 in decoding.\nCommunication.\nThe communication cost mostly comes\nfrom the all-reduce operation in tensor parallelism. It can\nbe modeled as the transferred data volume divided by the\nbandwidth. We denote it as Tnw(TP), and approximate it as\nb \u00b7 A/Bar(TP) where A is the size of the activation of one\nrequest within a batch and Bar(TP) is the all-reduce band-\nwidth. Tnw(TP) is monotonically increasing with TP as\nadditional GPUs and more replicas of activations are added\nto all-reduce. We omit the peer-to-peer communication over\nin pipeline parallel since it is negligible compared to the\nall-reduce operation of tensor parallel.\nA.2\nBatching Analysis\nBatching is critical in decoding. It significantly affects the\nlatency and throughput. Batch size represents how many\n\n=== Page 15 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nTP1DP8\nTP2DP4\nTP4DP2\nTP8DP1\n0.00\n0.25\n0.50\n0.75\n1.00\nRuntime per Request\nOOM\nload weight\ncompute\nallreduce\n0\n50\n100\n150\nBatch Size\nBatch Size\nFigure 15. How data parallelism affects the decoding throughput.\nData parallelism has minimal communication overhead but suffers\nfrom caused by inefficient memory access caused by duplicating\nmodel weights. Model duplicates occupy more GPU memory,\nleaving less space for KV cache and smaller batch sizes. With\nmore data parallelism, the overhead of loading data from GPU\nglobal memory to compute units significantly increases.\nrequests are processed in one forward pass, and larger batch\nsizes can amortize the cost of transferring weights, thus\nimproving the throughput.\nGlobal and micro-batch size.\nIn distributed inference\nsuch as multi-GPU settings, we define the global batch size\nb as the number of requests being actively processed by the\nwhole cluster. It is a tunable hyper-parameter that represents\nthe overall workload of the system. It is bounded by the\nmaximal batch size, which is determined by the memory\nbudget. On the other side, the micro batch size is defined\nat the device level as the batch size processed during each\nforward pass. Tensor parallelism does not affect the batch\nsize while DP and PP shrink the micro batch size.\nA.3\nParallelism Analysis\nWe consider three types of parallelism: data parallelism,\ntensor parallelism, and pipeline parallelism, and denote their\ndegree of parallelism as DP, TP, and PP respectively.\nTensor parallelism\ncan accelerate both data moving\n(T linear\ndm\nand T attn\ndm are reduced to 1/TP) and computation\nTcomp (reduced to Tcomp/TP), at the cost of all reduce over-\nhead Tnw.\nData parallelism\ndistributes the global batch size b onto\nDP micro-batches processed in parallel. The model is du-\nplicated so T linear\ndm\nremains unchanged. T attn\ndw , T linear\ncomp, T attn\ncomp,\nTnw are reduced as the batch size is smaller. Due to the\nneed to duplicate model weights, the GPU memory left for\nthe KV cache is smaller. The spare space for KV cache on\neach GPU is Mkv = M \u2212\n2LW\nT P \u00b7P P . The maximal batch size\nis\nbmax = DP \u00b7 Mkv \u00b7 TP \u00b7 PP\n4Lhkvds\n= DP \u00b7 M \u00b7 TP \u00b7 PP \u22122LW\n4Lhkvds\nWhile TP and PP can super-linearly scale the batch size, DP\ncan only linearly scale the batch size. The trade-off between\nlimited batch sizes and reduced communication overhead is\nshown in Figure 15.\nPipeline parallelism\ndistributes different layers to dif-\nferent devices, and each device will have L/PP layers. It\ncannot reduce single-request latency but is more suitable for\nthroughput-oriented scenarios as it introduces less commu-\nnication overhead. However, it is not the ultimate answer\nof high-throughput applications because of an important\nobservation that pipeline parallelism harms maximal batch\nsize. A tricky nuance is that given a batch size b, pipeline\nparallelism can only process b/PP of them simultaneously\nin order to utilize and pipeline all PP GPUs, which is harm-\nful to batching. If the workload is not uniformly distributed\nacross GPUs, there will be bubbles, or in the worst case,\nsome GPUs might be idle. When the pipeline is fully and\nstably pipelining, each time the last pipeline stage finishes\nits L/PP layers of forward pass, a micro-batch of b/PP will\nbe finished.\nThroughput.\nThe micro-batch size on each GPU is\nb/(PP \u00b7 DP). The total runtime of generating one micro\nbatch with size b/(PP \u00b7 DP) on one DP replica (or more\nspecifically, the time of the last pipeline stage finishing a\nmicro-batch) is\nTstage = L\nPP \u00b7\n\"\nmax(T linear\ndm\nTP ,\nT linear\ncomp\nDP \u00b7 TP \u00b7 PP)+\n+ max(T attn\ndm, T attn\ncomp)\nDP \u00b7 TP \u00b7 PP\n+ Tnw(TP)\nPP \u00b7 DP\n\u0015\nThe throughput (number of processed requests per unit time)\nis b/PP/T. For simplicity, we calculate the inverse of it as\nthroughput\u22121 = Tstage\nb/PP = L\nb \u00b7\n\"\nmax(T linear\ndm\nTP ,\nT linear\ncomp\nDP \u00b7 TP \u00b7 PP)\n+ max(T attn\ndm, T attn\ncomp)\nDP \u00b7 TP \u00b7 PP\n+ Tnw(TP)\nPP \u00b7 DP\n\u0015\n(1)\nIf we approximate the roof-line model with a simplified\nadditional model, this expression can be simplified as:\nthroughput\u22121 \u221dT linear\ndm\nTP\n+ T linear\ncomp + T attn\ndm + T attn\ncomp\nDP \u00b7 TP \u00b7 PP\n+ Tnw(TP)\nPP \u00b7 DP\n(2)\n",
      "searchable_content": "page 1 seesaw high-throughput llm inference via model re-sharding qidong su123 wei zhao34 xin li3 muralidhar andoorveedu3 chenhao jiang12 zhanda zhu123 kevin song12 christina giannoula123 gennady pekhimenko123 abstract to improve the efficiency of distributed large language model llm inference various parallelization strategies such as tensor and pipeline parallelism have been proposed. however the distinct computational characteristics inherent in the two stages of llm inference prefilling and decoding render a single static parallelization strategy insufficient for the effective optimization of both stages. in this work we present seesaw an llm inference engine optimized for throughput-oriented tasks. the key idea behind seesaw is dynamic model re- sharding a technique that facilitates the dynamic reconfiguration of parallelization strategies across stages thereby maximizing throughput at both phases. to mitigate re-sharding overhead and optimize computational efficiency we employ tiered kv cache buffering and transition-minimizing scheduling. these approaches work synergistically to reduce the overhead caused by frequent stage transitions while ensuring maximum batching efficiency. our evaluation demonstrates that seesaw achieves a throughput increase of up to 1.78 1.36 on average compared to vllm the most widely used state-of-the-art llm inference engine. 1 introduction large language models llms such as the llama tou- vron et al. 2023a and gpt achiam et al. 2023 families have demonstrated exceptional performance across a wide range of tasks. beyond their prevalent use in interactive applications like chatbots openai 2024 llms are also gaining high interest in throughput-oriented offline inference workloads such as information extraction narayan et al. 2022 database querying liu et al. 2024 and knowledge graph processing edge et al. 2024 . unlike interactive applications where low latency is crucial these offline in- ference tasks prioritize high throughput over response time. these offline inference workloads are widely adopted in in- dustry kamsetty et al. 2023 yu et al. 2024 dell technolo- gies 2024 chan et al. 2024 leading mlperf to develop benchmarks specifically for them mlcommons 2024 . in this work we focus on improving inference efficiency for offline throughput-oriented llm inference workloads. as llms often exceed the memory capacity of individual gpus parallelization is essential for their deployment ben- nun hoefler 2019 shoeybi et al. 2019 . several paral- lelization strategies including tensor parallelism shoeybi et al. 2019 and pipeline parallelism narayanan et al. 2019 huang et al. 2019 have been proposed each presenting distinct trade-offs in memory efficiency inter-device com- munication and computational efficiency. tensor paral- lelism distributes model weights across devices but suffers 1 university of toronto 2 vector institute 3 centml 4 stan- ford university tp1pp8 tp2pp4 tp4pp2 tp8pp1 0.00 0.25 0.50 0.75 1.00 normalized time communication compute weight transfer a prefill tp1pp8 tp2pp4 tp4pp2 tp8pp1 0.00 0.25 0.50 0.75 1.00 normalized time communication compute weight transfer b decode figure 1. breakdown of execution time for the prefill and decode stages for llama2-13b inference on 8 l4 gpus the global batch size is 16. pipeline parallelism further divides the data into micro-batches of size 16 pp to fully utilize pipelining . from high communication costs due to frequent all-reduce operations at each layer pope et al. 2023 chang et al. 2024 . the communication cost becomes particularly severe in systems connected via pcie dell technologies 2023 or with partial high-speed connections nvidia corporation 2020 . in contrast pipeline parallelism partitions the model into sequential stages reducing inter-device communica- tion by passing only activations between them. however to enable pipelining each data batch needs to be divided into micro-batches leading to extra execution overheads since every micro-batch repeatedly loads weights into the compute units see section 3.1 for details . while numerous studies have proposed methods to optimize parallelization strategies for llms miao et al. 2023 kwon et al. 2023 li et al. 2023 pope et al. 2023 prior works typically rely on a single static configuration throughout arxiv 2503.06433v1 cs.dc 9 mar 2025 page 2 seesaw high-throughput llm inference via model re-sharding the entire generation process. however our findings indi- cate that this one-size-fits-all approach is often inefficient for throughput-oriented llm inference because it fails to leverage the distinct patterns between the two stages in llm generation the prefill stage where the input sequence is pro- cessed at once to produce the initial token and the decode stage where subsequent tokens are generated sequentially based on prior tokens. these two stages exhibit fundamen- tally different computational characteristics yuan et al. 2024 . during the prefill stage multiple tokens from the input prompt are processed simultaneously making com- putation and communication the dominant contributors to runtime. in contrast the decode stage processes one token at a time for each sequence increasing the relative time spent on weight transfer. this difference indicates that the optimal parallelization strategy for each stage may also vary. to illustrate the performance limitations of applying a uni- form parallelization strategy for both prefill and decode we measure the execution time of each stage under various combinations of tensor and pipeline parallelism as shown in figure 1. in the prefill stage as the degree of tensor par- allelism increases the communication overhead increases significantly due to additional gpus participating in all- reduce operations. as a result tensor parallelism performs significantly worse than pipeline parallelism. in contrast during the decode stage pipeline parallelism is slower than tensor parallelism largely due to increased weight trans- ferring overhead caused by micro-batching required for pipelining see section 3.1 for more details . therefore we need stage-specific parallelization strategies to provide better llm inference throughput. an existing approach is disaggregated prefill-decode zhong et al. 2024 qin et al. 2024 which assigns prefill and de- code computation to different gpu instances. the prefill instances and decode instances form a two-stage pipeline to serve inference requests. therefore the overall through- put of disaggregated prefill-decode is constrained by the slower of the two stages and balancing throughput between these two stages is essential. the key drawback of disag- gregated prefill-decode is that it can cause large amounts of pipeline bubbles under resource-constrained environments. for example when deploying a 70b model on 8 40gb gpus even the most balanced configuration results in a 6 difference in throughput between the prefill and decode stages. in this setup the decode stage operates at one-sixth the throughput of the prefill stage resulting in a significant bottleneck at the prefill stage that slows down the entire system see section 3.2 for details . to address these challenges we present seesaw a high- throughput llm inference engine that dynamically recon- figures parallelization strategies between the prefill and de- code stages. the key idea behind seesaw is model re- time gpu max seqs time gpu max seqs time cpu max seqs seqs seqs seqs under-utilize too frequent transitions a prefill-prioritizing b decode-prioritizing c tiered kv cache buffering transition-minimizing scheduling p d p d p d p d p d p d p d p d gpu max seqs p d p d p figure 2. different scheduling policies considering transition over- head. decoding throughput is positively correlated with the num- ber of sequences in gpu memory the maximal batch size which is highlighted as light green area. sharding a novel technique that dynamically re-partitions model weights and kv cache 1 between prefill and decode stages. by tailoring parallelization strategies to the dis- tinct computational demands of each stage seesaw reduces communication overhead during the prefill stage while en- hancing memory efficiency in the decode stage resulting in a substantial increase in overall throughput. however the overhead associated with model re-sharding can be high due to frequent transitions between prefill and decode. to maximize throughput existing systems typi- cally adopt prefill-prioritized scheduling yu et al. 2022 kwon et al. 2023 which interleaves prefill and decode stages across batches to achieve continuous batching. yet as illustrated in figure 2 a integrating this approach with model re-sharding can result in significant overhead due to frequent transitions between prefill and decode. on the other hand decode-prioritized scheduling nvidia 2024a completes all decode steps for a batch before proceeding to the next resulting in lower re-sharding overhead. however as depicted in figure 2 b this method suffers from low resource utilization due to smaller batch sizes. to overcome this constraint and achieve both minimal re-sharding overhead and large batch size we propose two synergetic techniques tiered kv cache buffering and transition-minimizing scheduling. tiered kv cache buffering leverages cpu memory as auxiliary storage for the kv cache enabling seesaw to store the kv cache for a large number of prefill requests. transition-minimizing scheduling reduces re-sharding overhead by minimizing the number of transitions to the decode stage. seesaw transi- tions from prefill to decode only after the cpu kv cache is full. during decoding the large number of kv cache in the cpu buffer enables seesaw to perform decode with large batch sizes and thus enabling high throughput. as depicted 1 the tensors cached for each sequence s decoding steps. page 3 seesaw high-throughput llm inference via model re-sharding in figure 2 c this approach maintains the maximal batch size during the decode stage while significantly reducing the frequency of stage transitions thereby minimizing re- sharding overhead. additionally to mitigate the overhead of kv cache transfers between cpu and gpu seesaw em- ploys asynchronous pipelining to overlap data transfers with computation. in summary we make the following contributions. we identify and quantitatively analyze the different pref- erences for parallelisms in the prefill and decode stages of throughput-oriented llm inference tasks. our analysis comprehensively accounts for data movement computa- tion and communication costs. we propose dynamic model re-sharding a novel technique that dynamically reconfigures the parallelization strategies for prefill and decode stages. we address the challenge of transition overhead in model re-sharding with continuous batching by introducing tiered kv cache buffering and transition-minimizing scheduling. based on these tech- niques we implement seesaw a high-throughput offline inference system that optimizes parallelization strategies for each llm inference stage. we conduct a comprehensive evaluation of seesaw across a variety of workloads and hardware configurations. our results show seesaw achieves an average speedup of 1.36 and a throughput improvement of up to 1.78 compared to the state-of-the-art llm inference engines. 2 background 2.1 llm inference transformer architecture. modern large language mod- els are based on the transformer architecture vaswani et al. 2017 which typically consists of multiple identical decoder layers openai 2024 . each layer includes several linear layers and an attention layer. the weights of the linear layers account for the majority of the model s parameters. auto-regressive generation. llm inference follows an auto-regressive paradigm bengio et al. 2000 which takes an input prompt and generates a sequence of output tokens. this process is divided into two stages prefilling which processes the input tokens and decoding which generates a token per step. these stages exhibit distinct computational properties zhong et al. 2024 yuan et al. 2024 . prefilling processes the prompt that are typically hundreds to thou- sands of tokens long. the computation and communication costs both of which scale with the number of tokens domi- nate the runtime during this stage. since the cost of loading weights is amortized over a larger set of tokens the overall performance is primarily bound by compute and or commu- nication. in contrast decoding processes only the newly generated tokens in each auto-regressive step and has com- paratively smaller computation in each step. therefore the cost for loading the weight data from off-chip memory to computation units has a relatively higher percentage. in each generation step the intermediate tensors k and v in each attention operator can be cached for reuse in the future gen- eration which is called key-value cache kv cache pope et al. 2023 . while being able to accelerate computation it occupies a substantial amount of gpu memory which is proportional to the total number of tokens. 2.2 llm inference optimization parallelism. as the size of llms grows the memory capacity on a single gpu becomes insufficient. conse- quently various techniques are developed to partition mod- els onto multiple gpus zheng et al. 2022 . these paral- lelization strategies can be classified as 1 inter-operator which places different operators or layers across multiple gpus overlapping them with pipelining known as pipeline parallelism pp huang et al. 2019 narayanan et al. 2019 li et al. 2023 and 2 intra-operator which partitions different dimensions of tensors involved in computation including data parallelism srivatsa et al. 2024 tensor parallelism shoeybi et al. 2019 etc. data parallelism du- plicates models on different devices and dispatches requests among them. tensor parallelism shards model weights and each device performs a portion of the computation then aggregates these partial results to produce the final output. batching. batching more tokens in a single forward pass increases inference efficiency by for example amortizing the time required to load model weights sheng et al. 2023 fang et al. 2021 . however its effectiveness differs be- tween the prefilling and decoding stages yuan et al. 2024 he zhai 2024 agrawal et al. 2023 . in decoding where weight-loading overhead occupies a larger portion of the runtime batching significantly boosts throughput by effec- tively amortizing this overhead. conversely in the prefilling stage batching has a less pronounced impact since the token count in input prompts is generally sufficient to keep the process compute-bound. overall larger batch sizes yield higher throughput though the maximum batch size is lim- ited by available gpu memory as it requires additional space for activations and the kv cache. continuous batching and scheduling. continuous batching is an essential optimization for throughput-oriented llm inference yu et al. 2022 kwon et al. 2023 . by batching multiple sequences at the token level it allows the system to onboard new sequences and clear the kv cache of completed sequences at any generation step. this approach enables prefill-prioritizing scheduling which removes se- quences as they finish frees up their kv cache and eagerly page 4 seesaw high-throughput llm inference via model re-sharding tensor parallel pipeline parallel l1 l1 gpu1 gpu2 gpu1 gpu2 gpu1 gpu2 gpu1 gpu2 l1 1 2 l1 2 2 l1 l1 prefill decode s1 s2 s1 s2 s2 s3 l1 s1 l2 s1 l2 s2 0.5x load weight 0.5x compute allreduce per-sequence time time 1x load weight 0.5x compute s2 s1 s1 s2 s1 s1 s2 s1 s2 s2 l2 s1 l2 s2 l1 s1 time 0.5x load weight 0.5x compute allreduce per-sequence time 1x load weight 0.5x compute layer token sequence batch allreduce pp is better because of lower allreduce overhead tp is better because of more effective batching proportion of allreduce is larger in prefilling proportion of loading weights is higher in decoding tp shards weights so load weights is parallelized l2 1 2 l2 2 2 s2 s1 s2 s1 l1 1 2 l1 2 2 l2 1 2 l2 2 2 pp has smaller batch sizes figure 3. different effects of tensor and pipeline parallelisms on prefilling and decoding. tensor parallelism incurs all-reduce overhead which has a higher percentage in prefilling therefore pipeline parallelism is better for prefilling. conversely pipeline parallelism splits batches into smaller micro-batches which leads to more forward passes and repetitive loading weights which is insufficient in decoding. schedules the prefilling of new sequences whenever gpu memory becomes available. this strategy maximizes the number of concurrent sequences being processed resulting in higher throughput. another alternative is to use decode- prioritizing scheduling which minimizes the frequency of transitions. instead of scheduling to prefilling eagerly this approach waits until all sequences in a batch have finished decoding before initiating the next round of prefilling. how- ever this scheduling policy results in suboptimal decoding throughput agrawal et al. 2024 . 3 motivation and analysis in this section we provide an in-depth analysis of two key observations we identify from figure 1 in section 1 1 tensor parallelism often exhibits significantly worse per- formance than pipeline parallelism during the prefill stage due to its substantial communication overhead 2 pipeline parallelism tends to fall short in the decode stage owing to the considerable weight loading overhead it incurs. we then argue that a dynamic parallelization strategy is essential to attain optimal performance across both stages. given the importance of batching in throughput-oriented tasks it can be useful to consider how different paralleliza- tion strategies impact the maximum batch size rather than assuming batch size as a tunable parameter as is often done in online-serving contexts such as distserve zhong et al. 2024 and sarathi-serve agrawal et al. 2024 . 3.1 parallelism analysis observation 1 tensor parallelism incurs substantial communication overhead during the prefill stage. in tensor parallelism each device performs a part of computa- tion and aggregate the partial result. the activations at each layer are synchronized across all gpus using all-reduce operations. the overhead associated with this operation can be quantified as tokens activation size all-reduce bandwidth where all-reduce bandwidth refers to the rate of data transfer during all-reduce operations calculated as the size of the tensor being all-reduced divided by the all-reduce runtime. as the degree of tensor parallelism increases the proportion of execution time of all-reduce operations grows substan- tially. this growth is attributed to two main factors. first while model weights are partitioned activations in tensor parallelism remain fully replicated across gpus leading to a constant activation size regardless of the degree of tensor parallelism. second all-reduce bandwidth decreases as the number of gpus grows due to more complex communi- cation schemes. therefore increasing the degree of tensor parallelism not only fails to reduce the traffic of all-reduce operations but further limits the communication bandwidth resulting in escalated communication overhead. this is- sue is particularly pronounced in the prefill stage where a large number of tokens are processed simultaneously mak- ing communication overhead the primary bottleneck. thus tensor parallelism tends to perform worse than pipeline par- allelism due to its large communication overhead. observation 2 pipeline parallelism suffers from signifi- cant weight transferring overhead in the decode stage. pipeline parallelism distributes model layers sequentially across devices with each device responsible for processing a set of consecutive layers before passing the output to the next device. due to the auto-regressive nature of llm infer- ence a sequence cannot enter the pipeline until its preceding token is generated. as a result at any given time step a sequence can appear in only one stage of the pipeline mak- ing the batches processed by each device mutually exclusive. however the total number of sequences that the pipeline can handle at a time referred to as the global batch size page 5 seesaw high-throughput llm inference via model re-sharding is constrained by the size of kv cache. given the mutual exclusion of batches at each device pipeline parallelism can process only approximately 1 pp of the global batch per forward pass. we denote this reduced batch size in pipeline parallelism as the micro-batch size. dividing batches into micro-batches increases the number of llm forward passes required to process the same amount of requests. specifically a pipeline parallelism degree of pp necessitates pp times more forward passes for a given global batch. this repeated execution degrades inference performance as model weight matrices must be loaded from global memory repeatedly. this inefficiency is especially significant in the decode stage where weight-loading over- head accounts for a substantial portion of total execution time. as a result pipeline parallelism generally underper- forms relative to tensor parallelism in the decode stage due to the amplified weight loading overhead. discussion on data parallelism. unlike tensor and pipeline parallelism which distribute the model across de- vices data parallelism distributes the data while duplicating the model. while data parallelism has minimal commu- nication overhead it has two key disadvantages 1 the volume of weight transferring is higher by the number of duplicates compared to tensor parallelism and 2 it occu- pies more gpu memory reducing the available space for the kv cache and thus limiting the maximum batch size re- sulting in lower throughput. data parallelism can be applied orthogonally alongside both tensor and pipeline parallelism. we do not dynamically adjust data parallelism which will be explained in section 4.1. conclusion no one-size-fits-all when comparing these three parallelism strategies for high-throughput llm infer- ence a key observation is that prefilling and decoding stages benefit from different parallelism approaches. this differ- ence arises from the distinct characteristics of each stage as illustrated in figure 3. tensor parallelism is preferred for decoding due to its ability to efficiently accelerate weight matrix loading. however it incurs significant communica- tion overhead as it requires all-reduce operations at each layer. in contrast pipeline and data parallelism have much lower communication overhead making them preferable for prefilling. however their decoding throughput is limited by inefficient batching and additional weight-loading overhead. to quantitatively analyze the trade-offs across different par- allelisms we model the average runtime per sequence the inverse of throughput as follows. derivations and further details are provided in the appendix a. t t linear dm tp t attn dm tcomp dp tp pp tcomm tp pp dp here t linear dm represents data movement for linear layers pri- gpu0 gpu1 gpu2 gpu3 gpu4 gpu5 gpu6 gpu7 prefill worker decode worker prefill throughput decode throughput 0.0 0.5 1.0 1.5 2.0 throughput reqs sec decode 8 gpus decode 4 gpus prefill 4 gpus throughput mismatch figure 4. an example of spatially disaggregating prefilling and decoding has a restricted search space. deploying a 70b model on eight 40gib gpus allows only one disaggregation strategy four gpus for prefilling and four for decoding. however this causes severe throughput mismatch between the two stages. marily model weights t attn dm represents data movement for attention layers primarily kv cache tcomp represents computation time tcomm represents communication time. note that tcomm is a monotonically increasing function with respect to tp as all-reduce operations require more time as tp increases. tensor parallelism can effectively accelerate loading model weights which is t linear dm while pipeline and data parallelism cannot. on the other hand pipeline and data parallelism effectively reduce the overhead of communication while tensor parallelism contrarily increases the communication overhead. in prefilling t linear dm is negligible and tcomm be- comes larger so pipeline and data parallelisms are more preferred while in decoding t linear dm occupies a larger pro- portion so tensor parallelism is more advantageous. 3.2 why not disaggregate prefilling and decoding spatially disaggregating prefilling and decoding with sepa- rate hardware resources as done in online serving systems such as distserve zhong et al. 2024 and mooncake qin et al. 2024 is one approach to separately select paralleliza- tion strategies for prefilling and decoding. sequences are first processed by the devices dedicated for prefilling before being transferred to decoding devices. however there are two obstacles when applying prefill- decode disaggregation to purely throughput-oriented sce- narios. first since the overall throughput is bound by the slower stage the throughput of prefilling and decod- ing needs to be matched by adjusting the devices allocated for each stage. however it can be impractical in resource- constrained scenarios. as shown in figure 4 to deploy a 70b model which takes 140gib memory for model weights on eight 40gib gpus there is only one disag- gregation strategy that is four gpus for prefilling and four page 6 seesaw high-throughput llm inference via model re-sharding l1 kv1 l2 kv2 l1 1 2 kv1 1 2 l2 1 2 kv2 1 2 l1 2 2 kv1 2 2 l2 2 2 kv2 2 2 model resharding gpu 1 gpu 2 prefill pipeline parallelism decode tensor parallelism figure 5. model weights and kv cache need to be re-sharded when switching between different parallelism. for decoding2. however it causes severe throughput mis- match where prefilling has more than 6 higher throughput than decoding. second disaggregation duplicates the model weights similarly to data parallelism bringing similar draw- backs such as limited kv cache space and increased weight transfer. as a result decoding throughput with four gpus is only 15 of that with eight gpus. in conclusion although disaggregation allows for select- ing different parallelization strategies for each stage the throughput mismatch between stages and limited resources allocated to each can lead to suboptimal performance. this calls for a method that offers flexibility in parallelization while maximizing hardware resource utilization. 4 seesaw key ideas 4.1 dynamic model re-sharding observing that prefilling and decoding have distinct pref- erences for parallelism we propose a technique called dy- namic model re-sharding. this technique enables the se- lection of different parallelism strategies for each stage and automatically transitions between them. this approach ex- pands the configuration space allowing for separate opti- mization of the two stages potentially improving overall throughput compared to using a single configuration. in the following paragraphs we denote the parallelization strategy used in prefilling as cp and that in decoding as cd. to support transitions between different parallelization con- figurations the cluster must rearrange the data stored on each device to align with the new parallelism which involves both model weights and kv cache as illustrated in figure 5. in seesaw model weights are re-sharded by reloading the re- quired shards from cpu memory and kv cache re-sharding is performed through cpu shared memory. the inter-device movement of tensors incurs overhead. to mitigate this re-sharding cost we design an asynchronous pipeline to overlap data transfer with computation as de- tailed in section 5.2. discussion on data parallelism. unlike switching be- tween tensor and pipeline parallelism adjusting the degree 2 at least four gpus 160 gib memory are needed to fit the model weights. asynchronous swap in cpu gpus prefill cpu gpus cpu memory is empty cpu gpus decoding cpu gpus cpu memory is filled cpu gpus prefill warm up figure 6. tiered kv cache buffering and transition-minimizing scheduling and the change of kv cache occupancy. request cp cd gpu 1 worker 1 gpu 2 worker 2 cpu write back kv after prefill load kv before decode cp cp prefill cd cd decode cpu kv cache is empty cpu kv cache is full scheduler figure 7. kv cache re-sharding is completed during swapping leveraging cpu shared memory. of data parallelism alters the proportion of gpu memory al- located to model weights versus kv cache. this adjustment increases system complexity or necessitates additional data movement between the cpu and gpu. therefore we only dynamically adjust tensor and pipeline parallelism. 4.2 tiered kv cache buffering and transition-minimizing scheduling challenge transition overhead. in practice dynamic model resharding encounters an obstacle of transition overhead which is amplified by the widely-used contin- uous batching and prefill-prioritizing scheduling. prefill- prioritizing scheduling eagerly schedules new prefilling tasks causing frequent transitions between the two stages. as a result directly applying model re-sharding with this in- terleaved prefill-decode scheduling policy would introduce significant re-sharding overhead. on the other hand decode- prioritizing scheduling minimizes the frequency of transi- tions but results in suboptimal decoding throughput. other compromise solutions involve setting a threshold-based ap- proach for managing the prefill-decode transition cheng et al. 2024 . however they still involve a trade-off be- tween reducing transition overhead and maximizing decod- ing throughput. to address this problem we propose 1 tiered kv cache buffering which leverages cpu memory offloading 2 transition-minimizing scheduling policy. these two syn- ergistic techniques prevent frequent stage transitions and maintain a high decoding throughput. tiered kv cache buffering uses cpu memory as auxiliary storage for the kv cache enabling the pre-computation of a large batch of prefilling consecutively. during the page 7 seesaw high-throughput llm inference via model re-sharding prefill stage the generated kv cache is offloaded to cpu kv cache storage freeing it from the limitations of gpu memory space. during decoding continuous batching runs as normal except that new sequences are on-boarded by swapping in its kv cache from the cpu memory. transition-minimizing scheduling controls the transition to only happen when the cpu kv storage is either full or empty. during prefill once the cpu kv cache storage is fully utilized re-sharding is triggered and the cluster transitions to decoding. during decoding gpus continue processing requests and loading kv cache from cpu mem- ory keeping gpu kv cache fully utilized for high decod- ing throughput. when the entire cpu kv cache has been transferred to gpu memory the cluster switches back to prefilling. the whole process is illustrated in figure 6. kv cache re-sharding occurs throughout this process. as illustrated in figure 7 in a multi-gpu setup the cpu kv cache storage is shared among all gpus. during swap-out each gpu pushes its shard based on cp of the generated kv cache to the shared cpu storage where these shards collectively form the complete kv cache. during swap- in each gpu retrieves its required kv shard based on cd from the shared storage. we implement the shared kv cache using shared memory of the operating system. 5 system design and implementation 5.1 scheduler-worker architecture in order to support dynamically switching parallelization configurations for prefilling and decoding we build seesaw a new llm inference engine designed for high-throughput llm inference. the overall architecture of seesaw fol- lows a single-scheduler multi-worker design. the sched- uler manages all generation requests organizes them into batches and sends instructions to the workers. to fully utilize pipelining each decoding step processes 1 pp of the sequences in gpu kv storage. once a batch is formed it is sent to workers through shared queues. each worker is re- sponsible for controlling a single gpu and maintains a task queue to receive and execute instructions sequentially. this architecture facilitates the implementation of asynchronous features such as pipeline parallelism and the asynchronous pipeline for tiered kv cache buffering. 5.2 asynchronous pipeline while re-sharding and tiered kv cache buffering offer sub- stantial benefits they also introduce new overhead related to moving model weights and kv cache. the overhead of reloading model weights remains constant relative to batch size allowing it to be amortized with larger batches. in con- trast swapping the kv cache incurs overhead proportional to batch size making it harder to amortize. fortunately swap out qkv_proj attn ffn swap in main thread prefetcher thread scheduler prefill scheduler decode decode cpu kv gpu kv non-blocking copy cpu kv is empty cpu kv is full figure 8. async pipeline of seesaw swap-in overlaps with prefill computation while swap-out occurs in a separate asynchronous prefetcher thread. these overheads can be mitigated through computation- communication overlap. we implement an asynchronous pipeline to overlap kv cache transfer with ongoing compu- tation as illustrated in figure 8. overlap swap-out with computation. the kv cache generated during the prefilling stage is not used until decod- ing begins allowing the kv cache swap-out to overlap with other computations during prefilling. although cpu-gpu data transfer is relatively slow due to pcie bandwidth limi- tations it can still be overlapped with computation given the high flops involved in prefilling. in practice cpu-gpu data transfer can only overlap with computation when using pinned memory but shared mem- ory cannot be pinned alband 2023 . to address this we split the transfer into two stages gpu to pinned memory overlapped with computation and then pinned to shared memory which is a host-side operation that also runs con- currently with gpu kernels. asynchronous swap-in. we implement swap-in using a background thread called the prefetcher on each worker op- erating in a fully asynchronous paradigm. the prefetcher is controlled directly by the scheduler and runs independently of the main thread whether the main thread is handling pre- filling or decoding. in each iteration the scheduler creates new prefetching tasks when there are free slots in the gpu kv store. once the prefetcher completes moving the kv cache for certain sequences it notifies the scheduler via a shared queue allowing those sequences to be scheduled for decoding tasks later. as long as the output length is not too short the swap-in can also be well overlapped. bandwidth-aware kv cache layout. the data layout of the kv cache significantly impacts the bandwidth efficiency of data movement. there are two common layouts for stor- ing kv cache seq len num heads head dim nhd and num heads seq len head dim hnd . nhd is less opti- mal for memory access because tensor parallelism shards the kv cache along the h dimension number of heads which is the second-to-last dimension leading to more non- contiguous memory access. therefore we use the hnd page 8 seesaw high-throughput llm inference via model re-sharding layout for storing the kv cache in cpu memory. 6 evaluation in this section we evaluate the performance of seesaw under a variety of hardware configurations and workloads. 6.1 experiment settings hardware. we use three types of gpus nvidia a10 l4 and a100. the a10 and l4 are deployed on aws ec2 instances g5.48xlarge and g6.48xlarge amazon web services 2024 and the a100 is used on gcp google cloud 2024 . gpu specifications are listed in table 1. the pcie connection for each gpu is pcie 4.0 8 providing 16 gib s bandwidth pci-sig 2017 while nvlink nvidia corporation 2024 offers a bandwidth of 600 gib s. addi- tionally we allocate 80 gib of cpu memory per gpu. model. we use three different llms with different sizes 1 a 15b variety of llama3 elinas 2024 2 codellama-34b roziere et al. 2023 3 llama2- 70b touvron et al. 2023b . they all use grouped query attention gqa ainslie et al. 2023 . for brevity we refer to them as 15b 34b and 70b respectively in the following sections. we use float16 as the data type. workload. we use two different datasets in our eval- uation namely sharegpt sharegpt 2023 and arxiv-summarization cohan et al. 2018 . they correspond to two different distributions of work- load. sharegpt is a dataset of chatting history so its input and output have comparable lengths while arxiv-summarization dataset is a summarization dataset where inputs are much longer than outputs. the characteristics of these two datasets are shown in figure 9. we sample 2000 requests from the sharegpt dataset and 500 requests from arxiv-summarization and also use constant-length workloads in section 6.5. since seesaw is purely throughput-oriented we measure the end-to-end throughput as the metrics. baselines. we use vllm 0.5.4 kwon et al. 2023 as the baseline. it is the most widely used open-source llm serving engine with wide support for different parallelisms. we also directly use the vllm s model implementation for a straightforward comparison. sglang zheng et al. 2023 and deepspeed-fastgen holmes et al. 2024 do not sup- port pipeline parallelism. tensorrt-llm nvidia 2024b is not included in the comparison because it uses a simi- lar scheduling policy as vllm and vllm demonstrates comparable performance vllm team 2024 in throughput- oriented tasks. the techniques proposed in seesaw can also be applied to modifying tensorrt-llm. table 1. gpu hardware specification gpu model memory size memory bandwidth flops nvlink a10 24 gib 600 gib s 125t l4 24 gib 300 gib s 121t a100 40 gib 1 555 gib s 312t 0 2000 4000 tokens 0 2 4 density 1e 3 input tokens output tokens a arxiv-summarization 0 2000 4000 tokens 0.0 0.5 1.0 density 1e 2 input tokens output tokens b sharegpt figure 9. input and output length distributions of the datasets we enable chunked prefill and tune the chunk size for vllm to get the optimal throughput following the practice of sarathi-serve agrawal et al. 2024 . otherwise suboptimal chunk sizes would cause severe throughput degradation. 6.2 end-to-end throughput on pcie systems first we measure the end-to-end throughput of seesaw. we sweep over all available single parallelism configurations for vllm and show the result of the best configuration. we use four gpus for the 15b model and eight gpus for the 34b and 70b models. the result is shown in figure 10 with the used parallelism labeled above each bar. on a10 compared with the highest single parallelism base- line seesaw achieves a geometrically average speedup of 1.45 with up to 1.78 speedup. on l4 seesaw achieves a geometrically average speedup of 1.29 with up to 1.52 speedup. the overall average speedup is 1.36 . the speedup is more significant on a10 because a10 has better single gpu performance than l4 while they have similar pcie inter-connection bandwidth causing a higher percent- age of communication overhead. 6.3 speedup breakdown an example figure 12 illustrates how seesaw merges the advantages of different parallelisms. using codellama34b on the arxiv-summarization dataset with four a10 gpus as an example we measured the runtime of each stage. tp4 is optimal for decoding but significantly slower for prefilling while pp4 excels at prefilling but is slower during decoding. seesaw uses a mixed parallelism strategy applying pp4 for prefilling and tp4 for decoding achieving performance comparable to the best configuration for each stage. compared to the optimal single parallelism configuration tp2pp2 with chunked prefill seesaw is still faster because 1 chunked prefill does not piggy-back all decoding steps page 9 seesaw high-throughput llm inference via model re-sharding 15b 34b 70b 15b 34b 70b 0 1 2 normalized throughput d2t2 p4- t4 d2t2p2 d2p4- d2t4 t2p4 p8- t4p2 d2p2 p4- t4 d2t2p2 d2p4- d2t4 t4p2 p8- t4p2 arxiv sharegpt vllm seesaw a end-to-end throughput on a10 15b 34b 70b 15b 34b 70b 0 1 2 normalized throughput t2p2 p4- t4 d2t4 d2p4- d2t4 t4p2 p8- t4p2 p4 p4- t4 d2t4 p8- t4p2 t4p2 p8- t4p2 arxiv sharegpt vllm seesaw b end-to-end throughput on l4 figure 10. end-to-end throughput comparison on pcie systems. the used parallelization strategies are labelled above each bar. labels such as p4 d4 represent the parallelization strategies for prefilling and decoding respectively in seesaw. arxiv sharegpt 0.0 0.5 1.0 throughput normalized 0.61 0.62 0.89 0.82 1.00 1.00 1.00 1.13 vllm pcie seesaw pcie vllm nvlink seesaw nvlink figure 11. throughput comparison on a100. leaving some purely decoding steps and 2 chunked prefill with tp2pp2 is slower than prefilling with pp4. 6.4 end-to-end throughput on a100 speedup on a100 nvlink the nvlink interconnec- tion across a100 gpus significantly reduces the all-reduce overhead and further scales tensor parallelism. usually tensor parallelism alone is enough to achieve optimal per- formance when there are no more than four gpus. never- theless there is still a noticeable percentage of all-reduce overhead in prefilling when tensor parallelism scales be- yond four gpus. seesaw can still provide speedup in this case. as shown in figure 11 seesaw still achieves a 13 throughput increase over vllm for the sharegpt dataset on llama3-70b on eight a100s. speedup on a100 pcie besides a100 sxm with nvlink inter-connection there is also another version of a100 that is inter-connected with pcie links where seesaw can achieve noticeable speedup. as shown in figure 11 see- saw provides 46 speedup on arxiv-summarization and 30 speedup on sharegpt. seesaw brings the per- formance of the a100 pcie version much closer to the per- formance level of the nvlink version. vllm gets roughly 60 throughput on a100 pcie compared with a100 sxm while seesaw boosts it up to 82 89 . tp4 pp4 p4- t4 tp2pp2 chunked prefill 0 500 1000 1500 end-to-end time s prefill mix decode other figure 12. speedup breakdown. mix represents batches contain- ing both prefilling and decoding when chunked prefill is enabled. we disable chunked prefill for tp4 and pp4 in order to show the reference prefilling and decoding time. tp2pp2 with chunked prefill is the optimal parallelism for vllm. 0.0 0.1 0.2 0.3 d p 0.5 1.0 throughput normalized tp4pp2 tp2pp4 pp8 pp8- tp4pp2 figure 13. throughput of various parallelization strategies with different ratios between output and input lengths d p mea- sured on 70b model and eight a10 gpus. 6.5 sensitivity study ratio between input and output length the speedup of seesaw depends on the ratio between the input and output length or p d. model re-sharding has the opportunity to provide speedup when prefilling and decoding have bal- anced time. to investigate to what extent model re-sharding would be effective we measure the throughput of various parallelization strategies on synthesized datasets with uni- form lengths and different p d ratios. we fix the input length as 3000 and vary the output length. as shown in figure 13 pp8 achieves the highest throughput during prefilling while tp4pp2 excels in decoding. when the output length equals one prefilling only seesaw and pp8 show similar throughput and tp4pp2 performs worse due to high communication overhead. as output length increases the inefficiency of pp in decoding outweighs its advantage in prefilling causing pp8 s throughput to drop rapidly. there is a range where tp2pp4 becomes optimal before decoding dominates the runtime and tp4pp2 takes over as the fastest. nonetheless seesaw achieves the highest overall throughput across all data points. in real scenarios with variable input and output lengths seesaw is even more advantageous due to its adaptive capabilities. inter-connection bandwidth the effectiveness of see- saw also depends on the inter-connection bandwidth. we investigate this by measuring the runtime and tracing all- reduce operations of running arxiv-summarization and 34b model on eight a10s. we then mutate the all- reduce time to project the end-to-end throughput with dif- page 10 seesaw high-throughput llm inference via model re-sharding 10 1 100 101 bandwidth scale all_reduce through pcie 0.0 0.5 1.0 throughput normalized d2t1p4 d2t2p2 d2t4p1 d1t1p8 d1t2p4 d1t4p2 d1t8p1 d2p4- d2t4 figure 14. projected throughput of various parallelization strate- gies with different inter-connection bandwidth measured and traced on 34b model and eight a10 gpus. ferent inter-connection bandwidths. as shown in figure 14 when the inter-connection bandwidth is slow for example among geographically distributed devices borzunov et al. 2022 pipeline parallelism is optimal when the bandwidth is very high tensor parallelism is optimal. the throughput of seesaw is superior to fixed parallelization strategies on a wide range from 0.1 to 50 of pcie bandwidth. 7 related work 7.1 heterogenity between prefilling and decoding due to the different computational characteristics between prefilling and decoding leading to under-utilization of hard- ware resources prior research has investigated two direc- tions to address this problem namely disaggregating or merging the two stages. disaggregation places prefilling and decoding onto different devices to avoid their interfer- ence while merging processes prefilling and decoding in one batch. disaggregate prefill and decoding distserve zhong et al. 2024 proposed placing prefilling and decoding on different devices to prevent interference and leverage dif- ferent characteristics of the two stages. mooncake qin et al. 2024 uses similar through a distributed kv cache pool. p d-serve jin et al. 2024 uses the device-to-device network to transfer the kv cache between prefill and decode devices. splitwise patel et al. 2024 proposes using dif- ferent gpu models for the two stages. tetriinfer hu et al. 2024 further disaggregates different downstream tasks to avoid interference. these works are designed for online serving while seesaw focuses on offline inference. more- over they are usually designed for large clusters. merge prefill and decode chunked prefill as proposed by splitfuse holmes et al. 2024 sarathi agrawal et al. 2023 and sarathi-serve agrawal et al. 2024 splits long prompts in the prefilling stage into smaller chunks combin- ing them with decoding steps to strike a balance between data movement and computation and reduce pipeline bub- bles in pipeline parallelism. however determining the opti- mal chunk size is challenging. a chunk size that s too large results in excessive decode-only steps closely resembling traditional prefill-decode scheduling. conversely a chunk size that s too small reduces kernel efficiency. 7.2 parallel and distributed llm inference aside from tensor parallelism pipeline parallelism and data parallelism discussed in section 2.2 there are also other types of parallelisms such as sequence parallelism sp li et al. 2021 liu et al. 2023 lin et al. 2024 brandon et al. 2023 xue et al. 2024 and fully sharded data parallelism fsdp zhao et al. 2023 rajbhandari et al. 2020 . se- quence parallelism is especially designed for long sequence lengths and is orthogonal with our work. fsdp requires frequently transferring weight matrices across gpus thus mainly used in training. hexgen jiang et al. 2023 llm-pq zhao et al. 2024 helix mei et al. 2024 investigate parallelisms in hetero- geneous clusters. intra-device parallelism leverages over- lapping functions using different resources within each de- vice including nanoflow zhu et al. 2024 and liger du et al. 2024 . petals borzunov et al. 2022 explores llm inference in geographically distributed setups em- ploying pipeline parallelism to minimize communication costs. spotserve miao et al. 2024 runs llm inference on preemptible instances. 7.3 offloading in llm inference offloading is a widely used technique to run llm applica- tions in resource-constrained scenarios ren et al. 2021 . flexgen sheng et al. 2023 swaps tensors across gpu memory cpu memory and disks. fiddler kamahori et al. 2024 hetegen xuanlei et al. 2024 powerinfer song et al. 2023 and fastdecoder he zhai 2024 perform part of computation in cpu which require cpus with strong compute capability or external cpu nodes connected with high-bandwidth networking. instinfer pan et al. 2024 offloads computation to computational storage drives. 8 conclusion this paper proposes seesaw a high-throughput llm infer- ence engine to address the inefficiencies of fixed paralleliza- tion by selecting different parallelization strategies for the prefilling and decoding stages and switching between them using model re-sharding. it uses tiered kv cache buffering to minimize re-sharding overheads. our experiments show that seesaw outperforms widely-used open-source inference engines with a throughput increase of 1.06-1.78 and an average throughput improvement of 1.36 . these results highlight seesaw s effectiveness and adaptability. page 11 seesaw high-throughput llm inference via model re-sharding references achiam j. adler s. agarwal s. ahmad l. akkaya i. aleman f. l. almeida d. altenschmidt j. altman s. anadkat s. et al. gpt-4 technical report. arxiv preprint arxiv 2303.08774 2023. agrawal a. panwar a. mohan j. kwatra n. gulavani b. s. and ramjee r. sarathi efficient llm inference by piggybacking decodes with chunked prefills. arxiv preprint arxiv 2308.16369 2023. agrawal a. kedia n. panwar a. mohan j. kwatra n. gulavani b. s. tumanov a. and ramjee r. taming throughput-latency tradeoff in llm inference with sarathi- serve. arxiv preprint arxiv 2403.02310 2024. ainslie j. lee-thorp j. de jong m. zemlyanskiy y. lebr on f. and sanghai s. gqa training generalized multi-query transformer models from multi-head check- points. arxiv preprint arxiv 2305.13245 2023. alband. why not multiprocess pin memory in data loader https discuss.pytorch.org t why-not- multiprocess-pin-memory-in-data-loader 197345 2 2023. accessed 2024-10-14. amazon web services. amazon ec2 instance types 2024. url https aws.amazon.com ec2 instance-types . accessed 2024-10-26. ben-nun t. and hoefler t. demystifying parallel and dis- tributed deep learning an in-depth concurrency analysis. acm computing surveys csur 52 4 1 43 2019. bengio y. ducharme r. and vincent p. a neural proba- bilistic language model. advances in neural information processing systems 13 2000. borzunov a. baranchuk d. dettmers t. ryabinin m. belkada y. chumachenko a. samygin p. and raffel c. petals collaborative inference and fine-tuning of large models. arxiv preprint arxiv 2209.01188 2022. brandon w. nrusimha a. qian k. ankner z. jin t. song z. and ragan-kelley j. striped attention faster ring attention for causal transformers. arxiv preprint arxiv 2311.09431 2023. chan v. zhang h. and wang f. snowflake llm inference optimizing gpu capacity for interactive workloads. https www.snowflake.com engineering- blog snowflake-llm-inference-interactive-workloads september 2024. accessed 2024-10-30. chang l. bao w. hou q. jiang c. zheng n. zhong y. zhang x. song z. jiang z. lin h. et al. flux fast software-based communication overlap on gpus through kernel fusion. arxiv preprint arxiv 2406.06858 2024. cheng k. hu w. wang z. peng h. li j. and zhang s. slice-level scheduling for high throughput and load balanced llm serving. arxiv preprint arxiv 2406.13511 2024. cohan a. dernoncourt f. kim d. s. bui t. kim s. chang w. and goharian n. a discourse-aware attention model for abstractive summarization of long documents. arxiv preprint arxiv 1804.05685 2018. dell technologies. poweredge server gpu matrix 2023. url https www. delltechnologies.com asset en-ca products servers briefs-summaries poweredge-server-gpu-matrix.pdf. ac- cessed 2024-10-24. dell technologies. inferencing performance for gen- erative ai in the enterprise with amd accelerators. https infohub.delltechnologies.com en-au l generative- ai-in-the-enterprise-with-amd-accelerators inferencing- performance 2024. accessed 2024-10-30. du j. wei j. jiang j. cheng s. huang d. chen z. and lu y. liger interleaving intra-and inter-operator parallelism for distributed large model inference. in pro- ceedings of the 29th acm sigplan annual symposium on principles and practice of parallel programming pp. 42 54 2024. edge d. trinh h. cheng n. bradley j. chao a. mody a. truitt s. and larson j. from local to global a graph rag approach to query-focused summarization. arxiv preprint arxiv 2404.16130 2024. elinas. llama-3-15b instruct-zeroed. https huggingface.co elinas llama-3-15b-instruct-zeroed 2024. fang j. yu y. zhao c. and zhou j. turbotransformers an efficient gpu serving system for transformer models. in proceedings of the 26th acm sigplan symposium on principles and practice of parallel programming pp. 389 402 2021. google cloud. gpu platforms a100 gpus 2024. url https cloud.google.com compute docs gpus a100-gpus. accessed 2024-10-26. he j. and zhai j. fastdecode high-throughput gpu- efficient llm serving using heterogeneous pipelines. arxiv preprint arxiv 2403.11421 2024. holmes c. tanaka m. wyatt m. awan a. a. rasley j. rajbhandari s. aminabadi r. y. qin h. bakhtiari a. kurilenko l. et al. deepspeed-fastgen high-throughput text generation for llms via mii and deepspeed-inference. arxiv preprint arxiv 2401.08671 2024. page 12 seesaw high-throughput llm inference via model re-sharding hu c. huang h. xu l. chen x. xu j. chen s. feng h. wang c. wang s. bao y. et al. inference without interference disaggregate llm inference for mixed down- stream workloads. arxiv preprint arxiv 2401.11181 2024. huang y. cheng y. bapna a. firat o. chen d. chen m. lee h. ngiam j. le q. v. wu y. et al. gpipe efficient training of giant neural networks using pipeline parallelism. advances in neural information processing systems 32 2019. jiang y. yan r. yao x. zhou y. chen b. and yuan b. hexgen generative inference of large language model over heterogeneous environment. in forty-first interna- tional conference on machine learning 2023. jin y. wang t. lin h. song m. li p. ma y. shan y. yuan z. li c. sun y. et al. p d-serve serving disag- gregated large language model at scale. arxiv preprint arxiv 2408.08147 2024. kamahori k. gu y. zhu k. and kasikci b. fiddler cpu-gpu orchestration for fast inference of mixture-of- experts models. arxiv preprint arxiv 2402.07033 2024. kamsetty a. chen h. and xie l. how bytedance scales offline inference with multi-modal llms to 200tb data. https www.anyscale.com blog how-bytedance-scales- offline-inference-with-multi-modal-llms-to-200tb-data august 2023. accessed 2024-10-30. kwon w. li z. zhuang s. sheng y. zheng l. yu c. h. gonzalez j. zhang h. and stoica i. efficient memory management for large language model serving with pagedattention. in proceedings of the 29th sym- posium on operating systems principles pp. 611 626 2023. li s. xue f. baranwal c. li y. and you y. sequence parallelism long sequence training from system perspec- tive. arxiv preprint arxiv 2105.13120 2021. li z. zheng l. zhong y. liu v. sheng y. jin x. huang y. chen z. zhang h. gonzalez j. e. et al. alpaserve statistical multiplexing with model paral- lelism for deep learning serving. in 17th usenix sympo- sium on operating systems design and implementation osdi 23 pp. 663 679 2023. lin b. peng t. zhang c. sun m. li l. zhao h. xiao w. xu q. qiu x. li s. et al. infinite-llm efficient llm service for long context with distattention and distributed kvcache. arxiv preprint arxiv 2401.02669 2024. liu h. zaharia m. and abbeel p. ring attention with blockwise transformers for near-infinite context. arxiv preprint arxiv 2310.01889 2023. liu s. biswal a. cheng a. mo x. cao s. gonzalez j. e. stoica i. and zaharia m. optimizing llm queries in relational workloads. arxiv preprint arxiv 2403.05821 2024. mei y. zhuang y. miao x. yang j. jia z. and vinayak r. helix distributed serving of large language models via max-flow on heterogeneous gpus. arxiv preprint arxiv 2406.01566 2024. miao x. oliaro g. zhang z. cheng x. jin h. chen t. and jia z. towards efficient generative large language model serving a survey from algorithms to systems. arxiv preprint arxiv 2312.15234 2023. miao x. shi c. duan j. xi x. lin d. cui b. and jia z. spotserve serving generative large language models on preemptible instances. in proceedings of the 29th acm international conference on architectural support for programming languages and operating systems volume 2 pp. 1112 1127 2024. mlcommons. mlperf inference datacenter benchmark suite. https mlcommons.org benchmarks inference-datacenter 2024. accessed 2024- 10-30. narayan a. chami i. orr l. arora s. and r e c. can foundation models wrangle your data arxiv preprint arxiv 2205.09911 2022. narayanan d. harlap a. phanishayee a. seshadri v. devanur n. r. ganger g. r. gibbons p. b. and za- haria m. pipedream generalized pipeline parallelism for dnn training. in proceedings of the 27th acm symposium on operating systems principles pp. 1 15 2019. nvidia. fastertransformer transformer related optimiza- tion including bert gpt. https github.com nvidia fastertransformer 2024a. nvidia. tensorrt-llm optimized inference for large lan- guage models. https github.com nvidia tensorrt-llm 2024b. nvidia corporation. nvidia a100 pcie product brief 2020. url https www.nvidia.com content dam en-zz solutions data-center a100 pdf a100-pcie-prduct-brief.pdf. accessed 2024-10-24. nvidia corporation. nvidia nvlink high-speed gpu interconnect 2024. url https www.nvidia. com en-us data-center nvlink . accessed 2024-10-26. openai. chatgpt gpt-4 2024. url https www. openai.com research gpt-4. accessed 2024- 08-02. page 13 seesaw high-throughput llm inference via model re-sharding pan x. li e. li q. liang s. shan y. zhou k. luo y. wang x. and zhang j. instinfer in-storage attention offloading for cost-effective long-context llm inference. arxiv preprint arxiv 2409.04992 2024. patel p. choukse e. zhang c. shah a. goiri i. maleki s. and bianchini r. splitwise efficient generative llm inference using phase splitting. in 2024 acm ieee 51st annual international symposium on computer architec- ture isca pp. 118 132. ieee 2024. pci-sig. pci-sig releases pcie 4.0 ver- sion 1.0 2017. url https pcisig. com pci-sig-releases-pcie c2 ae-40-version-10. pope r. douglas s. chowdhery a. devlin j. bradbury j. heek j. xiao k. agrawal s. and dean j. efficiently scaling transformer inference. proceedings of machine learning and systems 5 606 624 2023. qin r. li z. he w. zhang m. wu y. zheng w. and xu x. mooncake kimi s kvcache-centric architecture for llm serving. arxiv preprint arxiv 2407.00079 2024. rajbhandari s. rasley j. ruwase o. and he y. zero memory optimizations toward training trillion parameter models. in sc20 international conference for high per- formance computing networking storage and analysis pp. 1 16. ieee 2020. ren j. rajbhandari s. aminabadi r. y. ruwase o. yang s. zhang m. li d. and he y. zero-offload democratizing billion-scale model training. in 2021 usenix annual technical conference usenix atc 21 pp. 551 564 2021. roziere b. gehring j. gloeckle f. sootla s. gat i. tan x. e. adi y. liu j. sauvestre r. remez t. et al. code llama open foundation models for code. arxiv preprint arxiv 2308.12950 2023. sharegpt. sharegpt vicuna unfiltered dataset. https huggingface.co datasets anon8231489123 sharegpt_vicuna_ unfiltered 2023. apache 2.0 license. sheng y. zheng l. yuan b. li z. ryabinin m. chen b. liang p. r e c. stoica i. and zhang c. flexgen high-throughput generative inference of large language models with a single gpu. in international conference on machine learning pp. 31094 31116. pmlr 2023. shoeybi m. patwary m. puri r. legresley p. casper j. and catanzaro b. megatron-lm training multi- billion parameter language models using model paral- lelism. arxiv preprint arxiv 1909.08053 2019. song y. mi z. xie h. and chen h. powerinfer fast large language model serving with a consumer-grade gpu. arxiv preprint arxiv 2312.12456 2023. srivatsa v. he z. abhyankar r. li d. and zhang y. preble efficient distributed prompt scheduling for llm serving. 2024. touvron h. lavril t. izacard g. martinet x. lachaux m.-a. lacroix t. rozi ere b. goyal n. hambro e. azhar f. et al. llama open and efficient foundation lan- guage models. arxiv preprint arxiv 2302.13971 2023a. touvron h. martin l. stone k. albert p. almahairi a. babaei y. bashlykov n. batra s. bhargava p. bhosale s. et al. llama 2 open foundation and fine- tuned chat models. arxiv preprint arxiv 2307.09288 2023b. vaswani a. shazeer n. parmar n. uszkoreit j. jones l. gomez a. n. kaiser \u0142. and polosukhin i. at- tention is all you need. advances in neural information processing systems 30 2017. vllm team. performance update bringing vllm to the next level. https blog.vllm.ai 2024 09 05 perf-update.html 2024. accessed 2024-10- 14. xuanlei z. jia b. zhou h. liu z. cheng s. and you y. hetegen efficient heterogeneous parallel inference for large language models on resource-constrained devices. proceedings of machine learning and systems 6 162 172 2024. xue f. chen y. li d. hu q. zhu l. li x. fang y. tang h. yang s. liu z. et al. longvila scaling long- context visual language models for long videos. arxiv preprint arxiv 2408.10188 2024. yu c. lee s. xu r. lin w. gorthy p. and liaw r. batch llm inference on anyscale slashes aws bedrock costs by up to 6x october 2024. url https www.anyscale.com blog batch-llm-inference-announcement. ac- cessed 2024-10-30. yu g.-i. jeong j. s. kim g.-w. kim s. and chun b.- g. orca a distributed serving system for transformer- based generative models. in 16th usenix symposium on operating systems design and implementation osdi 22 pp. 521 538 2022. yuan z. shang y. zhou y. dong z. xue c. wu b. li z. gu q. lee y. j. yan y. et al. llm inference unveiled survey and roofline model insights. arxiv preprint arxiv 2402.16363 2024. page 14 seesaw high-throughput llm inference via model re-sharding zhao j. wan b. peng y. lin h. and wu c. llm- pq serving llm on heterogeneous clusters with phase- aware partition and adaptive quantization. arxiv preprint arxiv 2403.01136 2024. zhao y. gu a. varma r. luo l. huang c.-c. xu m. wright l. shojanazeri h. ott m. shleifer s. et al. pytorch fsdp experiences on scaling fully sharded data parallel. arxiv preprint arxiv 2304.11277 2023. zheng l. li z. zhang h. zhuang y. chen z. huang y. wang y. xu y. zhuo d. xing e. p. et al. alpa automating inter-and intra-operator parallelism for distributed deep learning. in 16th usenix symposium on operating systems design and implementation osdi 22 pp. 559 578 2022. zheng l. yin l. xie z. huang j. sun c. yu c. h. cao s. kozyrakis c. stoica i. gonzalez j. e. et al. efficiently programming large language models using sglang. arxiv preprint arxiv 2312.07104 2023. zhong y. liu s. chen j. hu j. zhu y. liu x. jin x. and zhang h. distserve disaggregating prefill and decoding for goodput-optimized large language model serving. arxiv preprint arxiv 2401.09670 2024. zhu k. zhao y. zhao l. zuo g. gu y. xie d. gao y. xu q. tang t. ye z. et al. nanoflow towards optimal large language model serving throughput. arxiv preprint arxiv 2408.12757 2024. a performance model in this section we examine the trade-offs of various paral- lelism strategies by developing an analytical performance model. we break down the model s inference time into multiple components and analyze the impact of each paral- lelism type on these components. the results reveal that the proportion of these components differs across workloads resulting in distinct scaling behaviors for each parallelism strategy. table 2 lists the notations used in our analysis. we assume the data type is float16. table 2. notations n number of output tokens t linear dm time of moving weights r number of requests t attn dm time of moving kvcache b global batch size t linear c time of computation s average sequence length t attn c computation of attention hq number of heads tnw time of communication d head dimension hkv number of kv heads l number of layers w parameters of one layer pp pipeline parallel degree dp data parallel degree tp tensor parallel degree table 3. different components of the runtime of a forward pass. the batch size b representing the batching effect is emphasized. t linear dm t linear comp t attn dm t attn comp tnw tp prefill 2w bhbm 2bw s flops 2bs hq 2hkv d bhbm bhqs2d2 flops 4bshqd bar t p decode 2w bhbm 2bw flops 4bshkvd bhbm 2bhqsd2 flops 4bhqd bar t p a.1 runtime break-down the runtime of each decoding layer can be divided into three components 1 data movement tdm from gpu global memory hbm to compute units which includes transfer- ring weights t linear dm and kv cache t attn dm 2 computation tcomp including t linear comp and t attn comp and 3 communication cost tnw nw stands for network primarily arising from the all-reduce operation in tensor parallelism. based on the roof- line model the runtime of each layer can be approximated as tl max t linear dm t linear comp max t attn dm t attn comp tnw. data movement. the runtime of data movement can be approximated as transferred data volume divided by the bandwidth which is the hbm bandwidth for gpus. for lin- ear layers the transferred data is mostly weight matrices of which the size is 2w bytes which is constant. for attention layers the transferred data is most the q k and v matrices which is 2bs hq 2hkv d bytes in prefilling and 4bshkvd in decoding. compute. the computation time can be approximated as the number of floating operations flops divided by the number of floating operations per second of the hardware flop s . for linear layers the flops is proportional to the weight parameters times the number of tokens which is 2wbs in prefilling and 2wb in decoding. for attention layers most operations come from computing the attention score which is approximated as bhqs2d2 in prefilling and 2bhqsd2 in decoding. communication. the communication cost mostly comes from the all-reduce operation in tensor parallelism. it can be modeled as the transferred data volume divided by the bandwidth. we denote it as tnw tp and approximate it as b a bar tp where a is the size of the activation of one request within a batch and bar tp is the all-reduce band- width. tnw tp is monotonically increasing with tp as additional gpus and more replicas of activations are added to all-reduce. we omit the peer-to-peer communication over in pipeline parallel since it is negligible compared to the all-reduce operation of tensor parallel. a.2 batching analysis batching is critical in decoding. it significantly affects the latency and throughput. batch size represents how many page 15 seesaw high-throughput llm inference via model re-sharding tp1dp8 tp2dp4 tp4dp2 tp8dp1 0.00 0.25 0.50 0.75 1.00 runtime per request oom load weight compute allreduce 0 50 100 150 batch size batch size figure 15. how data parallelism affects the decoding throughput. data parallelism has minimal communication overhead but suffers from caused by inefficient memory access caused by duplicating model weights. model duplicates occupy more gpu memory leaving less space for kv cache and smaller batch sizes. with more data parallelism the overhead of loading data from gpu global memory to compute units significantly increases. requests are processed in one forward pass and larger batch sizes can amortize the cost of transferring weights thus improving the throughput. global and micro-batch size. in distributed inference such as multi-gpu settings we define the global batch size b as the number of requests being actively processed by the whole cluster. it is a tunable hyper-parameter that represents the overall workload of the system. it is bounded by the maximal batch size which is determined by the memory budget. on the other side the micro batch size is defined at the device level as the batch size processed during each forward pass. tensor parallelism does not affect the batch size while dp and pp shrink the micro batch size. a.3 parallelism analysis we consider three types of parallelism data parallelism tensor parallelism and pipeline parallelism and denote their degree of parallelism as dp tp and pp respectively. tensor parallelism can accelerate both data moving t linear dm and t attn dm are reduced to 1 tp and computation tcomp reduced to tcomp tp at the cost of all reduce over- head tnw. data parallelism distributes the global batch size b onto dp micro-batches processed in parallel. the model is du- plicated so t linear dm remains unchanged. t attn dw t linear comp t attn comp tnw are reduced as the batch size is smaller. due to the need to duplicate model weights the gpu memory left for the kv cache is smaller. the spare space for kv cache on each gpu is mkv m 2lw t p p p . the maximal batch size is bmax dp mkv tp pp 4lhkvds dp m tp pp 2lw 4lhkvds while tp and pp can super-linearly scale the batch size dp can only linearly scale the batch size. the trade-off between limited batch sizes and reduced communication overhead is shown in figure 15. pipeline parallelism distributes different layers to dif- ferent devices and each device will have l pp layers. it cannot reduce single-request latency but is more suitable for throughput-oriented scenarios as it introduces less commu- nication overhead. however it is not the ultimate answer of high-throughput applications because of an important observation that pipeline parallelism harms maximal batch size. a tricky nuance is that given a batch size b pipeline parallelism can only process b pp of them simultaneously in order to utilize and pipeline all pp gpus which is harm- ful to batching. if the workload is not uniformly distributed across gpus there will be bubbles or in the worst case some gpus might be idle. when the pipeline is fully and stably pipelining each time the last pipeline stage finishes its l pp layers of forward pass a micro-batch of b pp will be finished. throughput. the micro-batch size on each gpu is b pp dp . the total runtime of generating one micro batch with size b pp dp on one dp replica or more specifically the time of the last pipeline stage finishing a micro-batch is tstage l pp max t linear dm tp t linear comp dp tp pp max t attn dm t attn comp dp tp pp tnw tp pp dp the throughput number of processed requests per unit time is b pp t. for simplicity we calculate the inverse of it as throughput 1 tstage b pp l b max t linear dm tp t linear comp dp tp pp max t attn dm t attn comp dp tp pp tnw tp pp dp 1 if we approximate the roof-line model with a simplified additional model this expression can be simplified as throughput 1 t linear dm tp t linear comp t attn dm t attn comp dp tp pp tnw tp pp dp 2",
      "keywords": [
        "page",
        "seesaw",
        "high-throughput",
        "llm",
        "inference",
        "via",
        "model",
        "re-sharding",
        "qidong",
        "su123",
        "wei",
        "zhao34",
        "xin",
        "li3",
        "muralidhar",
        "andoorveedu3",
        "chenhao",
        "jiang12",
        "zhanda",
        "zhu123",
        "kevin",
        "song12",
        "christina",
        "giannoula123",
        "gennady",
        "pekhimenko123",
        "abstract",
        "improve",
        "efficiency",
        "distributed",
        "large",
        "language",
        "various",
        "parallelization",
        "strategies",
        "such",
        "tensor",
        "pipeline",
        "parallelism",
        "proposed.",
        "however",
        "distinct",
        "computational",
        "characteristics",
        "inherent",
        "two",
        "stages",
        "prefilling",
        "decoding",
        "render",
        "single",
        "static",
        "strategy",
        "insufficient",
        "effective",
        "optimization",
        "both",
        "stages.",
        "work",
        "present",
        "engine",
        "optimized",
        "throughput-oriented",
        "tasks.",
        "key",
        "idea",
        "behind",
        "dynamic",
        "re-",
        "sharding",
        "technique",
        "facilitates",
        "reconfiguration",
        "across",
        "thereby",
        "maximizing",
        "throughput",
        "phases.",
        "mitigate",
        "overhead",
        "optimize",
        "employ",
        "tiered",
        "cache",
        "buffering",
        "transition-minimizing",
        "scheduling.",
        "approaches",
        "synergistically",
        "reduce",
        "caused",
        "frequent",
        "stage",
        "transitions",
        "while",
        "ensuring",
        "maximum",
        "batching",
        "efficiency.",
        "evaluation"
      ],
      "metadata": {
        "filename": "2503.06433v1.pdf",
        "original_filename": "2503.06433v1.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 825289,
        "upload_file_id": "upload_1752159535243_pac3qpoa3",
        "upload_timestamp": "2025-07-10T14:58:55.288528",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T14:58:55.550159+00:00",
      "content_length": 73234,
      "content_type": ".pdf",
      "filename": "2503.06433v1.pdf"
    },
    "2": {
      "file_id": "2",
      "content": "\n=== Page 1 ===\nLUMOS: EFFICIENT PERFORMANCE MODELING AND ESTIMATION FOR\nLARGE-SCALE LLM TRAINING\nMingyu Liang 1 Hiwot Tadese Kassa 2 Wenyin Fu 2 Brian Coutinho 2 Louis Feng 2 Christina Delimitrou 3\nABSTRACT\nTraining LLMs in distributed environments presents significant challenges due to the complexity of model\nexecution, deployment systems, and the vast space of configurable strategies. Although various optimization\ntechniques exist, achieving high efficiency in practice remains difficult. Accurate performance models that\neffectively characterize and predict a model\u2019s behavior are essential for guiding optimization efforts and system-\nlevel studies. We propose Lumos, a trace-driven performance modeling and estimation toolkit for large-scale LLM\ntraining, designed to accurately capture and predict the execution behaviors of modern LLMs. We evaluate Lumos\non a production ML cluster with up to 512 NVIDIA H100 GPUs using various GPT-3 variants, demonstrating that\nit can replay execution time with an average error of just 3.3%, along with other runtime details, across different\nmodels and configurations. Additionally, we validate its ability to estimate performance for new setups from\nexisting traces, facilitating efficient exploration of model and deployment configurations.\n1\nINTRODUCTION\nIn recent years, large language models (LLMs) have trans-\nformed many aspects of daily life.\nThe availability of\nvast datasets, along with advancements in computational\nresources, has enabled the development of increasingly\ncomplex models, such as ChatGPT (Ouyang et al., 2022),\nLLaMA (Touvron et al., 2023), and PaLM (Chowdhery\net al., 2023). However, efficiently training these LLMs\npresents significant challenges, necessitating both hardware\nand software innovations across the system stack.\nTo meet these demands, efforts have focused on address-\ning various bottlenecks. Key areas of optimization include\nthe development of AI-specific hardware (e.g., NVIDIA\nGPUs (NVIDIA, c) and SmartNICs (Ma et al., 2022)), im-\nprovements in memory systems (Kwon et al., 2023; Ra-\njbhandari et al., 2020), the design of optimal parallelism\nstrategies (Zheng et al., 2022; Isaev et al., 2023), over-\nlapping communication with computation (Hashemi et al.,\n2019; Narayanan et al., 2021), and advancements in algo-\nrithms (Beltagy et al., 2020; You et al., 2019).\nDespite these innovations, ensuring training efficiency re-\nmains a significant challenge. Diagnosing inefficiencies in\nLLMs is particularly difficult because runtime traces pro-\nduced by machine learning (ML) frameworks (PyTorch;\n1Cornell University, Ithaca, USA 2Meta, Menlo Park, USA\n3Massachusetts Institute of Technology, Cambridge, USA. Corre-\nspondence to: Mingyu Liang <ml2585@cornell.edu>.\nProceedings of the 8 th MLSys Conference, Santa Clara, CA, USA,\n2025. Copyright 2025 by the author(s).\nTensorFlow, a) are often dense and require deep expertise\nto interpret effectively. Moreover, runtime behavior can\nvary significantly across different model architectures, de-\nployment configurations, accelerator types, network infras-\ntructures, and other system components. These variations\ncan cause performance bottlenecks to shift unpredictably,\nmaking them difficult to identify and address. An additional\nchallenge lies in the vast search space of optimization pos-\nsibilities. Finding the optimal solution within this space\nis time-consuming and resource-intensive, as it requires\nextensive experimentation on real hardware, demanding\nsignificant resources and incurring high costs.\nA key step toward achieving efficiency is to accurately char-\nacterize and understand the behavior of these models. One\ncommon approach is to build performance models that cap-\nture model execution, which also provide a solid founda-\ntion for further optimization studies. While existing ef-\nforts (Moolchandani et al., 2023; Isaev et al., 2023) develop\nanalytical models to predict high-level performance based\non exposed model parameters, they often miss essential un-\nderlying execution details. To address this limitation, recent\nwork (Hu et al., 2022; Zhu et al., 2020; Lin et al., 2022;\nBang et al., 2023) has leveraged runtime traces to construct\nfine-grained execution graphs, providing deeper insights\ninto the execution process.\nHowever, current modeling methods struggle to address the\nnew complexities in modern LLMs. Training LLMs at scale\ninvolves deploying across multiple machines, introducing\nsubstantial communication overhead. As one example of\noptimization, overlapping computation with communication\narXiv:2504.09307v1  [cs.DC]  12 Apr 2025\n\n=== Page 2 ===\nLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nExecution Time Breakdown (ms)\ndPRO\nActual\n3235\n1691\n1417\n81\n4287\n885\n2261\n178\nExposed Compute\nOverlapped\nExposed Comm\nOther\nFigure 1. Execution breakdown for one training iteration of GPT-3\n175B, configured with tensor parallelism = 8, pipeline parallelism\n= 4, and data parallelism = 8.\ncan reduce end-to-end training time, but it also introduces\ncomplex inter-stream dependencies that are challenging to\nmodel accurately. Figure 1 shows the execution time break-\ndown for a single training iteration of the GPT-3 (175B)\nmodel, along with replayed results from dPRO (Hu et al.,\n2022), a state-of-the-art trace-driven performance modeling\ntool. The comparison reveals significant gaps between the\nsimulated and actual results, highlighting the challenges in\ncapturing the full intricacies of LLM execution.\nTo overcome these difficulties, we propose Lumos, a trace-\ndriven performance modeling and estimation toolkit for\nlarge-scale LLM training. To the best of our knowledge,\nLumos is the first system to provide accurate performance\nmodels that effectively capture the execution behaviors of\nLLMs. It leverages built-in profiling tools from ML frame-\nworks, such as PyTorch Kineto (Kineto), without requir-\ning any custom instrumentation in models or frameworks,\nthereby minimizing the profiling overhead.\nFurthermore, to streamline the exploration of optimization\nopportunities, Lumos also offers the flexibility to modify\nand generate new execution graphs from existing traces.\nThis capability facilitates the exploration of optimal config-\nurations, such as adjusting parallelism strategies (e.g., data\nand pipeline parallelism) and fine-tuning model architec-\ntures (e.g., number of layers, hidden size). By estimating\nperformance through simulation rather than experimenting\non real hardware, Lumos can significantly reduce cost and\naccelerate the optimization process.\nThe main contributions of our work are the following:\n\u2022 By utilizing only built-in profiling traces from ML\nframeworks, Lumos constructs a comprehensive exe-\ncution graph that identifies all dependencies between\nexecuted tasks, enabling accurate performance model-\ning of large-scale LLM training. Beyond estimating\noverall execution time, the fine granularity of Lumos al-\nlows it to reproduce detailed execution characteristics,\nfacilitating deeper analysis and downstream optimiza-\ntion studies.\n\u2022 With a detailed execution graph, Lumos offers users\na convenient way to explore various model configura-\ntions, including adjustments to parallelism strategies\nand model architectures. By manipulating the existing\ngraph to generate new ones for different configurations\nand by predicting performance through simulation, Lu-\nmos streamlines the optimization process and enables\nefficient and low-cost configuration exploration.\n\u2022 We evaluate Lumos using various GPT-3 model vari-\nants on a production ML cluster with up to 512\nNVIDIA H100 GPUs. Our results show that Lumos\ncan accurately replay execution, achieving an average\nerror of only 3.3% across different models and deploy-\nment configurations. Additionally, we demonstrate that\nLumos accurately reproduces detailed execution statis-\ntics, such as execution time breakdown and SM utiliza-\ntion, showing significant improvements over existing\napproaches. Finally, we validate its ability to estimate\nperformance for new configurations and deployments,\nachieving high accuracy when adjusting parallelism\nstrategies and tuning various model architectures.\n2\nRELATED WORK\n2.1\nProfiling Tools and Traces\nAs the ML system stack evolves rapidly, profiling tools play\na crucial role in understanding model execution characteris-\ntics and identifying performance bottlenecks. As hardware\naccelerators like GPUs (NVIDIA, c) and TPUs (Jouppi et al.,\n2023) become increasingly essential, vendors offer special-\nized tools\u2014such as NVProf (NVIDIA, d), CUPTI (NVIDIA,\na), and Nsight (NVIDIA, e)\u2014to expose hardware perfor-\nmance counters, providing developers with critical insights\ninto performance metrics and enabling effective optimiza-\ntion.\nTo improve the interpretability of profiling results, ML\nframeworks also provide built-in tools for collecting ex-\necution statistics at the operator level. These tools often\nintegrate hardware-level traces, offering a complete view of\nthe entire stack\u2014from host to device. For instance, PyTorch\nKineto (Kineto) leverages CUPTI (NVIDIA, a) to capture\nruntime information for PyTorch operators, CUDA events,\nand GPU kernels, seamlessly linking them to provide a\nholistic perspective on model execution.\n2.2\nLLMs and Parallelism Strategies\nMost modern LLMs are built on transformer architec-\ntures (Vaswani, 2017), which rely on self-attention mecha-\nnisms to capture long-range dependencies in sequential data.\nThese models feature multiple stacked layers of attention\nand feedforward networks, with parameter sizes growing\nrapidly over the years. For example, GPT-2 (Radford et al.,\n2019) introduced in 2019 had 1.5 billion parameters, GPT-\n\n=== Page 3 ===\nLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training\n3 (Brown, 2020) in 2020 expanded to 175 billion parameters,\nand PaLM (Chowdhery et al., 2023) reached 540 billion pa-\nrameters by 2022.\nTraining LLMs presents significant computational and mem-\nory challenges, especially as model sizes grow beyond the\ncapacity of individual GPUs. To address these limitations,\n3D parallelism\u2014a hybrid approach combining data, tensor,\nand pipeline parallelism\u2014has become essential for efficient\nlarge-scale training (Narayanan et al., 2021; Shoeybi et al.,\n2019; Smith et al., 2022; Chowdhery et al., 2023). Each\nform of parallelism contributes uniquely: data parallelism\n(DP) distributes training batches across devices, synchro-\nnizing gradients during updates; tensor parallelism (TP)\nsplits large tensors across multiple GPUs, allowing shared\ncomputation with frequent communication; and pipeline\nparallelism (PP) partitions the model into sequential stages,\nwith each stage processed on different devices in a coordi-\nnated pipeline.\nDespite the benefits, configuring 3D parallelism introduces\nsignificant complexity, requiring careful coordination across\nthese strategies to balance workloads and minimize commu-\nnication overhead. Recent research has focused on automat-\ning these configurations to reduce the burden on develop-\ners and ensure efficient distributed execution. For example,\nGSPMD (Xu et al., 2021) extends the XLA compiler (Sabne,\n2020) to support various parallelism paradigms through user\nannotations. Alpa (Zheng et al., 2022) automates model\nparallelization by optimizing intra- and inter-operator paral-\nlelism for efficient distributed execution. Galvatron (Miao\net al., 2022) introduces a decision tree to decompose the\nsearch space and designs a dynamic programming algorithm\nto generate the optimal plan.\nEmerging techniques like sequence parallelism (Li et al.,\n2021; Jacobs et al., 2023; Liu et al., 2023) further address\nthe challenges of training on long sequences by distribut-\ning computations along the sequence dimension, reducing\nmemory overhead and communication bottlenecks.\n2.3\nPerformance Modeling, Simulation, and\nOptimization\nThe complexity of LLMs poses challenges and opportuni-\nties in system design and optimization, with performance\nmodeling serving as a critical foundation for diagnosing and\noptimizing overall efficiency.\nThere are two primary approaches to building perfor-\nmance models.\nThe first relies on analytical models.\nAmPeD (Moolchandani et al., 2023) introduces an analytical\nmodel to estimate performance in distributed transformer\ntraining under various model parameters and parallelism\nstrategies. Similarly, Calculon (Isaev et al., 2023) provides\na parameterized analytical model that explores the co-design\nspace of software and hardware configurations to identify\noptimal system designs for LLMs. However, these analyt-\nical models are often tailored to specific implementations\nand hardware configurations, limiting their ability to gen-\neralize in the face of rapid model and system evolution.\nMoreover, they typically provide high-level performance\nestimates, making them inadequate for optimizations like\nmixed precision training (Das et al., 2018; Zhu et al., 2020)\nand operator fusion (Zhao et al., 2022; Jia et al., 2019).\nThe second approach leverages trace-based models to sim-\nulate execution and derive optimization insights. For ex-\nample, ASTRA-sim (Rashidi et al., 2020) and ASTRA-\nsim2.0 (Won et al., 2023) simulate distributed training with\na cycle-level and analytical network backend, evaluating\ncollective communication algorithms and network topolo-\ngies. In (Lin et al., 2022), the authors analyze critical paths\nwithin profiled traces to predict per-batch training time for\nDLRM. Daydream (Zhu et al., 2020) uses kernel-level de-\npendency graphs collected with CUPTI to predict runtime\nunder specific optimizations, while dPRO (Hu et al., 2022)\nbuilds a global dataflow graph by tracking dependencies\namong operators to estimate DNN training performance.\nHowever, these trace-based approaches fail to fully capture\nthe complexities inherent in LLM execution. To the best\nof our knowledge, this work is the first to leverage traces\nfor accurately modeling the intricate behaviors of LLMs,\naccounting for detailed operator and kernel interactions es-\nsential for precise performance prediction.\n3\nDESIGN\n3.1\nOverview\nFigure 2 presents the workflow of Lumos, our trace-driven\nperformance modeling and estimation toolkit for distributed\nLLM training. The process begins with collecting runtime\nprofiling traces from popular frameworks such as Tensor-\nFlow and PyTorch. These raw traces are then analyzed to\nextract key meta-information, which is used to construct a\ndetailed task-level execution graph. The execution graph\ncan be modified to adjust model configurations, such as\nparallelism strategies and architectures, generating new\nconfiguration-specific graphs. Finally, the simulator uses\nthese graphs to either replay the original execution or pre-\ndict performance under alternative configurations, providing\ninsights into potential optimizations and enabling effective\nexploration of what-if scenarios.\nWe initially focus on PyTorch due to its widespread use\nin both academia and industry, along with its advanced\nprofiling capabilities. However, our approach is flexible by\ndesign and can be extended to support other ML frameworks.\nWe will discuss the adaptability of Lumos in Section 5.\n\n=== Page 4 ===\nLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training\nTensorFlow\nPyTorch\nTrace Collection\ndef training_iteration_loop(): \n  with torch.profiler.profile(\n    activities=[\n      torch.profiler.ProfilerActivity.CPU,\n      torch.profiler.ProfilerActivity.CUDA,\n    ],\n    execution_trace_observer=ETObserver()\n  ) as pf:\n    for iter in range(N):\n      model.step()\n      pf.step()\nTrace Extraction\nTask\nDependency\nExecution Graph\nGraph Manipulation\nSimulator\nReplay\nPredict\n\u2022 Model Parallelism\n\u2022 Hyperparamter\nNew Configuration\n\u2022 Parallelism Strategy\n\u2022 Model Architecture\nNew Configuration\nNew Execution Graph\nFigure 2. Overview of Lumos\u2019s workflow.\n3.2\nTraces Collection\nWe collect profiling traces using PyTorch Kineto (Kineto),\nwhich captures comprehensive runtime information about\nPyTorch operators, CUDA runtime events, and GPU ker-\nnels, including name, start time, duration, CUDA stream\nID, thread ID, correlation ID, and more. Unlike previous\nmethods such as Daydream and dPRO, which necessitate\nextensive framework and model instrumentation, our profil-\ning involves adding only a few lines of code into the model,\nas shown in the code snippet at the top left of Figure 2,\nsignificantly improving usability with minimal effort.\n3.3\nExecution Graph\nThe essence of a model\u2019s execution lies in its execution\ngraph, which maps out the tasks being performed and\nthe dependencies between them. Motivated by prior ap-\nproaches (Zhu et al., 2020; Hu et al., 2022; Bang et al.,\n2023), we construct a low-level execution graph to accu-\nrately represent model execution. However, we have incor-\nporated several enhancements to capture the complex execu-\ntion characteristics of LLMs, ensuring more accurate mod-\neling and offering the flexibility to estimate performance for\nnew model configurations.\n3.3.1\nTasks\nTo streamline the design, our execution graph includes only\nthe following two types of tasks:\nCPU tasks: These include all executions happened on\nthe CPU, including PyTorch operators and CUDA runtime\nevents. For each CPU task, we record its metadata along\nwith the specific CPU thread on which it runs.\nGPU tasks: These include all executions happened on the\nGPU, which primarily consist of GPU kernels. For each\nGPU task, we log its metadata along with the corresponding\nCUDA stream responsible for its execution.\n3.3.2\nDependency\nop\nop\nop\nop\nop\nintra-thread\nthread2\nthread1\ninter-thread\nop\nop\nkernel\nkernel\nCUDA launch\nstream1\nthread1\nop\nop\nop\nkernel\nkernel\nCUDA synchronization\nstream1\nthread1\nkernel\nkernel\nkernel\nkernel\nintra-stream\nstream2\nstream1\ninter-stream\nop\nFigure 3. Four types of dependencies between the tasks.\nNext, we identify four types of dependencies that capture\nall possible relationships between tasks:\nCPU to CPU: This dependency includes both intra-thread\nand inter-thread relationships between CPU tasks. Tasks\nwithin the same thread naturally execute sequentially, form-\ning intra-thread dependencies between consecutive tasks.\nInter-thread dependencies occur when tasks on one thread\nblock tasks on another. For example, in PyTorch, the back-\nward pass runs on a separate thread, requiring the first\nbackward operator to wait until the last forward operator\ncompletes. We detect these dependencies by identifying\nsignificant execution gaps within threads and establishing\ncross-thread dependencies accordingly.\nCPU to GPU: GPU tasks are typically launched\nby corresponding CPU-side CUDA events, such as\ncudaLaunchKernel and cudaMemsetAsync.\nIn\nKineto traces, both CUDA runtime events and GPU ker-\n\n=== Page 5 ===\nLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training\nnels are tagged with a correlation ID, which we use to link\nCPU tasks with their corresponding GPU tasks.\nGPU to CPU: CUDA synchronization events, such as\ncudaDeviceSync or cudaStreamSync, are common\nduring model execution. When invoked on the CPU, these\nevents block execution until the relevant GPU kernels com-\nplete. As a result, they create dependencies from one or\nmore GPU tasks to the initiating CPU task.\nGPU to GPU: Similar to CPU-to-CPU dependencies,\nthis includes both intra-stream and inter-stream depen-\ndencies between GPU tasks.\nGPU kernels within the\nsame CUDA stream execute sequentially, meaning con-\nsecutive tasks in the same stream have direct dependen-\ncies. To identify inter-stream dependencies, we leverage a\nspecialized event-based synchronization mechanism cap-\ntured in the Kineto trace.\nSpecifically, we focus on a\npair of CUDA runtime events: cudaEventRecord and\ncudaStreamWaitEvent.\nThe cudaEventRecord\nmarks a synchronization point in one stream, recording\nan event after all preceding kernels on that stream have com-\npleted. The corresponding cudaStreamWaitEvent en-\nsures that a different stream waits until the recorded event\nis reached, creating a dependency between the two streams.\nThis mechanism allows us to accurately capture inter-stream\ndependencies, providing a precise representation of the exe-\ncution order across streams.\nTraining LLMs at scale typically spans a large number of\nmachines, resulting in significant communication overhead.\nTo mitigate this, overlapping the execution of computation\nand communication kernels is a common practice to reduce\nend-to-end iteration time. However, this overlap introduces\ncomplex inter-stream dependencies, which are overlooked\nby existing modeling approaches. Lumos is the first to target\nLLMs and capture their intricate dependencies, a critical\nstep toward accurate performance modeling and reliable\ndownstream optimization studies.\n3.4\nGraph Manipulation\nTo improve and optimize LLM training performance, re-\nsearchers and engineers can have many configurable options\nand optimization strategies. Commonly, they will ask what-\nif questions, such as:\nHow will the performance scale with additional GPUs?\nWhich parallelism configuration will deliver the best re-\nsults? How will changes to the model architecture impact\nperformance? Will a specific optimization improve perfor-\nmance, and by how much?\nWhile current distributed training frameworks make it eas-\nier to change configurations, deploying models with new\nsettings on real hardware requires substantial resources and\nincurs high costs, leading to long iteration cycles. The pro-\ncess becomes even more challenging if the desired changes,\nsuch as introducing a new operator fusion pattern, are not\nsupported by the framework, forcing developers to hack the\nunderlying code, which can be both time-consuming and\nprone to errors.\nTo address these challenges, the fine granularity and flexi-\nbility of execution graphs, combined with simulation, pro-\nvide an effective solution. By modifying the existing graph\nto reflect different model configurations, we can estimate\nperformance and explore what-if scenarios without requir-\ning large-scale physical deployments, accelerating iteration\nspeed and significantly reducing costs.\nLumos offers an interface that allows users to specify new\nmodel configurations, after which it manipulates the existing\nexecution graph to generate a new one reflecting the changes\nfor performance estimation. It currently supports modifi-\ncations to both model architectures\u2014such as adjusting the\nnumber of transformer layers and hidden size\u2014and paral-\nlelism strategies, including data parallelism and pipeline\nparallelism.\n1\n2\n3\n4\n1\n5\n2\n6\n3\n7\n4\n8\n5\n6\n7\n8\n1\n2\n1\n3\n2\n4\n3\n4\nForward\nBackward\ni: ith micro-batch\n2x PP\nrank_0\nrank_0\nFigure 4. Updated pipeline schedule for rank 0 with 2x PP, assum-\ning the number of micro-batches is equal to TP \u00d7 PP and 1F1B\nscheduling policy (Narayanan et al., 2021).\nFor changes in data parallelism, only the communication\nneeds adjustment by assigning new execution time to the\ncommunication tasks, as the local computation for each\nworker remain unchanged. For pipeline parallelism adjust-\nments, we first update the pipeline schedule to align with\nthe new configuration based on the scheduling policy, de-\ntermining the execution order of the forward and backward\npasses, as illustrated in Figure 4. Next, we group the tasks\nby layers and partition the original layers and their under-\nlying tasks into new stages. For example, assuming layers\nare evenly distributed, we calculate how many layers belong\nto each stage. The corresponding tasks are reassigned to\ntheir new stages, and communication tasks are inserted at\nappropriate points, to ensure correct synchronization and ex-\necution. We currently do not support modifications to tensor\nparallelism, as it is typically fixed in practice (e.g., within a\nsingle node) due to its high communication overhead. We\nleave the support for it as our future work.\nFor changes to model architecture, such as adjusting the\nhidden size, we modify the input tensor dimensions for the\nrelevant operators and kernels and update their execution\ntimes during simulation. When changing the number of\nlayers, we follow a process similar to that used for pipeline\n\n=== Page 6 ===\nLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training\nparallelism, dividing tasks into layers and applying the ad-\njustments accordingly.\nThroughout this manipulation process, we ensure that the\ndependency patterns from the original trace are preserved\nin the new graph to maintain correct execution, as we will\ndemonstrate in the evaluation.\n3.5\nSimulation\nAlgorithm 1 Lumos\u2019s Simulation Algorithm\nInput: Execution graph: G = (V, E)\nOutput: Trace with runtime details of all tasks\nR \u2190\u2205{Initialize the ready task set}\nP \u2190{0} {Initialize task processors}\nfor each task t \u2208G.tasks do\nt.dep \u2190|{t\u2032s fixed dependencies}|\nif t.dep = 0 then\nR \u2190R \u222a{t}\nend if\nend for\nwhile R \u0338= \u2205do\nt \u2190pick(R) {Select a ready task to execute}\np \u2190t.Processor\nR \u2190R \\ {t}\nfor each r \u2208get runtime dependencies(t) do\nr.dependents \u2190r.dependents \u222a{t}\nt.dep \u2190t.dep + 1\nend for\nif t.dep = 0 then\nt.start \u2190max(P[p], t.start)\nP[p] \u2190t.start + t.duration\nfor each c \u2208t.dependents do\nc.dep \u2190c.dep \u22121\nc.start \u2190max(c.start, t.start + t.duration)\nif c.dep = 0 then\nR \u2190R \u222a{c}\nend if\nend for\nend if\nend while\nBoth the original and modified execution graphs will be\nfed into the simulator to simulate execution and estimate\nperformance. During the simulation, the four types of depen-\ndencies outlined in Section 3.3.2 are maintained through two\nmechanisms. Fixed dependencies are determined at initial-\nization and remain unchanged throughout execution, such as\nthe sequential order of CPU tasks on the same thread. Run-\ntime dependencies, on the other hand, are assigned dynami-\ncally during runtime. For example, a cudaStreamSync\ntask must wait for the last kernel on a specific stream to\ncomplete, but which kernel will be last cannot be known\nprior to execution.\nAlgorithm 1 outlines the simulation process, beginning with\nthe assignment of fixed dependencies. In each iteration, a\nready task is selected and allocated to its respective proces-\nsor. The algorithm then checks for any runtime dependen-\ncies. If all dependencies are met, the task is executed, updat-\ning the processor\u2019s progress and the status of its dependent\ntasks. Otherwise, the task is deferred until all dependencies\nare resolved. The simulation generates a trace similar to\nthe input trace initially profiled from the real run, recording\nall runtime information of the tasks. This output trace can\nbe used not only to estimate the overall execution time but\nalso to analyze fine-grained execution characteristics, as we\nshow in Section 4.2.\n4\nEVALUATION\nWe implement Lumos in Python with approximately 5,200\nLoC. To leverage it, users need access to the source code\nto insert profiler hooks into their PyTorch models for col-\nlecting traces, typically requiring around 10 lines of code.\nLumos then offers a fully automated workflow: it begins\nby constructing the execution graph from the raw traces,\nmanipulates the graph based on new configurations, and\nconcludes with performance estimation through simulation.\nDepending on the complexity of the original traces, the\nentire process can range from a few seconds to several min-\nutes.\n4.1\nMethodology\nModels. We evaluate Lumos using NVIDIA\u2019s open-source\nGPT-3 implementation (NVIDIA, b) from the MLPerf Train-\ning Benchmarks. Our experiments involve training four\nmodel variants, adjusting the number of layers, hidden\nsize, feedforward network size, and attention heads, with\nmodel parameters ranging from 15 billion to the full 175\nbillion, as summarized in Table 1. We collect traces with\nPyTorch Kineto and evaluate performance across various\nparallelism strategies, exploring different combinations of\ntensor, pipeline, and data parallelism.\nInfrastructure. Our evaluation is conducted on a produc-\ntion ML cluster, using up to 512 NVIDIA H100 GPUs (on\n32 servers) interconnected with 8x 400Gbps per host in a\nRoCE DC-scale network. Our testing environment is based\non CUDA 12.4, PyTorch 2.5, Transformer Engine 0.12.0,\nand PyTorch Lightning 1.9.4.\nWe select dPRO (Hu et al., 2022) as the state-of-the-art base-\nline for comparison. In our evaluation, we first validate the\nreplay accuracy by comparing both iteration time and exe-\ncution breakdown against the ground truth and the baseline.\nNext, we evaluate the accuracy of our approach in estimat-\ning performance for new configurations, including changes\nin both parallelism strategies and model architectures.\n\n=== Page 7 ===\nLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training\nTable 1. Model sizes and architectures used in the evaluation. All other parameters follow the default values from the open-source GPT-3\nimplementation (NVIDIA, b).\nMODEL NAME\nnparams\nnlayers\ndmodel\ndffn\nnheads\ndhead\nGPT-3 15B\n15B\n48\n6144\n12288\n48\n128\nGPT-3 44B\n44B\n48\n12288\n24576\n48\n128\nGPT-3 117B\n117B\n96\n12288\n24576\n96\n128\nGPT-3 175B\n175B\n96\n12288\n49152\n96\n128\n2x2x4\n2x2x8\n2x4x2\n2x4x4\n4x2x2\n4x2x4\nGPT-3 15B Model Configuration (TPxPPxDP)\n0\n250\n500\n750\n1000\n1250\nExecution Time (ms)\n4x4x2\n4x4x4\n4x8x1\n4x8x2\n8x4x1\n8x4x2\nGPT-3 44B Model Configuration (TPxPPxDP)\n0\n500\n1000\n1500\n2000\n2500\n3000\nExecution Time (ms)\n4x8x2\n4x8x4\n8x4x2\n8x4x4\n8x8x1\n8x8x2\nGPT-3 117B Model Configuration (TPxPPxDP)\n0\n1000\n2000\n3000\n4000\n5000\n6000\nExecution Time (ms)\n4x8x4\n4x8x8\n4x8x16\n8x4x4\n8x4x8\n8x4x16\nGPT-3 175B Model Configuration (TPxPPxDP)\n0\n2000\n4000\n6000\n8000\nExecution Time (ms)\n0\n5\n10\n15\n20\n25\n30\nReplay Error (%)\n0\n5\n10\n15\n20\n25\n30\nReplay Error (%)\n0\n5\n10\n15\n20\n25\n30\nReplay Error (%)\n0\n5\n10\n15\n20\n25\n30\nReplay Error (%)\nExposed Compute (Actual)\nExposed Compute (dPRO)\nExposed Compute (Lumos)\nOverlapped (Actual)\nOverlapped (dPRO)\nOverlapped (Lumos)\nLumos Error\nExposed Comm (Actual)\nExposed Comm (dPRO)\nExposed Comm (Lumos)\ndPRO Error\nOther (Actual)\nOther (dPRO)\nOther (Lumos)\nFigure 5. Per-iteration training time with its breakdown across various model sizes and parallelism strategies: comparison of actual\nexecution, dPRO, and Lumos.\n4.2\nReplay\n4.2.1\nOverall Iteration Time\nIn Figure 5, we firstly compare the per-iteration execution\ntime replayed by Lumos and dPRO against the real execu-\ntion time measured from actual training across various GPT-\n3 model sizes (15B, 44B, 117B, and 175B) and parallelism\nstrategies. Across all configurations, Lumos maintains a re-\nplay error mostly under 5%, with an average error of 3.3%.\nIn contrast, dPRO\u2019s error reaches up to 21.8%, with an aver-\nage of 14%. While smaller models and simpler setups allow\ndPRO to predict the overall time relatively well, its accu-\nracy deteriorates as model size and complexity grow. The\ndiscrepancy highlights Lumos\u2019s robustness in accurately\ncapturing execution behaviors and modeling performance,\neven for larger models and complex deployment setups.\n4.2.2\nExecution Breakdown\nFigure 5 also provides a detailed breakdown of the execu-\ntion into key components: exposed compute (computation\nthat does not overlap with communication), exposed com-\nmunication (communication that does not overlap with com-\nputation), overlapped execution (where computation and\ncommunication run concurrently), and other (primarily idle\nperiods). This breakdown offers deeper insights into the\ndifferences in iteration times across configurations.\nThe analysis reveals that dPRO consistently overestimates\noverlapped execution and underestimates total iteration time,\nprimarily due to its inability to accurately model inter-\n\n=== Page 8 ===\nLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training\nstream dependencies, leading to overly optimistic predic-\ntions of parallel execution. In contrast, Lumos effectively\ncaptures the complex dependencies within the model and\nfaithfully replays the execution. It accurately reflects the\ndynamic interactions between computation and communi-\ncation, adapting to changes in model size and deployment\nconfiguration, and closely aligning with the real measure-\nment.\nIn large-scale distributed training, particularly for LLMs, a\nsubstantial portion of execution time is spent on commu-\nnication and synchronization between GPUs. To optimize\nperformance, engineers aim to maximize the overlap be-\ntween computation and communication kernels. Therefore,\nan accurate performance model that not only replays overall\nexecution time but also captures fine-grained details, such as\nthe degree of overlap, is essential. Such a model can provide\nvaluable insights for identifying performance bottlenecks\nand guiding optimization efforts.\n4.2.3\nSM Utilization\n0\n100\n200\n300\n400\n500\n600\n700\nTimeline (per 1 ms)\n0.0\n0.5\n1.0\nSM Utilization (%)\n0\n100\n200\n300\n400\n500\n600\n700\nTimeline (per 1 ms)\n0.0\n0.5\n1.0\nSM Utilization (%)\n0\n100\n200\n300\n400\n500\n600\n700\nTimeline (per 1 ms)\n0.0\n0.5\n1.0\nSM Utilization (%)\nActual\nLumos\ndPRO\nFigure 6. SM utilization of one iteration when training GPT-3 15B\nunder TP = 2, PP = 2 and DP = 4.\nAnalyzing SM (Streaming Multiprocessor) utilization is\nessential for identifying performance bottlenecks, such as\nidle periods or imbalanced workloads, to enhance GPU\nefficiency.\nIn this section, we examine the SM utilization over one\niteration of training the GPT-3 15B model, configured with\ntensor parallelism = 2, pipeline parallelism = 2, and data\nparallelism = 4. Utilization is defined as the fraction of\ntime, over 1ms intervals, during which at least one CUDA\nstream is actively executing tasks. This data is derived from\nprofiled and simulated traces by analyzing kernel activities\nthroughout the execution.\nAs shown in Figure 6, the SM utilization replayed by Lumos\nclosely match the actual measured utilization. In contrast,\ndPRO exhibits more fluctuations and significant discrepan-\ncies. This comparison, again, highlights Lumos\u2019s ability to\ncapture fine-grained execution details, validating its effec-\ntiveness in accurately modeling execution behavior.\n4.3\nGraph Manipulation\n4.3.1\nParallelism Strategy\n0\n200\n400\n600\n800\nExecution time (ms)\n2x2x8\n2x2x16\n2x2x32\nModel Configuration\n329\n148\n287\n38\n337\n138\n330\n33\n321\n156\n370\n32\n353\n123\n291\n30\n364\n112\n307\n30\n352\n124\n358\n30\nExposed Compute\nOverlapped\nExposed Comm\nOther\n(a) Execution breakdown for scaling data parallelism.\n0\n250\n500\n750\n1000\n1250\n1500\n1750\nExecution time (ms)\n2x4x4\n2x8x4\n2x16x4\nModel Configuration\n403\n56\n408\n46\n421\n46\n1008\n77\n452\n44\n865\n477\n386\n60\n378\n48\n405\n49\n927\n65\n422\n49\n853\n461\nExposed Compute\nOverlapped\nExposed Comm\nOther\n(b) Execution breakdown for scaling pipeline parallelism.\n0\n200\n400\n600\n800\n1000\n1200\n1400\nExecution time (ms)\n2x4x8\n2x8x8\n2x4x16\nModel Configuration\n379\n80\n415\n39\n414\n53\n547\n81\n387\n72\n813\n44\n359\n96\n378\n55\n399\n63\n481\n67\n371\n87\n823\n55\nExposed Compute\nOverlapped\nExposed Comm\nOther\n(c) Execution breakdown for simultaneous scaling of data and\npipeline parallelism.\nFigure 7. Runtime predictions for scale-out configurations. Each\nconfiguration (TPxPPxDP) is represented by two horizontal bars:\nthe upper bar shows the predicted value by Lumos, and the lower\nbar shows the actual value.\nNext, we evaluate Lumos\u2019s flexibility in generating new ex-\necution graphs from existing ones to estimate performance\nunder new configurations. Specifically, we demonstrate its\n\n=== Page 9 ===\nLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training\nability to predict scale-out performance by adjusting par-\nallelism strategies. Our experiments focus on GPT-3 15B,\nusing traces collected from a baseline configuration with\ntensor parallelism = 2, pipeline parallelism = 2, and data\nparallelism = 4.\nWe begin by exploring changes to data parallelism, where\nonly the execution time of communication tasks needs to be\nupdated. We currently estimate new communication time\nusing an in-house performance model built from fleet traces,\nas it is both readily available and accurate. While network\nsimulators like ASTRA-sim (Won et al., 2023) or analytical\nmodels (Moolchandani et al., 2023; Rashidi et al., 2022)\ncould also be used, predicting the runtime of individual\nkernels is beyond the scope of this work. We will explore\npotential integrations with these tools in the discussion sec-\ntion. To validate these predictions, we compare them against\nactual traces collected at larger scales. As shown in Fig-\nure 7a , Lumos accurately predicts both the total runtime\nand detailed performance breakdowns when scaling from\n16 GPUs to 32, 64, and 128 GPUs.\nSimilarly, Figure 7b demonstrates that Lumos can also accu-\nrately estimate execution time and breakdown when scaling\npipeline parallelism. We modify the baseline traces by split-\nting layers and underlying tasks into new stages, adding\ncommunication tasks, and reordering task execution accord-\ning to the new pipeline schedule. Finally, Figure 7c shows\nthat Lumos maintains high accuracy, with an average error\nof just 4.2% when scaling both data and pipeline paral-\nlelism simultaneously. These results prove that Lumos can\neffectively generate correct new executions through graph\nmanipulation for new parallelism strategies.\n4.3.2\nModel Architecture\nTable 2. Sizes and architectures for model variations.\nMODEL NAME\nnparams\nnlayers\ndmodel\ndffn\nGPT-3 15B\n15B\n48\n6144\n12288\nGPT-3 V1\n20B\n64\n6144\n12288\nGPT-3 V2\n30B\n96\n6144\n12288\nGPT-3 V3\n28B\n48\n9216\n18432\nGPT-3 V4\n44B\n48\n12288\n24576\nWe now validate Lumos\u2019s accuracy in estimating perfor-\nmance for different model architectures. Our evaluation\ncontinues with GPT-3 15B as the base model. To generate\nseveral variants, we modify the number of layers, hidden\nsizes, and feedforward network sizes. Table 2 summarizes\nthe sizes and architectures of the models used in this eval-\nuation. All models are trained using the configuration of\ntensor parallelism = 2, pipeline parallelism = 2, and data\nparallelism = 4.\nWhen increasing the number of layers, we duplicate the lay-\ners and corresponding tasks from the existing trace, insert\nthem into the graph at appropriate places, and reconstruct de-\npendencies with neighboring tasks according to the original\ndependency pattern. For changes in hidden size or feedfor-\nward network size, we adjust the input tensor dimensions\nfor all relevant operators and kernels. Ideally, new execution\ntimes should be assigned to all affected tasks to reflect the\ninput changes. However, we observe that only a few key\nkernels, such as GEMM and communication-related ones,\nexhibit significant runtime changes under different configu-\nrations. We similarly update the execution times for these\nkernels using the in-house performance model described in\nSection 4.3.1.\nGPT-3 v1\nGPT-3 v2\nGPT-3 v3\nGPT-3 v4\nModel Variants\n0\n500\n1000\n1500\n2000\n2500\nExecution time (ms)\nExposed Compute (Actual)\nExposed Compute (Lumos)\nOverlapped (Actual)\nOverlapped (Lumos)\nExposed Comm (Actual)\nExposed Comm (Lumos)\nOther (Actual)\nOther (Lumos)\nFigure 8. Iteration time breakdown of model variations. The left\nbars show the actual values, and the right hatched bars show the\npredicted values.\nFigure 8 presents the iteration time breakdown across these\nmodel variations, showing both the actual and predicted per-\nformance. The predicted values, represented with hatched\npatterns, align closely with the actual measurements, demon-\nstrating that Lumos accurately reproduces the execution and\nestimates the performance under different model architec-\nture changes.\nOverall, the results shown in Section 4.3.1 and Section 4.3.2\ndemonstrate Lumos\u2019s ability to leverage existing traces to\ngenerate new execution graphs for both parallelism con-\nfigurations and model architecture variations, to provide\naccurate performance estimates through simulation. This\npredictive capability significantly reduces the need for costly\nhardware resources, positioning Lumos as a practical tool\nfor efficient model configurations exploration.\n5\nDISCUSSION\nIn this section, we discuss the profiling overhead, adaptabil-\nity, scope, and limitations of Lumos.\nProfiling Overhead and Cost. Lumos leverages PyTorch\nKineto (Kineto) to collect traces and construct the execu-\ntion graph. Profiling requires only a few lines of hook\ncode, which makes it much more user-friendly than ex-\n\n=== Page 10 ===\nLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training\nisting approaches that require additional instrumentation.\nAlthough profiling can impact execution, capturing a sin-\ngle iteration\u2014or just a few\u2014is sufficient, as the model\u2019s\nexecution pattern remains consistent across iterations. This\nensures that the overall profiling overhead is negligible in\nthe context of the entire training process.\nAdaptability of Lumos. Lumos requires profiling traces\nthat capture both CPU and GPU activities, including frame-\nwork operators, CUDA runtime events, and GPU kernels.\nSimilar profiling capabilities are available in other ML\nframeworks, such as TensorFlow Profiler (TensorFlow, b).\nLumos\u2019s post-processing stages for constructing and manip-\nulating execution graphs are framework-agnostic, making it\neasy to adapt to other frameworks.\nOur methodology also extends well to other LLMs and\nML models in other domains, as it does not rely on model-\nspecific information to construct execution graphs for per-\nformance modeling. While we make certain assumptions\nduring graph manipulation, such as where to insert layers\nand which tasks would be affected, our method remains\nbroadly applicable, given the shared transformer-based ar-\nchitecture of most modern LLMs.\nSimilarly, although this paper focuses on LLM training,\nwhere communication overhead is higher and model be-\nhavior more complex, Lumos is also applicable to the in-\nference. We anticipate even broader use cases as LLM\ninference grows more complicated, such as distributed in-\nference (Wei et al., 2022; Li et al., 2023) and SSD-based\ninference (Wilkening et al., 2021; Sun et al., 2022).\nKernel Execution Time Prediction. Changing configu-\nrations can introduce new GPU kernels not present in the\noriginal trace, such as new communication kernels when\nadjusting parallelism strategies or new computation kernels\nwhen modifying model architectures. Accurately predict-\ning performance for new configurations requires estimating\nthe runtime of these new or altered kernels. Currently, we\nestimate the runtime of unseen kernels using an in-house\nGPU kernel performance model, built by analyzing fleet\nGPU traces, as it is available and accurate. However, al-\nternative methods are also available. For communication\nkernels, we can use metadata like message size, collective\nalgorithm, and networking environment to estimate per-\nformance with network simulators like ASTRA-sim (Won\net al., 2023) and HeterSim (Tang et al., 2024), or analytical\nmodels (Moolchandani et al., 2023; Rashidi et al., 2022).\nFor computation kernels, we can simply measure runtime\nthrough individual microbenchmarks. However, predicting\nkernel runtimes is beyond the scope of this work.\nThe primary goal of Lumos is to deliver a fine-grained execu-\ntion graph and an accurate performance model to capture the\ncomplexities of LLM execution and provide reliable perfor-\nmance estimates. Developers can implement optimized indi-\nvidual kernels, profile their runtime, and integrate the results\ninto Lumos to predict the overall runtime, saving the engi-\nneering efforts of porting them into the frameworks. More\nimportantly, it can offer invaluable insights for optimization\neven before implementation by answering what-if questions,\nsuch as how much the overall runtime would be reduced if\na kernel ran twice as fast, and identifying which optimiza-\ntion would yield the greatest performance improvement.\nBy modifying existing traces and estimating performance\nthrough simulation, Lumos makes performance evaluation\nand optimization more efficient and cost-effective.\nLimitations. Lumos currently focuses on modeling and\nsimulating the timing of model execution. In predicting per-\nformance for modified configurations, we assume the model\nwill function as expected under the new settings, without\nunforeseen issues such as out-of-memory errors. Estimating\nsystem-level metrics, such as FLOPS utilization, memory\nconsumption, bandwidth usage, or energy efficiency, lies\nbeyond Lumos\u2019s current scope. These metrics, essential for\noptimizing LLM efficiency, are part of our future plans to\nprovide more comprehensive performance insights.\n6\nCONCLUSION\nIn this paper, we introduced Lumos, a trace-driven perfor-\nmance modeling and estimation toolkit for large-scale LLM\ntraining. Lumos captures complex behaviors through de-\ntailed execution graphs built from profiling traces, enabling\naccurate performance modeling and estimation. Our evalu-\nation on a production ML cluster with up to 512 NVIDIA\nH100 GPUs shows an average replay error of only 3.3%\nacross diverse model architectures and parallelism config-\nurations. By manipulating existing graphs to generate new\nones for different configurations and predicting performance\nthrough simulation, Lumos supports efficient optimization\nexploration.\nACKNOWLEDGEMENTS\nWe sincerely thank the anonymous reviewers for their sug-\ngestions on earlier versions of this manuscript. This work\nwas in part sponsored by Meta through a student researcher\nappointment. This work was also in part supported by NSF\nCAREER Award CCF-2326182, an Intel Research Award,\na Sloan Research Fellowship, a Microsoft Research Fellow-\nship, and a Facebook Research Faculty Award.\nREFERENCES\nBang, J., Choi, Y., Kim, M., Kim, Y., and Rhu, M. vtrain:\nA simulation framework for evaluating cost-effective and\ncompute-optimal large language model training. arXiv\n\n=== Page 11 ===\nLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training\npreprint arXiv:2312.12391, 2023.\nBeltagy, I., Peters, M. E., and Cohan, A.\nLong-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150, 2020.\nBrown, T. B. Language models are few-shot learners. arXiv\npreprint arXiv:2005.14165, 2020.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. Journal of Machine Learning Research,\n24(240):1\u2013113, 2023.\nDas, D., Mellempudi, N., Mudigere, D., Kalamkar, D.,\nAvancha, S., Banerjee, K., Sridharan, S., Vaidyanathan,\nK., Kaul, B., Georganas, E., et al. Mixed precision train-\ning of convolutional neural networks using integer opera-\ntions. arXiv preprint arXiv:1802.00930, 2018.\nHashemi, S. H., Abdu Jyothi, S., and Campbell, R. Tictac:\nAccelerating distributed deep learning with communica-\ntion scheduling. Proceedings of Machine Learning and\nSystems, 1:418\u2013430, 2019.\nHu, H., Jiang, C., Zhong, Y., Peng, Y., Wu, C., Zhu, Y., Lin,\nH., and Guo, C. dpro: A generic performance diagnosis\nand optimization toolkit for expediting distributed dnn\ntraining. Proceedings of Machine Learning and Systems,\n4:623\u2013637, 2022.\nIsaev, M., McDonald, N., Dennison, L., and Vuduc, R. Cal-\nculon: a methodology and tool for high-level co-design\nof systems and large language models. In Proceedings\nof the International Conference for High Performance\nComputing, Networking, Storage and Analysis, pp. 1\u201314,\n2023.\nJacobs, S. A., Tanaka, M., Zhang, C., Zhang, M., Song,\nS. L., Rajbhandari, S., and He, Y. Deepspeed ulysses:\nSystem optimizations for enabling training of extreme\nlong sequence transformer models.\narXiv preprint\narXiv:2309.14509, 2023.\nJia, Z., Padon, O., Thomas, J., Warszawski, T., Zaharia, M.,\nand Aiken, A. Taso: optimizing deep learning computa-\ntion with automatic generation of graph substitutions. In\nProceedings of the 27th ACM Symposium on Operating\nSystems Principles, pp. 47\u201362, 2019.\nJouppi, N., Kurian, G., Li, S., Ma, P., Nagarajan, R., Nai,\nL., Patil, N., Subramanian, S., Swing, A., Towles, B.,\net al. Tpu v4: An optically reconfigurable supercomputer\nfor machine learning with hardware support for embed-\ndings. In Proceedings of the 50th Annual International\nSymposium on Computer Architecture, pp. 1\u201314, 2023.\nKineto.\nPytorch kineto.\nhttps://github.com/\npytorch/kineto.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention. In Proceedings of the ACM SIGOPS\n29th Symposium on Operating Systems Principles, 2023.\nLi, J., Jiang, Y., Zhu, Y., Wang, C., and Xu, H. Accelerating\ndistributed {MoE} training and inference with lina. In\n2023 USENIX Annual Technical Conference (USENIX\nATC 23), pp. 945\u2013959, 2023.\nLi, S., Xue, F., Baranwal, C., Li, Y., and You, Y. Sequence\nparallelism: Long sequence training from system perspec-\ntive. arXiv preprint arXiv:2105.13120, 2021.\nLin, Z., Feng, L., Ardestani, E. K., Lee, J., Lundell, J.,\nKim, C., Kejariwal, A., and Owens, J. D. Building a\nperformance model for deep learning recommendation\nmodel training on gpus. arXiv preprint arXiv:2201.07821,\n2022.\nLiu, H., Zaharia, M., and Abbeel, P. Ring attention with\nblockwise transformers for near-infinite context. arXiv\npreprint arXiv:2310.01889, 2023.\nMa, R., Georganas, E., Heinecke, A., Gribok, S., Boutros,\nA., and Nurvitadhi, E. Fpga-based ai smart nics for scal-\nable distributed ai training systems.\nIEEE Computer\nArchitecture Letters, 21(2):49\u201352, 2022.\nMiao, X., Wang, Y., Jiang, Y., Shi, C., Nie, X., Zhang, H.,\nand Cui, B. Galvatron: Efficient transformer training over\nmultiple gpus using automatic parallelism. arXiv preprint\narXiv:2211.13878, 2022.\nMoolchandani, D., Kundu, J., Ruelens, F., Vrancx, P., Even-\nblij, T., and Perumkunnil, M. Amped: An analytical\nmodel for performance in distributed training of trans-\nformers. In 2023 IEEE International Symposium on Per-\nformance Analysis of Systems and Software (ISPASS), pp.\n306\u2013315. IEEE, 2023.\nNarayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Pat-\nwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P.,\nBernauer, J., Catanzaro, B., et al. Efficient large-scale\nlanguage model training on gpu clusters using megatron-\nlm. In Proceedings of the International Conference for\nHigh Performance Computing, Networking, Storage and\nAnalysis, pp. 1\u201315, 2021.\nNVIDIA.\nCupti.\nhttps://docs.nvidia.com/\ncuda/cupti/, a.\nNVIDIA. Gpt-3 implementation.\nhttps://github.\ncom/mlcommons/training/tree/master/\nlarge_language_model/megatron-lm, b.\n\n=== Page 12 ===\nLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training\nNVIDIA.\nNvidia blackwell architecture.\nhttps:\n//www.nvidia.com/en-us/data-center/\ntechnologies/blackwell-architecture/,\nc.\nNVIDIA.\nNvprof.\nhttps://docs.nvidia.com/\ncuda/profiler-users-guide/index.html,\nd.\nNVIDIA.\nNsight.\nhttps://developer.nvidia.\ncom/nsight-systems, e.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in neural information\nprocessing systems, 35:27730\u201327744, 2022.\nPyTorch. Pytorch. https://pytorch.org/.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero:\nMemory optimizations toward training trillion parameter\nmodels. In SC20: International Conference for High Per-\nformance Computing, Networking, Storage and Analysis,\npp. 1\u201316. IEEE, 2020.\nRashidi, S., Sridharan, S., Srinivasan, S., and Krishna, T.\nAstra-sim: Enabling sw/hw co-design exploration for\ndistributed dl training platforms. In 2020 IEEE Interna-\ntional Symposium on Performance Analysis of Systems\nand Software (ISPASS), pp. 81\u201392. IEEE, 2020.\nRashidi, S., Won, W., Srinivasan, S., Sridharan, S., and Kr-\nishna, T. Themis: A network bandwidth-aware collective\nscheduling policy for distributed training of dl models. In\nProceedings of the 49th Annual International Symposium\non Computer Architecture, pp. 581\u2013596, 2022.\nSabne, A. Xla: Compiling machine learning for peak per-\nformance. Google Res, 2020.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,\nJ., and Catanzaro, B.\nMegatron-lm: Training multi-\nbillion parameter language models using model paral-\nlelism. arXiv preprint arXiv:1909.08053, 2019.\nSmith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhan-\ndari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G.,\nKorthikanti, V., et al. Using deepspeed and megatron to\ntrain megatron-turing nlg 530b, a large-scale generative\nlanguage model. arXiv preprint arXiv:2201.11990, 2022.\nSun, X., Wan, H., Li, Q., Yang, C.-L., Kuo, T.-W., and\nXue, C. J. Rm-ssd: In-storage computing for large-scale\nrecommendation inference. In 2022 IEEE International\nSymposium on High-Performance Computer Architecture\n(HPCA), pp. 1056\u20131070. IEEE, 2022.\nTang, Y., Yuan, T., Cao, F., Wang, L., Guo, Z., Zhao, Y.,\nand Li, R. Simulating llm training in cxl-based heteroge-\nneous computing cluster. In IEEE INFOCOM 2024-IEEE\nConference on Computer Communications Workshops\n(INFOCOM WKSHPS), pp. 1\u20136. IEEE, 2024.\nTensorFlow. Tensorflow. https://www.tensorflow.\norg/, a.\nTensorFlow.\nTensorflow profiler.\nhttps://www.\ntensorflow.org/guide/profiler, b.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023.\nVaswani, A. Attention is all you need. Advances in Neural\nInformation Processing Systems, 2017.\nWei, Y., Langer, M., Yu, F., Lee, M., Liu, J., Shi, J., and\nWang, Z. A gpu-specialized inference parameter server\nfor large-scale deep recommendation models. In Pro-\nceedings of the 16th ACM Conference on Recommender\nSystems, pp. 408\u2013419, 2022.\nWilkening, M., Gupta, U., Hsia, S., Trippel, C., Wu, C.-J.,\nBrooks, D., and Wei, G.-Y. Recssd: near data processing\nfor solid state drive based recommendation inference. In\nProceedings of the 26th ACM International Conference\non Architectural Support for Programming Languages\nand Operating Systems, pp. 717\u2013729, 2021.\nWon, W., Heo, T., Rashidi, S., Sridharan, S., Srinivasan, S.,\nand Krishna, T. Astra-sim2. 0: Modeling hierarchical net-\nworks and disaggregated systems for large-model training\nat scale. In 2023 IEEE International Symposium on Per-\nformance Analysis of Systems and Software (ISPASS), pp.\n283\u2013294. IEEE, 2023.\nXu, Y., Lee, H., Chen, D., Hechtman, B., Huang, Y., Joshi,\nR., Krikun, M., Lepikhin, D., Ly, A., Maggioni, M., et al.\nGspmd: general and scalable parallelization for ml com-\nputation graphs. arXiv preprint arXiv:2105.04663, 2021.\nYou, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli,\nS., Song, X., Demmel, J., Keutzer, K., and Hsieh, C.-J.\nLarge batch optimization for deep learning: Training bert\nin 76 minutes. arXiv preprint arXiv:1904.00962, 2019.\nZhao, J., Gao, X., Xia, R., Zhang, Z., Chen, D., Chen, L.,\nZhang, R., Geng, Z., Cheng, B., and Jin, X. Apollo:\nAutomatic partition-based operator fusion through layer\nby layer optimization. Proceedings of Machine Learning\nand Systems, 4:1\u201319, 2022.\n\n=== Page 13 ===\nLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training\nZheng, L., Li, Z., Zhang, H., Zhuang, Y., Chen, Z., Huang,\nY., Wang, Y., Xu, Y., Zhuo, D., Xing, E. P., et al. Alpa:\nAutomating inter-and {Intra-Operator} parallelism for\ndistributed deep learning. In 16th USENIX Symposium\non Operating Systems Design and Implementation (OSDI\n22), pp. 559\u2013578, 2022.\nZhu, H., Phanishayee, A., and Pekhimenko, G. Daydream:\nAccurately estimating the efficacy of optimizations for\n{DNN} training. In 2020 USENIX Annual Technical\nConference (USENIX ATC 20), pp. 337\u2013352, 2020.\n",
      "searchable_content": "page 1 lumos efficient performance modeling and estimation for large-scale llm training mingyu liang 1 hiwot tadese kassa 2 wenyin fu 2 brian coutinho 2 louis feng 2 christina delimitrou 3 abstract training llms in distributed environments presents significant challenges due to the complexity of model execution deployment systems and the vast space of configurable strategies. although various optimization techniques exist achieving high efficiency in practice remains difficult. accurate performance models that effectively characterize and predict a model s behavior are essential for guiding optimization efforts and system- level studies. we propose lumos a trace-driven performance modeling and estimation toolkit for large-scale llm training designed to accurately capture and predict the execution behaviors of modern llms. we evaluate lumos on a production ml cluster with up to 512 nvidia h100 gpus using various gpt-3 variants demonstrating that it can replay execution time with an average error of just 3.3 along with other runtime details across different models and configurations. additionally we validate its ability to estimate performance for new setups from existing traces facilitating efficient exploration of model and deployment configurations. 1 introduction in recent years large language models llms have trans- formed many aspects of daily life. the availability of vast datasets along with advancements in computational resources has enabled the development of increasingly complex models such as chatgpt ouyang et al. 2022 llama touvron et al. 2023 and palm chowdhery et al. 2023 . however efficiently training these llms presents significant challenges necessitating both hardware and software innovations across the system stack. to meet these demands efforts have focused on address- ing various bottlenecks. key areas of optimization include the development of ai-specific hardware e.g. nvidia gpus nvidia c and smartnics ma et al. 2022 im- provements in memory systems kwon et al. 2023 ra- jbhandari et al. 2020 the design of optimal parallelism strategies zheng et al. 2022 isaev et al. 2023 over- lapping communication with computation hashemi et al. 2019 narayanan et al. 2021 and advancements in algo- rithms beltagy et al. 2020 you et al. 2019 . despite these innovations ensuring training efficiency re- mains a significant challenge. diagnosing inefficiencies in llms is particularly difficult because runtime traces pro- duced by machine learning ml frameworks pytorch 1cornell university ithaca usa 2meta menlo park usa 3massachusetts institute of technology cambridge usa. corre- spondence to mingyu liang ml2585 cornell.edu . proceedings of the 8 th mlsys conference santa clara ca usa 2025. copyright 2025 by the author s . tensorflow a are often dense and require deep expertise to interpret effectively. moreover runtime behavior can vary significantly across different model architectures de- ployment configurations accelerator types network infras- tructures and other system components. these variations can cause performance bottlenecks to shift unpredictably making them difficult to identify and address. an additional challenge lies in the vast search space of optimization pos- sibilities. finding the optimal solution within this space is time-consuming and resource-intensive as it requires extensive experimentation on real hardware demanding significant resources and incurring high costs. a key step toward achieving efficiency is to accurately char- acterize and understand the behavior of these models. one common approach is to build performance models that cap- ture model execution which also provide a solid founda- tion for further optimization studies. while existing ef- forts moolchandani et al. 2023 isaev et al. 2023 develop analytical models to predict high-level performance based on exposed model parameters they often miss essential un- derlying execution details. to address this limitation recent work hu et al. 2022 zhu et al. 2020 lin et al. 2022 bang et al. 2023 has leveraged runtime traces to construct fine-grained execution graphs providing deeper insights into the execution process. however current modeling methods struggle to address the new complexities in modern llms. training llms at scale involves deploying across multiple machines introducing substantial communication overhead. as one example of optimization overlapping computation with communication arxiv 2504.09307v1 cs.dc 12 apr 2025 page 2 lumos efficient performance modeling and estimation for large-scale llm training 0 1000 2000 3000 4000 5000 6000 7000 execution time breakdown ms dpro actual 3235 1691 1417 81 4287 885 2261 178 exposed compute overlapped exposed comm other figure 1. execution breakdown for one training iteration of gpt-3 175b configured with tensor parallelism 8 pipeline parallelism 4 and data parallelism 8. can reduce end-to-end training time but it also introduces complex inter-stream dependencies that are challenging to model accurately. figure 1 shows the execution time break- down for a single training iteration of the gpt-3 175b model along with replayed results from dpro hu et al. 2022 a state-of-the-art trace-driven performance modeling tool. the comparison reveals significant gaps between the simulated and actual results highlighting the challenges in capturing the full intricacies of llm execution. to overcome these difficulties we propose lumos a trace- driven performance modeling and estimation toolkit for large-scale llm training. to the best of our knowledge lumos is the first system to provide accurate performance models that effectively capture the execution behaviors of llms. it leverages built-in profiling tools from ml frame- works such as pytorch kineto kineto without requir- ing any custom instrumentation in models or frameworks thereby minimizing the profiling overhead. furthermore to streamline the exploration of optimization opportunities lumos also offers the flexibility to modify and generate new execution graphs from existing traces. this capability facilitates the exploration of optimal config- urations such as adjusting parallelism strategies e.g. data and pipeline parallelism and fine-tuning model architec- tures e.g. number of layers hidden size . by estimating performance through simulation rather than experimenting on real hardware lumos can significantly reduce cost and accelerate the optimization process. the main contributions of our work are the following by utilizing only built-in profiling traces from ml frameworks lumos constructs a comprehensive exe- cution graph that identifies all dependencies between executed tasks enabling accurate performance model- ing of large-scale llm training. beyond estimating overall execution time the fine granularity of lumos al- lows it to reproduce detailed execution characteristics facilitating deeper analysis and downstream optimiza- tion studies. with a detailed execution graph lumos offers users a convenient way to explore various model configura- tions including adjustments to parallelism strategies and model architectures. by manipulating the existing graph to generate new ones for different configurations and by predicting performance through simulation lu- mos streamlines the optimization process and enables efficient and low-cost configuration exploration. we evaluate lumos using various gpt-3 model vari- ants on a production ml cluster with up to 512 nvidia h100 gpus. our results show that lumos can accurately replay execution achieving an average error of only 3.3 across different models and deploy- ment configurations. additionally we demonstrate that lumos accurately reproduces detailed execution statis- tics such as execution time breakdown and sm utiliza- tion showing significant improvements over existing approaches. finally we validate its ability to estimate performance for new configurations and deployments achieving high accuracy when adjusting parallelism strategies and tuning various model architectures. 2 related work 2.1 profiling tools and traces as the ml system stack evolves rapidly profiling tools play a crucial role in understanding model execution characteris- tics and identifying performance bottlenecks. as hardware accelerators like gpus nvidia c and tpus jouppi et al. 2023 become increasingly essential vendors offer special- ized tools such as nvprof nvidia d cupti nvidia a and nsight nvidia e to expose hardware perfor- mance counters providing developers with critical insights into performance metrics and enabling effective optimiza- tion. to improve the interpretability of profiling results ml frameworks also provide built-in tools for collecting ex- ecution statistics at the operator level. these tools often integrate hardware-level traces offering a complete view of the entire stack from host to device. for instance pytorch kineto kineto leverages cupti nvidia a to capture runtime information for pytorch operators cuda events and gpu kernels seamlessly linking them to provide a holistic perspective on model execution. 2.2 llms and parallelism strategies most modern llms are built on transformer architec- tures vaswani 2017 which rely on self-attention mecha- nisms to capture long-range dependencies in sequential data. these models feature multiple stacked layers of attention and feedforward networks with parameter sizes growing rapidly over the years. for example gpt-2 radford et al. 2019 introduced in 2019 had 1.5 billion parameters gpt- page 3 lumos efficient performance modeling and estimation for large-scale llm training 3 brown 2020 in 2020 expanded to 175 billion parameters and palm chowdhery et al. 2023 reached 540 billion pa- rameters by 2022. training llms presents significant computational and mem- ory challenges especially as model sizes grow beyond the capacity of individual gpus. to address these limitations 3d parallelism a hybrid approach combining data tensor and pipeline parallelism has become essential for efficient large-scale training narayanan et al. 2021 shoeybi et al. 2019 smith et al. 2022 chowdhery et al. 2023 . each form of parallelism contributes uniquely data parallelism dp distributes training batches across devices synchro- nizing gradients during updates tensor parallelism tp splits large tensors across multiple gpus allowing shared computation with frequent communication and pipeline parallelism pp partitions the model into sequential stages with each stage processed on different devices in a coordi- nated pipeline. despite the benefits configuring 3d parallelism introduces significant complexity requiring careful coordination across these strategies to balance workloads and minimize commu- nication overhead. recent research has focused on automat- ing these configurations to reduce the burden on develop- ers and ensure efficient distributed execution. for example gspmd xu et al. 2021 extends the xla compiler sabne 2020 to support various parallelism paradigms through user annotations. alpa zheng et al. 2022 automates model parallelization by optimizing intra- and inter-operator paral- lelism for efficient distributed execution. galvatron miao et al. 2022 introduces a decision tree to decompose the search space and designs a dynamic programming algorithm to generate the optimal plan. emerging techniques like sequence parallelism li et al. 2021 jacobs et al. 2023 liu et al. 2023 further address the challenges of training on long sequences by distribut- ing computations along the sequence dimension reducing memory overhead and communication bottlenecks. 2.3 performance modeling simulation and optimization the complexity of llms poses challenges and opportuni- ties in system design and optimization with performance modeling serving as a critical foundation for diagnosing and optimizing overall efficiency. there are two primary approaches to building perfor- mance models. the first relies on analytical models. amped moolchandani et al. 2023 introduces an analytical model to estimate performance in distributed transformer training under various model parameters and parallelism strategies. similarly calculon isaev et al. 2023 provides a parameterized analytical model that explores the co-design space of software and hardware configurations to identify optimal system designs for llms. however these analyt- ical models are often tailored to specific implementations and hardware configurations limiting their ability to gen- eralize in the face of rapid model and system evolution. moreover they typically provide high-level performance estimates making them inadequate for optimizations like mixed precision training das et al. 2018 zhu et al. 2020 and operator fusion zhao et al. 2022 jia et al. 2019 . the second approach leverages trace-based models to sim- ulate execution and derive optimization insights. for ex- ample astra-sim rashidi et al. 2020 and astra- sim2.0 won et al. 2023 simulate distributed training with a cycle-level and analytical network backend evaluating collective communication algorithms and network topolo- gies. in lin et al. 2022 the authors analyze critical paths within profiled traces to predict per-batch training time for dlrm. daydream zhu et al. 2020 uses kernel-level de- pendency graphs collected with cupti to predict runtime under specific optimizations while dpro hu et al. 2022 builds a global dataflow graph by tracking dependencies among operators to estimate dnn training performance. however these trace-based approaches fail to fully capture the complexities inherent in llm execution. to the best of our knowledge this work is the first to leverage traces for accurately modeling the intricate behaviors of llms accounting for detailed operator and kernel interactions es- sential for precise performance prediction. 3 design 3.1 overview figure 2 presents the workflow of lumos our trace-driven performance modeling and estimation toolkit for distributed llm training. the process begins with collecting runtime profiling traces from popular frameworks such as tensor- flow and pytorch. these raw traces are then analyzed to extract key meta-information which is used to construct a detailed task-level execution graph. the execution graph can be modified to adjust model configurations such as parallelism strategies and architectures generating new configuration-specific graphs. finally the simulator uses these graphs to either replay the original execution or pre- dict performance under alternative configurations providing insights into potential optimizations and enabling effective exploration of what-if scenarios. we initially focus on pytorch due to its widespread use in both academia and industry along with its advanced profiling capabilities. however our approach is flexible by design and can be extended to support other ml frameworks. we will discuss the adaptability of lumos in section 5. page 4 lumos efficient performance modeling and estimation for large-scale llm training tensorflow pytorch trace collection def training_iteration_loop with torch.profiler.profile activities torch.profiler.profileractivity.cpu torch.profiler.profileractivity.cuda execution_trace_observer etobserver as pf for iter in range n model.step pf.step trace extraction task dependency execution graph graph manipulation simulator replay predict model parallelism hyperparamter new configuration parallelism strategy model architecture new configuration new execution graph figure 2. overview of lumos s workflow. 3.2 traces collection we collect profiling traces using pytorch kineto kineto which captures comprehensive runtime information about pytorch operators cuda runtime events and gpu ker- nels including name start time duration cuda stream id thread id correlation id and more. unlike previous methods such as daydream and dpro which necessitate extensive framework and model instrumentation our profil- ing involves adding only a few lines of code into the model as shown in the code snippet at the top left of figure 2 significantly improving usability with minimal effort. 3.3 execution graph the essence of a model s execution lies in its execution graph which maps out the tasks being performed and the dependencies between them. motivated by prior ap- proaches zhu et al. 2020 hu et al. 2022 bang et al. 2023 we construct a low-level execution graph to accu- rately represent model execution. however we have incor- porated several enhancements to capture the complex execu- tion characteristics of llms ensuring more accurate mod- eling and offering the flexibility to estimate performance for new model configurations. 3.3.1 tasks to streamline the design our execution graph includes only the following two types of tasks cpu tasks these include all executions happened on the cpu including pytorch operators and cuda runtime events. for each cpu task we record its metadata along with the specific cpu thread on which it runs. gpu tasks these include all executions happened on the gpu which primarily consist of gpu kernels. for each gpu task we log its metadata along with the corresponding cuda stream responsible for its execution. 3.3.2 dependency op op op op op intra-thread thread2 thread1 inter-thread op op kernel kernel cuda launch stream1 thread1 op op op kernel kernel cuda synchronization stream1 thread1 kernel kernel kernel kernel intra-stream stream2 stream1 inter-stream op figure 3. four types of dependencies between the tasks. next we identify four types of dependencies that capture all possible relationships between tasks cpu to cpu this dependency includes both intra-thread and inter-thread relationships between cpu tasks. tasks within the same thread naturally execute sequentially form- ing intra-thread dependencies between consecutive tasks. inter-thread dependencies occur when tasks on one thread block tasks on another. for example in pytorch the back- ward pass runs on a separate thread requiring the first backward operator to wait until the last forward operator completes. we detect these dependencies by identifying significant execution gaps within threads and establishing cross-thread dependencies accordingly. cpu to gpu gpu tasks are typically launched by corresponding cpu-side cuda events such as cudalaunchkernel and cudamemsetasync. in kineto traces both cuda runtime events and gpu ker- page 5 lumos efficient performance modeling and estimation for large-scale llm training nels are tagged with a correlation id which we use to link cpu tasks with their corresponding gpu tasks. gpu to cpu cuda synchronization events such as cudadevicesync or cudastreamsync are common during model execution. when invoked on the cpu these events block execution until the relevant gpu kernels com- plete. as a result they create dependencies from one or more gpu tasks to the initiating cpu task. gpu to gpu similar to cpu-to-cpu dependencies this includes both intra-stream and inter-stream depen- dencies between gpu tasks. gpu kernels within the same cuda stream execute sequentially meaning con- secutive tasks in the same stream have direct dependen- cies. to identify inter-stream dependencies we leverage a specialized event-based synchronization mechanism cap- tured in the kineto trace. specifically we focus on a pair of cuda runtime events cudaeventrecord and cudastreamwaitevent. the cudaeventrecord marks a synchronization point in one stream recording an event after all preceding kernels on that stream have com- pleted. the corresponding cudastreamwaitevent en- sures that a different stream waits until the recorded event is reached creating a dependency between the two streams. this mechanism allows us to accurately capture inter-stream dependencies providing a precise representation of the exe- cution order across streams. training llms at scale typically spans a large number of machines resulting in significant communication overhead. to mitigate this overlapping the execution of computation and communication kernels is a common practice to reduce end-to-end iteration time. however this overlap introduces complex inter-stream dependencies which are overlooked by existing modeling approaches. lumos is the first to target llms and capture their intricate dependencies a critical step toward accurate performance modeling and reliable downstream optimization studies. 3.4 graph manipulation to improve and optimize llm training performance re- searchers and engineers can have many configurable options and optimization strategies. commonly they will ask what- if questions such as how will the performance scale with additional gpus which parallelism configuration will deliver the best re- sults how will changes to the model architecture impact performance will a specific optimization improve perfor- mance and by how much while current distributed training frameworks make it eas- ier to change configurations deploying models with new settings on real hardware requires substantial resources and incurs high costs leading to long iteration cycles. the pro- cess becomes even more challenging if the desired changes such as introducing a new operator fusion pattern are not supported by the framework forcing developers to hack the underlying code which can be both time-consuming and prone to errors. to address these challenges the fine granularity and flexi- bility of execution graphs combined with simulation pro- vide an effective solution. by modifying the existing graph to reflect different model configurations we can estimate performance and explore what-if scenarios without requir- ing large-scale physical deployments accelerating iteration speed and significantly reducing costs. lumos offers an interface that allows users to specify new model configurations after which it manipulates the existing execution graph to generate a new one reflecting the changes for performance estimation. it currently supports modifi- cations to both model architectures such as adjusting the number of transformer layers and hidden size and paral- lelism strategies including data parallelism and pipeline parallelism. 1 2 3 4 1 5 2 6 3 7 4 8 5 6 7 8 1 2 1 3 2 4 3 4 forward backward i ith micro-batch 2x pp rank_0 rank_0 figure 4. updated pipeline schedule for rank 0 with 2x pp assum- ing the number of micro-batches is equal to tp pp and 1f1b scheduling policy narayanan et al. 2021 . for changes in data parallelism only the communication needs adjustment by assigning new execution time to the communication tasks as the local computation for each worker remain unchanged. for pipeline parallelism adjust- ments we first update the pipeline schedule to align with the new configuration based on the scheduling policy de- termining the execution order of the forward and backward passes as illustrated in figure 4. next we group the tasks by layers and partition the original layers and their under- lying tasks into new stages. for example assuming layers are evenly distributed we calculate how many layers belong to each stage. the corresponding tasks are reassigned to their new stages and communication tasks are inserted at appropriate points to ensure correct synchronization and ex- ecution. we currently do not support modifications to tensor parallelism as it is typically fixed in practice e.g. within a single node due to its high communication overhead. we leave the support for it as our future work. for changes to model architecture such as adjusting the hidden size we modify the input tensor dimensions for the relevant operators and kernels and update their execution times during simulation. when changing the number of layers we follow a process similar to that used for pipeline page 6 lumos efficient performance modeling and estimation for large-scale llm training parallelism dividing tasks into layers and applying the ad- justments accordingly. throughout this manipulation process we ensure that the dependency patterns from the original trace are preserved in the new graph to maintain correct execution as we will demonstrate in the evaluation. 3.5 simulation algorithm 1 lumos s simulation algorithm input execution graph g v e output trace with runtime details of all tasks r initialize the ready task set p 0 initialize task processors for each task t g.tasks do t.dep t s fixed dependencies if t.dep 0 then r r t end if end for while r do t pick r select a ready task to execute p t.processor r r t for each r get runtime dependencies t do r.dependents r.dependents t t.dep t.dep 1 end for if t.dep 0 then t.start max p p t.start p p t.start t.duration for each c t.dependents do c.dep c.dep 1 c.start max c.start t.start t.duration if c.dep 0 then r r c end if end for end if end while both the original and modified execution graphs will be fed into the simulator to simulate execution and estimate performance. during the simulation the four types of depen- dencies outlined in section 3.3.2 are maintained through two mechanisms. fixed dependencies are determined at initial- ization and remain unchanged throughout execution such as the sequential order of cpu tasks on the same thread. run- time dependencies on the other hand are assigned dynami- cally during runtime. for example a cudastreamsync task must wait for the last kernel on a specific stream to complete but which kernel will be last cannot be known prior to execution. algorithm 1 outlines the simulation process beginning with the assignment of fixed dependencies. in each iteration a ready task is selected and allocated to its respective proces- sor. the algorithm then checks for any runtime dependen- cies. if all dependencies are met the task is executed updat- ing the processor s progress and the status of its dependent tasks. otherwise the task is deferred until all dependencies are resolved. the simulation generates a trace similar to the input trace initially profiled from the real run recording all runtime information of the tasks. this output trace can be used not only to estimate the overall execution time but also to analyze fine-grained execution characteristics as we show in section 4.2. 4 evaluation we implement lumos in python with approximately 5 200 loc. to leverage it users need access to the source code to insert profiler hooks into their pytorch models for col- lecting traces typically requiring around 10 lines of code. lumos then offers a fully automated workflow it begins by constructing the execution graph from the raw traces manipulates the graph based on new configurations and concludes with performance estimation through simulation. depending on the complexity of the original traces the entire process can range from a few seconds to several min- utes. 4.1 methodology models. we evaluate lumos using nvidia s open-source gpt-3 implementation nvidia b from the mlperf train- ing benchmarks. our experiments involve training four model variants adjusting the number of layers hidden size feedforward network size and attention heads with model parameters ranging from 15 billion to the full 175 billion as summarized in table 1. we collect traces with pytorch kineto and evaluate performance across various parallelism strategies exploring different combinations of tensor pipeline and data parallelism. infrastructure. our evaluation is conducted on a produc- tion ml cluster using up to 512 nvidia h100 gpus on 32 servers interconnected with 8x 400gbps per host in a roce dc-scale network. our testing environment is based on cuda 12.4 pytorch 2.5 transformer engine 0.12.0 and pytorch lightning 1.9.4. we select dpro hu et al. 2022 as the state-of-the-art base- line for comparison. in our evaluation we first validate the replay accuracy by comparing both iteration time and exe- cution breakdown against the ground truth and the baseline. next we evaluate the accuracy of our approach in estimat- ing performance for new configurations including changes in both parallelism strategies and model architectures. page 7 lumos efficient performance modeling and estimation for large-scale llm training table 1. model sizes and architectures used in the evaluation. all other parameters follow the default values from the open-source gpt-3 implementation nvidia b . model name nparams nlayers dmodel dffn nheads dhead gpt-3 15b 15b 48 6144 12288 48 128 gpt-3 44b 44b 48 12288 24576 48 128 gpt-3 117b 117b 96 12288 24576 96 128 gpt-3 175b 175b 96 12288 49152 96 128 2x2x4 2x2x8 2x4x2 2x4x4 4x2x2 4x2x4 gpt-3 15b model configuration tpxppxdp 0 250 500 750 1000 1250 execution time ms 4x4x2 4x4x4 4x8x1 4x8x2 8x4x1 8x4x2 gpt-3 44b model configuration tpxppxdp 0 500 1000 1500 2000 2500 3000 execution time ms 4x8x2 4x8x4 8x4x2 8x4x4 8x8x1 8x8x2 gpt-3 117b model configuration tpxppxdp 0 1000 2000 3000 4000 5000 6000 execution time ms 4x8x4 4x8x8 4x8x16 8x4x4 8x4x8 8x4x16 gpt-3 175b model configuration tpxppxdp 0 2000 4000 6000 8000 execution time ms 0 5 10 15 20 25 30 replay error 0 5 10 15 20 25 30 replay error 0 5 10 15 20 25 30 replay error 0 5 10 15 20 25 30 replay error exposed compute actual exposed compute dpro exposed compute lumos overlapped actual overlapped dpro overlapped lumos lumos error exposed comm actual exposed comm dpro exposed comm lumos dpro error other actual other dpro other lumos figure 5. per-iteration training time with its breakdown across various model sizes and parallelism strategies comparison of actual execution dpro and lumos. 4.2 replay 4.2.1 overall iteration time in figure 5 we firstly compare the per-iteration execution time replayed by lumos and dpro against the real execu- tion time measured from actual training across various gpt- 3 model sizes 15b 44b 117b and 175b and parallelism strategies. across all configurations lumos maintains a re- play error mostly under 5 with an average error of 3.3 . in contrast dpro s error reaches up to 21.8 with an aver- age of 14 . while smaller models and simpler setups allow dpro to predict the overall time relatively well its accu- racy deteriorates as model size and complexity grow. the discrepancy highlights lumos s robustness in accurately capturing execution behaviors and modeling performance even for larger models and complex deployment setups. 4.2.2 execution breakdown figure 5 also provides a detailed breakdown of the execu- tion into key components exposed compute computation that does not overlap with communication exposed com- munication communication that does not overlap with com- putation overlapped execution where computation and communication run concurrently and other primarily idle periods . this breakdown offers deeper insights into the differences in iteration times across configurations. the analysis reveals that dpro consistently overestimates overlapped execution and underestimates total iteration time primarily due to its inability to accurately model inter- page 8 lumos efficient performance modeling and estimation for large-scale llm training stream dependencies leading to overly optimistic predic- tions of parallel execution. in contrast lumos effectively captures the complex dependencies within the model and faithfully replays the execution. it accurately reflects the dynamic interactions between computation and communi- cation adapting to changes in model size and deployment configuration and closely aligning with the real measure- ment. in large-scale distributed training particularly for llms a substantial portion of execution time is spent on commu- nication and synchronization between gpus. to optimize performance engineers aim to maximize the overlap be- tween computation and communication kernels. therefore an accurate performance model that not only replays overall execution time but also captures fine-grained details such as the degree of overlap is essential. such a model can provide valuable insights for identifying performance bottlenecks and guiding optimization efforts. 4.2.3 sm utilization 0 100 200 300 400 500 600 700 timeline per 1 ms 0.0 0.5 1.0 sm utilization 0 100 200 300 400 500 600 700 timeline per 1 ms 0.0 0.5 1.0 sm utilization 0 100 200 300 400 500 600 700 timeline per 1 ms 0.0 0.5 1.0 sm utilization actual lumos dpro figure 6. sm utilization of one iteration when training gpt-3 15b under tp 2 pp 2 and dp 4. analyzing sm streaming multiprocessor utilization is essential for identifying performance bottlenecks such as idle periods or imbalanced workloads to enhance gpu efficiency. in this section we examine the sm utilization over one iteration of training the gpt-3 15b model configured with tensor parallelism 2 pipeline parallelism 2 and data parallelism 4. utilization is defined as the fraction of time over 1ms intervals during which at least one cuda stream is actively executing tasks. this data is derived from profiled and simulated traces by analyzing kernel activities throughout the execution. as shown in figure 6 the sm utilization replayed by lumos closely match the actual measured utilization. in contrast dpro exhibits more fluctuations and significant discrepan- cies. this comparison again highlights lumos s ability to capture fine-grained execution details validating its effec- tiveness in accurately modeling execution behavior. 4.3 graph manipulation 4.3.1 parallelism strategy 0 200 400 600 800 execution time ms 2x2x8 2x2x16 2x2x32 model configuration 329 148 287 38 337 138 330 33 321 156 370 32 353 123 291 30 364 112 307 30 352 124 358 30 exposed compute overlapped exposed comm other a execution breakdown for scaling data parallelism. 0 250 500 750 1000 1250 1500 1750 execution time ms 2x4x4 2x8x4 2x16x4 model configuration 403 56 408 46 421 46 1008 77 452 44 865 477 386 60 378 48 405 49 927 65 422 49 853 461 exposed compute overlapped exposed comm other b execution breakdown for scaling pipeline parallelism. 0 200 400 600 800 1000 1200 1400 execution time ms 2x4x8 2x8x8 2x4x16 model configuration 379 80 415 39 414 53 547 81 387 72 813 44 359 96 378 55 399 63 481 67 371 87 823 55 exposed compute overlapped exposed comm other c execution breakdown for simultaneous scaling of data and pipeline parallelism. figure 7. runtime predictions for scale-out configurations. each configuration tpxppxdp is represented by two horizontal bars the upper bar shows the predicted value by lumos and the lower bar shows the actual value. next we evaluate lumos s flexibility in generating new ex- ecution graphs from existing ones to estimate performance under new configurations. specifically we demonstrate its page 9 lumos efficient performance modeling and estimation for large-scale llm training ability to predict scale-out performance by adjusting par- allelism strategies. our experiments focus on gpt-3 15b using traces collected from a baseline configuration with tensor parallelism 2 pipeline parallelism 2 and data parallelism 4. we begin by exploring changes to data parallelism where only the execution time of communication tasks needs to be updated. we currently estimate new communication time using an in-house performance model built from fleet traces as it is both readily available and accurate. while network simulators like astra-sim won et al. 2023 or analytical models moolchandani et al. 2023 rashidi et al. 2022 could also be used predicting the runtime of individual kernels is beyond the scope of this work. we will explore potential integrations with these tools in the discussion sec- tion. to validate these predictions we compare them against actual traces collected at larger scales. as shown in fig- ure 7a lumos accurately predicts both the total runtime and detailed performance breakdowns when scaling from 16 gpus to 32 64 and 128 gpus. similarly figure 7b demonstrates that lumos can also accu- rately estimate execution time and breakdown when scaling pipeline parallelism. we modify the baseline traces by split- ting layers and underlying tasks into new stages adding communication tasks and reordering task execution accord- ing to the new pipeline schedule. finally figure 7c shows that lumos maintains high accuracy with an average error of just 4.2 when scaling both data and pipeline paral- lelism simultaneously. these results prove that lumos can effectively generate correct new executions through graph manipulation for new parallelism strategies. 4.3.2 model architecture table 2. sizes and architectures for model variations. model name nparams nlayers dmodel dffn gpt-3 15b 15b 48 6144 12288 gpt-3 v1 20b 64 6144 12288 gpt-3 v2 30b 96 6144 12288 gpt-3 v3 28b 48 9216 18432 gpt-3 v4 44b 48 12288 24576 we now validate lumos s accuracy in estimating perfor- mance for different model architectures. our evaluation continues with gpt-3 15b as the base model. to generate several variants we modify the number of layers hidden sizes and feedforward network sizes. table 2 summarizes the sizes and architectures of the models used in this eval- uation. all models are trained using the configuration of tensor parallelism 2 pipeline parallelism 2 and data parallelism 4. when increasing the number of layers we duplicate the lay- ers and corresponding tasks from the existing trace insert them into the graph at appropriate places and reconstruct de- pendencies with neighboring tasks according to the original dependency pattern. for changes in hidden size or feedfor- ward network size we adjust the input tensor dimensions for all relevant operators and kernels. ideally new execution times should be assigned to all affected tasks to reflect the input changes. however we observe that only a few key kernels such as gemm and communication-related ones exhibit significant runtime changes under different configu- rations. we similarly update the execution times for these kernels using the in-house performance model described in section 4.3.1. gpt-3 v1 gpt-3 v2 gpt-3 v3 gpt-3 v4 model variants 0 500 1000 1500 2000 2500 execution time ms exposed compute actual exposed compute lumos overlapped actual overlapped lumos exposed comm actual exposed comm lumos other actual other lumos figure 8. iteration time breakdown of model variations. the left bars show the actual values and the right hatched bars show the predicted values. figure 8 presents the iteration time breakdown across these model variations showing both the actual and predicted per- formance. the predicted values represented with hatched patterns align closely with the actual measurements demon- strating that lumos accurately reproduces the execution and estimates the performance under different model architec- ture changes. overall the results shown in section 4.3.1 and section 4.3.2 demonstrate lumos s ability to leverage existing traces to generate new execution graphs for both parallelism con- figurations and model architecture variations to provide accurate performance estimates through simulation. this predictive capability significantly reduces the need for costly hardware resources positioning lumos as a practical tool for efficient model configurations exploration. 5 discussion in this section we discuss the profiling overhead adaptabil- ity scope and limitations of lumos. profiling overhead and cost. lumos leverages pytorch kineto kineto to collect traces and construct the execu- tion graph. profiling requires only a few lines of hook code which makes it much more user-friendly than ex- page 10 lumos efficient performance modeling and estimation for large-scale llm training isting approaches that require additional instrumentation. although profiling can impact execution capturing a sin- gle iteration or just a few is sufficient as the model s execution pattern remains consistent across iterations. this ensures that the overall profiling overhead is negligible in the context of the entire training process. adaptability of lumos. lumos requires profiling traces that capture both cpu and gpu activities including frame- work operators cuda runtime events and gpu kernels. similar profiling capabilities are available in other ml frameworks such as tensorflow profiler tensorflow b . lumos s post-processing stages for constructing and manip- ulating execution graphs are framework-agnostic making it easy to adapt to other frameworks. our methodology also extends well to other llms and ml models in other domains as it does not rely on model- specific information to construct execution graphs for per- formance modeling. while we make certain assumptions during graph manipulation such as where to insert layers and which tasks would be affected our method remains broadly applicable given the shared transformer-based ar- chitecture of most modern llms. similarly although this paper focuses on llm training where communication overhead is higher and model be- havior more complex lumos is also applicable to the in- ference. we anticipate even broader use cases as llm inference grows more complicated such as distributed in- ference wei et al. 2022 li et al. 2023 and ssd-based inference wilkening et al. 2021 sun et al. 2022 . kernel execution time prediction. changing configu- rations can introduce new gpu kernels not present in the original trace such as new communication kernels when adjusting parallelism strategies or new computation kernels when modifying model architectures. accurately predict- ing performance for new configurations requires estimating the runtime of these new or altered kernels. currently we estimate the runtime of unseen kernels using an in-house gpu kernel performance model built by analyzing fleet gpu traces as it is available and accurate. however al- ternative methods are also available. for communication kernels we can use metadata like message size collective algorithm and networking environment to estimate per- formance with network simulators like astra-sim won et al. 2023 and hetersim tang et al. 2024 or analytical models moolchandani et al. 2023 rashidi et al. 2022 . for computation kernels we can simply measure runtime through individual microbenchmarks. however predicting kernel runtimes is beyond the scope of this work. the primary goal of lumos is to deliver a fine-grained execu- tion graph and an accurate performance model to capture the complexities of llm execution and provide reliable perfor- mance estimates. developers can implement optimized indi- vidual kernels profile their runtime and integrate the results into lumos to predict the overall runtime saving the engi- neering efforts of porting them into the frameworks. more importantly it can offer invaluable insights for optimization even before implementation by answering what-if questions such as how much the overall runtime would be reduced if a kernel ran twice as fast and identifying which optimiza- tion would yield the greatest performance improvement. by modifying existing traces and estimating performance through simulation lumos makes performance evaluation and optimization more efficient and cost-effective. limitations. lumos currently focuses on modeling and simulating the timing of model execution. in predicting per- formance for modified configurations we assume the model will function as expected under the new settings without unforeseen issues such as out-of-memory errors. estimating system-level metrics such as flops utilization memory consumption bandwidth usage or energy efficiency lies beyond lumos s current scope. these metrics essential for optimizing llm efficiency are part of our future plans to provide more comprehensive performance insights. 6 conclusion in this paper we introduced lumos a trace-driven perfor- mance modeling and estimation toolkit for large-scale llm training. lumos captures complex behaviors through de- tailed execution graphs built from profiling traces enabling accurate performance modeling and estimation. our evalu- ation on a production ml cluster with up to 512 nvidia h100 gpus shows an average replay error of only 3.3 across diverse model architectures and parallelism config- urations. by manipulating existing graphs to generate new ones for different configurations and predicting performance through simulation lumos supports efficient optimization exploration. acknowledgements we sincerely thank the anonymous reviewers for their sug- gestions on earlier versions of this manuscript. this work was in part sponsored by meta through a student researcher appointment. this work was also in part supported by nsf career award ccf-2326182 an intel research award a sloan research fellowship a microsoft research fellow- ship and a facebook research faculty award. references bang j. choi y. kim m. kim y. and rhu m. vtrain a simulation framework for evaluating cost-effective and compute-optimal large language model training. arxiv page 11 lumos efficient performance modeling and estimation for large-scale llm training preprint arxiv 2312.12391 2023. beltagy i. peters m. e. and cohan a. long- former the long-document transformer. arxiv preprint arxiv 2004.05150 2020. brown t. b. language models are few-shot learners. arxiv preprint arxiv 2005.14165 2020. chowdhery a. narang s. devlin j. bosma m. mishra g. roberts a. barham p. chung h. w. sutton c. gehrmann s. et al. palm scaling language modeling with pathways. journal of machine learning research 24 240 1 113 2023. das d. mellempudi n. mudigere d. kalamkar d. avancha s. banerjee k. sridharan s. vaidyanathan k. kaul b. georganas e. et al. mixed precision train- ing of convolutional neural networks using integer opera- tions. arxiv preprint arxiv 1802.00930 2018. hashemi s. h. abdu jyothi s. and campbell r. tictac accelerating distributed deep learning with communica- tion scheduling. proceedings of machine learning and systems 1 418 430 2019. hu h. jiang c. zhong y. peng y. wu c. zhu y. lin h. and guo c. dpro a generic performance diagnosis and optimization toolkit for expediting distributed dnn training. proceedings of machine learning and systems 4 623 637 2022. isaev m. mcdonald n. dennison l. and vuduc r. cal- culon a methodology and tool for high-level co-design of systems and large language models. in proceedings of the international conference for high performance computing networking storage and analysis pp. 1 14 2023. jacobs s. a. tanaka m. zhang c. zhang m. song s. l. rajbhandari s. and he y. deepspeed ulysses system optimizations for enabling training of extreme long sequence transformer models. arxiv preprint arxiv 2309.14509 2023. jia z. padon o. thomas j. warszawski t. zaharia m. and aiken a. taso optimizing deep learning computa- tion with automatic generation of graph substitutions. in proceedings of the 27th acm symposium on operating systems principles pp. 47 62 2019. jouppi n. kurian g. li s. ma p. nagarajan r. nai l. patil n. subramanian s. swing a. towles b. et al. tpu v4 an optically reconfigurable supercomputer for machine learning with hardware support for embed- dings. in proceedings of the 50th annual international symposium on computer architecture pp. 1 14 2023. kineto. pytorch kineto. https github.com pytorch kineto. kwon w. li z. zhuang s. sheng y. zheng l. yu c. h. gonzalez j. e. zhang h. and stoica i. efficient memory management for large language model serving with pagedattention. in proceedings of the acm sigops 29th symposium on operating systems principles 2023. li j. jiang y. zhu y. wang c. and xu h. accelerating distributed moe training and inference with lina. in 2023 usenix annual technical conference usenix atc 23 pp. 945 959 2023. li s. xue f. baranwal c. li y. and you y. sequence parallelism long sequence training from system perspec- tive. arxiv preprint arxiv 2105.13120 2021. lin z. feng l. ardestani e. k. lee j. lundell j. kim c. kejariwal a. and owens j. d. building a performance model for deep learning recommendation model training on gpus. arxiv preprint arxiv 2201.07821 2022. liu h. zaharia m. and abbeel p. ring attention with blockwise transformers for near-infinite context. arxiv preprint arxiv 2310.01889 2023. ma r. georganas e. heinecke a. gribok s. boutros a. and nurvitadhi e. fpga-based ai smart nics for scal- able distributed ai training systems. ieee computer architecture letters 21 2 49 52 2022. miao x. wang y. jiang y. shi c. nie x. zhang h. and cui b. galvatron efficient transformer training over multiple gpus using automatic parallelism. arxiv preprint arxiv 2211.13878 2022. moolchandani d. kundu j. ruelens f. vrancx p. even- blij t. and perumkunnil m. amped an analytical model for performance in distributed training of trans- formers. in 2023 ieee international symposium on per- formance analysis of systems and software ispass pp. 306 315. ieee 2023. narayanan d. shoeybi m. casper j. legresley p. pat- wary m. korthikanti v. vainbrand d. kashinkunti p. bernauer j. catanzaro b. et al. efficient large-scale language model training on gpu clusters using megatron- lm. in proceedings of the international conference for high performance computing networking storage and analysis pp. 1 15 2021. nvidia. cupti. https docs.nvidia.com cuda cupti a. nvidia. gpt-3 implementation. https github. com mlcommons training tree master large_language_model megatron-lm b. page 12 lumos efficient performance modeling and estimation for large-scale llm training nvidia. nvidia blackwell architecture. https www.nvidia.com en-us data-center technologies blackwell-architecture c. nvidia. nvprof. https docs.nvidia.com cuda profiler-users-guide index.html d. nvidia. nsight. https developer.nvidia. com nsight-systems e. ouyang l. wu j. jiang x. almeida d. wainwright c. mishkin p. zhang c. agarwal s. slama k. ray a. et al. training language models to follow instructions with human feedback. advances in neural information processing systems 35 27730 27744 2022. pytorch. pytorch. https pytorch.org . radford a. wu j. child r. luan d. amodei d. sutskever i. et al. language models are unsupervised multitask learners. openai blog 1 8 9 2019. rajbhandari s. rasley j. ruwase o. and he y. zero memory optimizations toward training trillion parameter models. in sc20 international conference for high per- formance computing networking storage and analysis pp. 1 16. ieee 2020. rashidi s. sridharan s. srinivasan s. and krishna t. astra-sim enabling sw hw co-design exploration for distributed dl training platforms. in 2020 ieee interna- tional symposium on performance analysis of systems and software ispass pp. 81 92. ieee 2020. rashidi s. won w. srinivasan s. sridharan s. and kr- ishna t. themis a network bandwidth-aware collective scheduling policy for distributed training of dl models. in proceedings of the 49th annual international symposium on computer architecture pp. 581 596 2022. sabne a. xla compiling machine learning for peak per- formance. google res 2020. shoeybi m. patwary m. puri r. legresley p. casper j. and catanzaro b. megatron-lm training multi- billion parameter language models using model paral- lelism. arxiv preprint arxiv 1909.08053 2019. smith s. patwary m. norick b. legresley p. rajbhan- dari s. casper j. liu z. prabhumoye s. zerveas g. korthikanti v. et al. using deepspeed and megatron to train megatron-turing nlg 530b a large-scale generative language model. arxiv preprint arxiv 2201.11990 2022. sun x. wan h. li q. yang c.-l. kuo t.-w. and xue c. j. rm-ssd in-storage computing for large-scale recommendation inference. in 2022 ieee international symposium on high-performance computer architecture hpca pp. 1056 1070. ieee 2022. tang y. yuan t. cao f. wang l. guo z. zhao y. and li r. simulating llm training in cxl-based heteroge- neous computing cluster. in ieee infocom 2024-ieee conference on computer communications workshops infocom wkshps pp. 1 6. ieee 2024. tensorflow. tensorflow. https www.tensorflow. org a. tensorflow. tensorflow profiler. https www. tensorflow.org guide profiler b. touvron h. lavril t. izacard g. martinet x. lachaux m.-a. lacroix t. rozi ere b. goyal n. hambro e. azhar f. et al. llama open and efficient foundation lan- guage models. arxiv preprint arxiv 2302.13971 2023. vaswani a. attention is all you need. advances in neural information processing systems 2017. wei y. langer m. yu f. lee m. liu j. shi j. and wang z. a gpu-specialized inference parameter server for large-scale deep recommendation models. in pro- ceedings of the 16th acm conference on recommender systems pp. 408 419 2022. wilkening m. gupta u. hsia s. trippel c. wu c.-j. brooks d. and wei g.-y. recssd near data processing for solid state drive based recommendation inference. in proceedings of the 26th acm international conference on architectural support for programming languages and operating systems pp. 717 729 2021. won w. heo t. rashidi s. sridharan s. srinivasan s. and krishna t. astra-sim2. 0 modeling hierarchical net- works and disaggregated systems for large-model training at scale. in 2023 ieee international symposium on per- formance analysis of systems and software ispass pp. 283 294. ieee 2023. xu y. lee h. chen d. hechtman b. huang y. joshi r. krikun m. lepikhin d. ly a. maggioni m. et al. gspmd general and scalable parallelization for ml com- putation graphs. arxiv preprint arxiv 2105.04663 2021. you y. li j. reddi s. hseu j. kumar s. bhojanapalli s. song x. demmel j. keutzer k. and hsieh c.-j. large batch optimization for deep learning training bert in 76 minutes. arxiv preprint arxiv 1904.00962 2019. zhao j. gao x. xia r. zhang z. chen d. chen l. zhang r. geng z. cheng b. and jin x. apollo automatic partition-based operator fusion through layer by layer optimization. proceedings of machine learning and systems 4 1 19 2022. page 13 lumos efficient performance modeling and estimation for large-scale llm training zheng l. li z. zhang h. zhuang y. chen z. huang y. wang y. xu y. zhuo d. xing e. p. et al. alpa automating inter-and intra-operator parallelism for distributed deep learning. in 16th usenix symposium on operating systems design and implementation osdi 22 pp. 559 578 2022. zhu h. phanishayee a. and pekhimenko g. daydream accurately estimating the efficacy of optimizations for dnn training. in 2020 usenix annual technical conference usenix atc 20 pp. 337 352 2020.",
      "keywords": [
        "page",
        "lumos",
        "efficient",
        "performance",
        "modeling",
        "estimation",
        "large-scale",
        "llm",
        "training",
        "mingyu",
        "liang",
        "hiwot",
        "tadese",
        "kassa",
        "wenyin",
        "brian",
        "coutinho",
        "louis",
        "feng",
        "christina",
        "delimitrou",
        "abstract",
        "llms",
        "distributed",
        "environments",
        "presents",
        "significant",
        "challenges",
        "due",
        "complexity",
        "model",
        "execution",
        "deployment",
        "systems",
        "vast",
        "space",
        "configurable",
        "strategies.",
        "although",
        "various",
        "optimization",
        "techniques",
        "exist",
        "achieving",
        "high",
        "efficiency",
        "practice",
        "remains",
        "difficult.",
        "accurate",
        "models",
        "effectively",
        "characterize",
        "predict",
        "behavior",
        "essential",
        "guiding",
        "efforts",
        "system-",
        "level",
        "studies.",
        "propose",
        "trace-driven",
        "toolkit",
        "designed",
        "accurately",
        "capture",
        "behaviors",
        "modern",
        "llms.",
        "evaluate",
        "production",
        "cluster",
        "512",
        "nvidia",
        "h100",
        "gpus",
        "using",
        "gpt-3",
        "variants",
        "demonstrating",
        "replay",
        "time",
        "average",
        "error",
        "just",
        "3.3",
        "along",
        "other",
        "runtime",
        "details",
        "across",
        "different",
        "configurations.",
        "additionally",
        "validate",
        "ability",
        "estimate",
        "new",
        "setups"
      ],
      "metadata": {
        "filename": "2504.09307v1.pdf",
        "original_filename": "2504.09307v1.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 592584,
        "upload_file_id": "upload_1752159646779_mveoznsi2",
        "upload_timestamp": "2025-07-10T15:00:46.853808",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:00:47.045657+00:00",
      "content_length": 55816,
      "content_type": ".pdf",
      "filename": "2504.09307v1.pdf"
    },
    "4": {
      "file_id": "4",
      "content": "\n=== Page 1 ===\n4/26/25, 9:38\u202fAM\nOfficial EVUS Enrollment Website - Enrollment Status\nPage 1 of 1\nhttps://www.evus.gov/evusStatus\nDownload/Print \uf02f\nPENDING\nYour EVUS enrollment is under review because an immediate determination could not be made. This response does not indicate negative \ufb01ndings. A determination will be available within 72 hours.\nReturn to this website to retrieve the status of an EVUS enrollment previously submitted for one person or for a group of two or more persons.\nYou have successfully submitted the enrollment listed below. Please print this page for your personal records.\n\uf02f\nDHS recommends you print this screen for your records.\nNEED HELP?\nRELATED TOPICS:\nPaperwork Reduction Act Statement: An agency may not conduct or sponsor an information collection and a person is not required to respond to this information unless it displays a current valid OMB\ncontrol number. The control number for this collection is 1651-0139. The estimated average time to complete this enrollment is 20 minutes. If you have any comments regarding this burden estimate, you\ncan write to U.S. Customs and Border Protection, O\ufb03ce of Regulations and Rulings, 90 K Street, NE, 10th Floor, Washington DC 20229. Expiration\u00a0April 30, 2025\nThe EVUS logo is a registered trademark of the U.S. Department of Homeland Security. Its use, without permission, is unauthorized and in violation of trademark law. For more information, or to request the\nuse of the logo, please go to help.cbp.gov and submit a request by clicking on 'Ask a Question.' When selecting the product (under Additional Information) use 'EVUS' and the sub-product 'Logo Assistance' to\nexpedite the handling of your request.\nOfficial Website of the Department of Homeland Security\nCHANGE LANGUAGE\u00a0\u00a0 \nENGLISH (U.S./U.K.)\n (http://www.cbp.gov)\nName\nDate of Birth\nEnrollment Number\nPassport Number\nB1/B2 Visa Foil Number\nStatus\nExpires\nJIE PAN\nOct 17, 1972\nAQXKHVF4TJPHDABS\nEN4354998\nV0345571\nPending\nN/A\n\uf002View\nEXIT\nSubmitting a Group of Enrollments\nWhat is the process to enroll for a group of travelers?\nChecking Your EVUS Status\nHow do I update the information on my enrollment?\nTraveling with an EVUS\nHow long will my EVUS enrollment be valid for?\nIf Enrolled, does my travel authorization guarantee me admission to the United States?\nHave additional questions?\u00a0\u00a0 \nVIEW ALL TOPICS (/FAQ?LANG=EN)\nCONTACT SUPPORT\nTraveler Communications Center\nCall: 00-1-202-325-0180\nTo submit an inquiry:\nCBP Info Center (https://help.cbp.gov)\n24 hours per day, 7 days a week\nHELPFUL LINKS\nDept. of Homeland Security:\nDHS.gov (http://www.dhs.gov)\nCustoms and Border Protection:\nCBP.gov (http://www.cbp.gov)\nCBP.gov/travel (http://www.cbp.gov/travel)\nSITE POLICIES\nACCESSIBILITY\n(http://www.cbp.gov/site-policy-\nnotices/accessibility)\nPRIVACY STATEMENT\n(/privacystatement)\nVULNERABILITY DISCLOSURE\n(https://www.cbp.gov/document/directi\ndisclosure-program-policy-and-\nrules-engagement)\nPrivacy - Terms\n",
      "searchable_content": "page 1 4 26 25 9 38 am official evus enrollment website - enrollment status page 1 of 1 https www.evus.gov evusstatus download print pending your evus enrollment is under review because an immediate determination could not be made. this response does not indicate negative \ufb01ndings. a determination will be available within 72 hours. return to this website to retrieve the status of an evus enrollment previously submitted for one person or for a group of two or more persons. you have successfully submitted the enrollment listed below. please print this page for your personal records. dhs recommends you print this screen for your records. need help related topics paperwork reduction act statement an agency may not conduct or sponsor an information collection and a person is not required to respond to this information unless it displays a current valid omb control number. the control number for this collection is 1651-0139. the estimated average time to complete this enrollment is 20 minutes. if you have any comments regarding this burden estimate you can write to u.s. customs and border protection o\ufb03ce of regulations and rulings 90 k street ne 10th floor washington dc 20229. expiration april 30 2025 the evus logo is a registered trademark of the u.s. department of homeland security. its use without permission is unauthorized and in violation of trademark law. for more information or to request the use of the logo please go to help.cbp.gov and submit a request by clicking on ask a question. when selecting the product under additional information use evus and the sub-product logo assistance to expedite the handling of your request. official website of the department of homeland security change language english u.s. u.k. http www.cbp.gov name date of birth enrollment number passport number b1 b2 visa foil number status expires jie pan oct 17 1972 aqxkhvf4tjphdabs en4354998 v0345571 pending n a view exit submitting a group of enrollments what is the process to enroll for a group of travelers checking your evus status how do i update the information on my enrollment traveling with an evus how long will my evus enrollment be valid for if enrolled does my travel authorization guarantee me admission to the united states have additional questions view all topics faq lang en contact support traveler communications center call 00-1-202-325-0180 to submit an inquiry cbp info center https help.cbp.gov 24 hours per day 7 days a week helpful links dept. of homeland security dhs.gov http www.dhs.gov customs and border protection cbp.gov http www.cbp.gov cbp.gov travel http www.cbp.gov travel site policies accessibility http www.cbp.gov site-policy- notices accessibility privacy statement privacystatement vulnerability disclosure https www.cbp.gov document directi disclosure-program-policy-and- rules-engagement privacy - terms",
      "keywords": [
        "page",
        "official",
        "evus",
        "enrollment",
        "website",
        "status",
        "https",
        "www.evus.gov",
        "evusstatus",
        "download",
        "print",
        "pending",
        "under",
        "review",
        "because",
        "immediate",
        "determination",
        "not",
        "made.",
        "response",
        "indicate",
        "negative",
        "\ufb01ndings.",
        "available",
        "within",
        "hours.",
        "return",
        "retrieve",
        "previously",
        "submitted",
        "one",
        "person",
        "group",
        "two",
        "more",
        "persons.",
        "successfully",
        "listed",
        "below.",
        "please",
        "personal",
        "records.",
        "dhs",
        "recommends",
        "screen",
        "need",
        "help",
        "related",
        "topics",
        "paperwork",
        "reduction",
        "act",
        "statement",
        "agency",
        "conduct",
        "sponsor",
        "information",
        "collection",
        "required",
        "respond",
        "unless",
        "displays",
        "current",
        "valid",
        "omb",
        "control",
        "number.",
        "number",
        "1651-0139.",
        "estimated",
        "average",
        "time",
        "complete",
        "minutes.",
        "any",
        "comments",
        "regarding",
        "burden",
        "estimate",
        "write",
        "u.s.",
        "customs",
        "border",
        "protection",
        "o\ufb03ce",
        "regulations",
        "rulings",
        "street",
        "10th",
        "floor",
        "washington",
        "20229.",
        "expiration",
        "april",
        "2025",
        "logo",
        "registered",
        "trademark",
        "department",
        "homeland"
      ],
      "metadata": {
        "filename": "Official EVUS Enrollment Website - Enrollment Status.pdf",
        "original_filename": "Official EVUS Enrollment Website - Enrollment Status.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 113964,
        "upload_file_id": "upload_1752159749521_8gemcshlz",
        "upload_timestamp": "2025-07-10T15:02:29.566328",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:02:29.841967+00:00",
      "content_length": 2951,
      "content_type": ".pdf",
      "filename": "Official EVUS Enrollment Website - Enrollment Status.pdf"
    },
    "6": {
      "file_id": "6",
      "content": "\n=== Page 1 ===\nMAD: Memory-Augmented Detection of 3D Objects\nBen Agro\u2020\nSergio Casas\u2020\nPatrick Wang\u2020\nThomas Gilles\nRaquel Urtasun\n{tgilles, urtasun}@waabi.ai\n(a) 3D Detector\n(b) Object Memory\n(c) Memory Augmented Detections\n0.0s\n-2.4s\nMemory Age\nFalse Negative\nFigure 1.\nDetectors without long-term temporal fusion (a) miss heavily occluded objects. Our approach enhances detectors (b) to\nremember past predictions, (c) recovering from occlusion. Detections are in green , labels are in black , lidar points are in \u2022 gray.\nAbstract\nTo perceive, humans use memory to \ufb01ll in gaps caused by\nour limited visibility, whether due to occlusion or our nar-\nrow \ufb01eld of view. However, most 3D object detectors are\nlimited to using sensor evidence from a short temporal win-\ndow (0.1s-0.3s). In this work, we present a simple and ef-\nfective add-on for enhancing any existing 3D object detec-\ntor with long-term memory regardless of its sensor modality\n(e.g., LiDAR, camera) and network architecture. We pro-\npose a model to effectively align and fuse object proposals\nfrom a detector with object proposals from a memory bank\nof past predictions, exploiting trajectory forecasts to align\nproposals across time. We propose a novel schedule to train\nour model on temporal data that balances data diversity\nand the gap between training and inference. By applying\nour method to existing LiDAR and camera-based detectors\non the Waymo Open Dataset (WOD) and Argoverse 2 Sen-\nsor (AV2) dataset, we demonstrate signi\ufb01cant improvements\nin detection performance (+2.5 to +7.6 AP points). Our\nmethod attains the best performance on the WOD 3D de-\ntection leaderboard among online methods (excluding en-\nsembles or test-time augmentation).\n1\nIntroduction\nMost self-driving vehicles (SDVs) utilize a 3D object detec-\ntor to recognize and localize objects in 3D space. This task\nis challenging due to occlusion, large intra-class variability,\nand distant objects, which typically have limited sensor ob-\nservations. To overcome these challenges, human drivers\nrely on their memory. For example, they may drive more\ncautiously when remembering a previously observed but\nnow occluded cyclist, who may suddenly enter the road.\nA common approach for improving 3D object detectors\nis to aggregate a short temporal window of past sensor ob-\nservations. Towards this goal, most existing LiDAR-based\nmethods transform a short buffer of sensor data into the cur-\nrent ego (SDV) coordinate frame to align past and current\nevidence [1, 39, 58, 74, 75]. Similarly, camera-based meth-\nods stack multiple images [46, 78] as input to existing ar-\nchitectures. These methods cannot handle long temporal\nsequences due to computational and memory constraints.\nMoreover, temporal stacks of 3D/Bird\u2019s-Eye-View (BEV)\nrepresentations like point clouds or lifted camera features\nrequire a large receptive \ufb01eld, especially for fast-moving\nobjects [30], further increasing computational burden.\nThere is a growing interest in long-term temporal fusion.\nScene-level memory approaches [14, 17, 31] recurrently\nfuse scene-level features, but they can struggle to capture\nrelevant foreground objects. Other approaches associate ob-\njects in memory over time via tracking [8, 20, 30, 32], ag-\ngregating past information for each particular object. How-\never, the associations from the tracker may contain mis-\ntakes that can compound over time and lead to informa-\ntion loss. Other methods leverage attention from current\ndetection proposals to the past sensor or object informa-\ntion [15, 18, 74]. Still, they can be challenging to scale\nto long histories and suffer from false negatives as the pro-\nposals re\ufb01ned into the \ufb01nal detections only come from the\n\u2020Work done while at Waabi\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1449\n\n=== Page 2 ===\npresent time.\nIn this paper, we present a simple and sensor-agnostic\nadd-on for enhancing any existing 3D object detector with\nlong-term memory. We refer to it as MAD \u2014 short for\nMemory-Augmented Detection, and Fig. 1 illustrates the\nhigh-level idea. MAD is a transformer-based model that\nfuses proposals from a detector with proposals from a mem-\nory bank representing past beliefs. Inspired by recent devel-\nopments [4], we exploit joint detection and trajectory fore-\ncasting. By storing explicit trajectory forecasts in the mem-\nory bank, we can estimate object poses at arbitrary future\ntimestamps for all the objects in the memory. This enables\nus to enrich the set of proposals by aligning memory pro-\nposals with the current observations.\nTraining with temporal data can be challenging: back-\npropogation through consecutive training examples con-\nsumes prohibitive amounts of memory, training on long se-\nquences can cause over-\ufb01tting when back-propagating on\nevery example, and using memory warm-up can slow down\ntraining. We design a more effective training schedule that\nbegins with short temporal sequences and progressively in-\ncreases the length, exploiting high data diversity early and\nclosing the gap with inference towards the end.\nTo en-\nsure the model learns to trust the memory when training\non short sequences, we use cached model outputs from pre-\nvious training iterations.\nWe demonstrate the generality of our approach by en-\nhancing existing LiDAR-based and camera-based 3D ob-\nject detection networks with MAD, and show considerable\nimprovements over the base detectors on two large-scale\ndatasets: Waymo Open dataset (WOD) [57] and Argoverse\n2 Sensor dataset (AV2) [68]. Notably, SAFDNet [80] en-\nhanced with MAD achieves state-of-the-art performance on\nWOD for online detection methods without requiring en-\nsembles or test-time augmentation.\n2\nRelated Work\n3D Object Detection:\nWe can categorize 3D detectors by\ntheir input modality (e.g., LiDAR, camera), scene repre-\nsentation (e.g., point clouds, voxels), and number of stages\n(e.g., single-stage or multi-stage)\nLiDAR-based methods commonly represent the input as\nvoxels [12, 27, 56, 71, 80, 81], pillars [23, 25, 55, 61, 66,\n72], or point clouds [43, 53, 54, 69, 73]. A widely used ap-\nproach for including temporal LiDAR information is point\naggregation, which involves transforming past point clouds\ninto a common coordinate frame and processing the ag-\ngregated point cloud. These approaches are usually lim-\nited to \u21925 past LiDAR frames due to computational con-\nstraints [30] in online applications like autonomous driving.\nAnother drawback is that point aggregation does not align\nmoving objects, requiring a larger receptive \ufb01eld in the de-\ntector backbone the longer the temporal horizon is [30, 80].\nCamera-based 3D detection is challenging because of\nmissing depth information. One approach is to produce 3D\nbounding boxes from image features by estimating depth,\n3D size, and orientation [7, 41, 64, 70]. Other methods\nleverage voxel [47, 48, 51] or point cloud [65, 76] repre-\nsentations by predicting pixel depth distributions to lift 2D\nfeatures to 3D. Stacking and processing past camera images\nor feature maps is a common but expensive method for tem-\nporal fusion [2, 19, 46, 52, 78].\nRegardless of modality, we can further categorize 3D de-\ntectors as single-stage or multi-stage. Single-stage methods\nproduce detections from sensor data [6, 12, 64, 71, 80, 81]\nwith a single deep neural network. Multi-stage methods\nuse bounding box proposals from a \ufb01rst stage (or ran-\ndomly initialized proposals) to gather features (e.g., with\nRoIPool [49], RoIAlign [16], interpolations [75], or atten-\ntion [4, 28, 33\u201335, 67, 82]) and iteratively re\ufb01ne the bound-\ning boxes.\nMAD is a sensor-modality-agnostic module that we can\nadd to any detector as a subsequent re\ufb01nement stage. In\nour work we utilize CenterPoint [75], SAFDNet [80], HED-\nNet [81], FCOS3D [64], and BEVMap [6] as proposal net-\nworks.\nLong-Term Temporal Fusion for 3D Detection:\nVari-\nous works attempt to solve the shortcomings of sensor ag-\ngregation by learning to use multiple seconds of sensor ev-\nidence to improve object detection.\nIn this line of work, the scene-based paradigm uses\nrecurrent fusion of scene-level features [14, 17, 31, 75],\nwith some methods relying on multiple traversals of the\nscene [77]. A challenge of this approach is focusing and\naligning features from relevant and dynamic foreground\nobjects, which past works addressed by transforming fea-\nture maps, using segmentation to focus on foreground ob-\njects [17], and using deformable attention or convolution\nto align features of moving objects [22]. Processing both\nforeground and background areas can be computationally\nexpensive. It is worth noting that many of these methods\nare single-stage detectors [14, 17] and which we could use\nas a detection proposal network with MAD.\nAlternatively, the object-based paradigm focuses on the\nforeground by using detection proposals. Detect-track-fuse\nmethods are a sub-family of object-based methods that as-\nsociate previous detections over time to create tracks, and\nthese tracks summarize information from past sensor evi-\ndence [8, 20, 30, 30, 32, 79]. However, in complex situ-\nations like pedestrian crowds, association over time can be\ndif\ufb01cult due to heavy occlusions and erratic behavior, po-\ntentially leading to false negatives or identity switches in\nthe tracks. Object-to-scene approaches mitigate the short-\ncomings of association by directly using current detec-\ntion proposals to aggregate historical scene-level informa-\ntion using hand-crafted feature aggregation modules [15]\n1450\n\n=== Page 3 ===\nMemory Bank\nRetrieval\nAlignment\nFeature \nExtraction\n3D \nDetector\nDetection Proposals\nMemory Proposals\nProposal \nMerging\nRefinement Transformer\nMemory \nCross \nAttention\nFactorized \nSelf \nAttention\nProposal \nUpdate\nModel Outputs\n(Refined Proposals)\nMemory Bank Update\nMerged Proposals\nIn preparation for future inferences\nSensor Data\nTimestamp\nSDV pose\nFigure 2. MAD is a plug-and-play module that enhances any off-the-shelf 3D detector (kept frozen) with long-term memory.\nor attention mechanisms [82].\nThese approaches can be\ndif\ufb01cult to scale to long history horizons as they require\nre-processing past sensor evidence or dense feature maps\nbased on the current proposals (e.g., [15, 82] only use 0.7s\nof history). Finally, object-to-object methods use past ob-\nject detections to improve current object detections without\nexplicit tracking, e.g., by cross-attending from current de-\ntection proposals to past detections [63, 74], or using hand-\ncoded attention matrices based on distance [18]. Overall,\nmost object-based methods share some de\ufb01ciencies: Many\nonly re\ufb01ne detection proposals produced by current sen-\nsor evidence and struggle to recover from missing propos-\nals [8, 15, 18, 20, 74].\nOthers naively concatenate past\ndetections with current proposals [63], which can lead to\nalignment issues for dynamic objects and miss-calibration\nin the proposal scores, as the model should trust historical\nproposals less than current proposals.\nOur proposed method, MAD, performs object-based\ntemporal fusion without requiring explicit object associa-\ntion, aligns the memory in space and time by with trajectory\nforecasting, and can recover from missing proposals by us-\ning and rescoring proposals from the memory bank.\n3\nMemory Augmented 3D Object Detection\n3D object detectors take a short temporal window of sensor\ndata as input and produce a set of detections. Existing ap-\nproaches typically struggle to perceive occluded and distant\nobjects with limited sensor observations. To tackle these\nchallenges, we propose MAD, a plug-and-play module to\nenhance existing 3D object detectors with the ability to per-\nform long-horizon temporal fusion. Our only requirement\nfrom the detector is that each detection includes an object\nbounding box, multi-class con\ufb01dence scores, and a feature\nvector capturing local context. We demonstrate the general-\nity of MAD by augmenting and improving various LiDAR-\nbased and camera-based detectors.\nWe enable long-horizon temporal understanding through\na memory bank that captures all the relevant information on\nobjects, including where we expect them to move. These\ntrajectory forecasts allow us to align the memory objects\nwith the current detector proposals in space and time. Im-\nportantly, we do not require the object detector to provide\nmotion forecasts; instead, MAD computes them. To com-\npensate for ego-motion, we assume the ego is localized \u2014\nwhich is the norm in modern self-driving platforms [57, 68]\n\u2014 and store the ego pose in the memory along with the\nmodel outputs.\n3.1. Model\nWe start with an overview of our model; refer to Figure 2\nfor an illustration.\nAt every inference step, MAD takes\nas input the detection proposals, the current timestamp t\n(e.g., LiDAR sweep-end time or camera capture time), and\nthe ego pose Et in a global coordinate frame. It then re-\ntrieves objects from memory, aligns them spatially with Et\nand temporally to t, and extracts high-dimensional features\nfrom the aligned boxes and trajectory forecasts. We refer\nto the aligned boxes and trajectory forecasts with the ex-\ntracted features as memory proposals. A proposal merg-\ning mechanism then fuses detection and memory proposals\nby rescoring their con\ufb01dence scores and applying standard\npost-processing. Finally, our re\ufb01nement transformer itera-\ntive re\ufb01nes the object detections and trajectory forecasts in\nthe merged proposals with cross-attention to the memory\nand factorized self-attention. In preparation for future in-\nferences, the memory bank is then updated by appending\nthe model outputs (a.k.a. re\ufb01ned proposals) and removing\nolder model outputs to keep the memory bounded in size.\nProposal representation:\nWe de\ufb01ne object proposals\nP = (B, C, T, Q) with N bounding boxes B \u2191RN\u21927,\nwhere the last dimension corresponds to (x, y, z, l, w, h, \u03c9)\nwith object 3D centroids (x, y, z), headings \u03c9 in a BEV\nego-relative coordinate frame, and the 3D box dimensions\n(w, l, h); multi-class con\ufb01dence scores C \u2191[0, 1]N\u2192C,\nwhere C is the number of actor classes; trajectory fore-\ncasts T\n\u2191\nRN\u2192Tf \u21923 describing objects\u2019 BEV pose\n{(x, y, \u03c9)t+sf , . . . , (x, y, \u03c9)t+Tf sf } over Tf future way-\npoints at a time interval sf; and an object feature Q \u2191\nRN\u2192(Tf +1)\u2192d encoding both local and global features for\nevery object at the present and future timestamps, where d\n1451\n\n=== Page 4 ===\nis the feature dimensionality. We use superscripts to de-\nnote the source of the proposals: detection proposals Pdet\nfrom the 3D detector, memory proposals Pmem from the\nmemory bank, merged proposals Pmerge from the proposal\nmerging module, and re\ufb01ned proposals Pref from the out-\nput of the re\ufb01nement transformer. For detection proposals\nPdet, we generate Tdet by assuming the object is static over\ntime since detectors do not provide forecasts (and this is\njust an initialization before re\ufb01nement). The object features\nQdet are obtained by interpolating the feature map from be-\nfore the detector header at the projected object centroids,\nrepeating (Tf +1) times to get the features for future times-\ntamps, and adding a learned embedding of Bdet and Cdet.\nIn the paragraphs below we describe how we obtain Pmem,\nPmerge and Pref.\nMemory Bank and Retrieval:\nThe memory bank is a set\nof tuples\n!\ntm, Etm, Pref\ntm\n\"\nwith timestamped past model out-\nputs and ego pose, sorted by the timestamp tm at which the\noutputs were generated. During inference at timestamp t,\nwe retrieve memory entries Pref\ntm at a set of past target times-\ntamps tm \u2191Tm, where Tm = {t \u2193sm, t \u21932sm, . . . , t \u2193\nTmsm}. Tm is the number of past target timestamps, and\nsm is the time stride of the retrieved entries. To be precise,\nwe retrieve the closest memory entry to each timestamp in\nTm to be robust to small sensor delays.\nExtracting Memory Proposals:\nFor effective use of the\nmemory at inference, we should align each retrieved entry\n(tm, Etm, Pref\ntm ) in space and time with the current detec-\ntion proposals at time t. We handle ego-motion by applying\nthe relative transform Etm\u2191t = E\u21931\nt Etm to Bref\ntm and Tref\ntm.\nTo handle object motion, we linearly interpolate the stored\ntrajectory forecast to the current timestamp t to obtain the\nproposal box Bmem. To obtain the proposal forecast Tmem,\nwe also interpolate/extrapolate the stored trajectories as re-\nquired to obtain waypoints at Tf = {t + sf, . . . , t + Tfsf}\nfrom stored waypoints at {tm + sf, . . . , tm + Tfsf}.\nFinally, we extract latent features Qmem at t and every\nfuture time step tf \u2191Tf: First, we compute sinusoidal posi-\ntional embeddings [60] for the centroid coordinates Bmem\nx,y,z\nand encode them with a lightweight MLP. Separately, we\nconcatenate other features including Bmem\nl,w,h,\u03c9 (box dimen-\nsions and heading), con\ufb01dence scores Cmem, the memory\nage t \u2193tm, and a 2D vector pointing to where the pro-\nposal was in the current ego coordinate frame at the time\ntm. Finally, we encode the concatenated features with an-\nother MLP and add the features from both MLPs together.\nProposal Merging:\nThe memory and detection propos-\nals can be redundant, particularly in areas with good sen-\nsor coverage. To merge proposals, we learn to rescore their\nmulti-class con\ufb01dence scores. Rescoring is essential as the\ncon\ufb01dence the model should put in a memory proposal not\nonly depends on the con\ufb01dence score at a past timestamp\nCmem\ntm\n, but also on the proposal age t \u2193tm, as the fore-\ncasting uncertainty grows with the time horizon and other\nfactors. For example, the model should trust a fast-moving\ndetection less than a stationary object, or it should trust an\nobject observed 0.5 seconds ago more than one observed\n5 seconds ago. Furthermore, the detection proposals come\nfrom the 3D detector, while the memory proposals are pro-\nduced by MAD, and detectors have been found to be mis-\ncalibrated [24, 40, 42].\nTo make the scores comparable, we learn two small\nMLPs that separately map the features of the detection pro-\nposals Qdet and the memory proposals Qmem to new multi-\nclass scores Cmerge.\nAs explained in Section 3.2, these\nrescoring MLPs are trained under a single detection loss ap-\nplied to the merged proposals so that the model can decide\nwhich proposals to trust from both sources. Finally, we \ufb01lter\nproposals with score thresholding, non-maximum suppres-\nsion (NMS), and keep the top K merged proposals sorted\nby score (maximum over actor classes). Post-processing en-\nables the re\ufb01nement transformer to process a smaller num-\nber of queries.\nFinally, we add learned time positional embeddings to\nthe merged proposal features Qmerge to indicate the time of\nthe trajectory forecast. At this point, we have N merge def\n=\nN ref merged proposals Pmerge ready for re\ufb01nement.\nRe\ufb01nement Transformer:\nWe utilize a transformer de-\ncoder to re\ufb01ne the merged proposals Pmerge = Pref(0) it-\neratively over I blocks into Pref(1) ...Pref(I), where the\n\ufb01nal model outputs are Pref = Pref(I). We propose a novel\nmemory cross-attention mechanism to allow the queries \u2014\nproposal features Qref(i) \u2014 to aggregate information from\nall the memory proposals Qmem, including those that pro-\nposal merging \ufb01ltered out. We want to use this informa-\ntion in the re\ufb01nement transformer because multiple overlap-\nping memory proposals provide signi\ufb01cant evidence about\nan object\u2019s presence and location. To achieve this, we per-\nform cross attention from the object queries Qref(i) to the\nmemory proposal features Qmem. For ef\ufb01ciency, we limit\nthe cross attention to the nearest k keys to each object query\n(computing the nearest neighbors of Bref(i)\nx,y,z in Bmem\nx,y,z).\nSimilar to many works [4, 44], we perform factorized\nself-attention in each re\ufb01nement block, which separates\ntime self-attention and object self-attention for ef\ufb01ciency,\nwhere the former attends only to queries from the same ob-\nject (sequence length Tf + 1) and the latter only attends to\nqueries from the same time step (sequence length N). The\nupdated queries Qref(i+1) are input to the next block.\nFinally, we update the explicit proposal information\nas described in DeTra [4], by using a simple MLP to\nproduce Bref(i+1) and Cref(i+1) and a gated recurrent\nunit (GRU) [9] to update the future trajectory waypoints\nTref(i+1).\n1452\n\n=== Page 5 ===\nMemory Bank Update:\nWe post-process the re\ufb01ned pro-\nposals Pref as we did to the merged proposals: score thresh-\nolding, NMS, and top K based on con\ufb01dence score, adding\nthe result to the memory bank, along with the correspond-\ning timestamp t and ego pose Et. To limit the size of the\nmemory bank when running on long sequences, we remove\nany memory entries older than t\u2193Tmsm \u2193\u03b5 (the past time-\nhorizon used in memory retrieval with a small buffer \u03b5).\n3.2. Training\nWe \ufb01rst train an off-the-shelf 3D detector following their\noriginal training strategy. This stage can be omitted if a\npre-trained 3D detector is available. Then, we train all the\nparameters in MAD as a subsequent stage, with the 3D de-\ntector weights frozen. Pre-training and freezing the 3D de-\ntector is important to ensure the detection proposals do not\nchange throughout MAD training. Note that we train a sep-\narate MAD for each 3D detector, as each detector has differ-\nent features Qdet and detection distribution and calibration.\nBefore detailing our proposed MAD training, we discuss\nsome possibilities and trade-offs when training temporal fu-\nsion models. Training on unordered examples has the ad-\nvantage of satisfying the assumption of i.i.d examples (bet-\nter learning dynamics) [21, 50]. However, it differs from\nevaluation, where the model is rolled out on long sequences\nand consumes its previous outputs. Training on long se-\nquences of ordered data has the advantage of being closer to\nevaluation, but it has worse learning dynamics since consec-\nutive examples are heavily correlated (there are few changes\nin the scene from one frame to the next). If, instead, gradi-\nents are accumulated over a long sequence and used to up-\ndate the model parameters once per sequence, a sequence\nbecomes one example (satisfying the i.i.d assumption), but\nthe training duration is multiplied by the sequence length\nif the number of model updates is kept constant. Despite\nthis large space of possibilities and the importance of such\nchoices, prior works on learned temporal fusion neglect de-\ntails and discussion of their training recipe [15, 17, 20, 30].\nTo tackle these challenges, we design a novel training\nschedule. We propose to train MAD on increasingly long\nchunks of ordered data, using single frames1 at the begin-\nning and entire sequences at the end of training. To train ob-\nject memory on short chunks (or even single frames) of data\nwhile maintaining a reasonable amount of memory inputs,\nwe propose to maintain a cache of memory banks across\ntraining and using it to build the memory proposals for each\ntraining example. Below, we detail this proposed schedule,\nour cache of memory banks, how we handle augmentations\nwith memory, and our loss function.\nTraining Schedule:\nThe datasets we use (WOD [57],\nAV2 [68]) organize their data into driving logs, each around\n1We slightly abuse the term \u201cframe\u201d here, as some detectors use a win-\ndow of multiple past frames as input.\n20s in duration with data captured at 10Hz, meaning each\nlog has around 200 frames. Each log has a unique identi-\n\ufb01er (logID). For the \ufb01rst 25% of training, we sample single\nframes (that is, consecutive training examples are random\nframes from random logs). Throughout the rest of training,\nwe sample sequential chunks of gradually increasing size:\n48 frames for (25%, 50%] of training, 96 frames for (50%,\n75%], and 144 frames for (75%, 100%]. We train with a\nsingle cosine decay learning rate schedule with no resets.\nThe intuition behind this is that when the learning rate is\nhigh, and the model weights change the most, the model is\nexposed to more diverse data. Then, when the learning rate\nis lower, the model is tuned to be closer to the evaluation\nsetting, where it consumes its previous outputs.\nExploiting a Cache of Memory Banks:\nIf the schedule\ndescribed above is followed naively during the individual\nframe and short chunk training, the model cannot consume\nits previous outputs and thus would not learn to use memory\nduring this phase of training. To address this problem, we\nintroduce a cache of previous memory banks. This cache is\na mapping from the unique driving log identi\ufb01er logID to\na memory bank. At the start of training, we initialize the\ncache with empty memory banks for all logIDs. On a given\ntraining iteration, we index the cache with the logID of the\ncurrent training example to obtain the memory bank.\nIf\navailable, we retrieve the memory proposals from this mem-\nory bank as described in Sec. 3.1. We update the retrieved\nmemory bank at the end of the training iteration with the\nmodel outputs, replacing any existing entry with the same\ntimestamp. Note that during training we do not limit the\nsize of the memory bank.\nThere are a few challenges to training with the object\nmemory cache that we address:\n\u2022 To train these models ef\ufb01ciently on large datasets, we\nuse a distributed data training scheme, meaning we split\nexamples in the minibatch across multiple GPUs. Each\nGPU has a unique index called a rank. Each rank main-\ntains a separate cache to prevent the cache from \ufb01lling\nup the RAM and avoid synchronization costs. To guaran-\ntee high cache hit rates, we ensure that training examples\nfrom a given logID are always put on the same rank dur-\ning training.\n\u2022 The cache is \ufb01lled with MAD outputs, which are inac-\ncurate at the beginning of training. We do not want er-\nroneous model outputs to \ufb01ll the cache; otherwise, the\nmodel may not learn to trust the memory proposals. To\nmitigate this, we only start \ufb01lling the cache (and train-\ning with memory proposals) after 2.5% of training, after\nwhich performance is reasonable.\n\u2022 To make the model robust to variable latency and the pres-\nence and absence of memory proposals, we randomize the\ntarget timestamps Tm that we retrieve memory elements\nfor during training by randomly sampling the time stride\n1453\n\n=== Page 6 ===\nOverall L1\nOverall L2\nVehicle L1\nVehicle L2\nPedestrian L1\nPedestrian L2\nCyclist L1\nCyclist L2\nMethod\nAP\nAPH\nAP\nAPH\nAP\nAPH\nAP\nAPH\nAP\nAPH\nAP\nAPH\nAP\nAPH\nAP\nAPH\nCenterpoint 1f [75]\n76.1\n73.5\n70.0\n67.6\n75.7\n75.2\n67.9\n67.4\n77.6\n71.6\n70.1\n64.4\n74.9\n73.8\n72.1\n71.0\n+ MAD (Ours)\n82.9\n81.0\n77.6\n75.8\n81.1\n80.5\n74.0\n73.4\n83.8\n80.0\n77.2\n73.5\n83.8\n82.6\n81.6\n80.4\nCenterpoint 2f [75]\n77.5\n75.8\n71.7\n70.1\n76.4\n75.9\n68.7\n68.2\n79.2\n75.6\n71.9\n68.5\n76.8\n75.9\n74.4\n73.5\n+ MAD (Ours)\n82.8\n81.2\n77.5\n76.0\n81.4\n80.8\n74.3\n73.7\n84.7\n82.1\n78.2\n75.6\n82.2\n80.8\n80.1\n78.7\nHEDNet 1f [81]\n81.6\n79.7\n75.6\n73.7\n80.9\n80.5\n73.1\n72.7\n84.6\n80.2\n77.1\n72.8\n79.4\n78.5\n76.6\n75.6\n+ MAD (Ours)\n85.2\n83.3\n80.2\n78.3\n83.6\n82.9\n76.6\n76.0\n87.0\n83.4\n81.0\n77.4\n85.1\n83.7\n83.0\n81.6\nHEDNet 4f [81]\n83.6\n82.3\n78.1\n76.8\n82.4\n81.9\n75.1\n74.6\n86.3\n83.6\n79.4\n76.8\n82.2\n81.4\n79.9\n79.1\n+ MAD (Ours)\n85.5\n83.8\n80.6\n79.0\n83.6\n82.9\n76.8\n76.1\n87.7\n85.0\n81.9\n79.2\n85.1\n83.5\n83.2\n81.6\nSAFDNet 1f [80]\n81.7\n79.7\n75.5\n73.6\n80.5\n80.0\n72.5\n72.1\n84.7\n80.2\n77.1\n72.9\n79.8\n78.8\n76.9\n75.9\n+ MAD (Ours)\n85.3\n83.5\n80.3\n78.4\n83.4\n82.8\n76.5\n75.9\n86.8\n82.9\n80.7\n76.8\n85.8\n84.7\n83.7\n82.6\nSAFDNet 4f [80]\n83.9\n82.6\n78.4\n77.1\n82.8\n82.3\n75.4\n74.9\n86.8\n84.2\n80.1\n77.5\n82.0\n81.1\n79.6\n78.8\n+ MAD (Ours)\n85.8\n84.2\n81.0\n79.4\n84.2\n83.6\n77.4\n76.8\n87.9\n85.4\n82.2\n79.7\n85.2\n83.7\n83.3\n81.7\nTable 1. Comparing the performance of various off-the-shelf LiDAR object detectors with and without MAD on the WOD validation set.\nBase detector results are reproduced using of\ufb01cial code. MAD consistently boosts the performance of all detectors across all metrics.\nVehicle AP IoU 10 In Camera Field of View\nMethod\nOverall\n[0, 40)m\n[40, 80)m\n[80, 120)m\nFCOS3D [64]\n37.6\n73.9\n34.3\n4.65\n+MAD (Ours)\n43.6\n82.6\n40.0\n8.11\nBEVMap [6]\n51.5\n86.5\n54.0\n13.9\n+MAD (Ours)\n53.4\n88.0\n55.1\n17.2\nTable 2. Adding MAD to camera-based 3D detectors on AV2.\nsm, and the number of target timestamps Tm.\nHandling\nAugmentations\nwith\nMemory:\nPrior\nworks [26, 75, 80, 81] \ufb01nd that data augmentations\n(e.g., translation, rotation, \ufb02ipping, and re-scaling) are\nimportant for detection performance. We apply augmenta-\ntions to the boxes Bmem and trajectories Tmem in memory\nproposals after the memory alignment step.\nWe apply\nthe inverse of the augmentations to Bref and Tref before\nstoring them in the memory bank.\nLoss function:\nWe optimize a multi-task objective\nL = Lrescore(Cmerge) + #I\ni=1 Ldet(Bref(i), Cref(i)) +\nLfor(Tref(i)), which is a combination of a rescoring loss\nLscore, a detection re\ufb01nement loss Ldet, and a forecasting\nre\ufb01nement loss Lfor, where the detection and forecasting\nlosses are computed at every re\ufb01nement block.\nFollow-\ning [4], Ldet includes a binary focal loss for classi\ufb01cation,\nan L1 loss for regression and an IoU loss. To calculate the\ntargets for these losses, we \ufb01rst match the detections to the\nground truth bounding boxes through bipartite matching as\nproposed in DETR [3]. The rescoring loss is similar, ex-\ncept it consists only of the focal loss as we are only train-\ning the multi-class scores Cmerge output by the rescoring\nmodule. The trajectory re\ufb01nement loss is an L1 loss against\nthe ground-truth trajectory, supervised only for true-positive\ndetections (with IoU with a ground truth box higher than\nMethod\nAP L1\nAPH L1\nAP L2\nAPH L2\nCenterFormer [82]\n82.3\n80.9\n77.6\n76.3\nBEVFusion [36]\n82.7\n81.4\n77.7\n76.3\nMSF [15]\n83.1\n81.7\n78.3\n77.0\nFSD++ [13]\n83.5\n82.1\n78.4\n77.1\nLoGoNet [29]\n83.1\n81.8\n78.4\n77.1\nOctopus Noah\n83.1\n81.7\n78.7\n77.3\nSEED-L [38]\n83.5\n82.2\n78.7\n77.3\nLION [37]\n83.7\n82.4\n78.7\n77.4\nVeuronNet3D\n83.7\n82.2\n79.1\n77.7\nHIAC\n84.0\n82.6\n79.2\n77.8\nInceptioLidar\n83.8\n82.5\n79.2\n77.8\nVADet\n84.1\n82.8\n79.4\n78.2\nMT3D\n85.0\n83.7\n80.1\n78.7\nLIVOX Detection\n84.8\n83.5\n80.2\n79.0\nMAD (Ours)\n86.0\n84.3\n81.8\n80.2\nTable 3.\nResults on the WOD test set, as reported on the\nleaderboard2. We exclude entries that state they use ensembles,\ntest-time augmentations, or are of\ufb02ine (use future sensor data).\n\u201cOurs\u201d is using SAFDNet 4f as the 3D detector. APH L2 is the\nranking metric.\n0.5). See our supplementary for more details.\n4\nExperiments\nThis section provides a comprehensive quantitative analy-\nsis of MAD from three perspectives. First, we add MAD to\nexisting 3D detectors, showing signi\ufb01cant improvements.\nWe use both LiDAR-based and camera-based detectors on\nWOD [57] and AV2 [68], respectively. Second, we com-\npare the best version of MAD to the state-of-the-art meth-\nods on WOD, setting a new record on the WOD leaderboard\namong online methods without ensembles or test-time aug-\nmentation and outperforming prior learned temporal fusion\nmethods by a large margin. Finally, we conduct thorough\n1454\n\n=== Page 7 ===\nMethod\nAP L1\nAPH L1\nAP L2\nAPH L2\nValidation\nLEF [17]\n79.6\n79.2\n71.4\n70.9\nMoDAR [30]\n-\n-\n-\n72.5\nMPPNet [8]\n81.6\n81.1\n76.0\n74.8\nMSF [15]\n82.2\n80.7\n76.8\n75.5\nPTT [20]\n82.7\n80.7\n77.7\n75.7\nMAD (Ours)\n85.8\n84.2\n81.0\n79.4\nTesting\n3D-MAN [74]\n49.6\n48.1\n44.8\n43.4\nMPPNet [8]\n81.8\n80.6\n76.9\n75.7\nMSF [15]\n83.1\n81.7\n78.3\n77.0\nMAD (Ours)\n86.0\n84.3\n81.8\n80.2\nTable 4. Comparison of our method against various methods for\nlearned temporal fusion on WOD. \u201cOurs\u201d is using SAFDNet 4f.\nablation studies to understand the architectural choices that\nmake MAD effective and the impact of different training\nprocedures. Refer to our supplementary for more imple-\nmentation details, experimental results, and ablations.\nImplementation Details:\nThe re\ufb01nement transformer\nuses I = 3 re\ufb01nement blocks, and the dimension of all\nembeddings is d = 128.\nWe forecast Tf = 10 future\ntimestamps at stride of sf = 0.5s, yielding a 5s prediction\nhorizon. Unless otherwise stated, we use target timestamps\nof Tm = {\u21930.3s, \u21930.6s, . . . , \u21932.4s} (i.e., sm = 0.3s,\nTm = 8) for reading from the memory bank at inference.\nIn the memory cross attention we use the nearest k = 4\nneighbors.\nFollowing prior works [75, 80, 81], for any\ndetection post-processing, we use a 0.1 con\ufb01dence thresh-\nold; per-class NMS IoU thresholds of {0.75, 0.6, 0.55} for\nvehicles, pedestrians, and cyclists, respectively; and a top\nK = 500.\nMAD has 3.8M parameters, while the base\ndetectors have anywhere from 8M (Centerpoint [75]) to\n53M (BEVMap [6]) parameters. For each base detector,\nwe train MAD for 60k update steps (roughly equivalent to\n6 epochs on WOD and AV2), with batch size 16. We use\na cosine learning rate decay with a max learning rate of\n8 \u219410\u21934, and a linear warm-up for the \ufb01rst 1000 steps,\nbeginning with a learning rate of 8 \u219410\u21935. During train-\ning, we use a variable set of memory target timestamps\nTm \u2197uniform({6, 7, 8, 9, 10}) with a variable stride\nsm \u2197uniform({0.2s, 0.3s, 0.4s}).\nMetrics:\nWe report the detection metrics from the of\ufb01-\ncial WOD leaderboard [57], which include average preci-\nsion (AP) and AP weighted by heading error (APH) for ve-\nhicles (Veh.), pedestrians (Ped.), and cyclists (Cyc.). These\nmetrics use intersection-over-union (IoU) thresholds of 0.7,\n0.5, and 0.5, respectively. The metrics are broken down into\ntwo levels of dif\ufb01culty: Level 1 (L1) includes only labels\nthat have > 5 LiDAR points and are not marked as \u201chard\u201d,\nand Level (L2) includes all boxes that have > 0 LiDAR\npoints (a superset of L1). For camera experiments on AV2,\nwe report the mean average precision (AP) for vehicles in\nthe camera \ufb01eld of view at an IoU threshold of 0.1. We re-\nport the macro-average over all classes if the actor class is\nnot speci\ufb01ed.\nAugmenting off-the-shelf 3D Detectors with MAD:\nTab. 1 and Tab. 2 show the performance of MAD ap-\nplied to off-the-shelf 3D detectors on WOD and AV2, re-\nspectively.To show the generality of our approach, we ex-\nperiment with multiple base detectors trained on different\ndatasets and sensor modalities. We enhanced three LiDAR-\nbased methods with MAD on WOD: CenterPoint [75] with\nboth 1 LiDAR frame (1f) and 2 LiDAR frames (2f) as in-\nput, HEDNet [81] (1f and 4f), and SAFDNet [80] (1f and\n4f). We follow their of\ufb01cial protocols to train and eval-\nuate all models from scratch (due to the Waymo Dataset\nLicense Agreement, we cannot simply re-use pre-trained\nmodels). We also enhance two camera-based methods on\nAV2, FCOS3D [64] and BEVMap [6], which takes the\nmost recent image from the front camera as input.\nWe\nuse the of\ufb01cial implementation for both FCOS3D [64] and\nBEVMap [6]. Training details are in the supplementary.\nOur model brings signi\ufb01cant improvements to all detec-\ntors on both datasets. These gains are largest for single-\nframe detectors, where the memory provides the most ad-\nditional information.\nThe fact that the MAD-augmented\nsingle-frame detectors are better than the multi-frame detec-\ntors clearly shows the effectiveness of our method relative\nto the common point aggregation approach. Please visit the\nsupplementary materials for qualitative comparisons.\nComparison against SOTA:\nBy augmenting SAFDNet\n4f with MAD, we show in Tab. 3 that we achieve the best\nperformance on the WOD leaderboard2, among all online\nmethods that do not use ensembles or test-time augmenta-\ntion. Table 4 compares MAD to prior learned temporal fu-\nsion methods on the WOD validation and test set, where we\nachieve substantial gains. Please refer to our supplementary\nfor full Tabs. 3 and 4 with metrics for all actor classes.\nEffect of memory proposals and memory attention:\nWe ablate the different components of our memory pipeline\nin Tab. 5. Comparing rows 1, 2, and 5 shows that both the\nproposed memory attention and memory proposals have a\npositive effect. This is intuitive as the memory proposals\nlet MAD recover from false negative detection proposals,\nwhich is complementary to memory cross-attention that al-\nlows MAD to use all memory information for re\ufb01nement\n(bypassing the \ufb01ltering in proposal-merging).\nEffect of forecasting:\nComparing rows 3 and 5 in Tab. 5,\nwe \ufb01nd that using trajectory forecasting to align memory\nproposals to the current time is important, particularly for\nfast-moving objects. Without forecasting, the memory pro-\nposals from previous frames will be far from the current po-\n2https://waymo.com/open/challenges/2020/3d-detection/ as of submis-\nsion (14/11/2024)\n1455\n\n=== Page 8 ===\nMem. Prop.\nMem. Attn.\nForecast.\nRescore.\nVeh. AP\nPed. AP\nCyc. AP\nVeh. AP [20, 30) m/s\nCyc. AP [5, 10) m/s\n0\n\u2701\n\u2701\n\u2701\n\u2701\n75.4\n80.1\n79.6\n38.0\n72.4\n1\n\u2701\n\u2702\n\u2702\n\u2702\n76.5\n81.3\n82.0\n40.4\n82.1\n2\n\u2702\n\u2701\n\u2702\n\u2702\n75.8\n81.7\n81.6\n37.7\n75.9\n3\n\u2702\n\u2702\n\u2701\n\u2702\n76.9\n81.8\n81.9\n34.6\n78.7\n4\n\u2702\n\u2702\n\u2702\n\u2701\n72.7\n82.0\n81.2\n11.9\n73.9\n5\n\u2702\n\u2702\n\u2702\n\u2702\n77.0\n82.3\n83.3\n45.2\n86.2\nTable 5. Component ablation of MAD on the WOD validation set. All metrics are L2. Row 0 is the base 3D detector, SAFDNet 4f [80].\nAll ablations in this table (including the \ufb01nal method with all components) use a reduced training duration of 45k iterations to reduce costs.\nEvaluated Proposals\nAP L1\nAPH L1\nAP L2\nAPH L2\nDetection\n!\nPdet\"\n83.9\n82.6\n78.4\n77.1\nCombined\n!\nPmem \u2192Pdet\"\n18.1\n17.8\n16.8\n16.4\nMerged (Pmerge)\n84.1\n83.0\n78.8\n77.7\nAfter Block 0\n!\nPref(1)\"\n84.4\n83.2\n79.3\n78.1\nAfter Block 1\n!\nPref(2)\"\n85.8\n84.2\n80.9\n79.3\nAfter Block 2\n!\nPref\"\n85.8\n84.2\n81.0\n79.4\nTable 6. Evaluating various intermediate proposals from MAD.\nThe base detector is SAFDNet 4f.\nsition of those objects, making it challenging for the model\nto leverage the memory effectively.\nEffect of learned proposal merging:\nComparing rows 0,\n4, and 5 of Tab. 5 we \ufb01nd the proposed learned rescoring of\nthe merged detection and memory proposals is crucial for\ngood performance. Without it, MAD cannot enhance the\nbase detector (row 0) because the proposal scores from the\n3D detector and memory are miss-calibrated before being\npost-processed in the proposal merging step (i.e., NMS).\nWe illustrate this in Tab. 6, where we evaluate intermedi-\nate proposals of MAD: (1) the detection proposals Pdet,\n(2) naively taking the union of the detection proposals Pdet\nand memory proposals Pmem and post-processing them, (3)\nthe merged proposals Pmerge (which have been rescored),\nand (4) after each block of the re\ufb01nement transformer\nPref(1), . . . , Pref(I). Naively concatenating the combined\nproposals is much worse than the base detector because of\nthe miss-calibrated scores. After proposal merging, Pmerge\nalready improves over the base detector. Each re\ufb01nement\nblock brings further gains, illustrating the strength of our\nproposed re\ufb01nement transformer.\nTraining Procedure Study:\nTable 7 provides evidence\nsupporting the effectiveness of our proposed training sched-\nule. We \ufb01rst train MAD with three different chunk sizes\n(i.e., sequences with {144, 48, 1} frames), each with and\nwithout the memory bank cache. Training with long chunks\n(144 frames, Tab. 7.1a) provides good performance because\nthere is a low gap between training and evaluation. The\ncache provides no gains in this setting (Tab. 7.1b) because\nthe model already has memory proposals in most frames.\nTraining with shorter chunks (Tab. 7.2a,3a) performs worse\nbecause there is a more signi\ufb01cant gap between training\nand evaluation. Including the cache helps signi\ufb01cantly by\nChunk Length\nCache\nAP L1\nAPH L1\nAP L2\nAPH L2\n1a\n144\n\u2701\n85.4\n83.9\n80.5\n79.0\n1b\n144\n\u2702\n85.2\n83.7\n80.3\n78.9\n2a\n48\n\u2701\n85.1\n83.5\n80.2\n78.6\n2b\n48\n\u2702\n85.3\n83.8\n80.3\n78.9\n3a\n1\n\u2701\n83.9\n82.4\n78.4\n77.0\n3b\n1\n\u2702\n85.0\n83.3\n80.1\n78.5\n4a\n1)48)96)144\n\u2701\n84.9\n83.3\n79.9\n78.3\n4b\n1)48)96)144\n\u2702\n85.8\n84.2\n81.0\n79.4\nTable 7. Ablating chunk length and the memory cache on WOD,\nusing SAFDNet 4f.\nclosing the gap to evaluation but does not fully reach the\nlong chunk performance (Tab. 7.2b,3b). As hypothesized\nin Sec. 3.2, there is room for improvement by training with\nour proposed schedule and memory bank cache (Tab. 7.4b).\nThis strategy allows MAD to learn generalized patterns\nover a diverse set of examples quickly by training on short\nchunks (more i.i.d. data) at the beginning when the learn-\ning rate is higher while re\ufb01ning its understanding on long\nchunks (closer to the deployment setting) towards the end\nwhen the learning rate is lower. Table 7.4a shows the impor-\ntance of the cache when using this training schedule; other-\nwise, training with small chunks is ineffective as the model\nwould not learn to use the memory.\n5\nConclusion\nIn this paper, we propose MAD \u2014 a simple, effective, and\nsensor-modality-agnostic add-on for enhancing any existing\n3D object detector with long-term memory. To achieve this,\nwe design a transformer-based model that uses joint detec-\ntion and trajectory forecasting to populate a memory bank\nwith spatial-temporal object trajectories. Our model can ef-\nfectively fuse memory proposals with detection proposals\nby reading previous memory entries and aligning them with\nthe current time and ego pose. We also propose a novel\ntraining strategy that increases data diversity while keep-\ning the training-to-inference gap low. Our approach is very\ngeneral \u2014 bringing impressive improvements to a variety\nof LiDAR-based and camera-based detectors, and very ef-\nfective \u2014 achieving SOTA performance on Waymo Open\nDataset when paired to the base detector SAFDNet 4f [80].\n1456\n\n=== Page 9 ===\nReferences\n[1] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom.\nnuscenes: A multi-\nmodal dataset for autonomous driving. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11621\u201311631, 2020. 1\n[2] Yigit Baran Can, Alexander Liniger, Ozan Unal, Danda\nPaudel, and Luc Van Gool. Understanding bird\u2019s-eye view\nof road semantics using an onboard camera. IEEE Robotics\nand Automation Letters, 7(2):3302\u20133309, 2022. 2\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European confer-\nence on computer vision, pages 213\u2013229. Springer, 2020. 6,\n1\n[4] Sergio Casas, Ben Agro, Jiageng Mao, Thomas Gilles,\nAlexander Cui, Thomas Li, and Raquel Urtasun. Detra: A\nuni\ufb01ed model for object detection and trajectory forecasting.\n2024. 2, 4, 6, 5\n[5] Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir\nAnguelov.\nMultipath: Multiple probabilistic anchor tra-\njectory hypotheses for behavior prediction. arXiv preprint\narXiv:1910.05449, 2019. 2\n[6] Mincheol Chang, Seokha Moon, Reza Mahjourian, and\nJinkyu Kim. Bevmap: Map-aware bev modeling for 3d per-\nception. In Proceedings of the IEEE/CVF Winter Confer-\nence on Applications of Computer Vision, pages 7419\u20137428,\n2024. 2, 6, 7, 1\n[7] Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Andrew G\nBerneshawi, Huimin Ma, Sanja Fidler, and Raquel Urtasun.\n3d object proposals for accurate object class detection. Ad-\nvances in neural information processing systems, 28, 2015.\n2\n[8] Xuesong Chen, Shaoshuai Shi, Benjin Zhu, Ka Chun Che-\nung, Hang Xu, and Hongsheng Li. Mppnet: Multi-frame\nfeature intertwining with proxy points for 3d temporal ob-\nject detection. In European Conference on Computer Vision,\npages 680\u2013697. Springer, 2022. 1, 2, 3, 7\n[9] Kyunghyun Cho. Learning phrase representations using rnn\nencoder-decoder for statistical machine translation.\narXiv\npreprint arXiv:1406.1078, 2014. 4, 1\n[10] Alexander Cui, Sergio Casas, Kelvin Wong, Simon Suo, and\nRaquel Urtasun. Gorela: Go relative for viewpoint-invariant\nmotion forecasting.\nIn 2023 IEEE International Confer-\nence on Robotics and Automation (ICRA), pages 7801\u20137807.\nIEEE, 2023. 2, 5\n[11] Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou,\nTsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schnei-\nder, and Nemanja Djuric. Multimodal trajectory predictions\nfor autonomous driving using deep convolutional networks.\nIn 2019 international conference on robotics and automation\n(icra), pages 2090\u20132096. IEEE, 2019. 2\n[12] Martin Engelcke, Dushyant Rao, Dominic Zeng Wang,\nChi Hay Tong, and Ingmar Posner.\nVote3deep: Fast ob-\nject detection in 3d point clouds using ef\ufb01cient convolutional\nneural networks. In 2017 IEEE International Conference on\nRobotics and Automation (ICRA), pages 1355\u20131361. IEEE,\n2017. 2\n[13] Lue Fan, Yuxue Yang, Feng Wang, Naiyan Wang, and\nZhaoxiang Zhang. Super sparse 3d object detection. IEEE\ntransactions on pattern analysis and machine intelligence,\n45(10):12490\u201312505, 2023. 6, 3\n[14] Davi Frossard, Shun Da Suo, Sergio Casas, James Tu, and\nRaquel Urtasun. Strobe: Streaming object detection from li-\ndar packets. In Conference on Robot Learning, pages 1174\u2013\n1183. PMLR, 2021. 1, 2\n[15] Chenhang He, Ruihuang Li, Yabin Zhang, Shuai Li, and Lei\nZhang. Msf: Motion-guided sequential fusion for ef\ufb01cient\n3d object detection from point cloud sequences. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5196\u20135205, 2023. 1, 2, 3, 5, 6, 7\n[16] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961\u20132969, 2017. 2\n[17] Tong He, Pei Sun, Zhaoqi Leng, Chenxi Liu, Dragomir\nAnguelov, and Mingxing Tan.\nLef: Late-to-early tempo-\nral fusion for lidar 3d object detection. In 2023 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems\n(IROS), pages 1637\u20131644. IEEE, 2023. 1, 2, 5, 7, 3\n[18] Jinghua Hou, Zhe Liu, Zhikang Zou, Xiaoqing Ye, Xiang\nBai, et al. Query-based temporal fusion with explicit mo-\ntion for 3d object detection. Advances in Neural Information\nProcessing Systems, 36, 2024. 1, 3\n[19] Anthony Hu, Zak Murez, Nikhil Mohan, Sof\u00b4\u0131a Dudas, Jef-\nfrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and\nAlex Kendall. Fiery: Future instance prediction in bird\u2019s-\neye view from surround monocular cameras. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 15273\u201315282, 2021. 2\n[20] Kuan-Chih Huang, Weijie Lyu, Ming-Hsuan Yang, and\nYi-Hsuan Tsai.\nPtt: Point-trajectory transformer for ef-\n\ufb01cient temporal 3d object detection.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14938\u201314947, 2024. 1, 2, 3, 5, 7\n[21] Jack Kiefer and Jacob Wolfowitz. Stochastic estimation of\nthe maximum of a regression function. The Annals of Math-\nematical Statistics, pages 462\u2013466, 1952. 5\n[22] Junho Koh, Junhyung Lee, Youngwoo Lee, Jaekyum Kim,\nand Jun Won Choi.\nMgtanet: Encoding sequential lidar\npoints using long short-term motion-guided temporal atten-\ntion for 3d object detection. In Proceedings of the AAAI Con-\nference on Arti\ufb01cial Intelligence, pages 1179\u20131187, 2023. 2\n[23] Jason Ku, Melissa Mozi\ufb01an, Jungwook Lee, Ali Harakeh,\nand Steven L Waslander. Joint 3d proposal generation and\nobject detection from view aggregation. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems\n(IROS), pages 1\u20138. IEEE, 2018. 2\n[24] Fabian Kuppers, Jan Kronenberger, Amirhossein Shantia,\nand Anselm Haselhoff. Multivariate con\ufb01dence calibration\nfor object detection. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition workshops,\npages 326\u2013327, 2020. 4\n[25] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\n1457\n\n=== Page 10 ===\nfor object detection from point clouds. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 12697\u201312705, 2019. 2\n[26] Zhaoqi Leng, Guowang Li, Chenxi Liu, Ekin Dogus Cubuk,\nPei Sun, Tong He, Dragomir Anguelov, and Mingxing Tan.\nLidar augment: Searching for scalable 3d lidar data augmen-\ntations. In 2023 IEEE International Conference on Robotics\nand Automation (ICRA), pages 7039\u20137045. IEEE, 2023. 6\n[27] Bo Li. 3d fully convolutional network for vehicle detection\nin point cloud. In 2017 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), pages 1513\u20131518.\nIEEE, 2017. 2\n[28] Hongyang Li, Hao Zhang, Zhaoyang Zeng, Shilong Liu,\nFeng Li, Tianhe Ren, and Lei Zhang. Dfa3d: 3d deformable\nattention for 2d-to-3d feature lifting. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 6684\u20136693, 2023. 2\n[29] Xin Li, Tao Ma, Yuenan Hou, Botian Shi, Yuchen Yang,\nYouquan Liu, Xingjiao Wu, Qin Chen, Yikang Li, Yu Qiao,\net al.\nLogonet:\nTowards accurate 3d object detection\nwith local-to-global cross-modal fusion. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17524\u201317534, 2023. 6\n[30] Yingwei Li, Charles R Qi, Yin Zhou, Chenxi Liu, and\nDragomir Anguelov. Modar: Using motion forecasting for\n3d object detection in point cloud sequences. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9329\u20139339, 2023. 1, 2, 5, 7, 3\n[31] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-\nhao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:\nLearning bird\u2019s-eye-view representation from multi-camera\nimages via spatiotemporal transformers. In European con-\nference on computer vision, pages 1\u201318. Springer, 2022. 1,\n2\n[32] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu,\nSergio Casas, and Raquel Urtasun. Pnpnet: End-to-end per-\nception and prediction with tracking in the loop. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11553\u201311562, 2020. 1, 2\n[33] Xuewu Lin, Tianwei Lin, Zixiang Pei, Lichao Huang, and\nZhizhong Su.\nSparse4d:\nMulti-view 3d object detec-\ntion with sparse spatial-temporal fusion.\narXiv preprint\narXiv:2211.10581, 2022. 2\n[34] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.\nPetr: Position embedding transformation for multi-view 3d\nobject detection. In European Conference on Computer Vi-\nsion, pages 531\u2013548. Springer, 2022.\n[35] Yingfei Liu, Junjie Yan, Fan Jia, Shuailin Li, Aqi Gao, Tian-\ncai Wang, and Xiangyu Zhang. Petrv2: A uni\ufb01ed framework\nfor 3d perception from multi-camera images. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 3262\u20133272, 2023. 2\n[36] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,\nHuizi Mao, Daniela L Rus, and Song Han. Bevfusion: Multi-\ntask multi-sensor fusion with uni\ufb01ed bird\u2019s-eye view repre-\nsentation. In 2023 IEEE international conference on robotics\nand automation (ICRA), pages 2774\u20132781. IEEE, 2023. 6, 3\n[37] Zhe Liu, Jinghua Hou, Xinyu Wang, Xiaoqing Ye, Jingdong\nWang, Hengshuang Zhao, and Xiang Bai.\nLion: Linear\ngroup rnn for 3d object detection in point clouds.\narXiv\npreprint arXiv:2407.18232, 2024. 6, 3\n[38] Zhe Liu, Jinghua Hou, Xiaoqing Ye, Tong Wang, Jingdong\nWang, and Xiang Bai. Seed: A simple and effective 3d detr\nin point clouds. arXiv preprint arXiv:2407.10749, 2024. 6,\n3\n[39] Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furi-\nous: Real time end-to-end 3d detection, tracking and motion\nforecasting with a single convolutional net. In Proceedings of\nthe IEEE conference on Computer Vision and Pattern Recog-\nnition, pages 3569\u20133577, 2018. 1\n[40] Matthias Minderer,\nJosip Djolonga,\nRob Romijnders,\nFrances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran,\nand Mario Lucic. Revisiting the calibration of modern neu-\nral networks. Advances in Neural Information Processing\nSystems, 34:15682\u201315694, 2021. 4\n[41] Arsalan Mousavian, Dragomir Anguelov, John Flynn, and\nJana Kosecka. 3d bounding box estimation using deep learn-\ning and geometry. In Proceedings of the IEEE conference\non Computer Vision and Pattern Recognition, pages 7074\u2013\n7082, 2017. 2\n[42] Lukas Neumann, Andrew Zisserman, and Andrea Vedaldi.\nRelaxed softmax: Ef\ufb01cient con\ufb01dence auto-calibration for\nsafe pedestrian detection. 2018. 4\n[43] Jiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang,\nYuning Chai, Pei Sun, Yin Zhou, Xi Yi, Ouais Al-\nsharif, Patrick Nguyen, et al.\nStarnet: Targeted compu-\ntation for object detection in point clouds. arXiv preprint\narXiv:1908.11069, 2019. 2\n[44] Jiquan Ngiam, Benjamin Caine, Vijay Vasudevan, Zheng-\ndong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, Rebecca\nRoelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, et al.\nScene transformer: A uni\ufb01ed architecture for predicting mul-\ntiple agent trajectories.\narXiv preprint arXiv:2106.08417,\n2021. 4, 2\n[45] Tung Phan-Minh, Elena Corina Grigore, Freddy A Boulton,\nOscar Beijbom, and Eric M Wolff. Covernet: Multimodal\nbehavior prediction using trajectory sets. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 14074\u201314083, 2020. 2\n[46] AJ Piergiovanni, Vincent Casser, Michael S Ryoo, and\nAnelia Angelova. 4d-net for learned multi-modal alignment.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 15435\u201315445, 2021. 1, 2\n[47] Rui Qian, Divyansh Garg, Yan Wang, Yurong You, Serge\nBelongie, Bharath Hariharan, Mark Campbell, Kilian Q\nWeinberger, and Wei-Lun Chao.\nEnd-to-end pseudo-lidar\nfor image-based 3d object detection.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 5881\u20135890, 2020. 2\n[48] Cody Reading, Ali Harakeh, Julia Chae, and Steven L\nWaslander.\nCategorical depth distribution network for\nmonocular 3d object detection.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8555\u20138564, 2021. 2\n1458\n\n=== Page 11 ===\n[49] Shaoqing Ren.\nFaster r-cnn:\nTowards real-time object\ndetection with region proposal networks.\narXiv preprint\narXiv:1506.01497, 2015. 2\n[50] Herbert Robbins and Sutton Monro. A stochastic approxi-\nmation method. The annals of mathematical statistics, pages\n400\u2013407, 1951. 5\n[51] Thomas Roddick, Alex Kendall, and Roberto Cipolla. Ortho-\ngraphic feature transform for monocular 3d object detection.\narXiv preprint arXiv:1811.08188, 2018. 2\n[52] Avishkar Saha, Oscar Mendez, Chris Russell, and Richard\nBowden. Translating images into maps. In 2022 Interna-\ntional conference on robotics and automation (ICRA), pages\n9200\u20139206. IEEE, 2022. 2\n[53] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr-\ncnn: 3d object proposal generation and detection from point\ncloud. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 770\u2013779, 2019.\n2\n[54] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural net-\nwork for 3d object detection in a point cloud. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 1711\u20131719, 2020. 2\n[55] Martin Simony, Stefan Milzy, Karl Amendey, and Horst-\nMichael Gross. Complex-yolo: An euler-region-proposal for\nreal-time 3d object detection on point clouds. In Proceed-\nings of the European conference on computer vision (ECCV)\nworkshops, pages 0\u20130, 2018. 2\n[56] Shuran Song and Jianxiong Xiao. Deep sliding shapes for\namodal 3d object detection in rgb-d images.\nIn Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 808\u2013816, 2016. 2\n[57] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\nYuning Chai, Benjamin Caine, et al. Scalability in perception\nfor autonomous driving: Waymo open dataset. In CVPR,\n2020. 2, 3, 5, 6, 7\n[58] Pei Sun, Mingxing Tan, Weiyue Wang, Chenxi Liu, Fei Xia,\nZhaoqi Leng, and Dragomir Anguelov. Swformer: Sparse\nwindow transformer for 3d object detection in point clouds.\nIn European Conference on Computer Vision, pages 426\u2013\n442. Springer, 2022. 1\n[59] Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivas-\ntava, Khaled S Refaat, Nigamaa Nayakanti, Andre Cornman,\nKan Chen, Bertrand Douillard, Chi Pang Lam, Dragomir\nAnguelov, et al.\nMultipath++: Ef\ufb01cient information fu-\nsion and trajectory aggregation for behavior prediction. In\n2022 International Conference on Robotics and Automation\n(ICRA), pages 7814\u20137821. IEEE, 2022. 2\n[60] A Vaswani. Attention is all you need. Advances in Neural\nInformation Processing Systems, 2017. 4\n[61] Dominic Zeng Wang and Ingmar Posner. Voting for voting\nin online point cloud object detection. In Robotics: science\nand systems, pages 10\u201315. Rome, Italy, 2015. 2\n[62] Haiyang Wang, Chen Shi, Shaoshuai Shi, Meng Lei, Sen\nWang, Di He, Bernt Schiele, and Liwei Wang. Dsvt: Dy-\nnamic sparse voxel transformer with rotated sets. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 13520\u201313529, 2023. 1\n[63] Shihao Wang, Yingfei Liu, Tiancai Wang, Ying Li, and Xi-\nangyu Zhang. Exploring object-centric temporal modeling\nfor ef\ufb01cient multi-view 3d object detection. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 3621\u20133631, 2023. 3\n[64] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin.\nFcos3d: Fully convolutional one-stage monocular 3d object\ndetection.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 913\u2013922, 2021. 2, 6,\n7, 1\n[65] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hari-\nharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-\nlidar from visual depth estimation: Bridging the gap in 3d\nobject detection for autonomous driving. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8445\u20138453, 2019. 2\n[66] Yue Wang, Alireza Fathi, Abhijit Kundu, David A Ross,\nCaroline Pantofaru, Tom Funkhouser, and Justin Solomon.\nPillar-based object detection for autonomous driving.\nIn\nComputer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part XXII\n16, pages 18\u201334. Springer, 2020. 2\n[67] Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang,\nYilun Wang, Hang Zhao, and Justin Solomon.\nDetr3d:\n3d object detection from multi-view images via 3d-to-2d\nqueries. In Conference on Robot Learning, pages 180\u2013191.\nPMLR, 2022. 2\n[68] Benjamin Wilson, William Qi, Tanmay Agarwal, John\nLambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen\nPan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel\nPontes, et al.\nArgoverse 2: Next generation datasets for\nself-driving perception and forecasting.\narXiv preprint\narXiv:2301.00493, 2023. 2, 3, 5, 6\n[69] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xi-\nhui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang\nZhao. Point transformer v3: Simpler faster stronger. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4840\u20134851, 2024. 2\n[70] Bin Xu and Zhenzhong Chen. Multi-level fusion based 3d\nobject detection from monocular images. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 2345\u20132353, 2018. 2\n[71] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed-\nded convolutional detection. Sensors, 18(10):3337, 2018. 2,\n1\n[72] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-\ntime 3d object detection from point clouds. In Proceedings of\nthe IEEE conference on Computer Vision and Pattern Recog-\nnition, pages 7652\u20137660, 2018. 2\n[73] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia.\n3dssd:\nPoint-based 3d single stage object detector. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11040\u201311048, 2020. 2\n[74] Zetong Yang, Yin Zhou, Zhifeng Chen, and Jiquan Ngiam.\n3d-man: 3d multi-frame attention network for object de-\ntection.\nIn Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 1863\u20131872,\n2021. 1, 3, 7\n1459\n\n=== Page 12 ===\n[75] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-\nbased 3d object detection and tracking. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11784\u201311793, 2021. 1, 2, 6, 7\n[76] Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg,\nGeoff Pleiss, Bharath Hariharan, Mark Campbell, and Kil-\nian Q Weinberger.\nPseudo-lidar++: Accurate depth for\n3d object detection in autonomous driving. arXiv preprint\narXiv:1906.06310, 2019. 2\n[77] Yurong You, Katie Z Luo, Xiangyu Chen, Junan Chen, Wei-\nLun Chao, Wen Sun, Bharath Hariharan, Mark Campbell,\nand Kilian Q Weinberger.\nHindsight is 20/20: Leverag-\ning past traversals to aid 3d perception.\narXiv preprint\narXiv:2203.11405, 2022. 2\n[78] Yihan Zeng, Da Zhang, Chunwei Wang, Zhenwei Miao, Ting\nLiu, Xin Zhan, Dayang Hao, and Chao Ma. Lift: Learning\n4d lidar image fusion transformer for 3d object detection.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 17172\u201317181, 2022.\n1, 2\n[79] Ce Zhang, Chengjie Zhang, Yiluan Guo, Lingji Chen, and\nMichael Happold.\nMotiontrack: end-to-end transformer-\nbased multi-object tracking with lidar-camera fusion. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 151\u2013160, 2023. 2\n[80] Gang Zhang, Junnan Chen, Guohuan Gao, Jianmin Li, Si\nLiu, and Xiaolin Hu. Safdnet: A simple and effective net-\nwork for fully sparse 3d object detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14477\u201314486, 2024. 2, 6, 7, 8, 1\n[81] Gang Zhang, Chen Junnan, Guohuan Gao, Jianmin Li, and\nXiaolin Hu. Hednet: A hierarchical encoder-decoder net-\nwork for 3d object detection in point clouds. Advances in\nNeural Information Processing Systems, 36, 2024. 2, 6, 7, 1\n[82] Zixiang Zhou, Xiangchen Zhao, Yu Wang, Panqu Wang, and\nHassan Foroosh. Centerformer: Center-based transformer\nfor 3d object detection. In European Conference on Com-\nputer Vision, pages 496\u2013513. Springer, 2022. 2, 3, 6\n1460\n",
      "searchable_content": "page 1 mad memory-augmented detection of 3d objects ben agro sergio casas patrick wang thomas gilles raquel urtasun tgilles urtasun waabi.ai a 3d detector b object memory c memory augmented detections 0.0s -2.4s memory age false negative figure 1. detectors without long-term temporal fusion a miss heavily occluded objects. our approach enhances detectors b to remember past predictions c recovering from occlusion. detections are in green labels are in black lidar points are in gray. abstract to perceive humans use memory to \ufb01ll in gaps caused by our limited visibility whether due to occlusion or our nar- row \ufb01eld of view. however most 3d object detectors are limited to using sensor evidence from a short temporal win- dow 0.1s-0.3s . in this work we present a simple and ef- fective add-on for enhancing any existing 3d object detec- tor with long-term memory regardless of its sensor modality e.g. lidar camera and network architecture. we pro- pose a model to effectively align and fuse object proposals from a detector with object proposals from a memory bank of past predictions exploiting trajectory forecasts to align proposals across time. we propose a novel schedule to train our model on temporal data that balances data diversity and the gap between training and inference. by applying our method to existing lidar and camera-based detectors on the waymo open dataset wod and argoverse 2 sen- sor av2 dataset we demonstrate signi\ufb01cant improvements in detection performance 2.5 to 7.6 ap points . our method attains the best performance on the wod 3d de- tection leaderboard among online methods excluding en- sembles or test-time augmentation . 1 introduction most self-driving vehicles sdvs utilize a 3d object detec- tor to recognize and localize objects in 3d space. this task is challenging due to occlusion large intra-class variability and distant objects which typically have limited sensor ob- servations. to overcome these challenges human drivers rely on their memory. for example they may drive more cautiously when remembering a previously observed but now occluded cyclist who may suddenly enter the road. a common approach for improving 3d object detectors is to aggregate a short temporal window of past sensor ob- servations. towards this goal most existing lidar-based methods transform a short buffer of sensor data into the cur- rent ego sdv coordinate frame to align past and current evidence 1 39 58 74 75 . similarly camera-based meth- ods stack multiple images 46 78 as input to existing ar- chitectures. these methods cannot handle long temporal sequences due to computational and memory constraints. moreover temporal stacks of 3d bird s-eye-view bev representations like point clouds or lifted camera features require a large receptive \ufb01eld especially for fast-moving objects 30 further increasing computational burden. there is a growing interest in long-term temporal fusion. scene-level memory approaches 14 17 31 recurrently fuse scene-level features but they can struggle to capture relevant foreground objects. other approaches associate ob- jects in memory over time via tracking 8 20 30 32 ag- gregating past information for each particular object. how- ever the associations from the tracker may contain mis- takes that can compound over time and lead to informa- tion loss. other methods leverage attention from current detection proposals to the past sensor or object informa- tion 15 18 74 . still they can be challenging to scale to long histories and suffer from false negatives as the pro- posals re\ufb01ned into the \ufb01nal detections only come from the work done while at waabi this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 1449 page 2 present time. in this paper we present a simple and sensor-agnostic add-on for enhancing any existing 3d object detector with long-term memory. we refer to it as mad short for memory-augmented detection and fig. 1 illustrates the high-level idea. mad is a transformer-based model that fuses proposals from a detector with proposals from a mem- ory bank representing past beliefs. inspired by recent devel- opments 4 we exploit joint detection and trajectory fore- casting. by storing explicit trajectory forecasts in the mem- ory bank we can estimate object poses at arbitrary future timestamps for all the objects in the memory. this enables us to enrich the set of proposals by aligning memory pro- posals with the current observations. training with temporal data can be challenging back- propogation through consecutive training examples con- sumes prohibitive amounts of memory training on long se- quences can cause over-\ufb01tting when back-propagating on every example and using memory warm-up can slow down training. we design a more effective training schedule that begins with short temporal sequences and progressively in- creases the length exploiting high data diversity early and closing the gap with inference towards the end. to en- sure the model learns to trust the memory when training on short sequences we use cached model outputs from pre- vious training iterations. we demonstrate the generality of our approach by en- hancing existing lidar-based and camera-based 3d ob- ject detection networks with mad and show considerable improvements over the base detectors on two large-scale datasets waymo open dataset wod 57 and argoverse 2 sensor dataset av2 68 . notably safdnet 80 en- hanced with mad achieves state-of-the-art performance on wod for online detection methods without requiring en- sembles or test-time augmentation. 2 related work 3d object detection we can categorize 3d detectors by their input modality e.g. lidar camera scene repre- sentation e.g. point clouds voxels and number of stages e.g. single-stage or multi-stage lidar-based methods commonly represent the input as voxels 12 27 56 71 80 81 pillars 23 25 55 61 66 72 or point clouds 43 53 54 69 73 . a widely used ap- proach for including temporal lidar information is point aggregation which involves transforming past point clouds into a common coordinate frame and processing the ag- gregated point cloud. these approaches are usually lim- ited to 5 past lidar frames due to computational con- straints 30 in online applications like autonomous driving. another drawback is that point aggregation does not align moving objects requiring a larger receptive \ufb01eld in the de- tector backbone the longer the temporal horizon is 30 80 . camera-based 3d detection is challenging because of missing depth information. one approach is to produce 3d bounding boxes from image features by estimating depth 3d size and orientation 7 41 64 70 . other methods leverage voxel 47 48 51 or point cloud 65 76 repre- sentations by predicting pixel depth distributions to lift 2d features to 3d. stacking and processing past camera images or feature maps is a common but expensive method for tem- poral fusion 2 19 46 52 78 . regardless of modality we can further categorize 3d de- tectors as single-stage or multi-stage. single-stage methods produce detections from sensor data 6 12 64 71 80 81 with a single deep neural network. multi-stage methods use bounding box proposals from a \ufb01rst stage or ran- domly initialized proposals to gather features e.g. with roipool 49 roialign 16 interpolations 75 or atten- tion 4 28 33 35 67 82 and iteratively re\ufb01ne the bound- ing boxes. mad is a sensor-modality-agnostic module that we can add to any detector as a subsequent re\ufb01nement stage. in our work we utilize centerpoint 75 safdnet 80 hed- net 81 fcos3d 64 and bevmap 6 as proposal net- works. long-term temporal fusion for 3d detection vari- ous works attempt to solve the shortcomings of sensor ag- gregation by learning to use multiple seconds of sensor ev- idence to improve object detection. in this line of work the scene-based paradigm uses recurrent fusion of scene-level features 14 17 31 75 with some methods relying on multiple traversals of the scene 77 . a challenge of this approach is focusing and aligning features from relevant and dynamic foreground objects which past works addressed by transforming fea- ture maps using segmentation to focus on foreground ob- jects 17 and using deformable attention or convolution to align features of moving objects 22 . processing both foreground and background areas can be computationally expensive. it is worth noting that many of these methods are single-stage detectors 14 17 and which we could use as a detection proposal network with mad. alternatively the object-based paradigm focuses on the foreground by using detection proposals. detect-track-fuse methods are a sub-family of object-based methods that as- sociate previous detections over time to create tracks and these tracks summarize information from past sensor evi- dence 8 20 30 30 32 79 . however in complex situ- ations like pedestrian crowds association over time can be dif\ufb01cult due to heavy occlusions and erratic behavior po- tentially leading to false negatives or identity switches in the tracks. object-to-scene approaches mitigate the short- comings of association by directly using current detec- tion proposals to aggregate historical scene-level informa- tion using hand-crafted feature aggregation modules 15 1450 page 3 memory bank retrieval alignment feature extraction 3d detector detection proposals memory proposals proposal merging refinement transformer memory cross attention factorized self attention proposal update model outputs refined proposals memory bank update merged proposals in preparation for future inferences sensor data timestamp sdv pose figure 2. mad is a plug-and-play module that enhances any off-the-shelf 3d detector kept frozen with long-term memory. or attention mechanisms 82 . these approaches can be dif\ufb01cult to scale to long history horizons as they require re-processing past sensor evidence or dense feature maps based on the current proposals e.g. 15 82 only use 0.7s of history . finally object-to-object methods use past ob- ject detections to improve current object detections without explicit tracking e.g. by cross-attending from current de- tection proposals to past detections 63 74 or using hand- coded attention matrices based on distance 18 . overall most object-based methods share some de\ufb01ciencies many only re\ufb01ne detection proposals produced by current sen- sor evidence and struggle to recover from missing propos- als 8 15 18 20 74 . others naively concatenate past detections with current proposals 63 which can lead to alignment issues for dynamic objects and miss-calibration in the proposal scores as the model should trust historical proposals less than current proposals. our proposed method mad performs object-based temporal fusion without requiring explicit object associa- tion aligns the memory in space and time by with trajectory forecasting and can recover from missing proposals by us- ing and rescoring proposals from the memory bank. 3 memory augmented 3d object detection 3d object detectors take a short temporal window of sensor data as input and produce a set of detections. existing ap- proaches typically struggle to perceive occluded and distant objects with limited sensor observations. to tackle these challenges we propose mad a plug-and-play module to enhance existing 3d object detectors with the ability to per- form long-horizon temporal fusion. our only requirement from the detector is that each detection includes an object bounding box multi-class con\ufb01dence scores and a feature vector capturing local context. we demonstrate the general- ity of mad by augmenting and improving various lidar- based and camera-based detectors. we enable long-horizon temporal understanding through a memory bank that captures all the relevant information on objects including where we expect them to move. these trajectory forecasts allow us to align the memory objects with the current detector proposals in space and time. im- portantly we do not require the object detector to provide motion forecasts instead mad computes them. to com- pensate for ego-motion we assume the ego is localized which is the norm in modern self-driving platforms 57 68 and store the ego pose in the memory along with the model outputs. 3.1. model we start with an overview of our model refer to figure 2 for an illustration. at every inference step mad takes as input the detection proposals the current timestamp t e.g. lidar sweep-end time or camera capture time and the ego pose et in a global coordinate frame. it then re- trieves objects from memory aligns them spatially with et and temporally to t and extracts high-dimensional features from the aligned boxes and trajectory forecasts. we refer to the aligned boxes and trajectory forecasts with the ex- tracted features as memory proposals. a proposal merg- ing mechanism then fuses detection and memory proposals by rescoring their con\ufb01dence scores and applying standard post-processing. finally our re\ufb01nement transformer itera- tive re\ufb01nes the object detections and trajectory forecasts in the merged proposals with cross-attention to the memory and factorized self-attention. in preparation for future in- ferences the memory bank is then updated by appending the model outputs a.k.a. re\ufb01ned proposals and removing older model outputs to keep the memory bounded in size. proposal representation we de\ufb01ne object proposals p b c t q with n bounding boxes b rn 7 where the last dimension corresponds to x y z l w h \u03c9 with object 3d centroids x y z headings \u03c9 in a bev ego-relative coordinate frame and the 3d box dimensions w l h multi-class con\ufb01dence scores c 0 1 n c where c is the number of actor classes trajectory fore- casts t rn tf 3 describing objects bev pose x y \u03c9 t sf . . . x y \u03c9 t tf sf over tf future way- points at a time interval sf and an object feature q rn tf 1 d encoding both local and global features for every object at the present and future timestamps where d 1451 page 4 is the feature dimensionality. we use superscripts to de- note the source of the proposals detection proposals pdet from the 3d detector memory proposals pmem from the memory bank merged proposals pmerge from the proposal merging module and re\ufb01ned proposals pref from the out- put of the re\ufb01nement transformer. for detection proposals pdet we generate tdet by assuming the object is static over time since detectors do not provide forecasts and this is just an initialization before re\ufb01nement . the object features qdet are obtained by interpolating the feature map from be- fore the detector header at the projected object centroids repeating tf 1 times to get the features for future times- tamps and adding a learned embedding of bdet and cdet. in the paragraphs below we describe how we obtain pmem pmerge and pref. memory bank and retrieval the memory bank is a set of tuples tm etm pref tm with timestamped past model out- puts and ego pose sorted by the timestamp tm at which the outputs were generated. during inference at timestamp t we retrieve memory entries pref tm at a set of past target times- tamps tm tm where tm t sm t 2sm . . . t tmsm . tm is the number of past target timestamps and sm is the time stride of the retrieved entries. to be precise we retrieve the closest memory entry to each timestamp in tm to be robust to small sensor delays. extracting memory proposals for effective use of the memory at inference we should align each retrieved entry tm etm pref tm in space and time with the current detec- tion proposals at time t. we handle ego-motion by applying the relative transform etm t e 1 t etm to bref tm and tref tm. to handle object motion we linearly interpolate the stored trajectory forecast to the current timestamp t to obtain the proposal box bmem. to obtain the proposal forecast tmem we also interpolate extrapolate the stored trajectories as re- quired to obtain waypoints at tf t sf . . . t tfsf from stored waypoints at tm sf . . . tm tfsf . finally we extract latent features qmem at t and every future time step tf tf first we compute sinusoidal posi- tional embeddings 60 for the centroid coordinates bmem x y z and encode them with a lightweight mlp. separately we concatenate other features including bmem l w h \u03c9 box dimen- sions and heading con\ufb01dence scores cmem the memory age t tm and a 2d vector pointing to where the pro- posal was in the current ego coordinate frame at the time tm. finally we encode the concatenated features with an- other mlp and add the features from both mlps together. proposal merging the memory and detection propos- als can be redundant particularly in areas with good sen- sor coverage. to merge proposals we learn to rescore their multi-class con\ufb01dence scores. rescoring is essential as the con\ufb01dence the model should put in a memory proposal not only depends on the con\ufb01dence score at a past timestamp cmem tm but also on the proposal age t tm as the fore- casting uncertainty grows with the time horizon and other factors. for example the model should trust a fast-moving detection less than a stationary object or it should trust an object observed 0.5 seconds ago more than one observed 5 seconds ago. furthermore the detection proposals come from the 3d detector while the memory proposals are pro- duced by mad and detectors have been found to be mis- calibrated 24 40 42 . to make the scores comparable we learn two small mlps that separately map the features of the detection pro- posals qdet and the memory proposals qmem to new multi- class scores cmerge. as explained in section 3.2 these rescoring mlps are trained under a single detection loss ap- plied to the merged proposals so that the model can decide which proposals to trust from both sources. finally we \ufb01lter proposals with score thresholding non-maximum suppres- sion nms and keep the top k merged proposals sorted by score maximum over actor classes . post-processing en- ables the re\ufb01nement transformer to process a smaller num- ber of queries. finally we add learned time positional embeddings to the merged proposal features qmerge to indicate the time of the trajectory forecast. at this point we have n merge def n ref merged proposals pmerge ready for re\ufb01nement. re\ufb01nement transformer we utilize a transformer de- coder to re\ufb01ne the merged proposals pmerge pref 0 it- eratively over i blocks into pref 1 ...pref i where the \ufb01nal model outputs are pref pref i . we propose a novel memory cross-attention mechanism to allow the queries proposal features qref i to aggregate information from all the memory proposals qmem including those that pro- posal merging \ufb01ltered out. we want to use this informa- tion in the re\ufb01nement transformer because multiple overlap- ping memory proposals provide signi\ufb01cant evidence about an object s presence and location. to achieve this we per- form cross attention from the object queries qref i to the memory proposal features qmem. for ef\ufb01ciency we limit the cross attention to the nearest k keys to each object query computing the nearest neighbors of bref i x y z in bmem x y z . similar to many works 4 44 we perform factorized self-attention in each re\ufb01nement block which separates time self-attention and object self-attention for ef\ufb01ciency where the former attends only to queries from the same ob- ject sequence length tf 1 and the latter only attends to queries from the same time step sequence length n . the updated queries qref i 1 are input to the next block. finally we update the explicit proposal information as described in detra 4 by using a simple mlp to produce bref i 1 and cref i 1 and a gated recurrent unit gru 9 to update the future trajectory waypoints tref i 1 . 1452 page 5 memory bank update we post-process the re\ufb01ned pro- posals pref as we did to the merged proposals score thresh- olding nms and top k based on con\ufb01dence score adding the result to the memory bank along with the correspond- ing timestamp t and ego pose et. to limit the size of the memory bank when running on long sequences we remove any memory entries older than t tmsm \u03b5 the past time- horizon used in memory retrieval with a small buffer \u03b5 . 3.2. training we \ufb01rst train an off-the-shelf 3d detector following their original training strategy. this stage can be omitted if a pre-trained 3d detector is available. then we train all the parameters in mad as a subsequent stage with the 3d de- tector weights frozen. pre-training and freezing the 3d de- tector is important to ensure the detection proposals do not change throughout mad training. note that we train a sep- arate mad for each 3d detector as each detector has differ- ent features qdet and detection distribution and calibration. before detailing our proposed mad training we discuss some possibilities and trade-offs when training temporal fu- sion models. training on unordered examples has the ad- vantage of satisfying the assumption of i.i.d examples bet- ter learning dynamics 21 50 . however it differs from evaluation where the model is rolled out on long sequences and consumes its previous outputs. training on long se- quences of ordered data has the advantage of being closer to evaluation but it has worse learning dynamics since consec- utive examples are heavily correlated there are few changes in the scene from one frame to the next . if instead gradi- ents are accumulated over a long sequence and used to up- date the model parameters once per sequence a sequence becomes one example satisfying the i.i.d assumption but the training duration is multiplied by the sequence length if the number of model updates is kept constant. despite this large space of possibilities and the importance of such choices prior works on learned temporal fusion neglect de- tails and discussion of their training recipe 15 17 20 30 . to tackle these challenges we design a novel training schedule. we propose to train mad on increasingly long chunks of ordered data using single frames1 at the begin- ning and entire sequences at the end of training. to train ob- ject memory on short chunks or even single frames of data while maintaining a reasonable amount of memory inputs we propose to maintain a cache of memory banks across training and using it to build the memory proposals for each training example. below we detail this proposed schedule our cache of memory banks how we handle augmentations with memory and our loss function. training schedule the datasets we use wod 57 av2 68 organize their data into driving logs each around 1we slightly abuse the term frame here as some detectors use a win- dow of multiple past frames as input. 20s in duration with data captured at 10hz meaning each log has around 200 frames. each log has a unique identi- \ufb01er logid . for the \ufb01rst 25 of training we sample single frames that is consecutive training examples are random frames from random logs . throughout the rest of training we sample sequential chunks of gradually increasing size 48 frames for 25 50 of training 96 frames for 50 75 and 144 frames for 75 100 . we train with a single cosine decay learning rate schedule with no resets. the intuition behind this is that when the learning rate is high and the model weights change the most the model is exposed to more diverse data. then when the learning rate is lower the model is tuned to be closer to the evaluation setting where it consumes its previous outputs. exploiting a cache of memory banks if the schedule described above is followed naively during the individual frame and short chunk training the model cannot consume its previous outputs and thus would not learn to use memory during this phase of training. to address this problem we introduce a cache of previous memory banks. this cache is a mapping from the unique driving log identi\ufb01er logid to a memory bank. at the start of training we initialize the cache with empty memory banks for all logids. on a given training iteration we index the cache with the logid of the current training example to obtain the memory bank. if available we retrieve the memory proposals from this mem- ory bank as described in sec. 3.1. we update the retrieved memory bank at the end of the training iteration with the model outputs replacing any existing entry with the same timestamp. note that during training we do not limit the size of the memory bank. there are a few challenges to training with the object memory cache that we address to train these models ef\ufb01ciently on large datasets we use a distributed data training scheme meaning we split examples in the minibatch across multiple gpus. each gpu has a unique index called a rank. each rank main- tains a separate cache to prevent the cache from \ufb01lling up the ram and avoid synchronization costs. to guaran- tee high cache hit rates we ensure that training examples from a given logid are always put on the same rank dur- ing training. the cache is \ufb01lled with mad outputs which are inac- curate at the beginning of training. we do not want er- roneous model outputs to \ufb01ll the cache otherwise the model may not learn to trust the memory proposals. to mitigate this we only start \ufb01lling the cache and train- ing with memory proposals after 2.5 of training after which performance is reasonable. to make the model robust to variable latency and the pres- ence and absence of memory proposals we randomize the target timestamps tm that we retrieve memory elements for during training by randomly sampling the time stride 1453 page 6 overall l1 overall l2 vehicle l1 vehicle l2 pedestrian l1 pedestrian l2 cyclist l1 cyclist l2 method ap aph ap aph ap aph ap aph ap aph ap aph ap aph ap aph centerpoint 1f 75 76.1 73.5 70.0 67.6 75.7 75.2 67.9 67.4 77.6 71.6 70.1 64.4 74.9 73.8 72.1 71.0 mad ours 82.9 81.0 77.6 75.8 81.1 80.5 74.0 73.4 83.8 80.0 77.2 73.5 83.8 82.6 81.6 80.4 centerpoint 2f 75 77.5 75.8 71.7 70.1 76.4 75.9 68.7 68.2 79.2 75.6 71.9 68.5 76.8 75.9 74.4 73.5 mad ours 82.8 81.2 77.5 76.0 81.4 80.8 74.3 73.7 84.7 82.1 78.2 75.6 82.2 80.8 80.1 78.7 hednet 1f 81 81.6 79.7 75.6 73.7 80.9 80.5 73.1 72.7 84.6 80.2 77.1 72.8 79.4 78.5 76.6 75.6 mad ours 85.2 83.3 80.2 78.3 83.6 82.9 76.6 76.0 87.0 83.4 81.0 77.4 85.1 83.7 83.0 81.6 hednet 4f 81 83.6 82.3 78.1 76.8 82.4 81.9 75.1 74.6 86.3 83.6 79.4 76.8 82.2 81.4 79.9 79.1 mad ours 85.5 83.8 80.6 79.0 83.6 82.9 76.8 76.1 87.7 85.0 81.9 79.2 85.1 83.5 83.2 81.6 safdnet 1f 80 81.7 79.7 75.5 73.6 80.5 80.0 72.5 72.1 84.7 80.2 77.1 72.9 79.8 78.8 76.9 75.9 mad ours 85.3 83.5 80.3 78.4 83.4 82.8 76.5 75.9 86.8 82.9 80.7 76.8 85.8 84.7 83.7 82.6 safdnet 4f 80 83.9 82.6 78.4 77.1 82.8 82.3 75.4 74.9 86.8 84.2 80.1 77.5 82.0 81.1 79.6 78.8 mad ours 85.8 84.2 81.0 79.4 84.2 83.6 77.4 76.8 87.9 85.4 82.2 79.7 85.2 83.7 83.3 81.7 table 1. comparing the performance of various off-the-shelf lidar object detectors with and without mad on the wod validation set. base detector results are reproduced using of\ufb01cial code. mad consistently boosts the performance of all detectors across all metrics. vehicle ap iou 10 in camera field of view method overall 0 40 m 40 80 m 80 120 m fcos3d 64 37.6 73.9 34.3 4.65 mad ours 43.6 82.6 40.0 8.11 bevmap 6 51.5 86.5 54.0 13.9 mad ours 53.4 88.0 55.1 17.2 table 2. adding mad to camera-based 3d detectors on av2. sm and the number of target timestamps tm. handling augmentations with memory prior works 26 75 80 81 \ufb01nd that data augmentations e.g. translation rotation \ufb02ipping and re-scaling are important for detection performance. we apply augmenta- tions to the boxes bmem and trajectories tmem in memory proposals after the memory alignment step. we apply the inverse of the augmentations to bref and tref before storing them in the memory bank. loss function we optimize a multi-task objective l lrescore cmerge i i 1 ldet bref i cref i lfor tref i which is a combination of a rescoring loss lscore a detection re\ufb01nement loss ldet and a forecasting re\ufb01nement loss lfor where the detection and forecasting losses are computed at every re\ufb01nement block. follow- ing 4 ldet includes a binary focal loss for classi\ufb01cation an l1 loss for regression and an iou loss. to calculate the targets for these losses we \ufb01rst match the detections to the ground truth bounding boxes through bipartite matching as proposed in detr 3 . the rescoring loss is similar ex- cept it consists only of the focal loss as we are only train- ing the multi-class scores cmerge output by the rescoring module. the trajectory re\ufb01nement loss is an l1 loss against the ground-truth trajectory supervised only for true-positive detections with iou with a ground truth box higher than method ap l1 aph l1 ap l2 aph l2 centerformer 82 82.3 80.9 77.6 76.3 bevfusion 36 82.7 81.4 77.7 76.3 msf 15 83.1 81.7 78.3 77.0 fsd 13 83.5 82.1 78.4 77.1 logonet 29 83.1 81.8 78.4 77.1 octopus noah 83.1 81.7 78.7 77.3 seed-l 38 83.5 82.2 78.7 77.3 lion 37 83.7 82.4 78.7 77.4 veuronnet3d 83.7 82.2 79.1 77.7 hiac 84.0 82.6 79.2 77.8 inceptiolidar 83.8 82.5 79.2 77.8 vadet 84.1 82.8 79.4 78.2 mt3d 85.0 83.7 80.1 78.7 livox detection 84.8 83.5 80.2 79.0 mad ours 86.0 84.3 81.8 80.2 table 3. results on the wod test set as reported on the leaderboard2. we exclude entries that state they use ensembles test-time augmentations or are of\ufb02ine use future sensor data . ours is using safdnet 4f as the 3d detector. aph l2 is the ranking metric. 0.5 . see our supplementary for more details. 4 experiments this section provides a comprehensive quantitative analy- sis of mad from three perspectives. first we add mad to existing 3d detectors showing signi\ufb01cant improvements. we use both lidar-based and camera-based detectors on wod 57 and av2 68 respectively. second we com- pare the best version of mad to the state-of-the-art meth- ods on wod setting a new record on the wod leaderboard among online methods without ensembles or test-time aug- mentation and outperforming prior learned temporal fusion methods by a large margin. finally we conduct thorough 1454 page 7 method ap l1 aph l1 ap l2 aph l2 validation lef 17 79.6 79.2 71.4 70.9 modar 30 - - - 72.5 mppnet 8 81.6 81.1 76.0 74.8 msf 15 82.2 80.7 76.8 75.5 ptt 20 82.7 80.7 77.7 75.7 mad ours 85.8 84.2 81.0 79.4 testing 3d-man 74 49.6 48.1 44.8 43.4 mppnet 8 81.8 80.6 76.9 75.7 msf 15 83.1 81.7 78.3 77.0 mad ours 86.0 84.3 81.8 80.2 table 4. comparison of our method against various methods for learned temporal fusion on wod. ours is using safdnet 4f. ablation studies to understand the architectural choices that make mad effective and the impact of different training procedures. refer to our supplementary for more imple- mentation details experimental results and ablations. implementation details the re\ufb01nement transformer uses i 3 re\ufb01nement blocks and the dimension of all embeddings is d 128. we forecast tf 10 future timestamps at stride of sf 0.5s yielding a 5s prediction horizon. unless otherwise stated we use target timestamps of tm 0.3s 0.6s . . . 2.4s i.e. sm 0.3s tm 8 for reading from the memory bank at inference. in the memory cross attention we use the nearest k 4 neighbors. following prior works 75 80 81 for any detection post-processing we use a 0.1 con\ufb01dence thresh- old per-class nms iou thresholds of 0.75 0.6 0.55 for vehicles pedestrians and cyclists respectively and a top k 500. mad has 3.8m parameters while the base detectors have anywhere from 8m centerpoint 75 to 53m bevmap 6 parameters. for each base detector we train mad for 60k update steps roughly equivalent to 6 epochs on wod and av2 with batch size 16. we use a cosine learning rate decay with a max learning rate of 8 10 4 and a linear warm-up for the \ufb01rst 1000 steps beginning with a learning rate of 8 10 5. during train- ing we use a variable set of memory target timestamps tm uniform 6 7 8 9 10 with a variable stride sm uniform 0.2s 0.3s 0.4s . metrics we report the detection metrics from the of\ufb01- cial wod leaderboard 57 which include average preci- sion ap and ap weighted by heading error aph for ve- hicles veh. pedestrians ped. and cyclists cyc. . these metrics use intersection-over-union iou thresholds of 0.7 0.5 and 0.5 respectively. the metrics are broken down into two levels of dif\ufb01culty level 1 l1 includes only labels that have 5 lidar points and are not marked as hard and level l2 includes all boxes that have 0 lidar points a superset of l1 . for camera experiments on av2 we report the mean average precision ap for vehicles in the camera \ufb01eld of view at an iou threshold of 0.1. we re- port the macro-average over all classes if the actor class is not speci\ufb01ed. augmenting off-the-shelf 3d detectors with mad tab. 1 and tab. 2 show the performance of mad ap- plied to off-the-shelf 3d detectors on wod and av2 re- spectively.to show the generality of our approach we ex- periment with multiple base detectors trained on different datasets and sensor modalities. we enhanced three lidar- based methods with mad on wod centerpoint 75 with both 1 lidar frame 1f and 2 lidar frames 2f as in- put hednet 81 1f and 4f and safdnet 80 1f and 4f . we follow their of\ufb01cial protocols to train and eval- uate all models from scratch due to the waymo dataset license agreement we cannot simply re-use pre-trained models . we also enhance two camera-based methods on av2 fcos3d 64 and bevmap 6 which takes the most recent image from the front camera as input. we use the of\ufb01cial implementation for both fcos3d 64 and bevmap 6 . training details are in the supplementary. our model brings signi\ufb01cant improvements to all detec- tors on both datasets. these gains are largest for single- frame detectors where the memory provides the most ad- ditional information. the fact that the mad-augmented single-frame detectors are better than the multi-frame detec- tors clearly shows the effectiveness of our method relative to the common point aggregation approach. please visit the supplementary materials for qualitative comparisons. comparison against sota by augmenting safdnet 4f with mad we show in tab. 3 that we achieve the best performance on the wod leaderboard2 among all online methods that do not use ensembles or test-time augmenta- tion. table 4 compares mad to prior learned temporal fu- sion methods on the wod validation and test set where we achieve substantial gains. please refer to our supplementary for full tabs. 3 and 4 with metrics for all actor classes. effect of memory proposals and memory attention we ablate the different components of our memory pipeline in tab. 5. comparing rows 1 2 and 5 shows that both the proposed memory attention and memory proposals have a positive effect. this is intuitive as the memory proposals let mad recover from false negative detection proposals which is complementary to memory cross-attention that al- lows mad to use all memory information for re\ufb01nement bypassing the \ufb01ltering in proposal-merging . effect of forecasting comparing rows 3 and 5 in tab. 5 we \ufb01nd that using trajectory forecasting to align memory proposals to the current time is important particularly for fast-moving objects. without forecasting the memory pro- posals from previous frames will be far from the current po- 2https waymo.com open challenges 2020 3d-detection as of submis- sion 14 11 2024 1455 page 8 mem. prop. mem. attn. forecast. rescore. veh. ap ped. ap cyc. ap veh. ap 20 30 m s cyc. ap 5 10 m s 0 75.4 80.1 79.6 38.0 72.4 1 76.5 81.3 82.0 40.4 82.1 2 75.8 81.7 81.6 37.7 75.9 3 76.9 81.8 81.9 34.6 78.7 4 72.7 82.0 81.2 11.9 73.9 5 77.0 82.3 83.3 45.2 86.2 table 5. component ablation of mad on the wod validation set. all metrics are l2. row 0 is the base 3d detector safdnet 4f 80 . all ablations in this table including the \ufb01nal method with all components use a reduced training duration of 45k iterations to reduce costs. evaluated proposals ap l1 aph l1 ap l2 aph l2 detection pdet 83.9 82.6 78.4 77.1 combined pmem pdet 18.1 17.8 16.8 16.4 merged pmerge 84.1 83.0 78.8 77.7 after block 0 pref 1 84.4 83.2 79.3 78.1 after block 1 pref 2 85.8 84.2 80.9 79.3 after block 2 pref 85.8 84.2 81.0 79.4 table 6. evaluating various intermediate proposals from mad. the base detector is safdnet 4f. sition of those objects making it challenging for the model to leverage the memory effectively. effect of learned proposal merging comparing rows 0 4 and 5 of tab. 5 we \ufb01nd the proposed learned rescoring of the merged detection and memory proposals is crucial for good performance. without it mad cannot enhance the base detector row 0 because the proposal scores from the 3d detector and memory are miss-calibrated before being post-processed in the proposal merging step i.e. nms . we illustrate this in tab. 6 where we evaluate intermedi- ate proposals of mad 1 the detection proposals pdet 2 naively taking the union of the detection proposals pdet and memory proposals pmem and post-processing them 3 the merged proposals pmerge which have been rescored and 4 after each block of the re\ufb01nement transformer pref 1 . . . pref i . naively concatenating the combined proposals is much worse than the base detector because of the miss-calibrated scores. after proposal merging pmerge already improves over the base detector. each re\ufb01nement block brings further gains illustrating the strength of our proposed re\ufb01nement transformer. training procedure study table 7 provides evidence supporting the effectiveness of our proposed training sched- ule. we \ufb01rst train mad with three different chunk sizes i.e. sequences with 144 48 1 frames each with and without the memory bank cache. training with long chunks 144 frames tab. 7.1a provides good performance because there is a low gap between training and evaluation. the cache provides no gains in this setting tab. 7.1b because the model already has memory proposals in most frames. training with shorter chunks tab. 7.2a 3a performs worse because there is a more signi\ufb01cant gap between training and evaluation. including the cache helps signi\ufb01cantly by chunk length cache ap l1 aph l1 ap l2 aph l2 1a 144 85.4 83.9 80.5 79.0 1b 144 85.2 83.7 80.3 78.9 2a 48 85.1 83.5 80.2 78.6 2b 48 85.3 83.8 80.3 78.9 3a 1 83.9 82.4 78.4 77.0 3b 1 85.0 83.3 80.1 78.5 4a 1 48 96 144 84.9 83.3 79.9 78.3 4b 1 48 96 144 85.8 84.2 81.0 79.4 table 7. ablating chunk length and the memory cache on wod using safdnet 4f. closing the gap to evaluation but does not fully reach the long chunk performance tab. 7.2b 3b . as hypothesized in sec. 3.2 there is room for improvement by training with our proposed schedule and memory bank cache tab. 7.4b . this strategy allows mad to learn generalized patterns over a diverse set of examples quickly by training on short chunks more i.i.d. data at the beginning when the learn- ing rate is higher while re\ufb01ning its understanding on long chunks closer to the deployment setting towards the end when the learning rate is lower. table 7.4a shows the impor- tance of the cache when using this training schedule other- wise training with small chunks is ineffective as the model would not learn to use the memory. 5 conclusion in this paper we propose mad a simple effective and sensor-modality-agnostic add-on for enhancing any existing 3d object detector with long-term memory. to achieve this we design a transformer-based model that uses joint detec- tion and trajectory forecasting to populate a memory bank with spatial-temporal object trajectories. our model can ef- fectively fuse memory proposals with detection proposals by reading previous memory entries and aligning them with the current time and ego pose. we also propose a novel training strategy that increases data diversity while keep- ing the training-to-inference gap low. our approach is very general bringing impressive improvements to a variety of lidar-based and camera-based detectors and very ef- fective achieving sota performance on waymo open dataset when paired to the base detector safdnet 4f 80 . 1456 page 9 references 1 holger caesar varun bankiti alex h lang sourabh vora venice erin liong qiang xu anush krishnan yu pan gi- ancarlo baldan and oscar beijbom. nuscenes a multi- modal dataset for autonomous driving. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 11621 11631 2020. 1 2 yigit baran can alexander liniger ozan unal danda paudel and luc van gool. understanding bird s-eye view of road semantics using an onboard camera. ieee robotics and automation letters 7 2 3302 3309 2022. 2 3 nicolas carion francisco massa gabriel synnaeve nicolas usunier alexander kirillov and sergey zagoruyko. end-to- end object detection with transformers. in european confer- ence on computer vision pages 213 229. springer 2020. 6 1 4 sergio casas ben agro jiageng mao thomas gilles alexander cui thomas li and raquel urtasun. detra a uni\ufb01ed model for object detection and trajectory forecasting. 2024. 2 4 6 5 5 yuning chai benjamin sapp mayank bansal and dragomir anguelov. multipath multiple probabilistic anchor tra- jectory hypotheses for behavior prediction. arxiv preprint arxiv 1910.05449 2019. 2 6 mincheol chang seokha moon reza mahjourian and jinkyu kim. bevmap map-aware bev modeling for 3d per- ception. in proceedings of the ieee cvf winter confer- ence on applications of computer vision pages 7419 7428 2024. 2 6 7 1 7 xiaozhi chen kaustav kundu yukun zhu andrew g berneshawi huimin ma sanja fidler and raquel urtasun. 3d object proposals for accurate object class detection. ad- vances in neural information processing systems 28 2015. 2 8 xuesong chen shaoshuai shi benjin zhu ka chun che- ung hang xu and hongsheng li. mppnet multi-frame feature intertwining with proxy points for 3d temporal ob- ject detection. in european conference on computer vision pages 680 697. springer 2022. 1 2 3 7 9 kyunghyun cho. learning phrase representations using rnn encoder-decoder for statistical machine translation. arxiv preprint arxiv 1406.1078 2014. 4 1 10 alexander cui sergio casas kelvin wong simon suo and raquel urtasun. gorela go relative for viewpoint-invariant motion forecasting. in 2023 ieee international confer- ence on robotics and automation icra pages 7801 7807. ieee 2023. 2 5 11 henggang cui vladan radosavljevic fang-chieh chou tsung-han lin thi nguyen tzu-kuo huang jeff schnei- der and nemanja djuric. multimodal trajectory predictions for autonomous driving using deep convolutional networks. in 2019 international conference on robotics and automation icra pages 2090 2096. ieee 2019. 2 12 martin engelcke dushyant rao dominic zeng wang chi hay tong and ingmar posner. vote3deep fast ob- ject detection in 3d point clouds using ef\ufb01cient convolutional neural networks. in 2017 ieee international conference on robotics and automation icra pages 1355 1361. ieee 2017. 2 13 lue fan yuxue yang feng wang naiyan wang and zhaoxiang zhang. super sparse 3d object detection. ieee transactions on pattern analysis and machine intelligence 45 10 12490 12505 2023. 6 3 14 davi frossard shun da suo sergio casas james tu and raquel urtasun. strobe streaming object detection from li- dar packets. in conference on robot learning pages 1174 1183. pmlr 2021. 1 2 15 chenhang he ruihuang li yabin zhang shuai li and lei zhang. msf motion-guided sequential fusion for ef\ufb01cient 3d object detection from point cloud sequences. in proceed- ings of the ieee cvf conference on computer vision and pattern recognition pages 5196 5205 2023. 1 2 3 5 6 7 16 kaiming he georgia gkioxari piotr doll ar and ross gir- shick. mask r-cnn. in proceedings of the ieee international conference on computer vision pages 2961 2969 2017. 2 17 tong he pei sun zhaoqi leng chenxi liu dragomir anguelov and mingxing tan. lef late-to-early tempo- ral fusion for lidar 3d object detection. in 2023 ieee rsj international conference on intelligent robots and systems iros pages 1637 1644. ieee 2023. 1 2 5 7 3 18 jinghua hou zhe liu zhikang zou xiaoqing ye xiang bai et al. query-based temporal fusion with explicit mo- tion for 3d object detection. advances in neural information processing systems 36 2024. 1 3 19 anthony hu zak murez nikhil mohan sof \u0131a dudas jef- frey hawke vijay badrinarayanan roberto cipolla and alex kendall. fiery future instance prediction in bird s- eye view from surround monocular cameras. in proceedings of the ieee cvf international conference on computer vi- sion pages 15273 15282 2021. 2 20 kuan-chih huang weijie lyu ming-hsuan yang and yi-hsuan tsai. ptt point-trajectory transformer for ef- \ufb01cient temporal 3d object detection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 14938 14947 2024. 1 2 3 5 7 21 jack kiefer and jacob wolfowitz. stochastic estimation of the maximum of a regression function. the annals of math- ematical statistics pages 462 466 1952. 5 22 junho koh junhyung lee youngwoo lee jaekyum kim and jun won choi. mgtanet encoding sequential lidar points using long short-term motion-guided temporal atten- tion for 3d object detection. in proceedings of the aaai con- ference on arti\ufb01cial intelligence pages 1179 1187 2023. 2 23 jason ku melissa mozi\ufb01an jungwook lee ali harakeh and steven l waslander. joint 3d proposal generation and object detection from view aggregation. in 2018 ieee rsj international conference on intelligent robots and systems iros pages 1 8. ieee 2018. 2 24 fabian kuppers jan kronenberger amirhossein shantia and anselm haselhoff. multivariate con\ufb01dence calibration for object detection. in proceedings of the ieee cvf confer- ence on computer vision and pattern recognition workshops pages 326 327 2020. 4 25 alex h lang sourabh vora holger caesar lubing zhou jiong yang and oscar beijbom. pointpillars fast encoders 1457 page 10 for object detection from point clouds. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 12697 12705 2019. 2 26 zhaoqi leng guowang li chenxi liu ekin dogus cubuk pei sun tong he dragomir anguelov and mingxing tan. lidar augment searching for scalable 3d lidar data augmen- tations. in 2023 ieee international conference on robotics and automation icra pages 7039 7045. ieee 2023. 6 27 bo li. 3d fully convolutional network for vehicle detection in point cloud. in 2017 ieee rsj international conference on intelligent robots and systems iros pages 1513 1518. ieee 2017. 2 28 hongyang li hao zhang zhaoyang zeng shilong liu feng li tianhe ren and lei zhang. dfa3d 3d deformable attention for 2d-to-3d feature lifting. in proceedings of the ieee cvf international conference on computer vision pages 6684 6693 2023. 2 29 xin li tao ma yuenan hou botian shi yuchen yang youquan liu xingjiao wu qin chen yikang li yu qiao et al. logonet towards accurate 3d object detection with local-to-global cross-modal fusion. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 17524 17534 2023. 6 30 yingwei li charles r qi yin zhou chenxi liu and dragomir anguelov. modar using motion forecasting for 3d object detection in point cloud sequences. in proceed- ings of the ieee cvf conference on computer vision and pattern recognition pages 9329 9339 2023. 1 2 5 7 3 31 zhiqi li wenhai wang hongyang li enze xie chong- hao sima tong lu yu qiao and jifeng dai. bevformer learning bird s-eye-view representation from multi-camera images via spatiotemporal transformers. in european con- ference on computer vision pages 1 18. springer 2022. 1 2 32 ming liang bin yang wenyuan zeng yun chen rui hu sergio casas and raquel urtasun. pnpnet end-to-end per- ception and prediction with tracking in the loop. in proceed- ings of the ieee cvf conference on computer vision and pattern recognition pages 11553 11562 2020. 1 2 33 xuewu lin tianwei lin zixiang pei lichao huang and zhizhong su. sparse4d multi-view 3d object detec- tion with sparse spatial-temporal fusion. arxiv preprint arxiv 2211.10581 2022. 2 34 yingfei liu tiancai wang xiangyu zhang and jian sun. petr position embedding transformation for multi-view 3d object detection. in european conference on computer vi- sion pages 531 548. springer 2022. 35 yingfei liu junjie yan fan jia shuailin li aqi gao tian- cai wang and xiangyu zhang. petrv2 a uni\ufb01ed framework for 3d perception from multi-camera images. in proceedings of the ieee cvf international conference on computer vi- sion pages 3262 3272 2023. 2 36 zhijian liu haotian tang alexander amini xinyu yang huizi mao daniela l rus and song han. bevfusion multi- task multi-sensor fusion with uni\ufb01ed bird s-eye view repre- sentation. in 2023 ieee international conference on robotics and automation icra pages 2774 2781. ieee 2023. 6 3 37 zhe liu jinghua hou xinyu wang xiaoqing ye jingdong wang hengshuang zhao and xiang bai. lion linear group rnn for 3d object detection in point clouds. arxiv preprint arxiv 2407.18232 2024. 6 3 38 zhe liu jinghua hou xiaoqing ye tong wang jingdong wang and xiang bai. seed a simple and effective 3d detr in point clouds. arxiv preprint arxiv 2407.10749 2024. 6 3 39 wenjie luo bin yang and raquel urtasun. fast and furi- ous real time end-to-end 3d detection tracking and motion forecasting with a single convolutional net. in proceedings of the ieee conference on computer vision and pattern recog- nition pages 3569 3577 2018. 1 40 matthias minderer josip djolonga rob romijnders frances hubis xiaohua zhai neil houlsby dustin tran and mario lucic. revisiting the calibration of modern neu- ral networks. advances in neural information processing systems 34 15682 15694 2021. 4 41 arsalan mousavian dragomir anguelov john flynn and jana kosecka. 3d bounding box estimation using deep learn- ing and geometry. in proceedings of the ieee conference on computer vision and pattern recognition pages 7074 7082 2017. 2 42 lukas neumann andrew zisserman and andrea vedaldi. relaxed softmax ef\ufb01cient con\ufb01dence auto-calibration for safe pedestrian detection. 2018. 4 43 jiquan ngiam benjamin caine wei han brandon yang yuning chai pei sun yin zhou xi yi ouais al- sharif patrick nguyen et al. starnet targeted compu- tation for object detection in point clouds. arxiv preprint arxiv 1908.11069 2019. 2 44 jiquan ngiam benjamin caine vijay vasudevan zheng- dong zhang hao-tien lewis chiang jeffrey ling rebecca roelofs alex bewley chenxi liu ashish venugopal et al. scene transformer a uni\ufb01ed architecture for predicting mul- tiple agent trajectories. arxiv preprint arxiv 2106.08417 2021. 4 2 45 tung phan-minh elena corina grigore freddy a boulton oscar beijbom and eric m wolff. covernet multimodal behavior prediction using trajectory sets. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 14074 14083 2020. 2 46 aj piergiovanni vincent casser michael s ryoo and anelia angelova. 4d-net for learned multi-modal alignment. in proceedings of the ieee cvf international conference on computer vision pages 15435 15445 2021. 1 2 47 rui qian divyansh garg yan wang yurong you serge belongie bharath hariharan mark campbell kilian q weinberger and wei-lun chao. end-to-end pseudo-lidar for image-based 3d object detection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 5881 5890 2020. 2 48 cody reading ali harakeh julia chae and steven l waslander. categorical depth distribution network for monocular 3d object detection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 8555 8564 2021. 2 1458 page 11 49 shaoqing ren. faster r-cnn towards real-time object detection with region proposal networks. arxiv preprint arxiv 1506.01497 2015. 2 50 herbert robbins and sutton monro. a stochastic approxi- mation method. the annals of mathematical statistics pages 400 407 1951. 5 51 thomas roddick alex kendall and roberto cipolla. ortho- graphic feature transform for monocular 3d object detection. arxiv preprint arxiv 1811.08188 2018. 2 52 avishkar saha oscar mendez chris russell and richard bowden. translating images into maps. in 2022 interna- tional conference on robotics and automation icra pages 9200 9206. ieee 2022. 2 53 shaoshuai shi xiaogang wang and hongsheng li. pointr- cnn 3d object proposal generation and detection from point cloud. in proceedings of the ieee cvf conference on com- puter vision and pattern recognition pages 770 779 2019. 2 54 weijing shi and raj rajkumar. point-gnn graph neural net- work for 3d object detection in a point cloud. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 1711 1719 2020. 2 55 martin simony stefan milzy karl amendey and horst- michael gross. complex-yolo an euler-region-proposal for real-time 3d object detection on point clouds. in proceed- ings of the european conference on computer vision eccv workshops pages 0 0 2018. 2 56 shuran song and jianxiong xiao. deep sliding shapes for amodal 3d object detection in rgb-d images. in proceed- ings of the ieee conference on computer vision and pattern recognition pages 808 816 2016. 2 57 pei sun henrik kretzschmar xerxes dotiwalla aurelien chouard vijaysai patnaik paul tsui james guo yin zhou yuning chai benjamin caine et al. scalability in perception for autonomous driving waymo open dataset. in cvpr 2020. 2 3 5 6 7 58 pei sun mingxing tan weiyue wang chenxi liu fei xia zhaoqi leng and dragomir anguelov. swformer sparse window transformer for 3d object detection in point clouds. in european conference on computer vision pages 426 442. springer 2022. 1 59 balakrishnan varadarajan ahmed hefny avikalp srivas- tava khaled s refaat nigamaa nayakanti andre cornman kan chen bertrand douillard chi pang lam dragomir anguelov et al. multipath ef\ufb01cient information fu- sion and trajectory aggregation for behavior prediction. in 2022 international conference on robotics and automation icra pages 7814 7821. ieee 2022. 2 60 a vaswani. attention is all you need. advances in neural information processing systems 2017. 4 61 dominic zeng wang and ingmar posner. voting for voting in online point cloud object detection. in robotics science and systems pages 10 15. rome italy 2015. 2 62 haiyang wang chen shi shaoshuai shi meng lei sen wang di he bernt schiele and liwei wang. dsvt dy- namic sparse voxel transformer with rotated sets. in pro- ceedings of the ieee cvf conference on computer vision and pattern recognition pages 13520 13529 2023. 1 63 shihao wang yingfei liu tiancai wang ying li and xi- angyu zhang. exploring object-centric temporal modeling for ef\ufb01cient multi-view 3d object detection. in proceedings of the ieee cvf international conference on computer vi- sion pages 3621 3631 2023. 3 64 tai wang xinge zhu jiangmiao pang and dahua lin. fcos3d fully convolutional one-stage monocular 3d object detection. in proceedings of the ieee cvf international conference on computer vision pages 913 922 2021. 2 6 7 1 65 yan wang wei-lun chao divyansh garg bharath hari- haran mark campbell and kilian q weinberger. pseudo- lidar from visual depth estimation bridging the gap in 3d object detection for autonomous driving. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 8445 8453 2019. 2 66 yue wang alireza fathi abhijit kundu david a ross caroline pantofaru tom funkhouser and justin solomon. pillar-based object detection for autonomous driving. in computer vision eccv 2020 16th european conference glasgow uk august 23 28 2020 proceedings part xxii 16 pages 18 34. springer 2020. 2 67 yue wang vitor campagnolo guizilini tianyuan zhang yilun wang hang zhao and justin solomon. detr3d 3d object detection from multi-view images via 3d-to-2d queries. in conference on robot learning pages 180 191. pmlr 2022. 2 68 benjamin wilson william qi tanmay agarwal john lambert jagjeet singh siddhesh khandelwal bowen pan ratnesh kumar andrew hartnett jhony kaesemodel pontes et al. argoverse 2 next generation datasets for self-driving perception and forecasting. arxiv preprint arxiv 2301.00493 2023. 2 3 5 6 69 xiaoyang wu li jiang peng-shuai wang zhijian liu xi- hui liu yu qiao wanli ouyang tong he and hengshuang zhao. point transformer v3 simpler faster stronger. in pro- ceedings of the ieee cvf conference on computer vision and pattern recognition pages 4840 4851 2024. 2 70 bin xu and zhenzhong chen. multi-level fusion based 3d object detection from monocular images. in proceedings of the ieee conference on computer vision and pattern recog- nition pages 2345 2353 2018. 2 71 yan yan yuxing mao and bo li. second sparsely embed- ded convolutional detection. sensors 18 10 3337 2018. 2 1 72 bin yang wenjie luo and raquel urtasun. pixor real- time 3d object detection from point clouds. in proceedings of the ieee conference on computer vision and pattern recog- nition pages 7652 7660 2018. 2 73 zetong yang yanan sun shu liu and jiaya jia. 3dssd point-based 3d single stage object detector. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 11040 11048 2020. 2 74 zetong yang yin zhou zhifeng chen and jiquan ngiam. 3d-man 3d multi-frame attention network for object de- tection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 1863 1872 2021. 1 3 7 1459 page 12 75 tianwei yin xingyi zhou and philipp krahenbuhl. center- based 3d object detection and tracking. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 11784 11793 2021. 1 2 6 7 76 yurong you yan wang wei-lun chao divyansh garg geoff pleiss bharath hariharan mark campbell and kil- ian q weinberger. pseudo-lidar accurate depth for 3d object detection in autonomous driving. arxiv preprint arxiv 1906.06310 2019. 2 77 yurong you katie z luo xiangyu chen junan chen wei- lun chao wen sun bharath hariharan mark campbell and kilian q weinberger. hindsight is 20 20 leverag- ing past traversals to aid 3d perception. arxiv preprint arxiv 2203.11405 2022. 2 78 yihan zeng da zhang chunwei wang zhenwei miao ting liu xin zhan dayang hao and chao ma. lift learning 4d lidar image fusion transformer for 3d object detection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 17172 17181 2022. 1 2 79 ce zhang chengjie zhang yiluan guo lingji chen and michael happold. motiontrack end-to-end transformer- based multi-object tracking with lidar-camera fusion. in pro- ceedings of the ieee cvf conference on computer vision and pattern recognition pages 151 160 2023. 2 80 gang zhang junnan chen guohuan gao jianmin li si liu and xiaolin hu. safdnet a simple and effective net- work for fully sparse 3d object detection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 14477 14486 2024. 2 6 7 8 1 81 gang zhang chen junnan guohuan gao jianmin li and xiaolin hu. hednet a hierarchical encoder-decoder net- work for 3d object detection in point clouds. advances in neural information processing systems 36 2024. 2 6 7 1 82 zixiang zhou xiangchen zhao yu wang panqu wang and hassan foroosh. centerformer center-based transformer for 3d object detection. in european conference on com- puter vision pages 496 513. springer 2022. 2 3 6 1460",
      "keywords": [
        "page",
        "mad",
        "memory-augmented",
        "detection",
        "objects",
        "ben",
        "agro",
        "sergio",
        "casas",
        "patrick",
        "wang",
        "thomas",
        "gilles",
        "raquel",
        "urtasun",
        "tgilles",
        "waabi.ai",
        "detector",
        "object",
        "memory",
        "augmented",
        "detections",
        "0.0s",
        "-2.4s",
        "age",
        "false",
        "negative",
        "figure",
        "detectors",
        "without",
        "long-term",
        "temporal",
        "fusion",
        "miss",
        "heavily",
        "occluded",
        "objects.",
        "approach",
        "enhances",
        "remember",
        "past",
        "predictions",
        "recovering",
        "from",
        "occlusion.",
        "green",
        "labels",
        "black",
        "lidar",
        "points",
        "gray.",
        "abstract",
        "perceive",
        "humans",
        "use",
        "\ufb01ll",
        "gaps",
        "caused",
        "limited",
        "visibility",
        "whether",
        "due",
        "occlusion",
        "nar-",
        "row",
        "\ufb01eld",
        "view.",
        "however",
        "most",
        "using",
        "sensor",
        "evidence",
        "short",
        "win-",
        "dow",
        "0.1s-0.3s",
        "work",
        "present",
        "simple",
        "ef-",
        "fective",
        "add-on",
        "enhancing",
        "any",
        "existing",
        "detec-",
        "tor",
        "regardless",
        "modality",
        "e.g.",
        "camera",
        "network",
        "architecture.",
        "pro-",
        "pose",
        "model",
        "effectively",
        "align",
        "fuse",
        "proposals"
      ],
      "metadata": {
        "filename": "Agro_MAD_Memory-Augmented_Detection_of_3D_Objects_CVPR_2025_paper.pdf",
        "original_filename": "Agro_MAD_Memory-Augmented_Detection_of_3D_Objects_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 2862588,
        "upload_file_id": "upload_1752159938304_2s2dni0zh",
        "upload_timestamp": "2025-07-10T15:05:38.343768",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:05:38.709353+00:00",
      "content_length": 61240,
      "content_type": ".pdf",
      "filename": "Agro_MAD_Memory-Augmented_Detection_of_3D_Objects_CVPR_2025_paper.pdf"
    },
    "7": {
      "file_id": "7",
      "content": "\n=== Page 1 ===\nEvaluating Vision-Language Models as Evaluators in Path Planning\nMohamed Aghzal1, Xiang Yue2, Erion Plaku3*, Ziyu Yao1\n1George Mason University, 2Carnegie Mellon University, 3National Science Foundation\n{maghzal, ziyuyao}@gmu.edu, xyue2@andrew.cmu.edu, eplaku@nsf.gov\nAbstract\nDespite their promise to perform complex reasoning,\nlarge language models (LLMs) have been shown to have\nlimited effectiveness in end-to-end planning. This has in-\nspired an intriguing question: if these models cannot plan\nwell, can they still contribute to the planning framework as\na helpful plan evaluator? In this work, we generalize this\nquestion to consider LLMs augmented with visual under-\nstanding, i.e., Vision-Language Models (VLMs). We intro-\nduce PATHEVAL, a novel benchmark evaluating VLMs as\nplan evaluators in complex path-planning scenarios. Suc-\nceeding in the benchmark requires a VLM to be able to ab-\nstract traits of optimal paths from the scenario description,\ndemonstrate precise low-level perception on each path, and\nintegrate this information to decide the better path. Our\nanalysis of state-of-the-art VLMs reveals that these mod-\nels face signi\ufb01cant challenges on the benchmark. We ob-\nserve that the VLMs can precisely abstract given scenar-\nios to identify the desired traits and exhibit mixed perfor-\nmance in integrating the provided information. Yet, their vi-\nsion component presents a critical bottleneck, with models\nstruggling to perceive low-level details about a path. Our\nexperimental results show that this issue cannot be trivially\naddressed via end-to-end \ufb01ne-tuning; rather, task-speci\ufb01c\ndiscriminative adaptation of these vision encoders is needed\nfor these VLMs to become effective path evaluators.12\n1. Introduction\nThe impressive capabilities of Large Language Models\n(LLMs) [6, 8, 71] and Vision-Language Models (VLMs)\n[82] have led to an increasing interest in applying them to\nautomated motion planning and navigation tasks [50, 51].\nHowever, the inherent limitations of these models in long-\n*The work by E. Plaku is supported by (while serving at) the National\nScience Foundation. Any opinion, \ufb01ndings, and conclusions or recom-\nmendations expressed in this material are those of the authors and do not\nnecessarily re\ufb02ect the views of the National Science Foundation.\n1Source code: https://github.com/MohamedAghzal/PathEval\n2Dataset: https://huggingface.co/datasets/maghzal/PathEval\nhorizon planning have rendered them ineffective as end-\nto-end motion planners [2, 3, 11, 75]. This has made re-\nsearchers wonder: if these models cannot be good motion\nplanners themselves, can they still support a motion plan-\nning framework? Intuitively, using these models still holds\nthe promise of signi\ufb01cantly enhancing the motion plan-\nning framework, as they have learned extensive factual and\ncommonsense knowledge that could bene\ufb01t planning dur-\ning their pre-training. As a result, there has been an emerg-\ning paradigm exploring how these models can be leveraged\nin combination with traditional methods [27].\nOne particularly interesting approach within this line of\nwork involves using these models as plan evaluators. Mo-\ntivated by the intuition that \u201cevaluation is easier than gen-\neration\u201d [31], several efforts have explored leveraging these\nmodels as \u201ccritics\u201d to assess the quality of generated plans\n[5, 21, 73, 83]. However, most of these efforts have focused\non scenarios that require only limited, high-level visual per-\nception, without necessitating \ufb01ne-grained or precise per-\nceptual abilities. On the other hand, while there have been\nprior works similarly investigating VLMs\u2019 low-level per-\nception [26, 64], studies speci\ufb01cally about the use of these\nmodels in planning remain limited. Tasks such as motion\nplanning often require \ufb01ne-grained visual understanding in\nhighly speci\ufb01c contexts, while also drawing on broad com-\nmonsense knowledge acquired during pre-training. Hence,\nthere is a pressing need to investigate the potential of VLMs\nto understand both low-level visual details and leverage\nthese visual signals for high-level reasoning.\nIn this work, we explore whether we can utilize VLMs\nas evaluators in highly intricate continuous path planning\nproblems. We introduce PATHEVAL (Figure 1), a control-\nlable benchmark designed around path planning in complex\nenvironments under diverse commonsense decision-making\nscenarios (e.g., A \ufb01re\ufb01ghting robot has to cover as much of\nthe area as possible to extinguish \ufb01res and scout for sur-\nvivors, thus needs to prioritize paths with more coverage\nand higher clearance). Traditionally, encoding constraints\nin a planning algorithm requires signi\ufb01cant human effort in\nhand-crafting scenario-speci\ufb01c criteria for path evaluation,\nsuch that the constraints can be injected into the planner\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n6886\n\n=== Page 2 ===\nExample of an instance of PATHEVAL\n2D\n3D\nPath Descriptors:\nPath 1: Min. clearance: 1.07, Max. clearance: 8.91, Avg. clearance: 2.62, Path length: 243.46, Smoothness: 843.07, Number of sharp turns: 4,\nMaximum angle: 124.35.\nPath 2: Min. clearance: 1.57, Max. clearance: 8.91, Avg. clearance: 3.18, Path length: 243.75, Smoothness: 809.65, Number of sharp turns: 1,\nMaximum angle: 93.12.\nScenario #1: The agent navigating this maze is a large truck, so sharp turns (90 degrees or larger) are harder to make. It should also stay on a\nstraight line (unless it is making a turn) as it is driving through heavy traf\ufb01c.\nRequired Descriptors: Smoothness (\u2192), Number of Sharp turns (\u2192), Maximum angle (\u2192)\nGround Truth: Path 2\nScenario #2: An autonomous \ufb01re\ufb01ghting robot is designed to navigate and operate within burning buildings to extinguish \ufb01res and rescue trapped\nindividuals. It should explore as much of the area as possible, while maintaining a safe distance from the walls to avoid damage.\nRequired Descriptors: Avg. Clearance (\u2191), Min. Clearance (\u2191), Path length (\u2191)\nGround Truth: Path 1\nScenario #3: As the vehicle is traversing a warzone, it must stay concealed from enemy operatives, making use of covers like walls and avoiding\nopen spaces as much as possible. It should also reach its target as quickly as possible.\nRequired Descriptors: Avg. Clearance (\u2192), Max. Clearance (\u2192), Path length (\u2192)\nGround Truth: Path 2\nPrompt\nThe agent navigating this maze is a large truck, so sharp turns (90 degrees or larger) are harder to make. It should also stay on a straight line\n(unless it is making a turn) as it is driving through heavy traf\ufb01c. Which path better achieves the task? Path 1 is on the left side and Path 2 is on\nthe right side. Your answer should follow the format below:\nAnswer: Path 1 or Path 2.\nExplanation: Why you chose the path (1 or 2).\nGPT-4o\nAnswer: Path 1.\nExplanation: Path 1 has fewer sharp turns and maintains longer straight segments compared to Path 2. This is more suitable for a large truck\nnavigating through heavy traf\ufb01c, as it minimizes the need for dif\ufb01cult maneuvers and allows for smoother driving. Path 2, on the other hand, has\nmore frequent and sharper turns, which would be more challenging for a large truck to navigate.\nLLaVA-NeXT-7b\nAnswer: Path 1.\nExplanation: Path 1 is better suited for a large truck because it has fewer sharp turns (90 degrees or larger) compared to Path 2. The truck\nwould have to make only one 90-degree turn, which is easier to execute than the two 90-degree turns in Path 2. Additionally, Path 1 has more\nstraight sections, which would be more ef\ufb01cient for a large truck to navigate through heavy traf\ufb01c. The fewer turns and the greater number of\nstraight sections make Path 1 the more suitable route for a large truck.\nFigure 1. Example of PATHEVAL. The benchmark consists of 14,550 instances of path pairs rendered in both 2D and 3D and mapped to 15\ndecision-making scenarios. Success on this task is tied to three distinct levels: 1) Attribute abstraction: recognizing what aspects make\na path ideal, 2) Low-level perception: extracting the required attributes for each path from the images, and 2) Information integration:\nsynthesizing the collected information to make a decision. We test a set of VLMs on the task and \ufb01nd that they struggle particularly with\nlow-level perception. Incorrect answers by different models are shown (explanations indicating misperception are highlighted).\nduring the search process [36, 52, 54]. However, this not\nonly is a tedious and costly process but also cannot scale up\nto handle the countless intricate scenarios in meeting real-\nlife planning needs. An effective VLM as a \u201creward model\u201d\nrelieving humans from customizing evaluation criteria for\nspeci\ufb01c scenarios could enable scalable, general-purpose\nplanners that can adapt to diverse commonsense scenarios\ndescribed in natural language.\n6887\n\n=== Page 3 ===\nIn PATHEVAL, a VLM is tasked with comparing two\npaths within a given decision-making context and select-\ning the one that better satis\ufb01es the constraints outlined by\nthe scenario. Success on this task requires effective per-\nformance across three distinct levels: 1) attribute abstrac-\ntion: recognizing the attributes that de\ufb01ne a favorable path\nin a particular decision-making context; 2) low-level per-\nception: demonstrating precise low-level perception to de-\ntermine which path performs better based on the given cri-\nteria; and 3) information integration: integrating and syn-\nthesizing the perceived information to produce an answer.\nUsing this benchmark, we analyze the performance of 9\nstate-of-the-art (SOTA) VLMs, including both the closed-\nsource GPT-4o and GPT-4o-mini [47] and 7 different open-\nsource VLMs of different sizes (i.e., LLaVA-NeXT 7b and\n13b [39], Qwen2-VL-7b [69], LLaVA-OneVision-7b [32],\nLLaMA-3.2-11b [17], and Intern-VL2 8b and 40b [12]).\nWe \ufb01nd that these models struggle with the path evalua-\ntion task (e.g., Qwen2-VL-7b achieving only 50.2% accu-\nracy). However, when providing these VLMs with verbal-\nized path speci\ufb01cations, their performance signi\ufb01cantly im-\nproves (e.g., 74.2% accuracy for Qwen2-VL-7b), which re-\nveals a potential vision bottleneck of these VLMs. Our fur-\nther analysis con\ufb01rms these models\u2019 weakness in low-level\nperception, especially when they are tasked to perceive the\nclearance of a path with respect to surrounding obstacles,\nand this weakness could be more prominent when the envi-\nronment and the path representation become more complex.\nWe discover the source of this weakness from the vision\nencoders used by these VLMs, yet simply \ufb01ne-tuning the\nVLMs end-to-end with the vision encoders does not address\nthe issue. Rather, our experiments suggest performing task-\nspeci\ufb01c discriminative adaptation of these vision encoders.\n2. Related Work\n2.1. Vision-Language Models\nThe outstanding success of decoder-only LLMs [6, 48, 65]\nhas driven the development of Vision-Language Models\n(VLMs), which extend LLMs with a vision component in an\nattempt to generalize their performance into a multi-modal\nsetting [12, 17, 32, 37, 38, 62, 69]. VLMs are designed for\ntasks that require unifying visual and linguistic representa-\ntions (e.g. visual question answering [4]). Typically, this is\nachieved by connecting a vision encoder to a language de-\ncoder and projecting the representations into a shared space,\nallowing visual elements and linguistic components to be\nlinked in a semantically meaningful manner. One of the\nearliest successful models at achieving this was the CLIP\nencoder [55], which was trained using contrastive learning\nto learn representations that map images and their corre-\nsponding textual descriptions.\nSeveral varieties of CLIP\nhave then been introduced [25, 56, 76, 80]. These mod-\nels, while showing tremendous promises have shown sev-\neral limitations when it comes to visual tasks [57]; as such\nseveral works [64, 72] have sought ways to improve such\nrepresentations by combining them with vision foundation\nmodels such as DINO [8, 49]. In this work, we contribute\nto the research of VLMs with a new benchmark, namely\nPATHEVAL, focusing on evaluating VLMs as evaluators for\npath planning. This benchmark tests VLMs seamlessly on\ntheir commonsense understanding (i.e., being able to ab-\nstract critical concepts expressed in the described planning\nscenarios), low-level perception (i.e., precisely perceiving\ndetails about paths in complex environments), and the abil-\nity to reason about the collected textual and visual informa-\ntion for decision-making.\n2.2. Automated Planning with VLMs\nVision-language planning promises more \ufb02exible planning\nframeworks and enhanced human-AI interaction. There-\nfore, designing systems that can effectively understand nat-\nural language instruction and leverage perceptual inputs to\nconduct planning tasks has been a topic of interest in re-\ncent years [20, 61, 70]. The rise of VLMs has led many\nto investigate the use of these models as vision-language\nplanning agents [15, 16, 50, 51]. However, existing litera-\nture highlights the limitations of LLMs in spatial reasoning\n[2, 26, 74] as well as long-horizon planning [3, 67, 68]. This\npushed researchers to explore alternative ways to incorpo-\nrate VLMs and LLMs into planning frameworks more reli-\nably [27, 58]. One potential direction is their use as plan\nevaluators, either through the generation of reward func-\ntions [23, 33, 73], or by using them directly as off-the-shelf\ncritics [5, 21, 83]. The success of such frameworks assumes\nperfect perception and that the models can accurately per-\nceive visual information and reason about it in order to pro-\nduce an answer; nevertheless, it has been shown that the\nrepresentations used by these models fail in highly intricate\nvisual settings [64]. Several works have explored the use\nof VLMs as well as LLMs for path and motion planning\n[2, 3, 7, 9, 11, 14, 44, 75], however, to the knowledge of the\nauthors there is no work that explores the use of VLMs as\npath critics in this context. Accordingly, we aim to evaluate\nthe ability of VLMs to serve as evaluators in cases requir-\ning navigation in complex environments while adhering to\ndecision-making constraints speci\ufb01ed in natural language.\n2.3. Vision-Language Model Benchmarks\nThe introduction of multimodal models has prompted the\ndevelopment of several benchmarks that are capable of as-\nsessing the performance of these models on visual rea-\nsoning tasks such as visual question-answering datasets\n[4, 18, 19, 24, 42, 43, 84]. However, the rise of founda-\ntion models has produced the need for a more holistic eval-\nuation of the perceptual and reasoning capabilities of large\n6888\n\n=== Page 4 ===\nVLMs, leading to benchmarks such as MM-Vet [77], MM-\nBench [40] , MMMU [45, 78, 79, 81] and OmniBench [35].\nSeveral benchmarks have also speci\ufb01cally been designed to\nassess the perception capabilities of these models and ex-\nplore the limitations associated with visual hallucinations\nand optical illusions [10, 22, 34, 60, 64]. Our proposed\nbenchmark provides a \ufb02exible yet challenging framework\nfor interleavedly assessing the low-level perception and rea-\nsoning capabilities of VLMs.\n3. The PATHEVAL Benchmark\nMotivated by the need to evaluate VLMs as path evalua-\ntors in real-world planning scenarios, we introduce PATHE-\nVAL, a controllable and extensible benchmark focused on\npath planning [28\u201330, 41] in complex environments under\na diverse set of decision-making constraints. We list the\ndecision-making scenarios as well as the descriptors they\nattempt to optimize in Appendix A. In total, PATHEVAL\nincludes 14,550 tasks over more than 1,150 distinct envi-\nronments and 15 distinct scenarios. Below, we introduce a\nformal description of the task and the dataset construction.\n3.1. Task Formulation\nGiven two paths, P1 and P2, and a scenario S, the objective\nis to determine which path better satis\ufb01es the scenario\u2019s op-\ntimization criteria. Each scenario S is a high-level descrip-\ntion that aims to optimize over a set of path descriptors (or\nmetrics) M = {m1, m2, . . . , mk}, where each descriptor\nmj : P \u2192R evaluates a speci\ufb01c property of a path (e.g.,\nlength, smoothness, or proximity to obstacles). A VLM V\nis presented with two images presenting P1 and P2 in the\nsame environment, respectively. The model must then de-\ncide which path better satis\ufb01es the scenario\u2019s criteria. To\nexplore the sensitivity of VLMs to the way how a path is\npresented, PATHEVAL includes both the 2D and 3D images\nof the path illustration. The model is also prompted to gen-\nerate an explanation to justify its choice.\n3.2. Environment and Path Generation\nEnvironment Generation:\nAn environment, as shown\nin Figure 1,\nis de\ufb01ned by a set of walls O\n=\n{O1, O2, . . . , On}, where each wall Oi represents an ob-\nstacle in the 2D space. Each wall is a closed geometric\nshape described by its vertices, and the set O forms the ob-\nstacles that the path must avoid. In this work, we leverage\nthe environments of Plaku et. al. [53], which consists of\nfour types of obstacle arrangements: (1) rings, where the\nenvironments are structured as mazes with circular walls,\n(2) waves, which consist of wavy horizontal obstacles, (3)\nmazes, which consist of both vertical and horizontal walls\nforming a complex maze structure, as well as (4) random,\nwhich consist of randomly placed obstacles.\nPath Synthesis via the Randomly-exploring Rapid\nTree (RRT) algorithm:\nTo generate path candidates in\nPATHEVAL, we leverage the RRT path planning algo-\nrithm [30]. Starting from the initial location in the envi-\nronment, the algorithm works by building a tree which ex-\npands itself by randomly selecting the next location in the\nenvironment while avoiding obstacles, until it reaches the\ngoal. In this work, we use the Open Motion Planning Li-\nbrary (OMPL) [59] and implement the RRT-Connect algo-\nrithm [29]. We note that while we use RRT in the current\nbenchmark, our codebase is adaptable and can incorporate\nmost path-planning algorithms provided by OMPL. We en-\ncourage future research building on our benchmark to ex-\nperiment with other algorithms as well.\n3.3. Path Descriptors\nWe collect the following descriptors M for each of a gen-\nerated path: Minimum Clearance measures the smallest\ndistance between any point on the path and the nearest ob-\nstacle; Maximum Clearance measures the largest distance\nbetween any point on the path and the nearest obstacle; Av-\nerage Clearance computes the average distance between all\npoints on the path and the nearest obstacle; Path Length is\ncalculated by summing up the Euclidean distances between\nconsecutive points on the path; Smoothness is de\ufb01ned as\nthe sum of the angles between consecutive segments of the\npath, measuring how smoothly the path changes direction;\nNumber of Sharp Turns counts the number of turns in\nthe path where the angle between consecutive segments ex-\nceeds 90 degrees; and Maximum Angle denotes the largest\nangle between any two consecutive segments of the path.\nThe three Clearance metrics and Path Length share the same\nmeasuring unit, i.e., one grid size; Smoothness and Maxi-\nmum Angle are measured by degree; and the Number of\nSharp Turns is an integer count. We include the formula of\neach descriptor in Appendix B.\n3.4. Natural Language Descriptions of Scenarios\nTo create a suf\ufb01ciently challenging path-planning evalua-\ntion benchmark, we design a total of 15 decision-making\nscenarios that aim to optimize different combinations of the\npath descriptors. For instance, Scenario #2 (as shown in\nFigure 1) requires searching through an area affected with\n\ufb01re in search for survivors, and the agent thus must cover as\nmuch ground as possible. In contrast, Scenario #3 indicates\nthat the path is to be executed within a warzone; as a result,\nthe vehicle has to remain hidden and take the shortest route.\nAs such, given the same set of paths, the one minimizing\nthe path length is favored by Scenario #3 while Scenario #2\nneeds to maximize this value. A complete summary of the\n15 scenarios, along with the descriptors each scenario aims\nto optimize, is presented in Tables 6-7 in Appendix A.\n6889\n\n=== Page 5 ===\n3.5. Task Pairs Selection\nFor each environment we synthesized, we ran the RRT plan-\nner 30 times to generate different paths. Upon eliminating\npaths that did not reach the goal, we selected path pairs that\nexhibited the greatest dissimilarity in terms of path descrip-\ntors. Speci\ufb01cally, we \ufb01rst represented each path using a (7-\ndimension) vector of its path descriptor values. Given that\neach path descriptor ranged in a dramatically different scale,\nwe normalized the vector by performing the min-max scal-\ning, i.e., scaling each value x in the vector to x\u2192=\nx\u2191xmin\nxmax\u2191xmin ,\nwhere x represents each value in the vector, and xmin and\nxmax are the minimum and maximum values of each de-\nscriptor across paths sampled from the same environment,\nrespectively. We then measured the distance between two\npaths by calculating the Euclidean distance between their\nnormalized path descriptor vectors, and selected 5 path pairs\nwith the largest distances. Each path is included in only one\npair to avoid redundancy. We repeated the same selection\nprocedure for a total of 1,150 environments.\nUpon performing this generation, we matched each pair\nwith all \ufb01fteen scenarios; however, we only kept pairs where\nthere was a signi\ufb01cant difference in at least one of the\ndescriptors required for the particular scenario.\nIn other\nwords, we ensure that the absolute difference is greater than\na prede\ufb01ned threshold (0.8 for clearance descriptors, 50 for\npath length, 90 for smoothness, 1 for the number of sharp\nturns, and 30 for maximum angle) for at least one of the\nrequired descriptors. This makes it more likely that the dif-\nference is more noticeable to the naked eye, and thus the\npaths can be compared by visual inspection.\nWe constructed our \ufb01nal evaluation benchmark by ran-\ndomly selecting 70 task pairs from each scenario, resulting\nin 1,050 pairs in total. The remaining task pairs (a total\nof 13,500) are used as the training set to facilitate the \ufb01ne-\ntuning experiments in Section 5.\n4. Can SOTA VLMs Evaluate Planned Paths?\nIn order for VLMs to perform successfully in our bench-\nmark, they need to perform successfully at three differ-\nent levels, i.e, recognizing the critical descriptors required\nby each scenario (Attribute Abstraction), exhibiting sharp\nlow-level perception to precisely assess each path\u2019s proper-\nties (Low-level Perception), and integrating the prior in-\nformation to make a rational decision on the better path\n(Information Integration). Among them, the \ufb01rst two lev-\nels re\ufb02ect parallel properties that serve as a foundation for\nthe third level. In this section, we evaluated a set of 9 VLMs\non PATHEVAL in a zero-shot manner and analyzed their ca-\npabilities at these three levels. These VLMs include (1) two\nclosed-source VLMs, i.e., GPT-4o and GPT-4o-mini [47],\nand (2) seven open-source VLMs with various sizes, in-\ncluding LLaVA-NeXT-7b and LLaVA-NeXT-13b [38, 39],\nLLaVA-OneVision-7b [32], Qwen2-VL-7b [69], LLaMA-\n3.2-11b [17], and Intern-VL2-8b and Intern-VL2-40b [12].\nWe include all prompt scripts used in this Section in Ap-\npendix C.1.\n4.1. Overview of VLMs on PATHEVAL\nThe performance of the 9 VLMs on PATHEVAL is show-\ncased in Table 1.\nWe notice that all of the models, ex-\ncept GPT-4o, fail to perform signi\ufb01cantly better than a sim-\nple random baseline, indicating signi\ufb01cant limitations. For\nGPT-4o, we also notice a 4% higher accuracy on 2D im-\nages compared to prompting with 3D inputs. This obser-\nvation indicates that the model is prone to visual illusions\nintroduced by the 3D images, when it has to rely on solely\nthe image for decision-making (although we observe an op-\nposite effect of 2D vs. 3D when verbalized descriptor val-\nues are provided). In the remaining section, we will further\nbreak down these models\u2019 capabilities to gain a deeper un-\nderstanding of their failure on this task.\nProviding verbalized path information yields better task\naccuracy, implying reasonable VLM performance in at-\ntribute abstraction. In Table 1, we further show the per-\nformance of each model when we explicitly list the value\nfor each descriptor as part of the language prompt (i.e.,\n\u201cPATHEVAL w Desc.\u201d). We notice a 11.1%-27.1% improve-\nment across most models, indicating that when given low-\nlevel details, the models can better \ufb01lter out the information\nand make better comparisons. This points out that the bot-\ntleneck for these VLMs\u2019s better performance lies in their\ninability to accurately perceive low-level information about\nthe paths (we discuss this in more detail in Section 4.2),\nwhereas these models generally have a reasonable capabil-\nity in abstracting the critical attributes for decision making\nin various scenarios. In Appendix D.1, we include an ex-\nperiment where we explicitly query each VLM to identify\nthe critical path metrics for each scenario; the result corrob-\norates our hypothesis. In particular, we \ufb01nd that for most\nmodels, the success rate for identifying a required descrip-\ntor is over 92%. Finally, a surprising observation happens\nto LLaVa-NeXT-7b. We notice that this model suffers par-\nticularly severely from hallucination; even when the textual\ndescriptor values are provided and when the model can cor-\nrectly identify critical path metrics based on our analysis in\nAppendix D.1, it fails to pick the better paths. We show\nan example of its explanation when textual descriptors are\nprovided in Figure 5 of Appendix E.\nVLMs exhibit mixed performance in integrating vi-\nsual and textual path information.\nWe take a further\nlook into the model performance when both the image\nand the textual descriptor values are provided, and con-\ntrast it with their performance when only the textual path\ndescriptions are provided (i.e., \u201cDesc Only\u201d).\nInterest-\n6890\n\n=== Page 6 ===\nModel\n2D\n3D\nDesc Only\nPATHEVAL\nPATHEVAL w Desc.\nPATHEVAL\nPATHEVAL w Desc.\nGPT-4o-mini\n0.520\n0.750\n0.508\n0.745\n0.680\nGPT-4o\n0.665\n0.860\n0.624\n0.895\n0.894\nLLaVa-NeXT-7b\n0.501\n0.524\n0.499\n0.517\n0.514\nQwen2-VL-7b\n0.502\n0.731\n0.511\n0.742\n0.737\nLLaVA-OneVision-7b\n0.505\n0.718\n0.509\n0.739\n0.721\nIntern-VL2-8b\n0.489\n0.654\n0.505\n0.691\n0.648\nLLaMa-3.2-11b\n0.480\n0.695\n0.460\n0.680\n0.686\nLLaVa-NeXT-13b\n0.509\n0.620\n0.494\n0.601\n0.630\nIntern-VL2-40b\n0.506\n0.688\n0.496\n0.717\n0.679\nRandom Baseline\n0.500\n0.500\n0.500\n0.500\n0.500\nTable 1. Accuracy of VLMs on PATHEVAL based on 2D and 3D environment images. To investigate the potential vision bottleneck of\nVLMs, we additionally present each model\u2019s accuracy when explicit descriptor values are provided in the language prompts (\u201cPATHEVAL\nw Desc.\u201d). The last column (\u201cDesc Only\u201d) shows the model performance when only the textual descriptor values are provided.\ningly, we observe mixed information integration perfor-\nmance from these models.\nFor GPT-4o, Qwen2-VL-7b,\nLLaVA-OneVision-7b, LLaMa-3.2-11b, and LLaVa-NeXT-\n13b, their performance based on only descriptor values has\nno obvious difference to their best performance when (2D\nor 3D) images are also provided. This observation implies\nthat these models do not bene\ufb01t from the additional image\ninformation when the textual path descriptions are provided.\nInstead, sometimes the images (e.g., 2D images for GPT-4o\nand 3D images for LLaVa-NeXT-13b) may confuse their\nunderstanding of the textual descriptors, resulting in a worse\naccuracy compared to Desc Only. For GPT-4o-mini, Intern-\nVL2-8b, and Intern-VL2-40b, however, providing both the\nvisual and textual path information offers \u21914%-7% perfor-\nmance gain over Desc Only, indicating better information\nintegration from these VLMs.\nIn Appendix D.2, we present a breakdown of GPT-\n4o\u2019s performance by scenarios. We show that performance\nvaries greatly from one scenario to the other.\nInterest-\ningly, we observe that GPT-4o may overuse their common-\nsense knowledge. For instance, in the case of Scenario #2\nshown in Figure 1, where the agent is required to maxi-\nmize the path length for better coverage, GPT-4o still fa-\nvors the shorter path. This scenario represents a counter-\nfactual situation as models are often trained or instructed\nto seek the shortest paths. Evaluating VLMs in such coun-\nterfactual scenarios allows us to effectively probe their task\nunderstanding and reasoning, which we consider to be an\nimportant direction for future research.\nThe reasoning of VLMs can be unreliable.\nLimited\nby the vision bottleneck, we noticed these VLMs fabricat-\ning seemingly plausible explanations to justify their path\nevaluation, despite the fact that they could not actually per-\nceive the necessary path details to perform the task. This\nfabrication echos \ufb01ndings from other recent work, where\nLLMs were shown to produce reasoning traces that do not\nModel\nDefault\nFlipped\nRandom IDs\nGPT-4o-mini\n34/1016\n22/1026\n94/956\nGPT-4o\n278/772\n258/792\n291/759\nLLaVa-NeXT-7b\n1028/22\n1041/9\n580/470\nQwen2-VL-7b\n21/1029\n130/920\n127/923\nLLaVA-OneVision-7b\n438/612\n440/610\n262/788\nIntern-VL2-8b\n433/610\n430/620\n418/632\nLLaMA-3.2-11b*\n601/251\n620/264\n731/319\nLLaVa-NeXT-13b\n795/255\n939/111\n305/745\nIntern-VL2-40b\n394/656\n410/639\n510/540\nGround truth labels\n530/520\nTable 2. Performance on the 2D case (#of times \ufb01rst path is chosen\n/ #of times second path is chosen) when we \ufb02ip the path order or\nreplace their default names with random IDs. *There are several\ncases where LLaMA-3.2-11b does not follow the required format and/or\ndoes not give an answer, we omit those cases from this table.\naccurately re\ufb02ect the process of how the models reach an\nanswer [1, 66].\nTo gain further insights, we performed\nan analysis comparing model performance on PATHEVAL\nwith cases that consist of: 1) \ufb02ipping the order of paths\nin the pair, and 2) assigning random IDs to the paths\n(e.g. instead of referring to them as \u201cPath 1\u201d and \u201cPath\n2\u201d, we use a random sequence such as \u201cPath Xu2q\u201d and\n\u201cPath fP48\u201d). The results presented in Table 2 showcase\nthat VLMs demonstrate bias for a particular label, when\nthey actually do not have the capability to resolve the task.\nFor example, when no matter the default or the \ufb02ipping la-\nbels are used, LLaVA-NeXT-7b consistently selects Path\n1 98% of the time and fabricates incorrect observations of\nthe two paths in its explanations (Figure 1) to support this\nchoice. As we discussed earlier, LLaVA-NeXT-7b is par-\nticularly prone to hallucination in explanations, leading to\nits random-guess performance with or without textual de-\nscriptors. Introducing random IDs as path names mitigates\nthis bias for LLaVa-NeXT-7b (although the model still ob-\ntains a close-to-random accuracy on PATHEVAL) but does\nnot seem to help other models dramatically.\n6891\n\n=== Page 7 ===\nDescriptor\nTest Set\n\u03c91\n\u03c92\n\u03c93\nMin. Clearance\n0.46/0.46\n0.50/0.46\n0.74/0.70\n0.86/0.74\nMax. Clearance\n0.44/0.46\n0.41/0.49\n0.46/0.55\n0.50/0.60\nAvg. Clearance\n0.53/0.55\n0.50/0.52\n0.70/0.57\n0.73/0.60\nPath Length\n0.58/0.70\n0.86/0.91\n0.92/0.86\n0.94/0.94\nSmoothness\n0.74/0.72\n0.86/0.82\n0.90/0.90\n0.90/0.89\n# of Sharp Turns\n0.76/0.73\n0.80/0.84\n0.77/0.70\n0.60/0.55\nMax. Angle\n0.71/0.70\n0.82/0.84\n0.86/0.88\n0.94/0.96\nTable 3.\nGPT-4o\u2019s \ufb01ne-grained perception accuracy (2D/3D)\non the test set of PATHEVAL and three additionally synthesized\ndatasets with increasing metric differences.\n4.2. What Can Vision-Language Models See?\nThe previous subsection clearly highlights the vision com-\nponent as the bottleneck for path evaluation on PATHEVAL.\nIn this subsection, we conduct an analysis of the visual per-\nception capabilities of VLMs. We focus our analysis on\nGPT-4o since it is the only model that performs substan-\ntially better than the random baseline in the case where no\ndescriptors are required.\nGPT-4o can perceive paths that are signi\ufb01cantly differ-\nent. In order to get a better understanding of the model\u2019s\nperception capability, we break down its performance in\nterms of perception by individual metric. Speci\ufb01cally, we\nprompt GPT-4o to select which path in the pair provides\na lower value on each individual metric and report its ac-\ncuracy. We perform this analysis on both the task pairs in\nthe test set of PATHEVAL and an additionally synthesized\ndataset consisting of task pairs with three levels (denoted\nas \u03c91, \u03c92, and \u03c93) of increasing differences in their descrip-\ntor values. We describe the data generation process for this\ndataset in more detail in Appendix C.2.\nThe results for both experiments are shown in Table 3.\nUpon evaluating the performance on the test set of PATHE-\nVAL by individual metrics, we notice that GPT-4o particu-\nlarly struggles with Clearance metrics. These metrics typ-\nically require a lower level of perception and is naturally\nmore challenging to discriminate than other metrics. On the\nother hand, Smoothness, Number of Sharpest Turns, and\nMax Angle appear to be easier for the model to capture.\nWe also note that GPT-4o perceives the path length much\nmore easily in a 3D environment presentation.\nFurther-\nmore, GPT-4o\u2019s performance increases as we increase the\ndescriptor difference between paths. This, however, is not\nthe case for the Number of Sharp Turns. We conjecture that\nwhen we increase the number of sharp turns, we also en-\nforce the challenge of \u201ccounting\u201d the number of satisfying\nturns, which VLMs have been shown with limitations [46].\nDoes segment complexity affect performance? We look\ninto whether the complexity of the path segment is the key\nreason for GPT-4o\u2019s limited perception of clearance. To\nthis end, we test GPT-4o on segments of varying complex-\nity (i.e., points, straight lines, and curves), in a simpli\ufb01ed\n(a) Point\n(b) Straight line\n(c) Curve\n(d) GPT-4o performance on the distance to obstacle under different seg-\nment complexities.\nFigure 2. Example segment complexity test cases in simpli\ufb01ed\nenvironments and performance across the various settings.\nenvironment with only one rectangular obstacle at the cen-\nter (Figure 2), and evaluate its accuracy in identifying seg-\nments that are closer to the obstacle. For individual points,\nthe clearance is de\ufb01ned by the perpendicular distance from\nthe point to the obstacle; for straight lines and curves, we\nconsider a path closer if one of its endpoints is closer to the\nobstacle. For each segment type, we synthesize 100 pairs\nby \ufb01rst randomly generating 200 segments and then pairing\neach with the segment with the greatest distance difference\nfrom the obstacle relative to it (i.e., maximizing the absolute\ndifference between the distances of the two segments from\nthe obstacle). This increases the likelihood that the distance\nis signi\ufb01cant enough to be perceivable. The average differ-\nence in the clearances of the pairs of segments are 14.76,\n14.42, and 14.28 for points, lines, and curves respectively.\nThe results in Figure 2d show that GPT-4o can perform\nbetter in very easy scenarios; however, it struggles more as\nthe segment complexity increases. For instance, the model\nwas able to identify the closer points in 77% of the cases;\nhowever, when considering straight and curved lines, its\nperformance drops to 64% and 59%, respectively. A sur-\nprising observation is that, in the case of curved lines, GPT-\n4o\u2019s performance is dramatically better in 3D images. As\nshown in Appendix C.2, the average Clearance difference of\npath pairs in PATHEVAL is merely 0.12 \u2013 1.31. As the paths\nin PATHEVAL are much more complicated than the curves\nin this experiment, it is expected that GPT-4o exhibits dif-\n\ufb01culty in judging paths\u2019 clearances. The complexity of the\nenvironments (compared to a single square obstacle) could\nadd challenges.\n5. Fine-tuning a Path Evaluator\nOne intuitive question is whether simply \ufb01ne-tuning the\nVLMs can relieve their vision bottleneck. To answer this\n6892\n\n=== Page 8 ===\nModel\nFrozen\nFine-tuned\nAccuracy\nAvg. Cosine Similarity\nAccuracy\nAvg. Cosine Similarity\nclip-vit-base-patch32\n0.510\n0.914\n0.783\n0.514\nclip-vit-large-patch14-336\n0.498\n0.907\n0.749\n0.548\nOpenCLIP-vit-B-32\n0.540\n0.883\n0.743\n0.475\nsiglip-base-patch16-224\n0.529\n0.895\n0.731\n0.612\ndino-vit-16\n0.495\n0.911\n0.763\n0.754\ndino-v2-base\n0.510\n0.761\n0.721\n0.681\nTable 4. Probing accuracy and average cosine similarity between distinct path images (2D) when the vision encoder is frozen or \ufb01ne-tuned.\nSetting\nFrozen\nTunable\nImage Only\n0.52\n0.51\nImage w Descriptors\n0.96\n-\nImage Only (Random IDs)\n0.48\n0.52\nTable 5. Fine-tuned LLaVA-NeXT-7b performance (2D) when we\nkeep the vision encoder frozen or tunable.\nquestion, we experiment with LLaVA-NeXT-7b and \ufb01ne-\ntune it on the training set (13,500 pairs) of PATHEVAL. We\nfocus on the 2D case for the set of experiments described in\nthis section. We consider three separate settings for train-\ning: (1) training with only images as input, (2) training with\nimages and textual descriptor values as input, and (3) the\nsame setting as (1) with using random IDs as target labels.\nDetails in experimental setup are included in Appendix C.3.\n5.1. Overall Performance\nFine-tuning does not help with vision-language map-\nping. Table 5 shows that the model fails to learn meaningful\npatterns in the data, even after training 50 epochs. How-\never, when the textual descriptors are provided as input, the\nmodel can easily learn the function achieving 96% accu-\nracy (a 45% improvement from the zero-shot setting). This\nshows that the model is unable to extract the same descrip-\ntor values from the image input. Unfreezing the encoder for\n\ufb01ne-tuning also does not provide any signi\ufb01cant improve-\nment. We include a further discussion in Appendix D.3.\nThe results point to a limitation in the vision model\u2019s ability\nto encode the images, which we will investigate next.\n5.2. Understanding the Visual Representations\nIn order to further understand the limitations of the vision\ncomponent, we conduct an analysis to better understand\nhow well different vision encoder models can differentiate\nbetween different paths in the dataset. To this end, we \ufb01rst\napply a linear probe to see how easily distinguishable differ-\nent images are. Speci\ufb01cally, given a pair of images, we \ufb01rst\nuse the vision encoder to extract high-dimensional feature\nrepresentations for both images. These features are then\nconcatenated and passed through a simple binary classi\ufb01ca-\ntion layer (i.e., the probe). The probe is trained to predict a\nlabel of 1 if the images are the same and 0 otherwise. We ex-\nperiment with various SOTA vision encoders, namely CLIP\n[55] base and large varieties, LAION-OpenCLIP [25, 56],\nSigLip [80], DINO [8], and DINO-v2 [49] and analyze how\nwell their learned representations capture visual similarities\nand differences. We use a set of 1,000 randomly sampled\npath pairs with a balanced label distribution to train the\nprobe, and look at whether the model can learn to distin-\nguish between these paths. We also perform an experiment\nwhere we \ufb01ne-tune the vision encoder along with the probe.\nIn this setting, our goal is to gauge if carefully \ufb01ne-tuning a\nvision encoder can potentially improve the model\u2019s perfor-\nmance in low-level perception. Finally, in both settings, we\npresent the average cosine similarity between distinct paths.\nVision encoders cannot distinguish between paths. From\nTable 4, it can be seen that vision encoder models are unable\nto provide representations that are signi\ufb01cantly different for\nthe probe to tell if they are the same. This is further sup-\nported by the high values for the average cosine similarity\nacross all models.\nFine-tuning the encoders on a discrimination task can\nhelp disentangle the visual representations.\nBy un-\nfreezing the encoder weights and training them to identify\nwhether two paths are identical, we enhance their adapt-\nability to the task. The results in Table 4 demonstrate that\nthis approach effectively disentangles the learned represen-\ntations, resulting in signi\ufb01cantly improved performance and\nincreased separability, as evidenced by the notably lower\ncosine similarity between non-identical paths. The results\nthus imply the need for carefully \ufb01ne-tuning task-speci\ufb01c\nvision encoders for path evaluation on PATHEVAL.\n6. Conclusion\nThis work explored the use of VLMs as evaluators in path-\nplanning scenarios. We evaluated a number of VLMs on\nour proposed benchmark, PATHEVAL, and found that these\nmodels struggle with low-level perception. Speci\ufb01cally, we\n\ufb01nd that visual encoders used by SOTA models are unable\nto discern the differences between different paths in intri-\ncate scenarios. We hope that PATHEVAL will inspire re-\nsearchers to further explore ways to improve the visual ca-\npabilities of VLMs and contribute to \ufb01nding better ways to\nincorporate foundation models for developing more \ufb02exi-\nble, robust, and scalable planning paradigms.\n6893\n\n=== Page 9 ===\nAcknowledgments\nThis project was supported by resources provided by the\nOf\ufb01ce of Research Computing at George Mason University\n(URL: https://orc.gmu.edu) and funded in part by grants\nfrom the National Science Foundation (Award Number\n2018631).\nReferences\n[1] Chirag Agarwal, Sree Harsha Tanneru, and Himabindu\nLakkaraju.\nFaithfulness\nvs.\nPlausibility:\nOn\nthe\n(Un)Reliability of Explanations from Large Language Mod-\nels, 2024. 6\n[2] Mohamed Aghzal, Erion Plaku, and Ziyu Yao. Can Large\nLanguage Models be Good Path Planners? A Benchmark\nand Investigation on Spatial-temporal Reasoning. In ICLR\n2024 Workshop on Large Language Model (LLM) Agents,\n2024. 1, 3\n[3] Mohamed Aghzal, Erion Plaku, and Ziyu Yao. Look Further\nAhead: Testing the Limits of GPT-4 in Path Planning. IEEE\nCASE, 2024. 1, 3\n[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh. VQA: Visual Question Answering. In International\nConference on Computer Vision (ICCV), 2015. 3\n[5] Kate Baumli, Satinder Baveja, Feryal Behbahani, Harris\nChan, Gheorghe Comanici, Sebastian Flennerhag, Maxime\nGazeau, Kristian Holsheimer, Dan Horgan, Michael Laskin,\nClare Lyle, Hussain Masoom, Kay McKinney, Volodymyr\nMnih, Alexander Neitz, Dmitry Nikulin, Fabio Pardo, Jack\nParker-Holder, John Quan, Tim Rockt\u00a8aschel, Himanshu\nSahni, Tom Schaul, Yannick Schroecker, Stephen Spencer,\nRichie Steigerwald, Luyu Wang, and Lei Zhang.\nVision-\nLanguage Models as a Source of Rewards, 2024. 1, 3\n[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners, 2020. 1, 3\n[7] Xu Cao, Tong Zhou, Yunsheng Ma, Wenqian Ye, Can\nCui, Kun Tang, Zhipeng Cao, Kaizhao Liang, Ziran Wang,\nJames M. Rehg, and Chao Zheng. MAPLM: A Real-World\nLarge-Scale Vision-Language Benchmark for Map and Traf-\n\ufb01c Scene Understanding. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 21819\u201321830, 2024. 3\n[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the International Conference on Computer Vi-\nsion (ICCV), 2021. 1, 3, 8\n[9] Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xi-\naodan Liang, and Kwan-Yee Wong.\nMapGPT: Map-\nguided prompting with adaptive path planning for vision-\nand-language navigation. In Proceedings of the 62nd An-\nnual Meeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 9796\u20139810, Bangkok,\nThailand, 2024. Association for Computational Linguistics.\n3\n[10] Xuweiyi Chen, Ziqiao Ma, Xuejun Zhang, Sihan Xu,\nShengyi Qian, Jianing Yang, David Fouhey, and Joyce Chai.\nMulti-object hallucination in vision language models.\nIn\nThe Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024. 4\n[11] Yongchao Chen, Jacob Arkin, Charles Dawson, Yang Zhang,\nNicholas Roy, and Chuchu Fan. Autotamp: Autoregressive\ntask and motion planning with llms as translators and check-\ners. In 2024 IEEE International Conference on Robotics and\nAutomation (ICRA), pages 6695\u20136702, 2024. 1, 3\n[12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,\nSen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\nLewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng\nDai. InternVL: Scaling up Vision Foundation Models and\nAligning for Generic Visual-Linguistic Tasks. arXiv preprint\narXiv:2312.14238, 2023. 3, 5\n[13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing.\nVicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality, 2023. 4\n[14] Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang.\nTask and motion planning with large language models for ob-\nject rearrangement. In 2023 IEEE/RSJ International Confer-\nence on Intelligent Robots and Systems (IROS), pages 2086\u2013\n2092. IEEE, 2023. 3\n[15] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\nMarc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,\nand Pete Florence. Palm-e: an embodied multimodal lan-\nguage model. In Proceedings of the 40th International Con-\nference on Machine Learning. JMLR.org, 2023. 3\n[16] Yilun Du, Sherry Yang, Pete Florence, Fei Xia, Ayzaan\nWahid, brian ichter, Pierre Sermanet, Tianhe Yu, Pieter\nAbbeel, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Andy\nZeng, and Jonathan Tompson. Video language planning. In\nThe Twelfth International Conference on Learning Represen-\ntations, 2024. 3\n[17] Abhimanyu Dubey et. al.\nThe Llama 3 Herd of Models,\n2024. 3, 5\n[18] Difei Gao, Ruiping Wang, Shiguang Shan, and Xilin Chen.\nCric: A vqa dataset for compositional reasoning on vision\nand commonsense. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 45(5):5561\u20135578, 2022. 3\n[19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\n6894\n\n=== Page 10 ===\nthe role of image understanding in visual question answer-\ning. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2017. 3\n[20] Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, and Xin\nWang. Vision-and-language navigation: A survey of tasks,\nmethods, and future directions. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers). Association for Computa-\ntional Linguistics, 2022. 3\n[21] Lin Guan, Yifan Zhou, Denis Liu, Yantian Zha, Heni Ben\nAmor, and Subbarao Kambhampati.\nTask success is not\nenough: Investigating the use of video-language models as\nbehavior critics for catching undesirable agent behaviors. In\nFirst Conference on Language Modeling, 2024. 1, 3\n[22] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia\nLi, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang,\nYaser Yacoob, Dinesh Manocha, and Tianyi Zhou.\nHal-\nlusionbench: An advanced diagnostic suite for entangled\nlanguage hallucination & visual illusion in large vision-\nlanguage models, 2023. 4\n[23] Xu Han, Qiannan Yang, Xianda Chen, Xiaowen Chu, and\nMeixin Zhu. Generating and evolving reward functions for\nhighway driving with large language models, 2024. 3\n[24] Drew A. Hudson and Christopher D. Manning. GQA: A New\nDataset for Real-World Visual Reasoning and Compositional\nQuestion Answering.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2019. 3\n[25] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, 2021. If you use this software, please cite it as below.\n3, 8\n[26] Amita Kamath, Jack Hessel, and Kai-Wei Chang. What\u2019s\n\u201dup\u201d with vision-language models? Investigating their strug-\ngle with spatial reasoning. In The 2023 Conference on Em-\npirical Methods in Natural Language Processing, 2023. 1,\n3\n[27] Subbarao Kambhampati, Karthik Valmeekam, Lin Guan,\nMudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Paul\nSaldyt, and Anil B Murthy. Position: LLMs Can\u2019t Plan, But\nCan Help Planning in LLM-Modulo Frameworks. In Forty-\n\ufb01rst International Conference on Machine Learning, 2024.\n1, 3\n[28] L.E. Kavraki, P. Svestka, J.-C. Latombe, and M.H. Over-\nmars.\nProbabilistic roadmaps for path planning in high-\ndimensional con\ufb01guration spaces.\nIEEE Transactions on\nRobotics and Automation, 12(4):566\u2013580, 1996. 4\n[29] J.J. Kuffner and S.M. LaValle. RRT-connect: An ef\ufb01cient ap-\nproach to single-query path planning. In Proceedings 2000\nICRA. Millennium Conference. IEEE International Confer-\nence on Robotics and Automation. Symposia Proceedings\n(Cat. No.00CH37065), pages 995\u20131001 vol.2, 2000. 4\n[30] Steven M. LaValle. Rapidly-exploring random trees : a new\ntool for path planning. The annual research report, 1998. 4\n[31] Jan\nLeike.\nWhy\ni\u2019m\noptimistic\nabout\nour\nalign-\nment approach:\nEvaluation is easier than generation.\nhttps://aligned.substack.com/i/88447351/\nevaluation - is - easier - than - generation,\n2022. Accessed: 2024-11. 1\n[32] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng\nLi, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and\nChunyuan Li. LLaVA-OneVision: Easy Visual Task Trans-\nfer. arXiv preprint arXiv:2408.03326, 2024. 3, 5\n[33] Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu\nQiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, and Jifeng\nDai. Auto mc-reward: Automated dense reward design with\nlarge language models for minecraft. In IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 2024. 3\n[34] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucination in\nlarge vision-language models. In The 2023 Conference on\nEmpirical Methods in Natural Language Processing, 2023.\n4\n[35] Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu,\nHangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Wang,\nJian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue\nZhang, Zhenzhu Yang, Xiangzhou Wang, Zhaoxiang Zhang,\nZachary Liu, Emmanouil Benetos, Wenhao Huang, and\nChenghua Lin. OmniBench: Towards The Future of Uni-\nversal Omni-Language Models, 2024. 4\n[36] Hui Sheng Lim, Shuangshuang Fan, Christopher K.H.\nChin, Shuhong Chai, Neil Bose, and Eonjoo Kim.\nCon-\nstrained path planning of autonomous underwater vehicle us-\ning selectively-hybridized particle swarm optimization algo-\nrithms. IFAC-PapersOnLine, 52(21):315\u2013322, 2019. 12th\nIFAC Conference on Control Applications in Marine Sys-\ntems, Robotics, and Vehicles CAMS 2019. 2\n[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In Advances in Neural Information\nProcessing Systems, pages 34892\u201334916. Curran Associates,\nInc., 2023. 3\n[38] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved Baselines with Visual Instruction Tuning, 2024. 3,\n5\n[39] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Im-\nproved reasoning, OCR, and world knowledge, 2024. 3, 5\n[40] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your\nmulti-modal model an all-around player? In Computer Vi-\nsion \u2013 ECCV 2024, pages 216\u2013233, Cham, 2025. Springer\nNature Switzerland. 4\n[41] Tom\u00b4as Lozano-P\u00b4erez and Michael A. Wesley. An algorithm\nfor planning collision-free paths among polyhedral obsta-\ncles. Communications of the ACM, 22(10):560\u2013570, 1979.\n4\n[42] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li,\nHannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel\nGalley, and Jianfeng Gao.\nMathvista: Evaluating mathe-\nmatical reasoning of foundation models in visual contexts.\nIn The Twelfth International Conference on Learning Repre-\nsentations, 2024. 3\n6895\n\n=== Page 11 ===\n[43] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019. 3\n[44] Silin Meng, Yiwei Wang, Cheng-Fu Yang, Nanyun Peng,\nand Kai-Wei Chang. LLM-a*: Large language model en-\nhanced incremental heuristic search on path planning.\nIn\nFindings of the Association for Computational Linguistics:\nEMNLP 2024, pages 1087\u20131102, Miami, Florida, USA,\n2024. Association for Computational Linguistics. 3\n[45] Shota Onohara, Atsuyuki Miyai, Yuki Imajuku, Kazuki\nEgashira, Jeonghun Baek, Xiang Yue, Graham Neubig, and\nKiyoharu Aizawa.\nJmmmu: A japanese massive multi-\ndiscipline multimodal understanding benchmark for culture-\naware evaluation, 2024. 4\n[46] OpenAI. GPT-4V(ision) System Card. 2023. 7\n[47] OpenAI. GPT-4o System Card, 2024. 3, 5\n[48] OpenAI. GPT-4 Technical Report, 2024. 3\n[49] Maxime Oquab, Timoth\u00b4ee Darcet, Theo Moutakanni, Huy V.\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-\nsell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-\nWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-\nlas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,\nJulien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-\njanowski. Dinov2: Learning robust visual features without\nsupervision, 2023. 3, 8\n[50] Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris,\nAude Oliva, Phillip Isola, and Yoon Kim. LangNav: Lan-\nguage as a perceptual representation for navigation.\nIn\nFindings of the Association for Computational Linguistics:\nNAACL 2024, pages 950\u2013974, Mexico City, Mexico, 2024.\nAssociation for Computational Linguistics. 1, 3\n[51] Chenbin Pan, Burhaneddin Yaman, Tommaso Nesti, Abhirup\nMallik, Alessandro G Allievi, Senem Velipasalar, and Liu\nRen.\nVLP: Vision Language Planning for Autonomous\nDriving . In 2024 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 14760\u201314769,\nLos Alamitos, CA, USA, 2024. IEEE Computer Society. 1,\n3\n[52] Clment Petres, Yan Pailhas, Pedro Patron, Yvan Petil-\nlot, Jonathan Evans, and David Lane.\nPath planning for\nautonomous underwater vehicles.\nIEEE Transactions on\nRobotics, 23(2):331\u2013341, 2007. 2\n[53] Evis Plaku, Erion Plaku, and Patricio Simari.\nClearance-\ndriven motion planning for mobile robots with differential\nconstraints. Robotica, 36(7):971\u2013993, 2018. 4\n[54] Patrick A. Plonski, Pratap Tokekar, and Volkan Isler. Energy-\nEf\ufb01cient Path Planning for Solar-Powered Mobile Robots,\npages 717\u2013731. Springer International Publishing, Heidel-\nberg, 2013. 2\n[55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pam Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. Proceedings of\nthe 38th International Conference on Machine Learning,\n2021. 3, 8\n[56] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade W Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev.\nLAION-5b: An open large-scale dataset for train-\ning next generation image-text models. In Thirty-sixth Con-\nference on Neural Information Processing Systems Datasets\nand Benchmarks Track, 2022. 3, 8\n[57] Jie-Jing Shao, Jiang-Xin Shi, Xiao-Wen Yang, Lan-Zhe Guo,\nand Yu-Feng Li. Investigating the Limitation of CLIP Mod-\nels: The Worst-Performing Categories, 2023. 3\n[58] Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B. Tenen-\nbaum, Leslie Pack Kaelbling, and Michael Katz. General-\nized planning in pddl domains with pretrained large language\nmodels, 2023. 3\n[59] Ioan A. Sucan, Mark Moll, and Lydia E. Kavraki.\nThe\nOpen Motion Planning Library. IEEE Robotics & Automa-\ntion Magazine, 19(4):72\u201382, 2012. 4\n[60] Yinan Sun, Zicheng Zhang, Haoning Wu, Xiaohong Liu,\nWeisi Lin, Guangtao Zhai, and Xiongkuo Min. Explore the\nhallucination on low-level perception for mllms, 2024. 4\n[61] Hao Tan, Licheng Yu, and Mohit Bansal. Learning to nav-\nigate unseen environments: Back translation with environ-\nmental dropout. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 2610\u20132621, Minneapolis,\nMinnesota, 2019. Association for Computational Linguis-\ntics. 3\n[62] Gemini Team. Gemini: A family of highly capable multi-\nmodal models, 2024. 3\n[63] Shengbang Tong, Erik Jones, and Jacob Steinhardt. Mass-\nproducing failures of multimodal systems with language\nmodels. In Proceedings of the 37th International Confer-\nence on Neural Information Processing Systems, Red Hook,\nNY, USA, 2024. Curran Associates Inc. 4\n[64] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann\nLeCun, and Saining Xie. Eyes Wide Shut? Exploring the\nVisual Shortcomings of Multimodal LLMs, 2024. 1, 3, 4\n[65] Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer,\nMoya Chen, Guillem Cucurull, David Esiobu, Jude Fer-\nnandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia\nGao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Vik-\ntor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-\nrenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning\nMao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiao-\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,\n6896\n\n=== Page 12 ===\nJian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov,\nand Thomas Scialom. Llama 2: Open foundation and \ufb01ne-\ntuned chat models, 2023. 3\n[66] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R.\nBowman.\nLanguage models don\u2019t always say what they\nthink: unfaithful explanations in chain-of-thought prompt-\ning.\nIn Proceedings of the 37th International Conference\non Neural Information Processing Systems, Red Hook, NY,\nUSA, 2024. Curran Associates Inc. 6\n[67] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and\nSubbarao Kambhampati. Large Language Models Still Can\u2019t\nPlan (A Benchmark for LLMs on Planning and Reasoning\nabout Change). In NeurIPS 2022 Foundation Models for De-\ncision Making Workshop, 2022. 3\n[68] Karthik Valmeekam, Kaya Stechly, and Subbarao Kamb-\nhampati. LLMs Still Can\u2019t Plan; Can LRMs? A Preliminary\nEvaluation of OpenAI\u2019s o1 on PlanBench, 2024. 3\n[69] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui\nMen, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Jun-\nyang Lin. Qwen2-VL: Enhancing Vision-Language Model\u2019s\nPerception of the World at Any Resolution. arXiv preprint\narXiv:2409.12191, 2024. 3, 5\n[70] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao,\nDinghan Shen, Yuan-Fang Wang, William Yang Wang, and\nLei Zhang.\nReinforced cross-modal matching and self-\nsupervised imitation learning for vision-language navigation.\nIn 2019 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 6622\u20136631, 2019. 3\n[71] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and\nDenny Zhou. Chain of Thought Prompting Elicits Reasoning\nin Large Language Models. In Advances in Neural Informa-\ntion Processing Systems, 2022. 1\n[72] Monika Wysocza\u00b4nska, Oriane Sim\u00b4eoni, Micha\u00a8el Ramamon-\njisoa, Andrei Bursuc, Tomasz Trzci\u00b4nski, and Patrick P\u00b4erez.\nClip-dinoiser: Teaching clip a few dino tricks for open-\nvocabulary semantic segmentation. ECCV, 2024. 3\n[73] Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu,\nQian Luo, Victor Zhong, Yanchao Yang, and Tao Yu.\nText2Reward: Reward Shaping with Language Models for\nReinforcement Learning. In The Twelfth International Con-\nference on Learning Representations, 2024. 1, 3\n[74] Yutaro Yamada, Yihan Bao, Andrew Kyle Lampinen, Jungo\nKasai, and Ilker Yildirim. Evaluating spatial understanding\nof large language models. Transactions on Machine Learn-\ning Research, 2024. 3\n[75] Zhutian Yang, Caelan Garrett, Dieter Fox, Tom\u00b4as Lozano-\nP\u00b4erez, and Leslie Pack Kaelbling. Guiding long-horizon task\nand motion planning with vision language models, 2024. 1,\n3\n[76] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models. Transactions\non Machine Learning Research, 2022. 3\n[77] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. In International conference on machine learn-\ning. PMLR, 2024. 4\n[78] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Ren-\nliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo\nLiu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMMMU: A Massive Multi-discipline Multimodal Under-\nstanding and Reasoning Benchmark for Expert AGI. In Pro-\nceedings of CVPR, 2024. 4\n[79] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang,\nKai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge\nZhang, Huan Sun, Yu Su, Wenhu Chen, and Graham\nNeubig.\nMMMU-Pro: A More Robust Multi-discipline\nMultimodal Understanding Benchmark.\narXiv preprint\narXiv:2409.02813, 2024. 4\n[80] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 11975\u201311986, 2023. 3, 8\n[81] Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu\nLuo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu\nXu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang,\nRuibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan\nTsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, and Jie\nFu. CMMMU: A Chinese Massive Multi-discipline Multi-\nmodal Understanding Benchmark, 2024. 4\n[82] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George\nKarypis, and Alex Smola. Multimodal chain-of-thought rea-\nsoning in language models, 2024. 1\n[83] Victor Zhong, Dipendra Misra, Xingdi Yuan, and Marc-\nAlexandre C\u02c6ot\u00b4e. Policy Improvement using Language Feed-\nback Models, 2024. 1, 3\n[84] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.\nVisual7W: Grounded Question Answering in Images.\nIn\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, 2016. 3\n6897\n",
      "searchable_content": "page 1 evaluating vision-language models as evaluators in path planning mohamed aghzal1 xiang yue2 erion plaku3 ziyu yao1 1george mason university 2carnegie mellon university 3national science foundation maghzal ziyuyao gmu.edu xyue2 andrew.cmu.edu eplaku nsf.gov abstract despite their promise to perform complex reasoning large language models llms have been shown to have limited effectiveness in end-to-end planning. this has in- spired an intriguing question if these models cannot plan well can they still contribute to the planning framework as a helpful plan evaluator in this work we generalize this question to consider llms augmented with visual under- standing i.e. vision-language models vlms . we intro- duce patheval a novel benchmark evaluating vlms as plan evaluators in complex path-planning scenarios. suc- ceeding in the benchmark requires a vlm to be able to ab- stract traits of optimal paths from the scenario description demonstrate precise low-level perception on each path and integrate this information to decide the better path. our analysis of state-of-the-art vlms reveals that these mod- els face signi\ufb01cant challenges on the benchmark. we ob- serve that the vlms can precisely abstract given scenar- ios to identify the desired traits and exhibit mixed perfor- mance in integrating the provided information. yet their vi- sion component presents a critical bottleneck with models struggling to perceive low-level details about a path. our experimental results show that this issue cannot be trivially addressed via end-to-end \ufb01ne-tuning rather task-speci\ufb01c discriminative adaptation of these vision encoders is needed for these vlms to become effective path evaluators.12 1. introduction the impressive capabilities of large language models llms 6 8 71 and vision-language models vlms 82 have led to an increasing interest in applying them to automated motion planning and navigation tasks 50 51 . however the inherent limitations of these models in long- the work by e. plaku is supported by while serving at the national science foundation. any opinion \ufb01ndings and conclusions or recom- mendations expressed in this material are those of the authors and do not necessarily re\ufb02ect the views of the national science foundation. 1source code https github.com mohamedaghzal patheval 2dataset https huggingface.co datasets maghzal patheval horizon planning have rendered them ineffective as end- to-end motion planners 2 3 11 75 . this has made re- searchers wonder if these models cannot be good motion planners themselves can they still support a motion plan- ning framework intuitively using these models still holds the promise of signi\ufb01cantly enhancing the motion plan- ning framework as they have learned extensive factual and commonsense knowledge that could bene\ufb01t planning dur- ing their pre-training. as a result there has been an emerg- ing paradigm exploring how these models can be leveraged in combination with traditional methods 27 . one particularly interesting approach within this line of work involves using these models as plan evaluators. mo- tivated by the intuition that evaluation is easier than gen- eration 31 several efforts have explored leveraging these models as critics to assess the quality of generated plans 5 21 73 83 . however most of these efforts have focused on scenarios that require only limited high-level visual per- ception without necessitating \ufb01ne-grained or precise per- ceptual abilities. on the other hand while there have been prior works similarly investigating vlms low-level per- ception 26 64 studies speci\ufb01cally about the use of these models in planning remain limited. tasks such as motion planning often require \ufb01ne-grained visual understanding in highly speci\ufb01c contexts while also drawing on broad com- monsense knowledge acquired during pre-training. hence there is a pressing need to investigate the potential of vlms to understand both low-level visual details and leverage these visual signals for high-level reasoning. in this work we explore whether we can utilize vlms as evaluators in highly intricate continuous path planning problems. we introduce patheval figure 1 a control- lable benchmark designed around path planning in complex environments under diverse commonsense decision-making scenarios e.g. a \ufb01re\ufb01ghting robot has to cover as much of the area as possible to extinguish \ufb01res and scout for sur- vivors thus needs to prioritize paths with more coverage and higher clearance . traditionally encoding constraints in a planning algorithm requires signi\ufb01cant human effort in hand-crafting scenario-speci\ufb01c criteria for path evaluation such that the constraints can be injected into the planner this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 6886 page 2 example of an instance of patheval 2d 3d path descriptors path 1 min. clearance 1.07 max. clearance 8.91 avg. clearance 2.62 path length 243.46 smoothness 843.07 number of sharp turns 4 maximum angle 124.35. path 2 min. clearance 1.57 max. clearance 8.91 avg. clearance 3.18 path length 243.75 smoothness 809.65 number of sharp turns 1 maximum angle 93.12. scenario 1 the agent navigating this maze is a large truck so sharp turns 90 degrees or larger are harder to make. it should also stay on a straight line unless it is making a turn as it is driving through heavy traf\ufb01c. required descriptors smoothness number of sharp turns maximum angle ground truth path 2 scenario 2 an autonomous \ufb01re\ufb01ghting robot is designed to navigate and operate within burning buildings to extinguish \ufb01res and rescue trapped individuals. it should explore as much of the area as possible while maintaining a safe distance from the walls to avoid damage. required descriptors avg. clearance min. clearance path length ground truth path 1 scenario 3 as the vehicle is traversing a warzone it must stay concealed from enemy operatives making use of covers like walls and avoiding open spaces as much as possible. it should also reach its target as quickly as possible. required descriptors avg. clearance max. clearance path length ground truth path 2 prompt the agent navigating this maze is a large truck so sharp turns 90 degrees or larger are harder to make. it should also stay on a straight line unless it is making a turn as it is driving through heavy traf\ufb01c. which path better achieves the task path 1 is on the left side and path 2 is on the right side. your answer should follow the format below answer path 1 or path 2. explanation why you chose the path 1 or 2 . gpt-4o answer path 1. explanation path 1 has fewer sharp turns and maintains longer straight segments compared to path 2. this is more suitable for a large truck navigating through heavy traf\ufb01c as it minimizes the need for dif\ufb01cult maneuvers and allows for smoother driving. path 2 on the other hand has more frequent and sharper turns which would be more challenging for a large truck to navigate. llava-next-7b answer path 1. explanation path 1 is better suited for a large truck because it has fewer sharp turns 90 degrees or larger compared to path 2. the truck would have to make only one 90-degree turn which is easier to execute than the two 90-degree turns in path 2. additionally path 1 has more straight sections which would be more ef\ufb01cient for a large truck to navigate through heavy traf\ufb01c. the fewer turns and the greater number of straight sections make path 1 the more suitable route for a large truck. figure 1. example of patheval. the benchmark consists of 14 550 instances of path pairs rendered in both 2d and 3d and mapped to 15 decision-making scenarios. success on this task is tied to three distinct levels 1 attribute abstraction recognizing what aspects make a path ideal 2 low-level perception extracting the required attributes for each path from the images and 2 information integration synthesizing the collected information to make a decision. we test a set of vlms on the task and \ufb01nd that they struggle particularly with low-level perception. incorrect answers by different models are shown explanations indicating misperception are highlighted . during the search process 36 52 54 . however this not only is a tedious and costly process but also cannot scale up to handle the countless intricate scenarios in meeting real- life planning needs. an effective vlm as a reward model relieving humans from customizing evaluation criteria for speci\ufb01c scenarios could enable scalable general-purpose planners that can adapt to diverse commonsense scenarios described in natural language. 6887 page 3 in patheval a vlm is tasked with comparing two paths within a given decision-making context and select- ing the one that better satis\ufb01es the constraints outlined by the scenario. success on this task requires effective per- formance across three distinct levels 1 attribute abstrac- tion recognizing the attributes that de\ufb01ne a favorable path in a particular decision-making context 2 low-level per- ception demonstrating precise low-level perception to de- termine which path performs better based on the given cri- teria and 3 information integration integrating and syn- thesizing the perceived information to produce an answer. using this benchmark we analyze the performance of 9 state-of-the-art sota vlms including both the closed- source gpt-4o and gpt-4o-mini 47 and 7 different open- source vlms of different sizes i.e. llava-next 7b and 13b 39 qwen2-vl-7b 69 llava-onevision-7b 32 llama-3.2-11b 17 and intern-vl2 8b and 40b 12 . we \ufb01nd that these models struggle with the path evalua- tion task e.g. qwen2-vl-7b achieving only 50.2 accu- racy . however when providing these vlms with verbal- ized path speci\ufb01cations their performance signi\ufb01cantly im- proves e.g. 74.2 accuracy for qwen2-vl-7b which re- veals a potential vision bottleneck of these vlms. our fur- ther analysis con\ufb01rms these models weakness in low-level perception especially when they are tasked to perceive the clearance of a path with respect to surrounding obstacles and this weakness could be more prominent when the envi- ronment and the path representation become more complex. we discover the source of this weakness from the vision encoders used by these vlms yet simply \ufb01ne-tuning the vlms end-to-end with the vision encoders does not address the issue. rather our experiments suggest performing task- speci\ufb01c discriminative adaptation of these vision encoders. 2. related work 2.1. vision-language models the outstanding success of decoder-only llms 6 48 65 has driven the development of vision-language models vlms which extend llms with a vision component in an attempt to generalize their performance into a multi-modal setting 12 17 32 37 38 62 69 . vlms are designed for tasks that require unifying visual and linguistic representa- tions e.g. visual question answering 4 . typically this is achieved by connecting a vision encoder to a language de- coder and projecting the representations into a shared space allowing visual elements and linguistic components to be linked in a semantically meaningful manner. one of the earliest successful models at achieving this was the clip encoder 55 which was trained using contrastive learning to learn representations that map images and their corre- sponding textual descriptions. several varieties of clip have then been introduced 25 56 76 80 . these mod- els while showing tremendous promises have shown sev- eral limitations when it comes to visual tasks 57 as such several works 64 72 have sought ways to improve such representations by combining them with vision foundation models such as dino 8 49 . in this work we contribute to the research of vlms with a new benchmark namely patheval focusing on evaluating vlms as evaluators for path planning. this benchmark tests vlms seamlessly on their commonsense understanding i.e. being able to ab- stract critical concepts expressed in the described planning scenarios low-level perception i.e. precisely perceiving details about paths in complex environments and the abil- ity to reason about the collected textual and visual informa- tion for decision-making. 2.2. automated planning with vlms vision-language planning promises more \ufb02exible planning frameworks and enhanced human-ai interaction. there- fore designing systems that can effectively understand nat- ural language instruction and leverage perceptual inputs to conduct planning tasks has been a topic of interest in re- cent years 20 61 70 . the rise of vlms has led many to investigate the use of these models as vision-language planning agents 15 16 50 51 . however existing litera- ture highlights the limitations of llms in spatial reasoning 2 26 74 as well as long-horizon planning 3 67 68 . this pushed researchers to explore alternative ways to incorpo- rate vlms and llms into planning frameworks more reli- ably 27 58 . one potential direction is their use as plan evaluators either through the generation of reward func- tions 23 33 73 or by using them directly as off-the-shelf critics 5 21 83 . the success of such frameworks assumes perfect perception and that the models can accurately per- ceive visual information and reason about it in order to pro- duce an answer nevertheless it has been shown that the representations used by these models fail in highly intricate visual settings 64 . several works have explored the use of vlms as well as llms for path and motion planning 2 3 7 9 11 14 44 75 however to the knowledge of the authors there is no work that explores the use of vlms as path critics in this context. accordingly we aim to evaluate the ability of vlms to serve as evaluators in cases requir- ing navigation in complex environments while adhering to decision-making constraints speci\ufb01ed in natural language. 2.3. vision-language model benchmarks the introduction of multimodal models has prompted the development of several benchmarks that are capable of as- sessing the performance of these models on visual rea- soning tasks such as visual question-answering datasets 4 18 19 24 42 43 84 . however the rise of founda- tion models has produced the need for a more holistic eval- uation of the perceptual and reasoning capabilities of large 6888 page 4 vlms leading to benchmarks such as mm-vet 77 mm- bench 40 mmmu 45 78 79 81 and omnibench 35 . several benchmarks have also speci\ufb01cally been designed to assess the perception capabilities of these models and ex- plore the limitations associated with visual hallucinations and optical illusions 10 22 34 60 64 . our proposed benchmark provides a \ufb02exible yet challenging framework for interleavedly assessing the low-level perception and rea- soning capabilities of vlms. 3. the patheval benchmark motivated by the need to evaluate vlms as path evalua- tors in real-world planning scenarios we introduce pathe- val a controllable and extensible benchmark focused on path planning 28 30 41 in complex environments under a diverse set of decision-making constraints. we list the decision-making scenarios as well as the descriptors they attempt to optimize in appendix a. in total patheval includes 14 550 tasks over more than 1 150 distinct envi- ronments and 15 distinct scenarios. below we introduce a formal description of the task and the dataset construction. 3.1. task formulation given two paths p1 and p2 and a scenario s the objective is to determine which path better satis\ufb01es the scenario s op- timization criteria. each scenario s is a high-level descrip- tion that aims to optimize over a set of path descriptors or metrics m m1 m2 . . . mk where each descriptor mj p r evaluates a speci\ufb01c property of a path e.g. length smoothness or proximity to obstacles . a vlm v is presented with two images presenting p1 and p2 in the same environment respectively. the model must then de- cide which path better satis\ufb01es the scenario s criteria. to explore the sensitivity of vlms to the way how a path is presented patheval includes both the 2d and 3d images of the path illustration. the model is also prompted to gen- erate an explanation to justify its choice. 3.2. environment and path generation environment generation an environment as shown in figure 1 is de\ufb01ned by a set of walls o o1 o2 . . . on where each wall oi represents an ob- stacle in the 2d space. each wall is a closed geometric shape described by its vertices and the set o forms the ob- stacles that the path must avoid. in this work we leverage the environments of plaku et. al. 53 which consists of four types of obstacle arrangements 1 rings where the environments are structured as mazes with circular walls 2 waves which consist of wavy horizontal obstacles 3 mazes which consist of both vertical and horizontal walls forming a complex maze structure as well as 4 random which consist of randomly placed obstacles. path synthesis via the randomly-exploring rapid tree rrt algorithm to generate path candidates in patheval we leverage the rrt path planning algo- rithm 30 . starting from the initial location in the envi- ronment the algorithm works by building a tree which ex- pands itself by randomly selecting the next location in the environment while avoiding obstacles until it reaches the goal. in this work we use the open motion planning li- brary ompl 59 and implement the rrt-connect algo- rithm 29 . we note that while we use rrt in the current benchmark our codebase is adaptable and can incorporate most path-planning algorithms provided by ompl. we en- courage future research building on our benchmark to ex- periment with other algorithms as well. 3.3. path descriptors we collect the following descriptors m for each of a gen- erated path minimum clearance measures the smallest distance between any point on the path and the nearest ob- stacle maximum clearance measures the largest distance between any point on the path and the nearest obstacle av- erage clearance computes the average distance between all points on the path and the nearest obstacle path length is calculated by summing up the euclidean distances between consecutive points on the path smoothness is de\ufb01ned as the sum of the angles between consecutive segments of the path measuring how smoothly the path changes direction number of sharp turns counts the number of turns in the path where the angle between consecutive segments ex- ceeds 90 degrees and maximum angle denotes the largest angle between any two consecutive segments of the path. the three clearance metrics and path length share the same measuring unit i.e. one grid size smoothness and maxi- mum angle are measured by degree and the number of sharp turns is an integer count. we include the formula of each descriptor in appendix b. 3.4. natural language descriptions of scenarios to create a suf\ufb01ciently challenging path-planning evalua- tion benchmark we design a total of 15 decision-making scenarios that aim to optimize different combinations of the path descriptors. for instance scenario 2 as shown in figure 1 requires searching through an area affected with \ufb01re in search for survivors and the agent thus must cover as much ground as possible. in contrast scenario 3 indicates that the path is to be executed within a warzone as a result the vehicle has to remain hidden and take the shortest route. as such given the same set of paths the one minimizing the path length is favored by scenario 3 while scenario 2 needs to maximize this value. a complete summary of the 15 scenarios along with the descriptors each scenario aims to optimize is presented in tables 6-7 in appendix a. 6889 page 5 3.5. task pairs selection for each environment we synthesized we ran the rrt plan- ner 30 times to generate different paths. upon eliminating paths that did not reach the goal we selected path pairs that exhibited the greatest dissimilarity in terms of path descrip- tors. speci\ufb01cally we \ufb01rst represented each path using a 7- dimension vector of its path descriptor values. given that each path descriptor ranged in a dramatically different scale we normalized the vector by performing the min-max scal- ing i.e. scaling each value x in the vector to x x xmin xmax xmin where x represents each value in the vector and xmin and xmax are the minimum and maximum values of each de- scriptor across paths sampled from the same environment respectively. we then measured the distance between two paths by calculating the euclidean distance between their normalized path descriptor vectors and selected 5 path pairs with the largest distances. each path is included in only one pair to avoid redundancy. we repeated the same selection procedure for a total of 1 150 environments. upon performing this generation we matched each pair with all \ufb01fteen scenarios however we only kept pairs where there was a signi\ufb01cant difference in at least one of the descriptors required for the particular scenario. in other words we ensure that the absolute difference is greater than a prede\ufb01ned threshold 0.8 for clearance descriptors 50 for path length 90 for smoothness 1 for the number of sharp turns and 30 for maximum angle for at least one of the required descriptors. this makes it more likely that the dif- ference is more noticeable to the naked eye and thus the paths can be compared by visual inspection. we constructed our \ufb01nal evaluation benchmark by ran- domly selecting 70 task pairs from each scenario resulting in 1 050 pairs in total. the remaining task pairs a total of 13 500 are used as the training set to facilitate the \ufb01ne- tuning experiments in section 5. 4. can sota vlms evaluate planned paths in order for vlms to perform successfully in our bench- mark they need to perform successfully at three differ- ent levels i.e recognizing the critical descriptors required by each scenario attribute abstraction exhibiting sharp low-level perception to precisely assess each path s proper- ties low-level perception and integrating the prior in- formation to make a rational decision on the better path information integration . among them the \ufb01rst two lev- els re\ufb02ect parallel properties that serve as a foundation for the third level. in this section we evaluated a set of 9 vlms on patheval in a zero-shot manner and analyzed their ca- pabilities at these three levels. these vlms include 1 two closed-source vlms i.e. gpt-4o and gpt-4o-mini 47 and 2 seven open-source vlms with various sizes in- cluding llava-next-7b and llava-next-13b 38 39 llava-onevision-7b 32 qwen2-vl-7b 69 llama- 3.2-11b 17 and intern-vl2-8b and intern-vl2-40b 12 . we include all prompt scripts used in this section in ap- pendix c.1. 4.1. overview of vlms on patheval the performance of the 9 vlms on patheval is show- cased in table 1. we notice that all of the models ex- cept gpt-4o fail to perform signi\ufb01cantly better than a sim- ple random baseline indicating signi\ufb01cant limitations. for gpt-4o we also notice a 4 higher accuracy on 2d im- ages compared to prompting with 3d inputs. this obser- vation indicates that the model is prone to visual illusions introduced by the 3d images when it has to rely on solely the image for decision-making although we observe an op- posite effect of 2d vs. 3d when verbalized descriptor val- ues are provided . in the remaining section we will further break down these models capabilities to gain a deeper un- derstanding of their failure on this task. providing verbalized path information yields better task accuracy implying reasonable vlm performance in at- tribute abstraction. in table 1 we further show the per- formance of each model when we explicitly list the value for each descriptor as part of the language prompt i.e. patheval w desc. . we notice a 11.1 -27.1 improve- ment across most models indicating that when given low- level details the models can better \ufb01lter out the information and make better comparisons. this points out that the bot- tleneck for these vlms s better performance lies in their inability to accurately perceive low-level information about the paths we discuss this in more detail in section 4.2 whereas these models generally have a reasonable capabil- ity in abstracting the critical attributes for decision making in various scenarios. in appendix d.1 we include an ex- periment where we explicitly query each vlm to identify the critical path metrics for each scenario the result corrob- orates our hypothesis. in particular we \ufb01nd that for most models the success rate for identifying a required descrip- tor is over 92 . finally a surprising observation happens to llava-next-7b. we notice that this model suffers par- ticularly severely from hallucination even when the textual descriptor values are provided and when the model can cor- rectly identify critical path metrics based on our analysis in appendix d.1 it fails to pick the better paths. we show an example of its explanation when textual descriptors are provided in figure 5 of appendix e. vlms exhibit mixed performance in integrating vi- sual and textual path information. we take a further look into the model performance when both the image and the textual descriptor values are provided and con- trast it with their performance when only the textual path descriptions are provided i.e. desc only . interest- 6890 page 6 model 2d 3d desc only patheval patheval w desc. patheval patheval w desc. gpt-4o-mini 0.520 0.750 0.508 0.745 0.680 gpt-4o 0.665 0.860 0.624 0.895 0.894 llava-next-7b 0.501 0.524 0.499 0.517 0.514 qwen2-vl-7b 0.502 0.731 0.511 0.742 0.737 llava-onevision-7b 0.505 0.718 0.509 0.739 0.721 intern-vl2-8b 0.489 0.654 0.505 0.691 0.648 llama-3.2-11b 0.480 0.695 0.460 0.680 0.686 llava-next-13b 0.509 0.620 0.494 0.601 0.630 intern-vl2-40b 0.506 0.688 0.496 0.717 0.679 random baseline 0.500 0.500 0.500 0.500 0.500 table 1. accuracy of vlms on patheval based on 2d and 3d environment images. to investigate the potential vision bottleneck of vlms we additionally present each model s accuracy when explicit descriptor values are provided in the language prompts patheval w desc. . the last column desc only shows the model performance when only the textual descriptor values are provided. ingly we observe mixed information integration perfor- mance from these models. for gpt-4o qwen2-vl-7b llava-onevision-7b llama-3.2-11b and llava-next- 13b their performance based on only descriptor values has no obvious difference to their best performance when 2d or 3d images are also provided. this observation implies that these models do not bene\ufb01t from the additional image information when the textual path descriptions are provided. instead sometimes the images e.g. 2d images for gpt-4o and 3d images for llava-next-13b may confuse their understanding of the textual descriptors resulting in a worse accuracy compared to desc only. for gpt-4o-mini intern- vl2-8b and intern-vl2-40b however providing both the visual and textual path information offers 4 -7 perfor- mance gain over desc only indicating better information integration from these vlms. in appendix d.2 we present a breakdown of gpt- 4o s performance by scenarios. we show that performance varies greatly from one scenario to the other. interest- ingly we observe that gpt-4o may overuse their common- sense knowledge. for instance in the case of scenario 2 shown in figure 1 where the agent is required to maxi- mize the path length for better coverage gpt-4o still fa- vors the shorter path. this scenario represents a counter- factual situation as models are often trained or instructed to seek the shortest paths. evaluating vlms in such coun- terfactual scenarios allows us to effectively probe their task understanding and reasoning which we consider to be an important direction for future research. the reasoning of vlms can be unreliable. limited by the vision bottleneck we noticed these vlms fabricat- ing seemingly plausible explanations to justify their path evaluation despite the fact that they could not actually per- ceive the necessary path details to perform the task. this fabrication echos \ufb01ndings from other recent work where llms were shown to produce reasoning traces that do not model default flipped random ids gpt-4o-mini 34 1016 22 1026 94 956 gpt-4o 278 772 258 792 291 759 llava-next-7b 1028 22 1041 9 580 470 qwen2-vl-7b 21 1029 130 920 127 923 llava-onevision-7b 438 612 440 610 262 788 intern-vl2-8b 433 610 430 620 418 632 llama-3.2-11b 601 251 620 264 731 319 llava-next-13b 795 255 939 111 305 745 intern-vl2-40b 394 656 410 639 510 540 ground truth labels 530 520 table 2. performance on the 2d case of times \ufb01rst path is chosen of times second path is chosen when we \ufb02ip the path order or replace their default names with random ids. there are several cases where llama-3.2-11b does not follow the required format and or does not give an answer we omit those cases from this table. accurately re\ufb02ect the process of how the models reach an answer 1 66 . to gain further insights we performed an analysis comparing model performance on patheval with cases that consist of 1 \ufb02ipping the order of paths in the pair and 2 assigning random ids to the paths e.g. instead of referring to them as path 1 and path 2 we use a random sequence such as path xu2q and path fp48 . the results presented in table 2 showcase that vlms demonstrate bias for a particular label when they actually do not have the capability to resolve the task. for example when no matter the default or the \ufb02ipping la- bels are used llava-next-7b consistently selects path 1 98 of the time and fabricates incorrect observations of the two paths in its explanations figure 1 to support this choice. as we discussed earlier llava-next-7b is par- ticularly prone to hallucination in explanations leading to its random-guess performance with or without textual de- scriptors. introducing random ids as path names mitigates this bias for llava-next-7b although the model still ob- tains a close-to-random accuracy on patheval but does not seem to help other models dramatically. 6891 page 7 descriptor test set \u03c91 \u03c92 \u03c93 min. clearance 0.46 0.46 0.50 0.46 0.74 0.70 0.86 0.74 max. clearance 0.44 0.46 0.41 0.49 0.46 0.55 0.50 0.60 avg. clearance 0.53 0.55 0.50 0.52 0.70 0.57 0.73 0.60 path length 0.58 0.70 0.86 0.91 0.92 0.86 0.94 0.94 smoothness 0.74 0.72 0.86 0.82 0.90 0.90 0.90 0.89 of sharp turns 0.76 0.73 0.80 0.84 0.77 0.70 0.60 0.55 max. angle 0.71 0.70 0.82 0.84 0.86 0.88 0.94 0.96 table 3. gpt-4o s \ufb01ne-grained perception accuracy 2d 3d on the test set of patheval and three additionally synthesized datasets with increasing metric differences. 4.2. what can vision-language models see the previous subsection clearly highlights the vision com- ponent as the bottleneck for path evaluation on patheval. in this subsection we conduct an analysis of the visual per- ception capabilities of vlms. we focus our analysis on gpt-4o since it is the only model that performs substan- tially better than the random baseline in the case where no descriptors are required. gpt-4o can perceive paths that are signi\ufb01cantly differ- ent. in order to get a better understanding of the model s perception capability we break down its performance in terms of perception by individual metric. speci\ufb01cally we prompt gpt-4o to select which path in the pair provides a lower value on each individual metric and report its ac- curacy. we perform this analysis on both the task pairs in the test set of patheval and an additionally synthesized dataset consisting of task pairs with three levels denoted as \u03c91 \u03c92 and \u03c93 of increasing differences in their descrip- tor values. we describe the data generation process for this dataset in more detail in appendix c.2. the results for both experiments are shown in table 3. upon evaluating the performance on the test set of pathe- val by individual metrics we notice that gpt-4o particu- larly struggles with clearance metrics. these metrics typ- ically require a lower level of perception and is naturally more challenging to discriminate than other metrics. on the other hand smoothness number of sharpest turns and max angle appear to be easier for the model to capture. we also note that gpt-4o perceives the path length much more easily in a 3d environment presentation. further- more gpt-4o s performance increases as we increase the descriptor difference between paths. this however is not the case for the number of sharp turns. we conjecture that when we increase the number of sharp turns we also en- force the challenge of counting the number of satisfying turns which vlms have been shown with limitations 46 . does segment complexity affect performance we look into whether the complexity of the path segment is the key reason for gpt-4o s limited perception of clearance. to this end we test gpt-4o on segments of varying complex- ity i.e. points straight lines and curves in a simpli\ufb01ed a point b straight line c curve d gpt-4o performance on the distance to obstacle under different seg- ment complexities. figure 2. example segment complexity test cases in simpli\ufb01ed environments and performance across the various settings. environment with only one rectangular obstacle at the cen- ter figure 2 and evaluate its accuracy in identifying seg- ments that are closer to the obstacle. for individual points the clearance is de\ufb01ned by the perpendicular distance from the point to the obstacle for straight lines and curves we consider a path closer if one of its endpoints is closer to the obstacle. for each segment type we synthesize 100 pairs by \ufb01rst randomly generating 200 segments and then pairing each with the segment with the greatest distance difference from the obstacle relative to it i.e. maximizing the absolute difference between the distances of the two segments from the obstacle . this increases the likelihood that the distance is signi\ufb01cant enough to be perceivable. the average differ- ence in the clearances of the pairs of segments are 14.76 14.42 and 14.28 for points lines and curves respectively. the results in figure 2d show that gpt-4o can perform better in very easy scenarios however it struggles more as the segment complexity increases. for instance the model was able to identify the closer points in 77 of the cases however when considering straight and curved lines its performance drops to 64 and 59 respectively. a sur- prising observation is that in the case of curved lines gpt- 4o s performance is dramatically better in 3d images. as shown in appendix c.2 the average clearance difference of path pairs in patheval is merely 0.12 1.31. as the paths in patheval are much more complicated than the curves in this experiment it is expected that gpt-4o exhibits dif- \ufb01culty in judging paths clearances. the complexity of the environments compared to a single square obstacle could add challenges. 5. fine-tuning a path evaluator one intuitive question is whether simply \ufb01ne-tuning the vlms can relieve their vision bottleneck. to answer this 6892 page 8 model frozen fine-tuned accuracy avg. cosine similarity accuracy avg. cosine similarity clip-vit-base-patch32 0.510 0.914 0.783 0.514 clip-vit-large-patch14-336 0.498 0.907 0.749 0.548 openclip-vit-b-32 0.540 0.883 0.743 0.475 siglip-base-patch16-224 0.529 0.895 0.731 0.612 dino-vit-16 0.495 0.911 0.763 0.754 dino-v2-base 0.510 0.761 0.721 0.681 table 4. probing accuracy and average cosine similarity between distinct path images 2d when the vision encoder is frozen or \ufb01ne-tuned. setting frozen tunable image only 0.52 0.51 image w descriptors 0.96 - image only random ids 0.48 0.52 table 5. fine-tuned llava-next-7b performance 2d when we keep the vision encoder frozen or tunable. question we experiment with llava-next-7b and \ufb01ne- tune it on the training set 13 500 pairs of patheval. we focus on the 2d case for the set of experiments described in this section. we consider three separate settings for train- ing 1 training with only images as input 2 training with images and textual descriptor values as input and 3 the same setting as 1 with using random ids as target labels. details in experimental setup are included in appendix c.3. 5.1. overall performance fine-tuning does not help with vision-language map- ping. table 5 shows that the model fails to learn meaningful patterns in the data even after training 50 epochs. how- ever when the textual descriptors are provided as input the model can easily learn the function achieving 96 accu- racy a 45 improvement from the zero-shot setting . this shows that the model is unable to extract the same descrip- tor values from the image input. unfreezing the encoder for \ufb01ne-tuning also does not provide any signi\ufb01cant improve- ment. we include a further discussion in appendix d.3. the results point to a limitation in the vision model s ability to encode the images which we will investigate next. 5.2. understanding the visual representations in order to further understand the limitations of the vision component we conduct an analysis to better understand how well different vision encoder models can differentiate between different paths in the dataset. to this end we \ufb01rst apply a linear probe to see how easily distinguishable differ- ent images are. speci\ufb01cally given a pair of images we \ufb01rst use the vision encoder to extract high-dimensional feature representations for both images. these features are then concatenated and passed through a simple binary classi\ufb01ca- tion layer i.e. the probe . the probe is trained to predict a label of 1 if the images are the same and 0 otherwise. we ex- periment with various sota vision encoders namely clip 55 base and large varieties laion-openclip 25 56 siglip 80 dino 8 and dino-v2 49 and analyze how well their learned representations capture visual similarities and differences. we use a set of 1 000 randomly sampled path pairs with a balanced label distribution to train the probe and look at whether the model can learn to distin- guish between these paths. we also perform an experiment where we \ufb01ne-tune the vision encoder along with the probe. in this setting our goal is to gauge if carefully \ufb01ne-tuning a vision encoder can potentially improve the model s perfor- mance in low-level perception. finally in both settings we present the average cosine similarity between distinct paths. vision encoders cannot distinguish between paths. from table 4 it can be seen that vision encoder models are unable to provide representations that are signi\ufb01cantly different for the probe to tell if they are the same. this is further sup- ported by the high values for the average cosine similarity across all models. fine-tuning the encoders on a discrimination task can help disentangle the visual representations. by un- freezing the encoder weights and training them to identify whether two paths are identical we enhance their adapt- ability to the task. the results in table 4 demonstrate that this approach effectively disentangles the learned represen- tations resulting in signi\ufb01cantly improved performance and increased separability as evidenced by the notably lower cosine similarity between non-identical paths. the results thus imply the need for carefully \ufb01ne-tuning task-speci\ufb01c vision encoders for path evaluation on patheval. 6. conclusion this work explored the use of vlms as evaluators in path- planning scenarios. we evaluated a number of vlms on our proposed benchmark patheval and found that these models struggle with low-level perception. speci\ufb01cally we \ufb01nd that visual encoders used by sota models are unable to discern the differences between different paths in intri- cate scenarios. we hope that patheval will inspire re- searchers to further explore ways to improve the visual ca- pabilities of vlms and contribute to \ufb01nding better ways to incorporate foundation models for developing more \ufb02exi- ble robust and scalable planning paradigms. 6893 page 9 acknowledgments this project was supported by resources provided by the of\ufb01ce of research computing at george mason university url https orc.gmu.edu and funded in part by grants from the national science foundation award number 2018631 . references 1 chirag agarwal sree harsha tanneru and himabindu lakkaraju. faithfulness vs. plausibility on the un reliability of explanations from large language mod- els 2024. 6 2 mohamed aghzal erion plaku and ziyu yao. can large language models be good path planners a benchmark and investigation on spatial-temporal reasoning. in iclr 2024 workshop on large language model llm agents 2024. 1 3 3 mohamed aghzal erion plaku and ziyu yao. look further ahead testing the limits of gpt-4 in path planning. ieee case 2024. 1 3 4 stanislaw antol aishwarya agrawal jiasen lu margaret mitchell dhruv batra c. lawrence zitnick and devi parikh. vqa visual question answering. in international conference on computer vision iccv 2015. 3 5 kate baumli satinder baveja feryal behbahani harris chan gheorghe comanici sebastian flennerhag maxime gazeau kristian holsheimer dan horgan michael laskin clare lyle hussain masoom kay mckinney volodymyr mnih alexander neitz dmitry nikulin fabio pardo jack parker-holder john quan tim rockt aschel himanshu sahni tom schaul yannick schroecker stephen spencer richie steigerwald luyu wang and lei zhang. vision- language models as a source of rewards 2024. 1 3 6 tom b. brown benjamin mann nick ryder melanie sub- biah jared kaplan prafulla dhariwal arvind neelakan- tan pranav shyam girish sastry amanda askell sand- hini agarwal ariel herbert-voss gretchen krueger tom henighan rewon child aditya ramesh daniel m. ziegler jeffrey wu clemens winter christopher hesse mark chen eric sigler mateusz litwin scott gray benjamin chess jack clark christopher berner sam mccandlish alec rad- ford ilya sutskever and dario amodei. language models are few-shot learners 2020. 1 3 7 xu cao tong zhou yunsheng ma wenqian ye can cui kun tang zhipeng cao kaizhao liang ziran wang james m. rehg and chao zheng. maplm a real-world large-scale vision-language benchmark for map and traf- \ufb01c scene understanding. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr pages 21819 21830 2024. 3 8 mathilde caron hugo touvron ishan misra herv e j egou julien mairal piotr bojanowski and armand joulin. emerg- ing properties in self-supervised vision transformers. in pro- ceedings of the international conference on computer vi- sion iccv 2021. 1 3 8 9 jiaqi chen bingqian lin ran xu zhenhua chai xi- aodan liang and kwan-yee wong. mapgpt map- guided prompting with adaptive path planning for vision- and-language navigation. in proceedings of the 62nd an- nual meeting of the association for computational linguis- tics volume 1 long papers pages 9796 9810 bangkok thailand 2024. association for computational linguistics. 3 10 xuweiyi chen ziqiao ma xuejun zhang sihan xu shengyi qian jianing yang david fouhey and joyce chai. multi-object hallucination in vision language models. in the thirty-eighth annual conference on neural information processing systems 2024. 4 11 yongchao chen jacob arkin charles dawson yang zhang nicholas roy and chuchu fan. autotamp autoregressive task and motion planning with llms as translators and check- ers. in 2024 ieee international conference on robotics and automation icra pages 6695 6702 2024. 1 3 12 zhe chen jiannan wu wenhai wang weijie su guo chen sen xing muyan zhong qinglong zhang xizhou zhu lewei lu bin li ping luo tong lu yu qiao and jifeng dai. internvl scaling up vision foundation models and aligning for generic visual-linguistic tasks. arxiv preprint arxiv 2312.14238 2023. 3 5 13 wei-lin chiang zhuohan li zi lin ying sheng zhang- hao wu hao zhang lianmin zheng siyuan zhuang yong- hao zhuang joseph e. gonzalez ion stoica and eric p. xing. vicuna an open-source chatbot impressing gpt-4 with 90 chatgpt quality 2023. 4 14 yan ding xiaohan zhang chris paxton and shiqi zhang. task and motion planning with large language models for ob- ject rearrangement. in 2023 ieee rsj international confer- ence on intelligent robots and systems iros pages 2086 2092. ieee 2023. 3 15 danny driess fei xia mehdi s. m. sajjadi corey lynch aakanksha chowdhery brian ichter ayzaan wahid jonathan tompson quan vuong tianhe yu wenlong huang yevgen chebotar pierre sermanet daniel duck- worth sergey levine vincent vanhoucke karol hausman marc toussaint klaus greff andy zeng igor mordatch and pete florence. palm-e an embodied multimodal lan- guage model. in proceedings of the 40th international con- ference on machine learning. jmlr.org 2023. 3 16 yilun du sherry yang pete florence fei xia ayzaan wahid brian ichter pierre sermanet tianhe yu pieter abbeel joshua b. tenenbaum leslie pack kaelbling andy zeng and jonathan tompson. video language planning. in the twelfth international conference on learning represen- tations 2024. 3 17 abhimanyu dubey et. al. the llama 3 herd of models 2024. 3 5 18 difei gao ruiping wang shiguang shan and xilin chen. cric a vqa dataset for compositional reasoning on vision and commonsense. ieee transactions on pattern analysis and machine intelligence 45 5 5561 5578 2022. 3 19 yash goyal tejas khot douglas summers-stay dhruv ba- tra and devi parikh. making the v in vqa matter elevating 6894 page 10 the role of image understanding in visual question answer- ing. in proceedings of the ieee conference on computer vision and pattern recognition cvpr 2017. 3 20 jing gu eliana stefani qi wu jesse thomason and xin wang. vision-and-language navigation a survey of tasks methods and future directions. in proceedings of the 60th annual meeting of the association for computational lin- guistics volume 1 long papers . association for computa- tional linguistics 2022. 3 21 lin guan yifan zhou denis liu yantian zha heni ben amor and subbarao kambhampati. task success is not enough investigating the use of video-language models as behavior critics for catching undesirable agent behaviors. in first conference on language modeling 2024. 1 3 22 tianrui guan fuxiao liu xiyang wu ruiqi xian zongxia li xiaoyu liu xijun wang lichang chen furong huang yaser yacoob dinesh manocha and tianyi zhou. hal- lusionbench an advanced diagnostic suite for entangled language hallucination visual illusion in large vision- language models 2023. 4 23 xu han qiannan yang xianda chen xiaowen chu and meixin zhu. generating and evolving reward functions for highway driving with large language models 2024. 3 24 drew a. hudson and christopher d. manning. gqa a new dataset for real-world visual reasoning and compositional question answering. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr 2019. 3 25 gabriel ilharco mitchell wortsman ross wightman cade gordon nicholas carlini rohan taori achal dave vaishaal shankar hongseok namkoong john miller han- naneh hajishirzi ali farhadi and ludwig schmidt. open- clip 2021. if you use this software please cite it as below. 3 8 26 amita kamath jack hessel and kai-wei chang. what s up with vision-language models investigating their strug- gle with spatial reasoning. in the 2023 conference on em- pirical methods in natural language processing 2023. 1 3 27 subbarao kambhampati karthik valmeekam lin guan mudit verma kaya stechly siddhant bhambri lucas paul saldyt and anil b murthy. position llms can t plan but can help planning in llm-modulo frameworks. in forty- \ufb01rst international conference on machine learning 2024. 1 3 28 l.e. kavraki p. svestka j.-c. latombe and m.h. over- mars. probabilistic roadmaps for path planning in high- dimensional con\ufb01guration spaces. ieee transactions on robotics and automation 12 4 566 580 1996. 4 29 j.j. kuffner and s.m. lavalle. rrt-connect an ef\ufb01cient ap- proach to single-query path planning. in proceedings 2000 icra. millennium conference. ieee international confer- ence on robotics and automation. symposia proceedings cat. no.00ch37065 pages 995 1001 vol.2 2000. 4 30 steven m. lavalle. rapidly-exploring random trees a new tool for path planning. the annual research report 1998. 4 31 jan leike. why i m optimistic about our align- ment approach evaluation is easier than generation. https aligned.substack.com i 88447351 evaluation - is - easier - than - generation 2022. accessed 2024-11. 1 32 bo li yuanhan zhang dong guo renrui zhang feng li hao zhang kaichen zhang yanwei li ziwei liu and chunyuan li. llava-onevision easy visual task trans- fer. arxiv preprint arxiv 2408.03326 2024. 3 5 33 hao li xue yang zhaokai wang xizhou zhu jie zhou yu qiao xiaogang wang hongsheng li lewei lu and jifeng dai. auto mc-reward automated dense reward design with large language models for minecraft. in ieee cvf confer- ence on computer vision and pattern recognition 2024. 3 34 yifan li yifan du kun zhou jinpeng wang wayne xin zhao and ji-rong wen. evaluating object hallucination in large vision-language models. in the 2023 conference on empirical methods in natural language processing 2023. 4 35 yizhi li ge zhang yinghao ma ruibin yuan kang zhu hangyu guo yiming liang jiaheng liu zekun wang jian yang siwei wu xingwei qu jinjie shi xinyue zhang zhenzhu yang xiangzhou wang zhaoxiang zhang zachary liu emmanouil benetos wenhao huang and chenghua lin. omnibench towards the future of uni- versal omni-language models 2024. 4 36 hui sheng lim shuangshuang fan christopher k.h. chin shuhong chai neil bose and eonjoo kim. con- strained path planning of autonomous underwater vehicle us- ing selectively-hybridized particle swarm optimization algo- rithms. ifac-papersonline 52 21 315 322 2019. 12th ifac conference on control applications in marine sys- tems robotics and vehicles cams 2019. 2 37 haotian liu chunyuan li qingyang wu and yong jae lee. visual instruction tuning. in advances in neural information processing systems pages 34892 34916. curran associates inc. 2023. 3 38 haotian liu chunyuan li yuheng li and yong jae lee. improved baselines with visual instruction tuning 2024. 3 5 39 haotian liu chunyuan li yuheng li bo li yuanhan zhang sheng shen and yong jae lee. llava-next im- proved reasoning ocr and world knowledge 2024. 3 5 40 yuan liu haodong duan yuanhan zhang bo li songyang zhang wangbo zhao yike yuan jiaqi wang conghui he ziwei liu kai chen and dahua lin. mmbench is your multi-modal model an all-around player in computer vi- sion eccv 2024 pages 216 233 cham 2025. springer nature switzerland. 4 41 tom as lozano-p erez and michael a. wesley. an algorithm for planning collision-free paths among polyhedral obsta- cles. communications of the acm 22 10 560 570 1979. 4 42 pan lu hritik bansal tony xia jiacheng liu chunyuan li hannaneh hajishirzi hao cheng kai-wei chang michel galley and jianfeng gao. mathvista evaluating mathe- matical reasoning of foundation models in visual contexts. in the twelfth international conference on learning repre- sentations 2024. 3 6895 page 11 43 kenneth marino mohammad rastegari ali farhadi and roozbeh mottaghi. ok-vqa a visual question answering benchmark requiring external knowledge. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr 2019. 3 44 silin meng yiwei wang cheng-fu yang nanyun peng and kai-wei chang. llm-a large language model en- hanced incremental heuristic search on path planning. in findings of the association for computational linguistics emnlp 2024 pages 1087 1102 miami florida usa 2024. association for computational linguistics. 3 45 shota onohara atsuyuki miyai yuki imajuku kazuki egashira jeonghun baek xiang yue graham neubig and kiyoharu aizawa. jmmmu a japanese massive multi- discipline multimodal understanding benchmark for culture- aware evaluation 2024. 4 46 openai. gpt-4v ision system card. 2023. 7 47 openai. gpt-4o system card 2024. 3 5 48 openai. gpt-4 technical report 2024. 3 49 maxime oquab timoth ee darcet theo moutakanni huy v. vo marc szafraniec vasil khalidov pierre fernandez daniel haziza francisco massa alaaeldin el-nouby rus- sell howes po-yao huang hu xu vasu sharma shang- wen li wojciech galuba mike rabbat mido assran nico- las ballas gabriel synnaeve ishan misra herve jegou julien mairal patrick labatut armand joulin and piotr bo- janowski. dinov2 learning robust visual features without supervision 2023. 3 8 50 bowen pan rameswar panda souyoung jin rogerio feris aude oliva phillip isola and yoon kim. langnav lan- guage as a perceptual representation for navigation. in findings of the association for computational linguistics naacl 2024 pages 950 974 mexico city mexico 2024. association for computational linguistics. 1 3 51 chenbin pan burhaneddin yaman tommaso nesti abhirup mallik alessandro g allievi senem velipasalar and liu ren. vlp vision language planning for autonomous driving . in 2024 ieee cvf conference on computer vi- sion and pattern recognition cvpr pages 14760 14769 los alamitos ca usa 2024. ieee computer society. 1 3 52 clment petres yan pailhas pedro patron yvan petil- lot jonathan evans and david lane. path planning for autonomous underwater vehicles. ieee transactions on robotics 23 2 331 341 2007. 2 53 evis plaku erion plaku and patricio simari. clearance- driven motion planning for mobile robots with differential constraints. robotica 36 7 971 993 2018. 4 54 patrick a. plonski pratap tokekar and volkan isler. energy- ef\ufb01cient path planning for solar-powered mobile robots pages 717 731. springer international publishing heidel- berg 2013. 2 55 alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sas- try amanda askell pam mishkin jack clark gretchen krueger and ilya sutskever. learning transferable visual models from natural language supervision. proceedings of the 38th international conference on machine learning 2021. 3 8 56 christoph schuhmann romain beaumont richard vencu cade w gordon ross wightman mehdi cherti theo coombes aarush katta clayton mullis mitchell worts- man patrick schramowski srivatsa r kundurthy katherine crowson ludwig schmidt robert kaczmarczyk and jenia jitsev. laion-5b an open large-scale dataset for train- ing next generation image-text models. in thirty-sixth con- ference on neural information processing systems datasets and benchmarks track 2022. 3 8 57 jie-jing shao jiang-xin shi xiao-wen yang lan-zhe guo and yu-feng li. investigating the limitation of clip mod- els the worst-performing categories 2023. 3 58 tom silver soham dan kavitha srinivas joshua b. tenen- baum leslie pack kaelbling and michael katz. general- ized planning in pddl domains with pretrained large language models 2023. 3 59 ioan a. sucan mark moll and lydia e. kavraki. the open motion planning library. ieee robotics automa- tion magazine 19 4 72 82 2012. 4 60 yinan sun zicheng zhang haoning wu xiaohong liu weisi lin guangtao zhai and xiongkuo min. explore the hallucination on low-level perception for mllms 2024. 4 61 hao tan licheng yu and mohit bansal. learning to nav- igate unseen environments back translation with environ- mental dropout. in proceedings of the 2019 conference of the north american chapter of the association for computa- tional linguistics human language technologies volume 1 long and short papers pages 2610 2621 minneapolis minnesota 2019. association for computational linguis- tics. 3 62 gemini team. gemini a family of highly capable multi- modal models 2024. 3 63 shengbang tong erik jones and jacob steinhardt. mass- producing failures of multimodal systems with language models. in proceedings of the 37th international confer- ence on neural information processing systems red hook ny usa 2024. curran associates inc. 4 64 shengbang tong zhuang liu yuexiang zhai yi ma yann lecun and saining xie. eyes wide shut exploring the visual shortcomings of multimodal llms 2024. 1 3 4 65 hugo touvron louis martin kevin stone peter al- bert amjad almahairi yasmine babaei nikolay bash- lykov soumya batra prajjwal bhargava shruti bhos- ale dan bikel lukas blecher cristian canton ferrer moya chen guillem cucurull david esiobu jude fer- nandes jeremy fu wenyin fu brian fuller cynthia gao vedanuj goswami naman goyal anthony hartshorn saghar hosseini rui hou hakan inan marcin kardas vik- tor kerkez madian khabsa isabel kloumann artem ko- renev punit singh koura marie-anne lachaux thibaut lavril jenya lee diana liskovich yinghai lu yuning mao xavier martinet todor mihaylov pushkar mishra igor molybog yixin nie andrew poulton jeremy reizen- stein rashi rungta kalyan saladi alan schelten ruan silva eric michael smith ranjan subramanian xiao- qing ellen tan binh tang ross taylor adina williams 6896 page 12 jian xiang kuan puxin xu zheng yan iliyan zarov yuchen zhang angela fan melanie kambadur sharan narang aurelien rodriguez robert stojnic sergey edunov and thomas scialom. llama 2 open foundation and \ufb01ne- tuned chat models 2023. 3 66 miles turpin julian michael ethan perez and samuel r. bowman. language models don t always say what they think unfaithful explanations in chain-of-thought prompt- ing. in proceedings of the 37th international conference on neural information processing systems red hook ny usa 2024. curran associates inc. 6 67 karthik valmeekam alberto olmo sarath sreedharan and subbarao kambhampati. large language models still can t plan a benchmark for llms on planning and reasoning about change . in neurips 2022 foundation models for de- cision making workshop 2022. 3 68 karthik valmeekam kaya stechly and subbarao kamb- hampati. llms still can t plan can lrms a preliminary evaluation of openai s o1 on planbench 2024. 3 69 peng wang shuai bai sinan tan shijie wang zhihao fan jinze bai keqin chen xuejing liu jialin wang wenbin ge yang fan kai dang mengfei du xuancheng ren rui men dayiheng liu chang zhou jingren zhou and jun- yang lin. qwen2-vl enhancing vision-language model s perception of the world at any resolution. arxiv preprint arxiv 2409.12191 2024. 3 5 70 xin wang qiuyuan huang asli celikyilmaz jianfeng gao dinghan shen yuan-fang wang william yang wang and lei zhang. reinforced cross-modal matching and self- supervised imitation learning for vision-language navigation. in 2019 ieee cvf conference on computer vision and pat- tern recognition cvpr pages 6622 6631 2019. 3 71 jason wei xuezhi wang dale schuurmans maarten bosma brian ichter fei xia ed h. chi quoc v le and denny zhou. chain of thought prompting elicits reasoning in large language models. in advances in neural informa- tion processing systems 2022. 1 72 monika wysocza nska oriane sim eoni micha el ramamon- jisoa andrei bursuc tomasz trzci nski and patrick p erez. clip-dinoiser teaching clip a few dino tricks for open- vocabulary semantic segmentation. eccv 2024. 3 73 tianbao xie siheng zhao chen henry wu yitao liu qian luo victor zhong yanchao yang and tao yu. text2reward reward shaping with language models for reinforcement learning. in the twelfth international con- ference on learning representations 2024. 1 3 74 yutaro yamada yihan bao andrew kyle lampinen jungo kasai and ilker yildirim. evaluating spatial understanding of large language models. transactions on machine learn- ing research 2024. 3 75 zhutian yang caelan garrett dieter fox tom as lozano- p erez and leslie pack kaelbling. guiding long-horizon task and motion planning with vision language models 2024. 1 3 76 jiahui yu zirui wang vijay vasudevan legg yeung mo- jtaba seyedhosseini and yonghui wu. coca contrastive captioners are image-text foundation models. transactions on machine learning research 2022. 3 77 weihao yu zhengyuan yang linjie li jianfeng wang kevin lin zicheng liu xinchao wang and lijuan wang. mm-vet evaluating large multimodal models for integrated capabilities. in international conference on machine learn- ing. pmlr 2024. 4 78 xiang yue yuansheng ni kai zhang tianyu zheng ruoqi liu ge zhang samuel stevens dongfu jiang weiming ren yuxuan sun cong wei botao yu ruibin yuan ren- liang sun ming yin boyuan zheng zhenzhu yang yibo liu wenhao huang huan sun yu su and wenhu chen. mmmu a massive multi-discipline multimodal under- standing and reasoning benchmark for expert agi. in pro- ceedings of cvpr 2024. 4 79 xiang yue tianyu zheng yuansheng ni yubo wang kai zhang shengbang tong yuxuan sun botao yu ge zhang huan sun yu su wenhu chen and graham neubig. mmmu-pro a more robust multi-discipline multimodal understanding benchmark. arxiv preprint arxiv 2409.02813 2024. 4 80 xiaohua zhai basil mustafa alexander kolesnikov and lucas beyer. sigmoid loss for language image pre-training. in proceedings of the ieee cvf international conference on computer vision pages 11975 11986 2023. 3 8 81 ge zhang xinrun du bei chen yiming liang tongxu luo tianyu zheng kang zhu yuyang cheng chunpu xu shuyue guo haoran zhang xingwei qu junjie wang ruibin yuan yizhi li zekun wang yudong liu yu-hsuan tsai fengji zhang chenghua lin wenhao huang and jie fu. cmmmu a chinese massive multi-discipline multi- modal understanding benchmark 2024. 4 82 zhuosheng zhang aston zhang mu li hai zhao george karypis and alex smola. multimodal chain-of-thought rea- soning in language models 2024. 1 83 victor zhong dipendra misra xingdi yuan and marc- alexandre c\u02c6ot e. policy improvement using language feed- back models 2024. 1 3 84 yuke zhu oliver groth michael bernstein and li fei-fei. visual7w grounded question answering in images. in ieee conference on computer vision and pattern recog- nition 2016. 3 6897",
      "keywords": [
        "page",
        "evaluating",
        "vision-language",
        "models",
        "evaluators",
        "path",
        "planning",
        "mohamed",
        "aghzal1",
        "xiang",
        "yue2",
        "erion",
        "plaku3",
        "ziyu",
        "yao1",
        "1george",
        "mason",
        "university",
        "2carnegie",
        "mellon",
        "3national",
        "science",
        "foundation",
        "maghzal",
        "ziyuyao",
        "gmu.edu",
        "xyue2",
        "andrew.cmu.edu",
        "eplaku",
        "nsf.gov",
        "abstract",
        "despite",
        "promise",
        "perform",
        "complex",
        "reasoning",
        "large",
        "language",
        "llms",
        "shown",
        "limited",
        "effectiveness",
        "end-to-end",
        "planning.",
        "in-",
        "spired",
        "intriguing",
        "question",
        "cannot",
        "plan",
        "well",
        "still",
        "contribute",
        "framework",
        "helpful",
        "evaluator",
        "work",
        "generalize",
        "consider",
        "augmented",
        "visual",
        "under-",
        "standing",
        "i.e.",
        "vlms",
        "intro-",
        "duce",
        "patheval",
        "novel",
        "benchmark",
        "path-planning",
        "scenarios.",
        "suc-",
        "ceeding",
        "requires",
        "vlm",
        "able",
        "ab-",
        "stract",
        "traits",
        "optimal",
        "paths",
        "from",
        "scenario",
        "description",
        "demonstrate",
        "precise",
        "low-level",
        "perception",
        "each",
        "integrate",
        "information",
        "decide",
        "better",
        "path.",
        "analysis",
        "state-of-the-art",
        "reveals",
        "mod-",
        "els"
      ],
      "metadata": {
        "filename": "Aghzal_Evaluating_Vision-Language_Models_as_Evaluators_in_Path_Planning_CVPR_2025_paper.pdf",
        "original_filename": "Aghzal_Evaluating_Vision-Language_Models_as_Evaluators_in_Path_Planning_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 1520642,
        "upload_file_id": "upload_1752167758463_stelxhc6b",
        "upload_timestamp": "2025-07-10T17:15:58.495656",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T17:15:58.557793+00:00",
      "content_length": 63431,
      "content_type": ".pdf",
      "filename": "Aghzal_Evaluating_Vision-Language_Models_as_Evaluators_in_Path_Planning_CVPR_2025_paper.pdf"
    },
    "8": {
      "file_id": "8",
      "content": "\n=== Page 1 ===\nEfficient Event-Based Object Detection: A Hybrid Neural Network with Spatial\nand Temporal Attention\nSoikat Hasan Ahmed*, Jan Finkbeiner\u2020, Emre Neftci\nForschungszentrum J\u00a8ulich, RWTH Aachen University\n{s.ahmed, j.finkbeiner, e.neftci}@fz-juelich.de\nAbstract\nEvent cameras offer high temporal resolution and dy-\nnamic range with minimal motion blur, making them\npromising for robust object detection. While Spiking Neu-\nral Networks (SNNs) on neuromorphic hardware are often\nconsidered for energy efficient and low latency event-based\ndata processing, they often fall short of Artificial Neural\nNetworks (ANNs) in accuracy and flexibility. Here, we in-\ntroduce Attention-based Hybrid SNN-ANN backbones for\nevent-based object detection to leverage the strengths of\nboth SNN and ANN architectures. A novel Attention-based\nSNN-ANN bridge module captures sparse spatial and tem-\nporal relations from the SNN layer and converts them into\ndense feature maps for the ANN part of the backbone. Ad-\nditionally, we present a variant that integrates DWConvL-\nSTMs to the ANN blocks to capture slower dynamics. This\nmulti-timescale network combines fast SNN processing for\nshort timesteps with long-term dense RNN processing, ef-\nfectively capturing both fast and slow dynamics. Experi-\nmental results demonstrate that our proposed method sur-\npasses SNN-based approaches by significant margins, with\nresults comparable to existing ANN and RNN-based meth-\nods. Unlike ANN-only networks, the hybrid setup allows us\nto implement the SNN blocks on digital neuromorphic hard-\nware to investigate the feasibility of our approach. Exten-\nsive ablation studies and implementation on neuromorphic\nhardware confirm the effectiveness of our proposed modules\nand architectural choices. Our hybrid SNN-ANN architec-\ntures pave the way for ANN-like performance at a drasti-\ncally reduced parameter, latency, and power budget.\n1. Introduction\nOver the past decade, deep learning has made significant\nadvances in object detection. State-of-the-art approaches\npredominantly rely on frame-based cameras which cap-\n*Conceptual design, algorithm development and experimentation.\n\u2020Conceptual design, hardware analysis and deployment.\nture frames at a fixed rate. Frame cameras provide dense\nintensity data but have limitations in dynamic range and\nframe rates, leading to motion blur.\nDynamic Vision\nSensors (DVS), or event cameras, offer an alternative by\nasynchronously capturing pixel-level illumination changes,\nachieving low latency (\u223c10\u00b5s), higher temporal resolu-\ntion, and an extended dynamic range (140 dB vs. 60 dB)\n[12].\nThese characteristics make them well-suited for\nlow-light and fast-motion scenarios. However, due to the\nsparse, high-temporal-resolution data they generate, effec-\ntively processing event data for object detection remains a\nchallenging and emerging research area. Early adopters of\nevent-based object detection ANN models often naively re-\npurpose architectures originally designed for frame-based\ncameras [5, 18, 30, 35, 44]. ANN models generally achieve\ngood accuracy but tend to be large in terms of parameter\ncount and MAC operations, making them less suitable for\ndeployment on power-efficient edge or neuromorphic de-\nvices. Furthermore, the high sparsity and temporal reso-\nlution is often discarded in favor of dense representations\nto leverage GPUs\u2019 dense, vector-based representations. In\ncontrast, SNNs implemented in neuromorphic hardware are\nideally suited to leverage the sparsity of event-based inputs,\noffering significant reductions in computational cost, power\nconsumption, and latency [4, 8, 34]. However, SNNs tend\nto be less accurate at the task level compared to their ANN\ncounterparts.\nIn this work, we create a hybrid SNN-ANN-based back-\nbone architecture to combine the efficient, event-driven pro-\ncessing of SNNs on neuromorphic hardware with the effi-\ncient learning and representation capabilities of ANNs. The\nSNN extracts low-level features with high temporal resolu-\ntion from the event-based sensors and converts them into in-\ntermediate features, which then change to slower timescales\nbefore being processed by the ANN with dense activations.\nAdditionally, we feature a variant that adds DWConvL-\nSTMs [14, 36] to the ANN block.\nThis multi-timescale\nRNN variant combines the sparse SNN processing of short\ntimesteps with long time horizon processing via the dense\nRNN with the extracted long timesteps to efficiently cap-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n13970\n\n=== Page 2 ===\nture both fast and slow dynamics of the data. For hybrid\nmodels, the SNN can be efficiently deployed at the edge in\na power efficient manner, as we demonstrate with an imple-\nmentation on a digital neuromorphic chip. ANN processing\ncan occur either at the edge or, with reduced data rates, in\na cloud setting. By training the network jointly, the SNN\ncomponent can leverage backpropagated errors for efficient\ntraining via the surrogate gradient approach [31].\nInformation in SNNs is communicated by spike events.\nIn our hybrid models, these must be efficiently converted\ninto dense representations without discarding valuable spa-\ntiotemporal features. In our model, this is achieved by an\nattention-based SNN-ANN bridge module, which bridges\nSNN representations to ANN representations.\nThe attention module contains two attention modules\nnamed Event-Rate Spatial (ERS) attention and Spatial\nAware Temporal (SAT) attention. The SAT attention mod-\nule addresses the challenge of sparse event inputs by en-\nhancing the model\u2019s understanding of irregular structures\nand temporal attention to discern temporal relationships\nwithin the data. On the other hand, the ERS attention mod-\nule focuses on highlighting spatial areas by leveraging the\nactivity of events. Moreover, we implement the SNN blocks\nin digital neuromorphic hardware to demonstrate the feasi-\nbility of our approach. We report the performance of our\nmodel on large-scale event-based object detection bench-\nmarks. The contributions of our work can be summarized\nas follows:\n\u2022 A novel hybrid backbone-based event object detection\nmodel.\nTo the best of our knowledge, this is the first\nwork to propose a hybrid object detection approach for\nbenchmark object detection task. Evaluation on the Gen1\nand Gen4 Automotive Detection datasets [9, 33] shows\nthat the proposed method outperforms SNN-based meth-\nods and achieves comparable results to ANN and RNN-\nbased approaches.\n\u2022 An attention-based SNN-ANN bridge module (\u03b2asab) to\nconvert spatiotemporal spikes into a dense representation,\nenabling the ANN part of the network to process it effec-\ntively while preserving valuable spatial and temporal fea-\ntures through the Event-Rate Spatial (ERS) and Spatial-\nAware Temporal (SAT) attention mechanisms.\n\u2022 A multi-timescale RNN variant that includes both the\nhigh-temporal resolution SNN block followed by a\nslower, long time horizon DWConvLSTMs in the ANN\nblock, operating on larger timesteps extracted via the\n\u03b2asab module.\n\u2022 Implementation of the SNN blocks on digital neuromor-\nphic hardware to validate its performance and efficiency.\n2. Related Work\nRecent studies demonstrated the potential of event cameras\nin object detection tasks. In the earlier stages of adopting\nevent cameras, the focus primarily revolved around adapt-\ning existing frame-based feature extractors and a detection\nhead for object detection using event data [5, 18]. In [18],\nresearchers integrated event-based data into off-the-shelf\nframe-based object detection networks. They employed an\nInceptionNet-based backbone for feature extraction and a\nsingle-shot detector (SSD) for detection [29, 40]. Similarly,\n[33] utilized a frame-based object detection model called\nRetinaNet, which incorporates a spatial pooling-based fea-\nture extractor [27] along with a detection head, applied to\nevent data.\nAdditionally, methods such as [14, 25, 33] have incorpo-\nrated recurrent neural networks (RNNs) as feature extrac-\ntors for event data. [48] uses SSM to improve training time,\nand [42] proposes a training schema with efficient ground\ntruth label utilization. [30] introduced sparse convolution\nas a method for event feature extraction. To address the\nchallenges of efficiently extracting spatiotemporal features,\n[35] investigates the usability of a graph neural network-\nbased approach as a feature extractor.\nRecently, SNN-based methods have become popular for\nevent data processing due to their spike-based working prin-\nciple, similar to event cameras, which enables efficient\nprocessing.\nResearch conducted by [6] and [39] show-\ncases the effective utilization of SNNs in object detection\ntasks. Specifically, [6] and [39] delve into assessing the\nperformance of converting widely-used ANN-based back-\nbone architectures such as SqueezeNet [19], VGG [37],\nMobileNet [16], DenseNet [17], and ResNet [15] into SNN\narchitecture for event-based object detection. Nonetheless,\noptimizing intermediate and high-level features for detec-\ntion with SNNs results in a significant drop in accuracy.\nRecognizing the distinct advantages offered by both\nSNNs and ANNs, researchers have explored merging these\nnetworks into hybrid architectures [23].\nFor instance,\n[47] presents a framework that leverages hierarchical infor-\nmation abstraction for meta-continual learning with inter-\npretable multimodal reasoning. Building on this idea, [43]\nintroduces DashNet, which integrates SNNs with ANN-\nbased feature extraction for high-speed object tracking.\nSimilarly, [28] improves SNN performance through a hy-\nbrid top-down attention mechanism, while [24] demon-\nstrates that hybrid models can achieve energy-efficient op-\ntical flow estimation with enhanced robustness. Comple-\nmenting these advances, [2, 23] develops an architecture\nthat fuses SNN backbones with ANN heads for event-based\nvision tasks. By leveraging the complementary strengths of\neach, these hybrid networks show promise for simpler tasks.\nHowever, the bridge between SNNs and ANNs is still over-\nlooked to harness the best of both worlds.\nMoreover, the full extent of their capabilities remains\nlargely unexplored, especially in tackling state-of-the-art\nbenchmark vision tasks, such as object detection on popu-\n13971\n\n=== Page 3 ===\nlar datasets like Gen1 Automotive Detection dataset [9] and\nGen4 Automotive Detection dataset [33].\n3. Hybrid Object Detection Network\nThe overall hybrid network as shown in Figure 1 com-\nprises two key parts: an attention-based hybrid backbone\ndesigned to extract spatio-temporal features, and detection\nheads tasked with identifying objects. In the following sec-\ntion, we will delve into the details of the core components\nof the network.\n3.1. Event Representation\nAn event is represented as en = (xn, yn, tn, pn), where\n(xn, yn) is the pixel location, tn is the time, and pn is polar-\nity which indicates the change in light intensity (i.e., posi-\ntive or negative). The event data is pre-processed to convert\nit into a time-space tensor format. Following [14], we start\nby creating a 4D tensor Events[tk\u22121, tk] \u2208RT \u00d72\u00d7H\u00d7W ,\nwhere T represents number of time discretization steps, 2\ndenotes polarity features which contain the count of posi-\ntive and negative events in each discretized time step, and\nH and W signify the height and width of the event cam-\nera, respectively. Given the event set E = {e1, e2, . . . , eN},\nthe event tensor Events[tk\u22121, tk] is constructed from the\ndiscretized time variable t\u2032\nn =\nj\ntn\u2212ta\ntb\u2212ta \u00b7 T\nk\nas follows:\nEvents[tk\u22121, tk](t, p, x, y) =\nX\nen\u2208E\n\u03b4 (p \u2212pn) \u03b4 (x \u2212xn)\n\u03b4 (y \u2212yn) \u03b4 (t \u2212t\u2032\nn) .\n(1)\nWhile training, event tensors are created. However, during\ninference, given an input sparsity of \u223c98% (for Gen 1),\nthis results in significant efficiency gains due to sparse pro-\ncessing in neuromorphic hardware compared to the dense\nprocessing in a GPU.\n3.2. Attention-based Hybrid Backbone\nThe proposed hybrid backbone architecture, as shown in\nFigure 1, consists of three fundamental components: a\nlow-level spatio-temporal feature extractor fsnn, an ANN-\nbased high-level spatial feature extractor fann, and a novel\nAttention-based SNN-ANN Bridge (ASAB) module \u03b2asab.\nThe first module, denoted as fsnn, is an event-level\nfeature extractor operating in the spatio-temporal domain\nand consists of multiple convolutional SNN blocks. Each\nblock follows a structured sequence of operations: stan-\ndard convolution, batch normalization [20], and Paramet-\nric Leaky Integration and Fire (PLIF) spiking neuron [10].\nThe neural dynamics of PILF with trainable time constant\n\u03c4 = sigmoid(w)\u22121 given input X[t] can be expressed as\nfollows:\nV [t] = V [t \u22121] + 1\n\u03c4 (X[t] \u2212(V [t \u22121] \u2212Vreset).\n(2)\nThe fsnn module receives Events[tk\u22121, tk] as its input\nand generates events Espike = fsnn( Events[tk\u22121, tk]) \u2208\nRT \u00d7C\u00d7H\u2032\u00d7W \u2032.\nAs SNNs operate on a faster timescale\nand utilize sparse representations and ANNs operate\non dense representations, efficiently translating valuable\nspatio-temporal information into dense representations is\nessential. To achieve this translation, the Espike is subse-\nquently fed into a proposed \u03b2asab module, which bridges\nthe SNN and the ANN parts. The events Espike are con-\nverted into dense, non-binary features while preserving spa-\ntial and temporal information in the form of spatial fea-\nture maps. The output of \u03b2asab is represented by Fout =\n\u03b2asab(Espike), with dimensions C\u00d7H\u2032\u00d7W \u2032 which is com-\npatible with traditional 2D convolution-based networks, al-\nlowing for smooth processing and integration of informa-\ntion across both spatial and temporal dimensions. The at-\ntention module is further described in Section 3.3.\nThe third component, fann, extracts high-level spatial\nfeatures using multiple ANN blocks with standard ANN\ncomponents. Each ANN block consists of standard convo-\nlution operations, normalization [3, 20], and ReLU activa-\ntion functions, enabling the extraction of detailed high-level\nspatial features from the densely encoded Fout.\nIn addition to the proposed model, we explore a vari-\nant that features an added RNN module, incorporating two\nDepth-Wise separable Convolutional LSTM (DWConvL-\nSTM) units similar to those in [14], as illustrated in Figure\n1.\nThe fsnn processes fast dynamics with small timesteps\nfrom the event-based camera, while the DWConvLSTM op-\nerates on larger timesteps extracted from the \u03b2asab-module\nto capture slower dynamics. The resulting outputs from the\nANN blocks are then fed to the detection head for the final\nobject detection output.\n3.3. Attention-based SNN-ANN Bridge Module\nThe bridge module \u03b2asab comprises two attention modules:\ni) Spatial-aware Temporal (SAT) attention and ii) Event-\nRate Spatial (ERS) attention. The SAT attention module dy-\nnamically captures local spatial context within the irregular\nspatial spike-structure to uncover temporal relations. Mean-\nwhile, the ERS attention submodule focuses on attending to\nspatial areas utilizing the spatial event activities. Below, we\ndescribe these two submodules.\n3.3.1\nSpatial-aware Temporal (SAT) Attention\nThe SAT attention contains three crucial operations:\ni)\nChannel-wise Temporal Grouping to group relevant fea-\ntures from different time dimensions, ii) Time-wise Separa-\nble Deformable Convolution (TSDC) denoted as \u03a6tsdc for\ncapturing channel-independent local spatial context from\nsparse spike features, and iii) Temporal attention module\n13972\n\n=== Page 4 ===\nSNN Block\nSNN Block\nSNN Block\nSNN Block\nANN Block\nANN Block\nANN Block\nANN Block\nDWConvLSTM\nDWConvLSTM\nDetection\nFPN + Head\nTo Tensor\nTime\nFigure 1. Architecture of the hybrid model with object detection head and SNN-ANN hybrid backbone, including the SNN part, \u03b2asab\nbridge module and ANN part. The DWConvLSTM modules and dashed blue arrows are only part of the proposed hybrid + RNN variant.\nSoftmax\nMatmul\nMatmul\noffsets\nlearned\noffset field\noffset learning\nConv 1x1 + ReLu\nConv 1x1\nConv 1x1\nConv 1x1\n(a) Spatial-aware Temporal Attention.\nSigmoid\nSpatial-Aware Temporal\u00a0\nAttenton\nHadamard\nproduct\nSum(dim=T)\n(b) Event-rate Spatial Attention.\nFigure 2. Visualization of the proposed attention module\u2019s components. (a) Spatial-aware Temporal Attention: Highlights relevant temporal\nfeatures in spatial regions to enhance temporal coherence in event-based data. (b) Event-rate Spatial Attention: Emphasizes spatial regions\nbased on event rates, allowing for adaptive focus on areas with significant event activity. Together, these components improve feature\nextraction in spatiotemporal data processing.\n\u03a6ta, which uses local spatial context features to extract tem-\nporal relations to accumulate and translate temporal infor-\nmation into spatial information.\nTime-wise\nSeparable\nDeformable\nConvolutions\n(TSDC):\nAt first, we apply the channel-wise tempo-\nral grouping operation to the input data so that each\nfeature channel is processed separately while capturing\nspatial and temporal relations. This operation transforms\nthe input spike tensor Espike\n\u2208\nRT \u00d7C\u00d7H\u2032\u00d7W \u2032 into\nAin \u2208RC\u00d7T \u00d7H\u2032\u00d7W \u2032.\nAs shown in Figure 2a, the \u03a6tsdc operation operates on\nindividual channel-wise temporal groups Ain[ci, :, :, :], de-\nnoted as Ac, where i is the channel index. This operation\nextracts the local spatial context of the sparse, irregularly\nshaped spike-based representations. We posit that this irreg-\nular representation is better extracted using a deformed ker-\nnel rather than a standard square grid kernel, as discussed in\nSection 4.3. We implemented the TSDC as a time-wise sep-\narable convolution to capture spatial details independently\nof time-based changes, as motion occurs over time. Isolat-\ning spatial aspects enables a clearer understanding of the\nstructure and layout of features, separate from their move-\nment.\nThe time-wise separated spatial context is then passed\nto the \u03a6ta module for further processing to determine the\ntemporal relation of different time dimensions.\nFor the implementation of TSDC, we utilize deformable\nconvolution introduced by [7] which adjusts sampling\npoints on the standard grid by dynamically predicting ker-\nnel offsets based on input features. During training, an ad-\nditional convolutional layer called \u201doffset learning\u201d (refer to\nFigure 2a) is trained to predict these offsets. Moreover, to\nindependently process each temporal dimension, we set the\ngroup of deformable convolution kernels equal to the num-\nber of time steps T. This is done to encourage the network\nto focus on the spatial context of the data while maintaining\ntemporal relations intact for further processing.\nTemporal Attention:\nTo learn relationships between dif-\nferent time steps, we pass the local spatial context Asc =\n\u03a6tsdc(Ac) through a temporal attention module. This mod-\nule leverages the multi-head softmax self-attention mecha-\nnism introduced in [41]. In our case, we apply self-attention\nalong the temporal dimension to extract temporal relations.\nFirstly, we calculate the keys, queries, and values for\ntemporal self-attention by employing 1 \u00d7 1 convolutions,\nfollowed by a reshape operation, which we denote as Ak,\n13973\n\n=== Page 5 ===\nAq, and Av, respectively. These operations output tensors of\nshapes RH\u2032W \u2032\u00d7T , RT \u00d7H\u2032W \u2032, and RT \u00d7H\u2032W \u2032, respectively.\nAk = \u03c9k(Asc),\nAq = \u03a8(\u03c9q(Asc)),\nAv = \u03a8(\u03c9v(Asc))\n(3)\nWhere \u03a8 denotes the reshape operation and \u03c9 denotes the\n1 \u00d7 1 convolution operation.\nNext, the temporal atten-\ntion scores denoted as Ascore \u2208RT \u00d7T are computed by\nperforming matrix multiplication between Aq and Ak, fol-\nlowed by applying a softmax operation:\nAscore = softmax (AqAk) .\n(4)\nTo obtain the attended temporal features, Av is multiplied\nwith Ascore, followed by a reshape operation to output\nAattended \u2208RT \u00d7H\u2032\u00d7W \u2032:\nAattended = \u03a8(AvAscore).\n(5)\nFinally, a weighted-sum along the temporal dimension us-\ning a 1 \u00d7 1 convolution produces the output Aout[c, :, :] \u2208\nRH\u2032\u00d7W \u2032. This operation effectively combines the attended\ntemporal features to produce the final output.\n3.3.2\nEvent-rate Spatial Attention\nThis attention module extracts spatial correlation as spatial\nweights, utilizing dynamic event activity from intermediate\nspikes generated by the fsnn module. To identify active\nregions, an Event-rate Spatial Attention mechanism takes\nthe input Espike, and sums the time dimension to calculate\nthe event rates Srate, resulting in a shape of RC\u00d7H\u2032\u00d7W \u2032:\nSrate = PT\nt=1 Espike(t, :, :, :). The Srate is first normal-\nized using a sigmoid function to provide a spatial attention\nscore based on the event activity. This attention score is then\nutilized as a weight to adjust the output of the SAT module\nthrough a Hadamard product, as visualized in Figure 2b:\nEfeature = sigmoid(Srate) \u2299Aout\n(6)\nThe resulting tensor Fout is then fed into ANN blocks,\nwhich are subsequently utilized to predict the object detec-\ntion bounding box by a detection head [13].\n4. Experiments\n4.1. Setup\nDatasets:\nTo conduct the training and evaluation of our\nnetwork, we utilized two event-based object detection\ndatasets: Gen1 Automotive Detection dataset [9] and Gen4\nAutomotive Detection dataset [33]. The Gen1 and Gen4\ndatasets comprise 39 and 15 hours of event camera record-\nings at a resolution of 304 \u00d7 240 and 720 \u00d7 1280, respec-\ntively, with bounding box annotations for car, pedestrian,\nand two-wheeler (Gen4 only) classes.\nImplementation Details:\nThe model is implemented in\nPyTorch [32] with the SpikingJelly library [11] and trained\nend-to-end for 50 epochs on the Gen 1 dataset and 10\nepochs on the Gen 4 dataset. The ADAM optimizer [22]\nis used with a OneCycle learning rate schedule [38], which\ndecays linearly from a set maximum learning rate. The ker-\nnel size for \u03a6tsdc is set to 5. The training pipeline incorpo-\nrates data augmentation methods such as random horizon-\ntal flips, zoom, and crop, based on [14]. Event representa-\ntions for the SNN are constructed from 5 ms bins. During\ntraining, object detections are generated every 50 ms, us-\ning the SNN\u2019s output from the last 10 time bins, while in-\nference allows higher temporal resolution, bounded by the\nSNN timestep. The YOLOX framework [13] is used for\nobject detection, incorporating IOU loss, class loss, and re-\ngression loss. For the Gen 1 dataset, models are trained with\na batch size of 24 and a learning rate of 2 \u00d7 10\u22124, requiring\napproximately 8 hours on four 3090 GPUs. On the Gen 4\ndataset, the batch size is 8 with a learning rate of 3.5\u00d710\u22124,\ntaking around 1.5 days on four 3090 GPUs.\nWhen using an RNN variant, we follow previous meth-\nods with a sequence length of 21 for fair comparison. This\nRNN-based network, trained for 400,000 steps with a batch\nsize of 2, requires approximately 6 days to complete train-\ning.\n4.2. Benchmark comparisons\nComparison Design\nTo the best of our knowledge, this\nwork presents the first hybrid object detection model imple-\nmented in large-scale benchmark datasets, rendering com-\nparisons to other work challenging. Therefore, we design\nour comparison in three setups - (i) comparison with exist-\ning ANN-based methods (ii) comparison with SNN-based\nobject detection methods, and (iii) comparison with RNN-\nbased models.\nEvaluation Procedure:\nFollowing the evaluation proto-\ncol established in prior studies [6, 14, 33], the mean aver-\nage precision (mAP) [26] is used as the primary evaluation\nmetric to compare the proposed methods\u2019 effectiveness with\nexisting approaches. Since most methods do not offer open-\nsource code, the reported numbers from the corresponding\npapers were used.\nComparison design with ANN-based methods:\nThe ef-\nficacy of the proposed method was evaluated against ANN-\nbased models.\nThe results presented in Table 1 provide\na compelling comparison of various ANN-based networks\nand performance on the event-based Gen 1 dataset. No-\ntably, the Proposed hybrid model stands out with only 6.6M\nparameters, significantly smaller than other models such\nas SparseConv (133M) and RRC-Events (100M). Despite\n13974\n\n=== Page 6 ===\nTable 1. Comparative analysis of various ANN-based models for event-based object detection on the Gen1 [9] and Gen4 [33] Automotive\nDetection datasets, where mAP denotes mAP(.5:.05:.95). A\u2217suggests that this information was not directly available and estimated based\non the publication.\nGen 1\nGen 4\nModels\nType\nParams\nmAP\nmAP\nAEGNN [35]\nGNN\n20M\n0.16\n-\nSparseConv [30]\nANN\n133M\n0.15\n-\nInception + SSD [18]\nANN\n> 60M \u2217\n0.3\n0.34\nRRC-Events [5]\nANN\n> 100M \u2217\n0.31\n0.34\nEvents-RetinaNet [33]\nANN\n33M\n0.34\n0.18\nE2Vid-RetinaNet [33]\nANN\n44M\n0.27\n.25\nRVT-B W/O LSTM [14]\nTransformer\n16.2M \u2217\n0.32\n-\nProposed\nHybrid\n6.6M\n0.35\n.27\nTable 2. Comparative analysis of various SNN-based models for\nevent-based object detection on the Gen1 Automotive Detection\ndataset .\nModels\nType\nParams\nmAP\nVGG-11+SDD [6]\nSNN\n13M\n0.17\nMobileNet-64+SSD [6]\nSNN\n24M\n0.15\nDenseNet121-24+SSD [6]\nSNN\n8M\n0.19\nFP-DAGNet[45]\nSNN\n22M\n0.22\nEMS-RES10 [39]\nSNN\n6.20M\n0.27\nEMS-RES18 [39]\nSNN\n9.34M\n0.29\nEMS-RES34 [39]\nSNN\n14.4M\n0.31\nSpikeFPN [46]\nSNN\n22M\n0.22\nProposed\nHybrid\n6.6M\n0.35\nTable 3. Comprehensive evaluation of different RNN-based mod-\nels for event-based object detection tasks on the Gen1 Automotive\nDetection dataset. Here \u2019TF\u2019 denotes Transformer.\nModels\nType\nParams\nmAP\nS4D-ViT-B [48]\nTF + SSM\n16.5M\n0.46\nS5-ViT-B [48]\nTF + SSM\n18.2M\n0.48\nS5-ViT-S [48]\nTF + SSM\n9.7M\n0.47\nRVT-B [14]\nTF + RNN\n19M\n0.47\nRVT-S [14]\nTF + RNN\n10M\n0.46\nRVT-T [14]\nTF + RNN\n4M\n0.44\nASTMNet [25]\n(T)CNN + RNN\n100M\n0.48\nRED [33]\nCNN + RNN\n24M\n0.40\nProposed+RNN\nHybrid + RNN\n7.7M\n0.43\nits compact size, our proposed model achieves an accu-\nracy of 0.35, outperforming larger models like SparseC-\nonv, which achieves 0.15, and closely matching the perfor-\nmance of Events-RetinaNet (33M, 0.34). In contrast, for the\nGen 4 dataset, the analysis also includes architectures like\nEvents-RetinaNet and E2Vid-RetinaNet. Events-RetinaNet\nachieves a lower mean Average Precision (mAP) of 0.18,\nwhile E2Vid-RetinaNet performs slightly better with an\nmAP of 0.25. The AEGNN model, which utilizes a graph\nneural network approach, achieves an mAP of 0.16 with\n20 million parameters; however, its performance is over-\nshadowed by the proposed hybrid model, which achieves\nan mAP of 0.27 while maintaining a compact size of only\n6.6 million parameters. We observe that larger models tend\nto achieve higher accuracy due to their increased parame-\nter counts. However, their significant size makes them less\nsuitable for deployment on hardware with limited resources,\nsuch as edge devices or neuromorphic systems.\nComparison Design with SNN-based Methods:\nThe\nproposed hybrid method was compared against sev-\neral SNN-based methods, specifically, VGG-11+SDD [6],\nMobileNet-64+SSD [6], DenseNet121-24+SSD [6], FP-\nDAGNet[45], EMS-RES10 [39], EMS-RES18 [39], EMS-\nRES34 [39] and SpikeFPN [46].\nTable 2 displays the comparison results with vari-\nous state-of-the-art SNN-based object detection methods.\nMore specifically, VGG-11+SSD and MobileNet-64+SSD\nachieve mAP values of 0.17 and 0.15 with parameter counts\nof 13M and 24M, respectively.\nDenseNet121-24+SSD,\nwith a smaller parameter size of 8M, slightly outper-\nforms these models with an mAP of 0.19.\nFP-DAGNet\nand SpikeFPN, both at 22M parameters, attain an mAP\nof 0.22.\nThe EMS-RES series showcases incremental\nimprovements, with EMS-RES10 (6.2M) achieving 0.27,\nEMS-RES18 (9.34M) at 0.29, and EMS-RES34 (14.4M)\nachieving the highest mAP among SNNs at 0.31. In con-\n13975\n\n=== Page 7 ===\ntrast, the Proposed Hybrid Model surpasses all these SNN\nmodels with an mAP of 0.35 while maintaining an efficient\nparameter size of only 6.6M. This superiority can be at-\ntributed to our method\u2019s incorporation of a hybrid feature\nextraction approach with both spatial and temporal atten-\ntion modules, which are lacking in other methods.\nComparison design with RNN and SSM-based methods:\nAlthough the performance of the RNN-based models gen-\nerally outperforms models with spiking components, this\ncomparison aims to investigate how the proposed hybrid\nmodel is comparable to the RNN. We provide comparisons\nwith RNN-based methods such as RED [33], ASTMNet\n[25], RVT-B [14], RVT-S [14], RVT-T [14], S4D-ViT-B\n[48], S5-ViT-B [48], and S5-ViT-S [48]. Table 3 presents\na comparison of results obtained with RNN-based models.\nIt can be seen that two of the works, RED [33] and ASTM-\nNet [25], have substantially larger parameter counts and are\ntherefore expected to perform better.\nRVT [14] demon-\nstrates good accuracy at a parameter count comparable to\nthe proposed hybrid network.\nOur fully recurrent back-\nbone allows high-frequency detection without re-computing\nrecurrent layers, unlike RVT\u2019s non-causal attention which\nrequires re-computation for every prediction.\nCNNs are\nmore efficient than MLPs at small batch sizes due to higher\narithmetic intensity; in our method, less than 5% of MACs\nare from attention and MLPs versus 67% in RVT which is\nharder to deploy on energy-efficient edge and neuromorphic\nhardware.\n4.3. Ablation Study\nTable 4. Ablation study for ASAB module.\nModels\nmAP(.5)\nmAP\nVariant 1(w/o - \u03a6ta)\n0.57\n0.33\nVariant 2 (w/o deform)\n0.59\n0.34\nVariant 3 (w/o - ESA)\n0.59\n0.34\nVariant 4 (w/o - ASAB)\n0.53\n0.30\nVariant 5 (Proposed)\n0.61\n0.35\nASAB module:\nThe ablation study highlights the im-\nportance of each ASAB module component in enhancing\nmodel accuracy on the Gen1 Automotive Detection dataset.\nIn Table 4, Variant 1 (excluding \u03a8ta) achieves an mAP of\n0.33, revealing reduced temporal capture. In variant 2, we\nreplaced the deformable convolution with a standard con-\nvolution, which shows irregular sampling helps with sparse\ndata. In variant 3, removing the ERS module shows some\naccuracy drop, indicating limited spatial flexibility and at-\ntention. Variant 4, replacing ASAB with a simple accumu-\nlation operation, results in the lowest mAP of 0.30. The\ncomplete model (Variant 5) reaches the highest mAP of\n0.35, emphasizing the value of each component. Figure 3\nillustrates how the bridge module enhances detection by re-\nducing false predictions. Additionally, we performed an ab-\nlation study on various DWConvLSTM configurations by\ntoggling layers. The proposed setting achieved a mAP of\n0.43. Please refer to the supplementary (Table ??) for more\ndetails .\n(a) W/O ASAB module (b) W/ ASAB module\n(c) Ground Truth\nFigure 3. Visual comparison of object detection outputs between\nthe baseline hybrid event object detection method (left) and the\nproposed method (right) for the Gen 4 dataset. From left to right:\n(a) object detection output without the ASAB module, (b) object\ndetection output with the ASAB module, and (c) ground truth ob-\nject boundaries. More samples are in the Supplementary docu-\nment.\n5. Hardware Implementation, Energy and\nComputational Efficiency Analysis\n5.1. Hardware Implementation\nTable 5. Power and time measurements of the SNN block on Loihi\n2 for several input sizes and number of weight bits. The power is\nmeasured in Watts and the execution time per step is in millisec-\nonds. The mean and standard deviation of the measurements aver-\naged over 12 inputs for a total of 100k steps are reported.\nWeight\nquant.\n# chips\nPower [W]\nTime/Step\nint8\n6\n1.73 \u00b1 0.10\n2.06\nint6\n6\n1.71 \u00b1 0.11\n2.06\nint4\n6\n1.95 \u00b1 0.33\n1.16\nIn order to demonstrate the suitability of the chosen hy-\nbrid SNN-ANN approach for energy efficient inference on\nthe edge, we implemented the SNN backbone in hardware.\nIn the proposed architecture the SNN block transforms sen-\nsor data into intermediate representations and therefore un-\nderlies the strictest latency requirements. Due to the clear\n13976\n\n=== Page 8 ===\nTable 6. Comparison of different baselines complexities.\nModels\nmAP(.5)\nMACs\nACs\nBaselineann\n0.61\n15.34e9\n0.0\nBaselinew/o \u03b2asab\n0.53\n1.18e9\n0.97e9\nProposedw/\u03b2asab\n0.61\n1.63e9\n0.97e9\nProposedsnn+\n0.58\n0.87e9\n1.59e9\nseparation between SNN and ANN parts in the model\u2019s\narchitecture, the SNN blocks can be implemented in spe-\ncialized hardware.\nAs hardware, we chose Intel\u2019s Loihi\n2 [8], a digital, programmable chip based on event-based\ncommunication and computation. Only minor adjustments\nare necessary for execution on Loihi 2: The kernel-weights\nof the convolutional layers are quantized to int8 via a\nper-output-channel quantization scheme showing no result-\ning loss in accuracy (mAP: 0.348 (float16) vs 0.343\n(int8)). The batchnorm (BN) operations and quantization\nscaling are fused into the LIF-neuron dynamics by scaling\nand shifting the inputs according to the follwing equations:\nscale =\nqscale weightBN\n\u03c4 \u221aVarBN + \u03b5BN\n(7)\nshift = (biasconv \u2212meanBN)\nweightBN\n\u03c4 \u221aVarBN + \u03b5BN\n+ biasBN\n\u03c4\n(8)\nwhere qscale is the scaling factor introduced by the quan-\ntization and \u03c4 is the PLIF neurons time constant. Given\nthis approach, spike times are almost exactly reproduced\non Loihi 2 compared to the PyTorch int8 implementa-\ntion. For benchmarking purposes, the inputs to the network\nare simulated with an additional neuron population, due to\nthe current IO limitations of the chip. With this approach\nthe spiking statistics in the input and SNN layers are repro-\nduced.\nTable 5 reports power and time measurement results of\nthe 4-layer SNN block running on Loihi 2 for inputs of\nsize (2, 256, 160). The network runs at (1.7 \u00b1 0.1) W and\n(1.9 \u00b1 0.8) ms per step, which is faster than real-time in\nthe currently chosen architecture (5 ms per step).\nThese\nresults compare favorably to commercially available chips\nfor edge computing like the NVIDIA Jetson Orin Nano\n(7 W\u221215 W) [1] and demonstrate the suitability of an SNN\nbackbone for event-based data processing.\n5.2. Computational Analysis\nWe trained a variant of our model where the modified PLIF\nneuron acts like a ReLU with proposed attention mod-\nThis research was funded by the German Federal Ministry of\nEducation and Research (BMBF) under the projects \u201dGreenEdge-\nFuE\u201d (16ME0521) and \u201dCluster4Future\u201d (03ZU1106CB). Access to\nJUWELS [21] has been granted by GCS (www.gauss-centre.eu) under\nproject neuroml. We thank Intel for access to Loihi 2.\nule to investigate a comparison between a similar artifi-\ncial neural network (ANN) and the hybrid network. This\nvariant, Baselineann, with a mean 15.34 \u00d7 109 multiply-\naccumulate operations (MACs), makes it resource-intensive\nand unsuitable for energy-constrained hardware. In con-\ntrast, our Proposed hybrid model computes only 1.63 \u00d7\n109 MACs, making it more practical for edge devices.\nThe \u03b2asab module does incur additional operations but\nleads to significant accuracy improvement (compare to\nBaselinew/o \u03b2asab). Additionally, the spiking neural net-\nwork (SNN) variant, Proposedsnn+ (Increasing one SNN\nlayer and reducing one ANN layer), reduces computational\ndemands further to 0.87\u00d7109 MACs and is highly efficient\non neuromorphic hardware, running significantly faster and\nwith less energy on Intel\u2019s Loihi 2 compared to the dense-\nactivation ANNs.\nWe analyze power consumption across methods follow-\ning [2].\nAmong SNNs, DenseNet121+SSD uses 0.9 mJ\n(0.0 MACs, 2.3 \u00d7 109 ACs), while VGG-11+SSD requires\n4.2 mJ (11.1 \u00d7 109 ACs). In ANNs, Inception+SSD is the\nmost demanding at 19.3 mJ (11.4 \u00d7 109 MACs). Events-\nRetinaNet consumes 5.4 mJ (3.2 \u00d7 109 MACs), and RVT-B\nW/O LSTM requires 3.9 mJ (2.3\u00d7109 MACs). Our method\nachieves 1.6\u00d7109 MACs, 1.0\u00d7109 ACs, and 3.1 mJ, signif-\nicantly reducing energy and computational costs compared\nto most ANNs.\n6. Conclusion\nIn this work, we introduced a hybrid attention-based SNN-\nANN backbone for event-based visual object detection.\nA novel attention-based SNN-ANN bridge module is\nproposed to capture sparse spatial and temporal relations\nfrom the SNN layer and convert them into dense feature\nmaps for the ANN part of the backbone.\nAdditionally,\nwe demonstrate the effectiveness of combining RNNs\non multiple timescales: hardware-efficient SNNs for fast\ndynamics on short timescales with ConvLSTMs for longer\ntimescales that operate on the extracted features of the\nbridge-module. Experimental results demonstrate that our\nproposed method surpasses baseline hybrid and SNN-based\napproaches by significant margins, with results comparable\nto existing ANN-based methods.\nThe efficacy of our\nproposed modules and architectural choices is confirmed\nthrough extensive ablation studies.\nAdditionally, we\ndemonstrate the effectiveness of our architectural choice\nwith separate SNN and ANN blocks by implementing the\nSNN blocks on digital neuromorphic hardware, Intel\u2019s\nLoihi 2.\nThe neuromorphic hardware implementation\nachieves sub-real-time processing and improved power\nconsumption compared to commercially available edge\ncomputing hardware. The achieved accuracy and hardware\nimplementation results pave the way toward a hybrid\nSNN-ANN architecture that achieves ANN-like perfor-\nmance at a drastically reduced parameter and power budget.\n13977\n\n=== Page 9 ===\nReferences\n[1] Nvidia jetson orin nano datasheet, revision 4. https://\nopenzeka.com/wp-content/uploads/2023/03/\njetson- orin- nano- datasheet- r4- web.pdf,\n2023. Accessed: 2025-03-09. 8\n[2] Asude Aydin, Mathias Gehrig, Daniel Gehrig, and Da-\nvide Scaramuzza.\nA hybrid ann-snn architecture for low-\npower and low-latency visual perception. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops (CVPRW), 2024. 2, 8\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 3\n[4] Francesco Barchi, Gianvito Urgese, Alessandro Siino, Santa\nDi Cataldo, Enrico Macii, and Andrea Acquaviva.\nFlexi-\nble on-line reconfiguration of multi-core neuromorphic plat-\nforms. IEEE Transactions on Emerging Topics in Comput-\ning, 9(2):915\u2013927, 2019. 1\n[5] Nicholas FY Chen. Pseudo-labels for supervised learning\non dynamic vision sensor data, applied to object detection\nunder ego-motion. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition workshops, pages\n644\u2013653, 2018. 1, 2, 6\n[6] Lo\u00a8\u0131c Cordone, Beno\u02c6\u0131t Miramond, and Philippe Thierion. Ob-\nject detection with spiking neural networks on automotive\nevent data. In 2022 International Joint Conference on Neu-\nral Networks (IJCNN), pages 1\u20138. IEEE, 2022. 2, 5, 6\n[7] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In Proceedings of the IEEE international confer-\nence on computer vision, pages 764\u2013773, 2017. 4\n[8] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham\nChinya, Yongqiang Cao, Sri Harsha Choday, Georgios Di-\nmou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi:\nA neuromorphic manycore processor with on-chip learning.\nIeee Micro, 38(1):82\u201399, 2018. 1, 8\n[9] Pierre De Tournemire, Davide Nitti, Etienne Perot, Da-\nvide Migliore, and Amos Sironi.\nA large scale event-\nbased detection dataset for automotive.\narXiv preprint\narXiv:2001.08499, 2020. 2, 3, 5, 6\n[10] Wei Fang, Zhaofei Yu, Yanqi Chen, Timoth\u00b4ee Masquelier,\nTiejun Huang, and Yonghong Tian. Incorporating learnable\nmembrane time constant to enhance learning of spiking neu-\nral networks. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pages 2661\u20132671,\n2021. 3\n[11] Wei Fang, Yanqi Chen, Jianhao Ding, Zhaofei Yu, Timoth\u00b4ee\nMasquelier, Ding Chen, Liwei Huang, Huihui Zhou, Guoqi\nLi, and Yonghong Tian. Spikingjelly: An open-source ma-\nchine learning infrastructure platform for spike-based intel-\nligence. Science Advances, 9(40):eadi1480, 2023. 5\n[12] Guillermo Gallego, Tobi Delbr\u00a8uck, Garrick Orchard, Chiara\nBartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger,\nAndrew J Davison, J\u00a8org Conradt, Kostas Daniilidis, et al.\nEvent-based vision: A survey. IEEE transactions on pattern\nanalysis and machine intelligence, 44(1):154\u2013180, 2020. 1\n[13] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\nSun. Yolox: Exceeding yolo series in 2021. arXiv preprint\narXiv:2107.08430, 2021. 5\n[14] Mathias Gehrig and Davide Scaramuzza. Recurrent vision\ntransformers for object detection with event cameras. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 13884\u201313893, 2023. 1, 2, 3,\n5, 6, 7\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 2\n[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017. 2\n[17] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger.\nDensely connected convolutional net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4700\u20134708, 2017. 2\n[18] Massimiliano Iacono, Stefan Weber, Arren Glover, and\nChiara Bartolozzi.\nTowards event-driven object detection\nwith off-the-shelf deep learning. In 2018 IEEE/RSJ Interna-\ntional Conference on Intelligent Robots and Systems (IROS),\npages 1\u20139. IEEE, 2018. 1, 2, 6\n[19] Forrest N Iandola, Song Han, Matthew W Moskewicz,\nKhalid Ashraf,\nWilliam J Dally,\nand Kurt Keutzer.\nSqueezenet:\nAlexnet-level accuracy with 50x fewer pa-\nrameters and\u00a1 0.5 mb model size.\narXiv preprint\narXiv:1602.07360, 2016. 2\n[20] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In International conference on machine learn-\ning, pages 448\u2013456. pmlr, 2015. 3\n[21] J\u00a8ulich Supercomputing Centre.\nJUWELS Cluster and\nBooster: Exascale Pathfinder with Modular Supercomputing\nArchitecture at Juelich Supercomputing Centre. Journal of\nlarge-scale research facilities, 7(A138), 2021. 8\n[22] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 5\n[23] Alexander Kugele, Thomas Pfeil, Michael Pfeiffer, and Elis-\nabetta Chicca. Hybrid snn-ann: Energy-efficient classifica-\ntion and object detection for event-based vision. In DAGM\nGerman Conference on Pattern Recognition, pages 297\u2013312.\nSpringer, 2021. 2\n[24] Chankyu Lee, Adarsh Kumar Kosta, Alex Zihao Zhu, Ken-\nneth Chaney, Kostas Daniilidis, and Kaushik Roy. Spike-\nflownet: event-based optical flow estimation with energy-\nefficient hybrid neural networks. In European Conference\non Computer Vision, pages 366\u2013382. Springer, 2020. 2\n[25] Jianing Li, Jia Li, Lin Zhu, Xijie Xiang, Tiejun Huang, and\nYonghong Tian. Asynchronous spatio-temporal memory net-\nwork for continuous event-based object detection.\nIEEE\nTransactions on Image Processing, 31:2975\u20132987, 2022. 2,\n6, 7\n13978\n\n=== Page 10 ===\n[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 5\n[27] Tsung-Yi Lin, Piotr Doll\u00b4ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie.\nFeature pyra-\nmid networks for object detection.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 2117\u20132125, 2017. 2\n[28] Faqiang Liu and Rong Zhao. Enhancing spiking neural net-\nworks with hybrid top-down attention. Frontiers in Neuro-\nscience, 16:949142, 2022. 2\n[29] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\nSzegedy, Scott Reed, Cheng-Yang Fu, and Alexander C\nBerg.\nSsd: Single shot multibox detector.\nIn Computer\nVision\u2013ECCV 2016: 14th European Conference, Amster-\ndam, The Netherlands, October 11\u201314, 2016, Proceedings,\nPart I 14, pages 21\u201337. Springer, 2016. 2\n[30] Nico Messikommer, Daniel Gehrig, Antonio Loquercio, and\nDavide Scaramuzza. Event-based asynchronous sparse con-\nvolutional networks. In Computer Vision\u2013ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part VIII 16, pages 415\u2013431. Springer, 2020.\n1, 2, 6\n[31] E. O. Neftci, H. Mostafa, and F. Zenke. Surrogate gradient\nlearning in spiking neural networks: Bringing the power of\ngradientbased optimization to spiking neural networks. IEEE\nSignal Processing Magazine, 36(6):5163, 2019. 2\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library. Ad-\nvances in neural information processing systems, 32, 2019.\n5\n[33] Etienne Perot, Pierre De Tournemire, Davide Nitti, Jonathan\nMasci, and Amos Sironi. Learning to detect objects with a\n1 megapixel event camera. Advances in Neural Information\nProcessing Systems, 33:16639\u201316652, 2020. 2, 3, 5, 6, 7\n[34] Jun Sawada, Filipp Akopyan, Andrew S Cassidy, Brian Taba,\nMichael V Debole, Pallab Datta, Rodrigo Alvarez-Icaza,\nArnon Amir, John V Arthur, Alexander Andreopoulos, et al.\nTruenorth ecosystem for brain-inspired computing: scalable\nsystems, software, and applications. In SC\u201916: Proceedings\nof the International Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 130\u2013141.\nIEEE, 2016. 1\n[35] Simon Schaefer, Daniel Gehrig, and Davide Scaramuzza.\nAegnn: Asynchronous event-based graph neural networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 12371\u201312381, 2022. 1,\n2, 6\n[36] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung,\nWai-Kin Wong, and Wang-chun Woo. Convolutional lstm\nnetwork:\nA machine learning approach for precipitation\nnowcasting. Advances in neural information processing sys-\ntems, 28, 2015. 1\n[37] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 2\n[38] Leslie N Smith and Nicholay Topin.\nSuper-convergence:\nVery fast training of neural networks using large learn-\ning rates.\nIn Artificial intelligence and machine learning\nfor multi-domain operations applications, pages 369\u2013386.\nSPIE, 2019. 5\n[39] Qiaoyi Su, Yuhong Chou, Yifan Hu, Jianing Li, Shijie Mei,\nZiyang Zhang, and Guoqi Li. Deep directly-trained spik-\ning neural networks for object detection.\nIn Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 6555\u20136565, 2023. 2, 6\n[40] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich.\nGoing deeper with\nconvolutions.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1\u20139, 2015.\n2\n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 4\n[42] Ziyi Wu, Mathias Gehrig, Qing Lyu, Xudong Liu, and Igor\nGilitschenski.\nLeod: Label-efficient object detection for\nevent cameras. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16933\u2013\n16943, 2024. 2\n[43] Zheyu Yang, Yujie Wu, Guanrui Wang, Yukuan Yang, Guoqi\nLi, Lei Deng, Jun Zhu, and Luping Shi. Dashnet: A hybrid\nartificial and spiking neural network for high-speed object\ntracking. arXiv preprint arXiv:1909.12942, 2019. 2\n[44] Jason Yik, Soikat Hasan Ahmed, Zergham Ahmed, Brian\nAnderson, Andreas G Andreou, Chiara Bartolozzi, Arindam\nBasu, Douwe den Blanken, Petrut Bogdan, Sander Bohte,\net al.\nNeurobench: Advancing neuromorphic computing\nthrough collaborative, fair and representative benchmarking.\narXiv preprint arXiv:2304.04640, 2023. 1\n[45] Hu Zhang, Luziwei Leng, Kaiwei Che, Qian Liu, Jie\nCheng, Qinghai Guo, Jiangxing Liao, and Ran Cheng.\nAutomotive object detection via learning sparse events by\ntemporal dynamics of spiking neurons.\narXiv preprint\narXiv:2307.12900, 2023. 6\n[46] Hu Zhang, Yanchen Li, Luziwei Leng, Kaiwei Che, Qian\nLiu, Qinghai Guo, Jianxing Liao, and Ran Cheng. Auto-\nmotive object detection via learning sparse events by spiking\nneurons. IEEE Transactions on Cognitive and Developmen-\ntal Systems, 2024. 6\n[47] Rong Zhao, Zheyu Yang, Hao Zheng, Yujie Wu, Faqiang\nLiu, Zhenzhi Wu, Lukai Li, Feng Chen, Seng Song, Jun Zhu,\net al. A framework for the general design and computation\nof hybrid neural networks. Nature communications, 13(1):\n3427, 2022. 2\n[48] Nikola Zubic, Mathias Gehrig, and Davide Scaramuzza.\nState space models for event cameras.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5819\u20135828, 2024. 2, 6, 7\n13979\n",
      "searchable_content": "page 1 efficient event-based object detection a hybrid neural network with spatial and temporal attention soikat hasan ahmed jan finkbeiner emre neftci forschungszentrum j ulich rwth aachen university s.ahmed j.finkbeiner e.neftci fz-juelich.de abstract event cameras offer high temporal resolution and dy- namic range with minimal motion blur making them promising for robust object detection. while spiking neu- ral networks snns on neuromorphic hardware are often considered for energy efficient and low latency event-based data processing they often fall short of artificial neural networks anns in accuracy and flexibility. here we in- troduce attention-based hybrid snn-ann backbones for event-based object detection to leverage the strengths of both snn and ann architectures. a novel attention-based snn-ann bridge module captures sparse spatial and tem- poral relations from the snn layer and converts them into dense feature maps for the ann part of the backbone. ad- ditionally we present a variant that integrates dwconvl- stms to the ann blocks to capture slower dynamics. this multi-timescale network combines fast snn processing for short timesteps with long-term dense rnn processing ef- fectively capturing both fast and slow dynamics. experi- mental results demonstrate that our proposed method sur- passes snn-based approaches by significant margins with results comparable to existing ann and rnn-based meth- ods. unlike ann-only networks the hybrid setup allows us to implement the snn blocks on digital neuromorphic hard- ware to investigate the feasibility of our approach. exten- sive ablation studies and implementation on neuromorphic hardware confirm the effectiveness of our proposed modules and architectural choices. our hybrid snn-ann architec- tures pave the way for ann-like performance at a drasti- cally reduced parameter latency and power budget. 1. introduction over the past decade deep learning has made significant advances in object detection. state-of-the-art approaches predominantly rely on frame-based cameras which cap- conceptual design algorithm development and experimentation. conceptual design hardware analysis and deployment. ture frames at a fixed rate. frame cameras provide dense intensity data but have limitations in dynamic range and frame rates leading to motion blur. dynamic vision sensors dvs or event cameras offer an alternative by asynchronously capturing pixel-level illumination changes achieving low latency 10\u00b5s higher temporal resolu- tion and an extended dynamic range 140 db vs. 60 db 12 . these characteristics make them well-suited for low-light and fast-motion scenarios. however due to the sparse high-temporal-resolution data they generate effec- tively processing event data for object detection remains a challenging and emerging research area. early adopters of event-based object detection ann models often naively re- purpose architectures originally designed for frame-based cameras 5 18 30 35 44 . ann models generally achieve good accuracy but tend to be large in terms of parameter count and mac operations making them less suitable for deployment on power-efficient edge or neuromorphic de- vices. furthermore the high sparsity and temporal reso- lution is often discarded in favor of dense representations to leverage gpus dense vector-based representations. in contrast snns implemented in neuromorphic hardware are ideally suited to leverage the sparsity of event-based inputs offering significant reductions in computational cost power consumption and latency 4 8 34 . however snns tend to be less accurate at the task level compared to their ann counterparts. in this work we create a hybrid snn-ann-based back- bone architecture to combine the efficient event-driven pro- cessing of snns on neuromorphic hardware with the effi- cient learning and representation capabilities of anns. the snn extracts low-level features with high temporal resolu- tion from the event-based sensors and converts them into in- termediate features which then change to slower timescales before being processed by the ann with dense activations. additionally we feature a variant that adds dwconvl- stms 14 36 to the ann block. this multi-timescale rnn variant combines the sparse snn processing of short timesteps with long time horizon processing via the dense rnn with the extracted long timesteps to efficiently cap- this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 13970 page 2 ture both fast and slow dynamics of the data. for hybrid models the snn can be efficiently deployed at the edge in a power efficient manner as we demonstrate with an imple- mentation on a digital neuromorphic chip. ann processing can occur either at the edge or with reduced data rates in a cloud setting. by training the network jointly the snn component can leverage backpropagated errors for efficient training via the surrogate gradient approach 31 . information in snns is communicated by spike events. in our hybrid models these must be efficiently converted into dense representations without discarding valuable spa- tiotemporal features. in our model this is achieved by an attention-based snn-ann bridge module which bridges snn representations to ann representations. the attention module contains two attention modules named event-rate spatial ers attention and spatial aware temporal sat attention. the sat attention mod- ule addresses the challenge of sparse event inputs by en- hancing the model s understanding of irregular structures and temporal attention to discern temporal relationships within the data. on the other hand the ers attention mod- ule focuses on highlighting spatial areas by leveraging the activity of events. moreover we implement the snn blocks in digital neuromorphic hardware to demonstrate the feasi- bility of our approach. we report the performance of our model on large-scale event-based object detection bench- marks. the contributions of our work can be summarized as follows a novel hybrid backbone-based event object detection model. to the best of our knowledge this is the first work to propose a hybrid object detection approach for benchmark object detection task. evaluation on the gen1 and gen4 automotive detection datasets 9 33 shows that the proposed method outperforms snn-based meth- ods and achieves comparable results to ann and rnn- based approaches. an attention-based snn-ann bridge module \u03b2asab to convert spatiotemporal spikes into a dense representation enabling the ann part of the network to process it effec- tively while preserving valuable spatial and temporal fea- tures through the event-rate spatial ers and spatial- aware temporal sat attention mechanisms. a multi-timescale rnn variant that includes both the high-temporal resolution snn block followed by a slower long time horizon dwconvlstms in the ann block operating on larger timesteps extracted via the \u03b2asab module. implementation of the snn blocks on digital neuromor- phic hardware to validate its performance and efficiency. 2. related work recent studies demonstrated the potential of event cameras in object detection tasks. in the earlier stages of adopting event cameras the focus primarily revolved around adapt- ing existing frame-based feature extractors and a detection head for object detection using event data 5 18 . in 18 researchers integrated event-based data into off-the-shelf frame-based object detection networks. they employed an inceptionnet-based backbone for feature extraction and a single-shot detector ssd for detection 29 40 . similarly 33 utilized a frame-based object detection model called retinanet which incorporates a spatial pooling-based fea- ture extractor 27 along with a detection head applied to event data. additionally methods such as 14 25 33 have incorpo- rated recurrent neural networks rnns as feature extrac- tors for event data. 48 uses ssm to improve training time and 42 proposes a training schema with efficient ground truth label utilization. 30 introduced sparse convolution as a method for event feature extraction. to address the challenges of efficiently extracting spatiotemporal features 35 investigates the usability of a graph neural network- based approach as a feature extractor. recently snn-based methods have become popular for event data processing due to their spike-based working prin- ciple similar to event cameras which enables efficient processing. research conducted by 6 and 39 show- cases the effective utilization of snns in object detection tasks. specifically 6 and 39 delve into assessing the performance of converting widely-used ann-based back- bone architectures such as squeezenet 19 vgg 37 mobilenet 16 densenet 17 and resnet 15 into snn architecture for event-based object detection. nonetheless optimizing intermediate and high-level features for detec- tion with snns results in a significant drop in accuracy. recognizing the distinct advantages offered by both snns and anns researchers have explored merging these networks into hybrid architectures 23 . for instance 47 presents a framework that leverages hierarchical infor- mation abstraction for meta-continual learning with inter- pretable multimodal reasoning. building on this idea 43 introduces dashnet which integrates snns with ann- based feature extraction for high-speed object tracking. similarly 28 improves snn performance through a hy- brid top-down attention mechanism while 24 demon- strates that hybrid models can achieve energy-efficient op- tical flow estimation with enhanced robustness. comple- menting these advances 2 23 develops an architecture that fuses snn backbones with ann heads for event-based vision tasks. by leveraging the complementary strengths of each these hybrid networks show promise for simpler tasks. however the bridge between snns and anns is still over- looked to harness the best of both worlds. moreover the full extent of their capabilities remains largely unexplored especially in tackling state-of-the-art benchmark vision tasks such as object detection on popu- 13971 page 3 lar datasets like gen1 automotive detection dataset 9 and gen4 automotive detection dataset 33 . 3. hybrid object detection network the overall hybrid network as shown in figure 1 com- prises two key parts an attention-based hybrid backbone designed to extract spatio-temporal features and detection heads tasked with identifying objects. in the following sec- tion we will delve into the details of the core components of the network. 3.1. event representation an event is represented as en xn yn tn pn where xn yn is the pixel location tn is the time and pn is polar- ity which indicates the change in light intensity i.e. posi- tive or negative . the event data is pre-processed to convert it into a time-space tensor format. following 14 we start by creating a 4d tensor events tk 1 tk rt 2 h w where t represents number of time discretization steps 2 denotes polarity features which contain the count of posi- tive and negative events in each discretized time step and h and w signify the height and width of the event cam- era respectively. given the event set e e1 e2 . . . en the event tensor events tk 1 tk is constructed from the discretized time variable t n j tn ta tb ta t k as follows events tk 1 tk t p x y x en e \u03b4 p pn \u03b4 x xn \u03b4 y yn \u03b4 t t n . 1 while training event tensors are created. however during inference given an input sparsity of 98 for gen 1 this results in significant efficiency gains due to sparse pro- cessing in neuromorphic hardware compared to the dense processing in a gpu. 3.2. attention-based hybrid backbone the proposed hybrid backbone architecture as shown in figure 1 consists of three fundamental components a low-level spatio-temporal feature extractor fsnn an ann- based high-level spatial feature extractor fann and a novel attention-based snn-ann bridge asab module \u03b2asab. the first module denoted as fsnn is an event-level feature extractor operating in the spatio-temporal domain and consists of multiple convolutional snn blocks. each block follows a structured sequence of operations stan- dard convolution batch normalization 20 and paramet- ric leaky integration and fire plif spiking neuron 10 . the neural dynamics of pilf with trainable time constant \u03c4 sigmoid w 1 given input x t can be expressed as follows v t v t 1 1 \u03c4 x t v t 1 vreset . 2 the fsnn module receives events tk 1 tk as its input and generates events espike fsnn events tk 1 tk rt c h w . as snns operate on a faster timescale and utilize sparse representations and anns operate on dense representations efficiently translating valuable spatio-temporal information into dense representations is essential. to achieve this translation the espike is subse- quently fed into a proposed \u03b2asab module which bridges the snn and the ann parts. the events espike are con- verted into dense non-binary features while preserving spa- tial and temporal information in the form of spatial fea- ture maps. the output of \u03b2asab is represented by fout \u03b2asab espike with dimensions c h w which is com- patible with traditional 2d convolution-based networks al- lowing for smooth processing and integration of informa- tion across both spatial and temporal dimensions. the at- tention module is further described in section 3.3. the third component fann extracts high-level spatial features using multiple ann blocks with standard ann components. each ann block consists of standard convo- lution operations normalization 3 20 and relu activa- tion functions enabling the extraction of detailed high-level spatial features from the densely encoded fout. in addition to the proposed model we explore a vari- ant that features an added rnn module incorporating two depth-wise separable convolutional lstm dwconvl- stm units similar to those in 14 as illustrated in figure 1. the fsnn processes fast dynamics with small timesteps from the event-based camera while the dwconvlstm op- erates on larger timesteps extracted from the \u03b2asab-module to capture slower dynamics. the resulting outputs from the ann blocks are then fed to the detection head for the final object detection output. 3.3. attention-based snn-ann bridge module the bridge module \u03b2asab comprises two attention modules i spatial-aware temporal sat attention and ii event- rate spatial ers attention. the sat attention module dy- namically captures local spatial context within the irregular spatial spike-structure to uncover temporal relations. mean- while the ers attention submodule focuses on attending to spatial areas utilizing the spatial event activities. below we describe these two submodules. 3.3.1 spatial-aware temporal sat attention the sat attention contains three crucial operations i channel-wise temporal grouping to group relevant fea- tures from different time dimensions ii time-wise separa- ble deformable convolution tsdc denoted as \u03c6tsdc for capturing channel-independent local spatial context from sparse spike features and iii temporal attention module 13972 page 4 snn block snn block snn block snn block ann block ann block ann block ann block dwconvlstm dwconvlstm detection fpn head to tensor time figure 1. architecture of the hybrid model with object detection head and snn-ann hybrid backbone including the snn part \u03b2asab bridge module and ann part. the dwconvlstm modules and dashed blue arrows are only part of the proposed hybrid rnn variant. softmax matmul matmul offsets learned offset field offset learning conv 1x1 relu conv 1x1 conv 1x1 conv 1x1 a spatial-aware temporal attention. sigmoid spatial-aware temporal attenton hadamard product sum dim t b event-rate spatial attention. figure 2. visualization of the proposed attention module s components. a spatial-aware temporal attention highlights relevant temporal features in spatial regions to enhance temporal coherence in event-based data. b event-rate spatial attention emphasizes spatial regions based on event rates allowing for adaptive focus on areas with significant event activity. together these components improve feature extraction in spatiotemporal data processing. \u03c6ta which uses local spatial context features to extract tem- poral relations to accumulate and translate temporal infor- mation into spatial information. time-wise separable deformable convolutions tsdc at first we apply the channel-wise tempo- ral grouping operation to the input data so that each feature channel is processed separately while capturing spatial and temporal relations. this operation transforms the input spike tensor espike rt c h w into ain rc t h w . as shown in figure 2a the \u03c6tsdc operation operates on individual channel-wise temporal groups ain ci de- noted as ac where i is the channel index. this operation extracts the local spatial context of the sparse irregularly shaped spike-based representations. we posit that this irreg- ular representation is better extracted using a deformed ker- nel rather than a standard square grid kernel as discussed in section 4.3. we implemented the tsdc as a time-wise sep- arable convolution to capture spatial details independently of time-based changes as motion occurs over time. isolat- ing spatial aspects enables a clearer understanding of the structure and layout of features separate from their move- ment. the time-wise separated spatial context is then passed to the \u03c6ta module for further processing to determine the temporal relation of different time dimensions. for the implementation of tsdc we utilize deformable convolution introduced by 7 which adjusts sampling points on the standard grid by dynamically predicting ker- nel offsets based on input features. during training an ad- ditional convolutional layer called offset learning refer to figure 2a is trained to predict these offsets. moreover to independently process each temporal dimension we set the group of deformable convolution kernels equal to the num- ber of time steps t. this is done to encourage the network to focus on the spatial context of the data while maintaining temporal relations intact for further processing. temporal attention to learn relationships between dif- ferent time steps we pass the local spatial context asc \u03c6tsdc ac through a temporal attention module. this mod- ule leverages the multi-head softmax self-attention mecha- nism introduced in 41 . in our case we apply self-attention along the temporal dimension to extract temporal relations. firstly we calculate the keys queries and values for temporal self-attention by employing 1 1 convolutions followed by a reshape operation which we denote as ak 13973 page 5 aq and av respectively. these operations output tensors of shapes rh w t rt h w and rt h w respectively. ak \u03c9k asc aq \u03c8 \u03c9q asc av \u03c8 \u03c9v asc 3 where \u03c8 denotes the reshape operation and \u03c9 denotes the 1 1 convolution operation. next the temporal atten- tion scores denoted as ascore rt t are computed by performing matrix multiplication between aq and ak fol- lowed by applying a softmax operation ascore softmax aqak . 4 to obtain the attended temporal features av is multiplied with ascore followed by a reshape operation to output aattended rt h w aattended \u03c8 avascore . 5 finally a weighted-sum along the temporal dimension us- ing a 1 1 convolution produces the output aout c rh w . this operation effectively combines the attended temporal features to produce the final output. 3.3.2 event-rate spatial attention this attention module extracts spatial correlation as spatial weights utilizing dynamic event activity from intermediate spikes generated by the fsnn module. to identify active regions an event-rate spatial attention mechanism takes the input espike and sums the time dimension to calculate the event rates srate resulting in a shape of rc h w srate pt t 1 espike t . the srate is first normal- ized using a sigmoid function to provide a spatial attention score based on the event activity. this attention score is then utilized as a weight to adjust the output of the sat module through a hadamard product as visualized in figure 2b efeature sigmoid srate aout 6 the resulting tensor fout is then fed into ann blocks which are subsequently utilized to predict the object detec- tion bounding box by a detection head 13 . 4. experiments 4.1. setup datasets to conduct the training and evaluation of our network we utilized two event-based object detection datasets gen1 automotive detection dataset 9 and gen4 automotive detection dataset 33 . the gen1 and gen4 datasets comprise 39 and 15 hours of event camera record- ings at a resolution of 304 240 and 720 1280 respec- tively with bounding box annotations for car pedestrian and two-wheeler gen4 only classes. implementation details the model is implemented in pytorch 32 with the spikingjelly library 11 and trained end-to-end for 50 epochs on the gen 1 dataset and 10 epochs on the gen 4 dataset. the adam optimizer 22 is used with a onecycle learning rate schedule 38 which decays linearly from a set maximum learning rate. the ker- nel size for \u03c6tsdc is set to 5. the training pipeline incorpo- rates data augmentation methods such as random horizon- tal flips zoom and crop based on 14 . event representa- tions for the snn are constructed from 5 ms bins. during training object detections are generated every 50 ms us- ing the snn s output from the last 10 time bins while in- ference allows higher temporal resolution bounded by the snn timestep. the yolox framework 13 is used for object detection incorporating iou loss class loss and re- gression loss. for the gen 1 dataset models are trained with a batch size of 24 and a learning rate of 2 10 4 requiring approximately 8 hours on four 3090 gpus. on the gen 4 dataset the batch size is 8 with a learning rate of 3.5 10 4 taking around 1.5 days on four 3090 gpus. when using an rnn variant we follow previous meth- ods with a sequence length of 21 for fair comparison. this rnn-based network trained for 400 000 steps with a batch size of 2 requires approximately 6 days to complete train- ing. 4.2. benchmark comparisons comparison design to the best of our knowledge this work presents the first hybrid object detection model imple- mented in large-scale benchmark datasets rendering com- parisons to other work challenging. therefore we design our comparison in three setups - i comparison with exist- ing ann-based methods ii comparison with snn-based object detection methods and iii comparison with rnn- based models. evaluation procedure following the evaluation proto- col established in prior studies 6 14 33 the mean aver- age precision map 26 is used as the primary evaluation metric to compare the proposed methods effectiveness with existing approaches. since most methods do not offer open- source code the reported numbers from the corresponding papers were used. comparison design with ann-based methods the ef- ficacy of the proposed method was evaluated against ann- based models. the results presented in table 1 provide a compelling comparison of various ann-based networks and performance on the event-based gen 1 dataset. no- tably the proposed hybrid model stands out with only 6.6m parameters significantly smaller than other models such as sparseconv 133m and rrc-events 100m . despite 13974 page 6 table 1. comparative analysis of various ann-based models for event-based object detection on the gen1 9 and gen4 33 automotive detection datasets where map denotes map .5 .05 .95 . a suggests that this information was not directly available and estimated based on the publication. gen 1 gen 4 models type params map map aegnn 35 gnn 20m 0.16 - sparseconv 30 ann 133m 0.15 - inception ssd 18 ann 60m 0.3 0.34 rrc-events 5 ann 100m 0.31 0.34 events-retinanet 33 ann 33m 0.34 0.18 e2vid-retinanet 33 ann 44m 0.27 .25 rvt-b w o lstm 14 transformer 16.2m 0.32 - proposed hybrid 6.6m 0.35 .27 table 2. comparative analysis of various snn-based models for event-based object detection on the gen1 automotive detection dataset . models type params map vgg-11 sdd 6 snn 13m 0.17 mobilenet-64 ssd 6 snn 24m 0.15 densenet121-24 ssd 6 snn 8m 0.19 fp-dagnet 45 snn 22m 0.22 ems-res10 39 snn 6.20m 0.27 ems-res18 39 snn 9.34m 0.29 ems-res34 39 snn 14.4m 0.31 spikefpn 46 snn 22m 0.22 proposed hybrid 6.6m 0.35 table 3. comprehensive evaluation of different rnn-based mod- els for event-based object detection tasks on the gen1 automotive detection dataset. here tf denotes transformer. models type params map s4d-vit-b 48 tf ssm 16.5m 0.46 s5-vit-b 48 tf ssm 18.2m 0.48 s5-vit-s 48 tf ssm 9.7m 0.47 rvt-b 14 tf rnn 19m 0.47 rvt-s 14 tf rnn 10m 0.46 rvt-t 14 tf rnn 4m 0.44 astmnet 25 t cnn rnn 100m 0.48 red 33 cnn rnn 24m 0.40 proposed rnn hybrid rnn 7.7m 0.43 its compact size our proposed model achieves an accu- racy of 0.35 outperforming larger models like sparsec- onv which achieves 0.15 and closely matching the perfor- mance of events-retinanet 33m 0.34 . in contrast for the gen 4 dataset the analysis also includes architectures like events-retinanet and e2vid-retinanet. events-retinanet achieves a lower mean average precision map of 0.18 while e2vid-retinanet performs slightly better with an map of 0.25. the aegnn model which utilizes a graph neural network approach achieves an map of 0.16 with 20 million parameters however its performance is over- shadowed by the proposed hybrid model which achieves an map of 0.27 while maintaining a compact size of only 6.6 million parameters. we observe that larger models tend to achieve higher accuracy due to their increased parame- ter counts. however their significant size makes them less suitable for deployment on hardware with limited resources such as edge devices or neuromorphic systems. comparison design with snn-based methods the proposed hybrid method was compared against sev- eral snn-based methods specifically vgg-11 sdd 6 mobilenet-64 ssd 6 densenet121-24 ssd 6 fp- dagnet 45 ems-res10 39 ems-res18 39 ems- res34 39 and spikefpn 46 . table 2 displays the comparison results with vari- ous state-of-the-art snn-based object detection methods. more specifically vgg-11 ssd and mobilenet-64 ssd achieve map values of 0.17 and 0.15 with parameter counts of 13m and 24m respectively. densenet121-24 ssd with a smaller parameter size of 8m slightly outper- forms these models with an map of 0.19. fp-dagnet and spikefpn both at 22m parameters attain an map of 0.22. the ems-res series showcases incremental improvements with ems-res10 6.2m achieving 0.27 ems-res18 9.34m at 0.29 and ems-res34 14.4m achieving the highest map among snns at 0.31. in con- 13975 page 7 trast the proposed hybrid model surpasses all these snn models with an map of 0.35 while maintaining an efficient parameter size of only 6.6m. this superiority can be at- tributed to our method s incorporation of a hybrid feature extraction approach with both spatial and temporal atten- tion modules which are lacking in other methods. comparison design with rnn and ssm-based methods although the performance of the rnn-based models gen- erally outperforms models with spiking components this comparison aims to investigate how the proposed hybrid model is comparable to the rnn. we provide comparisons with rnn-based methods such as red 33 astmnet 25 rvt-b 14 rvt-s 14 rvt-t 14 s4d-vit-b 48 s5-vit-b 48 and s5-vit-s 48 . table 3 presents a comparison of results obtained with rnn-based models. it can be seen that two of the works red 33 and astm- net 25 have substantially larger parameter counts and are therefore expected to perform better. rvt 14 demon- strates good accuracy at a parameter count comparable to the proposed hybrid network. our fully recurrent back- bone allows high-frequency detection without re-computing recurrent layers unlike rvt s non-causal attention which requires re-computation for every prediction. cnns are more efficient than mlps at small batch sizes due to higher arithmetic intensity in our method less than 5 of macs are from attention and mlps versus 67 in rvt which is harder to deploy on energy-efficient edge and neuromorphic hardware. 4.3. ablation study table 4. ablation study for asab module. models map .5 map variant 1 w o - \u03c6ta 0.57 0.33 variant 2 w o deform 0.59 0.34 variant 3 w o - esa 0.59 0.34 variant 4 w o - asab 0.53 0.30 variant 5 proposed 0.61 0.35 asab module the ablation study highlights the im- portance of each asab module component in enhancing model accuracy on the gen1 automotive detection dataset. in table 4 variant 1 excluding \u03c8ta achieves an map of 0.33 revealing reduced temporal capture. in variant 2 we replaced the deformable convolution with a standard con- volution which shows irregular sampling helps with sparse data. in variant 3 removing the ers module shows some accuracy drop indicating limited spatial flexibility and at- tention. variant 4 replacing asab with a simple accumu- lation operation results in the lowest map of 0.30. the complete model variant 5 reaches the highest map of 0.35 emphasizing the value of each component. figure 3 illustrates how the bridge module enhances detection by re- ducing false predictions. additionally we performed an ab- lation study on various dwconvlstm configurations by toggling layers. the proposed setting achieved a map of 0.43. please refer to the supplementary table for more details . a w o asab module b w asab module c ground truth figure 3. visual comparison of object detection outputs between the baseline hybrid event object detection method left and the proposed method right for the gen 4 dataset. from left to right a object detection output without the asab module b object detection output with the asab module and c ground truth ob- ject boundaries. more samples are in the supplementary docu- ment. 5. hardware implementation energy and computational efficiency analysis 5.1. hardware implementation table 5. power and time measurements of the snn block on loihi 2 for several input sizes and number of weight bits. the power is measured in watts and the execution time per step is in millisec- onds. the mean and standard deviation of the measurements aver- aged over 12 inputs for a total of 100k steps are reported. weight quant. chips power w time step int8 6 1.73 0.10 2.06 int6 6 1.71 0.11 2.06 int4 6 1.95 0.33 1.16 in order to demonstrate the suitability of the chosen hy- brid snn-ann approach for energy efficient inference on the edge we implemented the snn backbone in hardware. in the proposed architecture the snn block transforms sen- sor data into intermediate representations and therefore un- derlies the strictest latency requirements. due to the clear 13976 page 8 table 6. comparison of different baselines complexities. models map .5 macs acs baselineann 0.61 15.34e9 0.0 baselinew o \u03b2asab 0.53 1.18e9 0.97e9 proposedw \u03b2asab 0.61 1.63e9 0.97e9 proposedsnn 0.58 0.87e9 1.59e9 separation between snn and ann parts in the model s architecture the snn blocks can be implemented in spe- cialized hardware. as hardware we chose intel s loihi 2 8 a digital programmable chip based on event-based communication and computation. only minor adjustments are necessary for execution on loihi 2 the kernel-weights of the convolutional layers are quantized to int8 via a per-output-channel quantization scheme showing no result- ing loss in accuracy map 0.348 float16 vs 0.343 int8 . the batchnorm bn operations and quantization scaling are fused into the lif-neuron dynamics by scaling and shifting the inputs according to the follwing equations scale qscale weightbn \u03c4 varbn \u03b5bn 7 shift biasconv meanbn weightbn \u03c4 varbn \u03b5bn biasbn \u03c4 8 where qscale is the scaling factor introduced by the quan- tization and \u03c4 is the plif neurons time constant. given this approach spike times are almost exactly reproduced on loihi 2 compared to the pytorch int8 implementa- tion. for benchmarking purposes the inputs to the network are simulated with an additional neuron population due to the current io limitations of the chip. with this approach the spiking statistics in the input and snn layers are repro- duced. table 5 reports power and time measurement results of the 4-layer snn block running on loihi 2 for inputs of size 2 256 160 . the network runs at 1.7 0.1 w and 1.9 0.8 ms per step which is faster than real-time in the currently chosen architecture 5 ms per step . these results compare favorably to commercially available chips for edge computing like the nvidia jetson orin nano 7 w 15 w 1 and demonstrate the suitability of an snn backbone for event-based data processing. 5.2. computational analysis we trained a variant of our model where the modified plif neuron acts like a relu with proposed attention mod- this research was funded by the german federal ministry of education and research bmbf under the projects greenedge- fue 16me0521 and cluster4future 03zu1106cb . access to juwels 21 has been granted by gcs www.gauss-centre.eu under project neuroml. we thank intel for access to loihi 2. ule to investigate a comparison between a similar artifi- cial neural network ann and the hybrid network. this variant baselineann with a mean 15.34 109 multiply- accumulate operations macs makes it resource-intensive and unsuitable for energy-constrained hardware. in con- trast our proposed hybrid model computes only 1.63 109 macs making it more practical for edge devices. the \u03b2asab module does incur additional operations but leads to significant accuracy improvement compare to baselinew o \u03b2asab . additionally the spiking neural net- work snn variant proposedsnn increasing one snn layer and reducing one ann layer reduces computational demands further to 0.87 109 macs and is highly efficient on neuromorphic hardware running significantly faster and with less energy on intel s loihi 2 compared to the dense- activation anns. we analyze power consumption across methods follow- ing 2 . among snns densenet121 ssd uses 0.9 mj 0.0 macs 2.3 109 acs while vgg-11 ssd requires 4.2 mj 11.1 109 acs . in anns inception ssd is the most demanding at 19.3 mj 11.4 109 macs . events- retinanet consumes 5.4 mj 3.2 109 macs and rvt-b w o lstm requires 3.9 mj 2.3 109 macs . our method achieves 1.6 109 macs 1.0 109 acs and 3.1 mj signif- icantly reducing energy and computational costs compared to most anns. 6. conclusion in this work we introduced a hybrid attention-based snn- ann backbone for event-based visual object detection. a novel attention-based snn-ann bridge module is proposed to capture sparse spatial and temporal relations from the snn layer and convert them into dense feature maps for the ann part of the backbone. additionally we demonstrate the effectiveness of combining rnns on multiple timescales hardware-efficient snns for fast dynamics on short timescales with convlstms for longer timescales that operate on the extracted features of the bridge-module. experimental results demonstrate that our proposed method surpasses baseline hybrid and snn-based approaches by significant margins with results comparable to existing ann-based methods. the efficacy of our proposed modules and architectural choices is confirmed through extensive ablation studies. additionally we demonstrate the effectiveness of our architectural choice with separate snn and ann blocks by implementing the snn blocks on digital neuromorphic hardware intel s loihi 2. the neuromorphic hardware implementation achieves sub-real-time processing and improved power consumption compared to commercially available edge computing hardware. the achieved accuracy and hardware implementation results pave the way toward a hybrid snn-ann architecture that achieves ann-like perfor- mance at a drastically reduced parameter and power budget. 13977 page 9 references 1 nvidia jetson orin nano datasheet revision 4. https openzeka.com wp-content uploads 2023 03 jetson- orin- nano- datasheet- r4- web.pdf 2023. accessed 2025-03-09. 8 2 asude aydin mathias gehrig daniel gehrig and da- vide scaramuzza. a hybrid ann-snn architecture for low- power and low-latency visual perception. in proceedings of the ieee cvf conference on computer vision and pattern recognition workshops cvprw 2024. 2 8 3 jimmy lei ba jamie ryan kiros and geoffrey e hin- ton. layer normalization. arxiv preprint arxiv 1607.06450 2016. 3 4 francesco barchi gianvito urgese alessandro siino santa di cataldo enrico macii and andrea acquaviva. flexi- ble on-line reconfiguration of multi-core neuromorphic plat- forms. ieee transactions on emerging topics in comput- ing 9 2 915 927 2019. 1 5 nicholas fy chen. pseudo-labels for supervised learning on dynamic vision sensor data applied to object detection under ego-motion. in proceedings of the ieee conference on computer vision and pattern recognition workshops pages 644 653 2018. 1 2 6 6 lo \u0131c cordone beno\u02c6\u0131t miramond and philippe thierion. ob- ject detection with spiking neural networks on automotive event data. in 2022 international joint conference on neu- ral networks ijcnn pages 1 8. ieee 2022. 2 5 6 7 jifeng dai haozhi qi yuwen xiong yi li guodong zhang han hu and yichen wei. deformable convolutional networks. in proceedings of the ieee international confer- ence on computer vision pages 764 773 2017. 4 8 mike davies narayan srinivasa tsung-han lin gautham chinya yongqiang cao sri harsha choday georgios di- mou prasad joshi nabil imam shweta jain et al. loihi a neuromorphic manycore processor with on-chip learning. ieee micro 38 1 82 99 2018. 1 8 9 pierre de tournemire davide nitti etienne perot da- vide migliore and amos sironi. a large scale event- based detection dataset for automotive. arxiv preprint arxiv 2001.08499 2020. 2 3 5 6 10 wei fang zhaofei yu yanqi chen timoth ee masquelier tiejun huang and yonghong tian. incorporating learnable membrane time constant to enhance learning of spiking neu- ral networks. in proceedings of the ieee cvf international conference on computer vision iccv pages 2661 2671 2021. 3 11 wei fang yanqi chen jianhao ding zhaofei yu timoth ee masquelier ding chen liwei huang huihui zhou guoqi li and yonghong tian. spikingjelly an open-source ma- chine learning infrastructure platform for spike-based intel- ligence. science advances 9 40 eadi1480 2023. 5 12 guillermo gallego tobi delbr uck garrick orchard chiara bartolozzi brian taba andrea censi stefan leutenegger andrew j davison j org conradt kostas daniilidis et al. event-based vision a survey. ieee transactions on pattern analysis and machine intelligence 44 1 154 180 2020. 1 13 zheng ge songtao liu feng wang zeming li and jian sun. yolox exceeding yolo series in 2021. arxiv preprint arxiv 2107.08430 2021. 5 14 mathias gehrig and davide scaramuzza. recurrent vision transformers for object detection with event cameras. in pro- ceedings of the ieee cvf conference on computer vision and pattern recognition pages 13884 13893 2023. 1 2 3 5 6 7 15 kaiming he xiangyu zhang shaoqing ren and jian sun. deep residual learning for image recognition. in proceed- ings of the ieee conference on computer vision and pattern recognition pages 770 778 2016. 2 16 andrew g howard menglong zhu bo chen dmitry kalenichenko weijun wang tobias weyand marco an- dreetto and hartwig adam. mobilenets efficient convolu- tional neural networks for mobile vision applications. arxiv preprint arxiv 1704.04861 2017. 2 17 gao huang zhuang liu laurens van der maaten and kil- ian q weinberger. densely connected convolutional net- works. in proceedings of the ieee conference on computer vision and pattern recognition pages 4700 4708 2017. 2 18 massimiliano iacono stefan weber arren glover and chiara bartolozzi. towards event-driven object detection with off-the-shelf deep learning. in 2018 ieee rsj interna- tional conference on intelligent robots and systems iros pages 1 9. ieee 2018. 1 2 6 19 forrest n iandola song han matthew w moskewicz khalid ashraf william j dally and kurt keutzer. squeezenet alexnet-level accuracy with 50x fewer pa- rameters and 0.5 mb model size. arxiv preprint arxiv 1602.07360 2016. 2 20 sergey ioffe and christian szegedy. batch normalization accelerating deep network training by reducing internal co- variate shift. in international conference on machine learn- ing pages 448 456. pmlr 2015. 3 21 j ulich supercomputing centre. juwels cluster and booster exascale pathfinder with modular supercomputing architecture at juelich supercomputing centre. journal of large-scale research facilities 7 a138 2021. 8 22 diederik p kingma and jimmy ba. adam a method for stochastic optimization. arxiv preprint arxiv 1412.6980 2014. 5 23 alexander kugele thomas pfeil michael pfeiffer and elis- abetta chicca. hybrid snn-ann energy-efficient classifica- tion and object detection for event-based vision. in dagm german conference on pattern recognition pages 297 312. springer 2021. 2 24 chankyu lee adarsh kumar kosta alex zihao zhu ken- neth chaney kostas daniilidis and kaushik roy. spike- flownet event-based optical flow estimation with energy- efficient hybrid neural networks. in european conference on computer vision pages 366 382. springer 2020. 2 25 jianing li jia li lin zhu xijie xiang tiejun huang and yonghong tian. asynchronous spatio-temporal memory net- work for continuous event-based object detection. ieee transactions on image processing 31 2975 2987 2022. 2 6 7 13978 page 10 26 tsung-yi lin michael maire serge belongie james hays pietro perona deva ramanan piotr doll ar and c lawrence zitnick. microsoft coco common objects in context. in computer vision eccv 2014 13th european conference zurich switzerland september 6-12 2014 proceedings part v 13 pages 740 755. springer 2014. 5 27 tsung-yi lin piotr doll ar ross girshick kaiming he bharath hariharan and serge belongie. feature pyra- mid networks for object detection. in proceedings of the ieee conference on computer vision and pattern recogni- tion pages 2117 2125 2017. 2 28 faqiang liu and rong zhao. enhancing spiking neural net- works with hybrid top-down attention. frontiers in neuro- science 16 949142 2022. 2 29 wei liu dragomir anguelov dumitru erhan christian szegedy scott reed cheng-yang fu and alexander c berg. ssd single shot multibox detector. in computer vision eccv 2016 14th european conference amster- dam the netherlands october 11 14 2016 proceedings part i 14 pages 21 37. springer 2016. 2 30 nico messikommer daniel gehrig antonio loquercio and davide scaramuzza. event-based asynchronous sparse con- volutional networks. in computer vision eccv 2020 16th european conference glasgow uk august 23 28 2020 proceedings part viii 16 pages 415 431. springer 2020. 1 2 6 31 e. o. neftci h. mostafa and f. zenke. surrogate gradient learning in spiking neural networks bringing the power of gradientbased optimization to spiking neural networks. ieee signal processing magazine 36 6 5163 2019. 2 32 adam paszke sam gross francisco massa adam lerer james bradbury gregory chanan trevor killeen zeming lin natalia gimelshein luca antiga et al. pytorch an im- perative style high-performance deep learning library. ad- vances in neural information processing systems 32 2019. 5 33 etienne perot pierre de tournemire davide nitti jonathan masci and amos sironi. learning to detect objects with a 1 megapixel event camera. advances in neural information processing systems 33 16639 16652 2020. 2 3 5 6 7 34 jun sawada filipp akopyan andrew s cassidy brian taba michael v debole pallab datta rodrigo alvarez-icaza arnon amir john v arthur alexander andreopoulos et al. truenorth ecosystem for brain-inspired computing scalable systems software and applications. in sc 16 proceedings of the international conference for high performance com- puting networking storage and analysis pages 130 141. ieee 2016. 1 35 simon schaefer daniel gehrig and davide scaramuzza. aegnn asynchronous event-based graph neural networks. in proceedings of the ieee cvf conference on computer vi- sion and pattern recognition pages 12371 12381 2022. 1 2 6 36 xingjian shi zhourong chen hao wang dit-yan yeung wai-kin wong and wang-chun woo. convolutional lstm network a machine learning approach for precipitation nowcasting. advances in neural information processing sys- tems 28 2015. 1 37 karen simonyan and andrew zisserman. very deep convo- lutional networks for large-scale image recognition. arxiv preprint arxiv 1409.1556 2014. 2 38 leslie n smith and nicholay topin. super-convergence very fast training of neural networks using large learn- ing rates. in artificial intelligence and machine learning for multi-domain operations applications pages 369 386. spie 2019. 5 39 qiaoyi su yuhong chou yifan hu jianing li shijie mei ziyang zhang and guoqi li. deep directly-trained spik- ing neural networks for object detection. in proceedings of the ieee cvf international conference on computer vi- sion pages 6555 6565 2023. 2 6 40 christian szegedy wei liu yangqing jia pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke and andrew rabinovich. going deeper with convolutions. in proceedings of the ieee conference on computer vision and pattern recognition pages 1 9 2015. 2 41 ashish vaswani noam shazeer niki parmar jakob uszko- reit llion jones aidan n gomez \u0142ukasz kaiser and illia polosukhin. attention is all you need. advances in neural information processing systems 30 2017. 4 42 ziyi wu mathias gehrig qing lyu xudong liu and igor gilitschenski. leod label-efficient object detection for event cameras. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 16933 16943 2024. 2 43 zheyu yang yujie wu guanrui wang yukuan yang guoqi li lei deng jun zhu and luping shi. dashnet a hybrid artificial and spiking neural network for high-speed object tracking. arxiv preprint arxiv 1909.12942 2019. 2 44 jason yik soikat hasan ahmed zergham ahmed brian anderson andreas g andreou chiara bartolozzi arindam basu douwe den blanken petrut bogdan sander bohte et al. neurobench advancing neuromorphic computing through collaborative fair and representative benchmarking. arxiv preprint arxiv 2304.04640 2023. 1 45 hu zhang luziwei leng kaiwei che qian liu jie cheng qinghai guo jiangxing liao and ran cheng. automotive object detection via learning sparse events by temporal dynamics of spiking neurons. arxiv preprint arxiv 2307.12900 2023. 6 46 hu zhang yanchen li luziwei leng kaiwei che qian liu qinghai guo jianxing liao and ran cheng. auto- motive object detection via learning sparse events by spiking neurons. ieee transactions on cognitive and developmen- tal systems 2024. 6 47 rong zhao zheyu yang hao zheng yujie wu faqiang liu zhenzhi wu lukai li feng chen seng song jun zhu et al. a framework for the general design and computation of hybrid neural networks. nature communications 13 1 3427 2022. 2 48 nikola zubic mathias gehrig and davide scaramuzza. state space models for event cameras. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 5819 5828 2024. 2 6 7 13979",
      "keywords": [
        "page",
        "efficient",
        "event-based",
        "object",
        "detection",
        "hybrid",
        "neural",
        "network",
        "spatial",
        "temporal",
        "attention",
        "soikat",
        "hasan",
        "ahmed",
        "jan",
        "finkbeiner",
        "emre",
        "neftci",
        "forschungszentrum",
        "ulich",
        "rwth",
        "aachen",
        "university",
        "s.ahmed",
        "j.finkbeiner",
        "e.neftci",
        "fz-juelich.de",
        "abstract",
        "event",
        "cameras",
        "offer",
        "high",
        "resolution",
        "dy-",
        "namic",
        "range",
        "minimal",
        "motion",
        "blur",
        "making",
        "promising",
        "robust",
        "detection.",
        "while",
        "spiking",
        "neu-",
        "ral",
        "networks",
        "snns",
        "neuromorphic",
        "hardware",
        "often",
        "considered",
        "energy",
        "low",
        "latency",
        "data",
        "processing",
        "fall",
        "short",
        "artificial",
        "anns",
        "accuracy",
        "flexibility.",
        "here",
        "in-",
        "troduce",
        "attention-based",
        "snn-ann",
        "backbones",
        "leverage",
        "strengths",
        "both",
        "snn",
        "ann",
        "architectures.",
        "novel",
        "bridge",
        "module",
        "captures",
        "sparse",
        "tem-",
        "poral",
        "relations",
        "from",
        "layer",
        "converts",
        "into",
        "dense",
        "feature",
        "maps",
        "part",
        "backbone.",
        "ad-",
        "ditionally",
        "present",
        "variant",
        "integrates",
        "dwconvl-",
        "stms"
      ],
      "metadata": {
        "filename": "Ahmed_Efficient_Event-Based_Object_Detection_A_Hybrid_Neural_Network_with_Spatial_CVPR_2025_paper.pdf",
        "original_filename": "Ahmed_Efficient_Event-Based_Object_Detection_A_Hybrid_Neural_Network_with_Spatial_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 1154949,
        "upload_file_id": "upload_1752160677309_byrypo4mp",
        "upload_timestamp": "2025-07-10T15:17:57.350721",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:17:57.774948+00:00",
      "content_length": 48705,
      "content_type": ".pdf",
      "filename": "Ahmed_Efficient_Event-Based_Object_Detection_A_Hybrid_Neural_Network_with_Spatial_CVPR_2025_paper.pdf"
    },
    "9": {
      "file_id": "9",
      "content": "\n=== Page 1 ===\nNearly Zero-Cost Protection Against Mimicry by Personalized Diffusion Models\nNamhyuk Ahn 1,2\nKiYoon Yoo 3\nWonhyuk Ahn 2\nDaesik Kim 2\nSeung-Hun Nam 2\n1 Inha University\n2 NAVER WEBTOON AI\n3 KRAFTON\n512\n768\n1024\n1660\n2048\n0.01\n0.1\n1\n10\n50\n100\n500\n1000\n5000\nTime\nAnti-DB\nMist\nFastProtect\nAdvDM\nPhotoGuard\nSDST\nImpasto\n(a) Inference latency (log-scaled) vs. image size\n0.15\n0.17\n0.19\n0.21\n0.23\n0.25\n185\n195\n205\n215\n225\n235\n245\n255\nFID\nDISTS\nFastProtect\nPhotoGuard\nMist\nImpasto\nSDST\nAdvDM\nAnti-DB\n(b) Protection efficacy vs. invisibility\nFigure 1. (a) FastProtect shows unprecedented speed in protection against diffusion models. On an A100 GPU, FastProtect achieves\nreal-time latency even for processing 20482-px image, while others require substantially longer time. (b) In terms of the trade-off between\nprotection efficacy (FID, \u2191is better) and invisibility (DISTS, \u2193is better), FastProtect exhibits improvement over other protection methods.\nAbstract\nRecent advancements in diffusion models revolutionize\nimage generation but pose risks of misuse, such as replicat-\ning artworks or generating deepfakes. Existing image pro-\ntection methods, though effective, struggle to balance pro-\ntection efficacy, invisibility, and latency, thus limiting prac-\ntical use. We introduce perturbation pre-training to reduce\nlatency and propose a mixture-of-perturbations approach\nthat dynamically adapts to input images to minimize perfor-\nmance degradation. Our novel training strategy computes\nprotection loss across multiple VAE feature spaces, while\nadaptive targeted protection at inference enhances robust-\nness and invisibility. Experiments show comparable protec-\ntion performance with improved invisibility and drastically\nreduced inference time. The code and demo are available\nat https://webtoon.github.io/impasto\n1. Introduction\nLarge-scale diffusion models [8, 22, 24, 29] have achieved\nremarkable success in the realm of image synthesis task,\nrevolutionizing the way we create and manipulate digital\nimagery [10, 11, 19, 21, 34]. By adopting recently emerged\npersonalization techniques, it is now possible to develop\ndiffusion models that can generate images in one\u2019s unique\nstyles [2, 5, 14, 25, 28]. However, the very capabilities that\nmake these models so valuable also pose significant risks.\nDiffusion models can allow malicious users to replicate an\nindividual\u2019s artwork without consent, easily stealing their\ncreative output [27]. Furthermore, diffusion models are ex-\nceedingly adept at mimicking faces and objects [14, 25].\nThis proficiency is not limited to benign applications but\ncan be extended to the making of deepfakes or fake news.\nThe implications for societal harm are profound, as these\ntechnologies could be used to undermine public trust, dis-\ntort political discourse, and violate personal privacy.\nCurrent efforts to protect images against diffusion mod-\nels have evolved through adversarial perturbation [6]. By\nintroducing perturbations to a given image, they render the\nprotected image resistant to diffusion-based personaliza-\ntion methods, causing the diffusion model to generate dis-\ntorted outputs. Existing methods primarily rely on two ap-\nproaches: minimizing the distance between the latent fea-\ntures of the protected and a pre-defined target image within\na VAE encoder (e.g. PhotoGuard [26] and Glaze [27]), or\nfooling the noise prediction network (e.g. AdvDM [16] and\nAnti-DB [30]). Despite numerous advancements, all cur-\nrent frameworks fundamentally start from an approach that\noptimizes perturbations during inference [18] when protec-\ntion is requested, casting doubt on the practicality of these\nmethods to be used in many real applications.\nFor a protection framework to become a practical so-\nlution, the following conditions should be satisfied: (1)\nProtection Efficacy: Obviously, image protection method\nshould effectively counteract against diffusion-based per-\nsonalization. Many recent studies have aimed to maximize\nthis aspect by analyzing the internal dynamics of diffusion\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n28801\n\n=== Page 2 ===\nmodels [33, 37]. (2) Invisibility: Although some studies\nstrive to be as invisible as possible, adversarial perturba-\ntions inevitably leave visible traces on images. However,\nfor commercial services, minimizing these traces is crucial.\nMoreover, as analyzed by Ahn et al. [1], the task of pre-\nventing mimicry tends to leave more noticeable traces, par-\nticularly in images with flat textures (e.g. cartoon or illustra-\ntion). Despite its importance, only a handful of studies high-\nlight this issue [1]. (3) Latency: All existing methods up-\ndate perturbations during the inference phase through itera-\ntive optimization. However, this process is inherently time-\nconsuming; e.g. protecting a 5122-px image can take up to\n5-120 minutes on a CPU and 7-200 seconds even on a high-\nend A100 GPU. The extensive time required by current pro-\ntection methods poses a significant barrier that prevents or-\ndinary users from utilizing image protection, thereby still\nexposing them to the risk of misuse of generative models.\nConsequently, in order to democratize this technology, it\nis crucial to develop protection framework that can operate\neffectively on less powerful devices. Unfortunately, despite\nextensive progresses in the image protection task, there is\nscant focus on reducing latency.\nIn this work, we propose a novel protection framework,\nFastProtect, which meets all the requirements, with a par-\nticular focus on latency. FastProtect diverges from preva-\nlent approaches by adopting a pre-trained perturbation. Al-\nthough universal adversarial perturbation (UAP) [20] is pro-\nposed in adversarial attack, we observed that it significantly\nreduces protection efficacy for our task. To counteract this,\nwe introduce a mixture-of-perturbations (MoP). Unlike\nUAP, which relies on a single perturbation, MoP prepares\nmultiple perturbations and selects one of them based on the\ninput image\u2019s latent code. We also propose a multi-layer\nprotection (MLP) loss that utilizes the intermediate fea-\ntures when calculating the protection loss. It intensifies the\ndistortion when the protected images go through fine-tuning\nby personalization methods, thus enhancing the protection\nefficacy without any additional cost at inference.\nFor the inference, we introduce an adaptive targeted\nprotection to enhance protection efficacy with minimal\ncomputational overhead. Our observations suggest that in\ntargeted protection, which minimizes the distance between\nthe input image\u2019s latent code and that of a (pre-defined) tar-\nget, selecting an appropriate target image significantly in-\nfluences protection performance. Taking this into account,\nwe adaptively determine the best target image based on the\ninput image\u2019s latent code and then apply the perturbation\ntrained for that specific target. To enhance invisibility, we\npropose an adaptive protection strength approach using\nthe LPIPS [36] distance. Unlike Ahn et al. [1] that lever-\nages multiple just-noticeable difference (JND) maps, our\napproach does not require ad-hoc modules nor does it sig-\nnificantly increase computational load; it only involves an\nadditional forward pass of LPIPS (i.e. AlexNet [13]).\nWe conduct extensive experiments to verify the effec-\ntiveness and efficiency of our protection method. To sim-\nulate various use-cases, we test across diverse domains\u2014\nnatural images, faces, paintings, and cartoons\u2014 under dif-\nferent personalization and countermeasure scenarios. Fast-\nProtect achieves similar protection efficacy to other meth-\nods but at nearly zero cost, with a 200 to 3500\u00d7 speedup\n(Figure 1a). It also shows an improvement in invisibility\ncompared to most methods (Figure 1b), proving its suitabil-\nity for practical applications. Our main contributions are:\n\u2022 We propose FastProtect, which achieves real-time pro-\ntection against diffusion models. Our work is the first\nto address the critical issue of latency in this task.\n\u2022 FastProtect integrates perturbations pre-training with\nadaptive inference schemes, meeting all requirements\nfor a practical protection solution.\n\u2022 We validate FastProtect in various scenarios, showing\nthat despite its speed and invisibility, it retains protec-\ntion efficacy, robustness, and generalization.\n2. Background\nDiffusion Models. These have gained prominence due to\ntheir ability to generate high-quality images. Latent Dif-\nfusion Model (LDM) [24] is particularly highligted for its\nexceptional quality and efficiency. Within LDM, a VAE\nencoder E, transforms an input image x into a latent code\nz = E(x). This code is then reconstructed back into the im-\nage domain by a decoder D; x\u2032 = D(z\u2032). The diffusion\nprocess makes a latent code z\u2032 by incorporating external\nfactors, y, such as textual prompts. The training of LDM\nis driven by a denoising loss function at each timestep t as:\nLSD = Ez\u223cE(x),y,\u03f5\u223cN(0,1),t[||\u03f5 \u2212\u03f5\u03b8(zt, t, c(y))||2\n2], (1)\nwhere a denoising network \u03f5\u03b8 restores the noised latent\ncode zt based on the timestep t and a conditioning vec-\ntor c(y). Recent advancements explore the personalization\n(or few-shot fine-tuning) of LDM with a few reference im-\nages through two primary approaches: textual inversion [5],\nwhich utilizes the embedding space of CLIP [23] while\nkeeping the denoising network fixed, and model optimiza-\ntion [25], which directly updates the denoising network.\nProtection Against Diffusion-based Mimicry.\nCurrent\nprotection frameworks apply adversarial perturbations \u03b4 to\nimage x, producing a protected image \u02c6x = x+\u03b4 using pro-\njected gradient descent (PGD) [18]. With a protection loss\nLP, current methods obtain the protected image of i-th opti-\nmization step by a signed gradient ascent with step function\nsgn and step length \u03b1 as given by\nx(i) = \u03a0N\u03b7(x)\nh\nx(i\u22121) + \u03b1sgn(\u2207x(i)LP(x(i\u22121))\ni\n,\n(2)\n28802\n\n=== Page 3 ===\nwhere \u03a0N\u03b7(x) projects onto the L\u221eneighborhood of x\nwith a radius of \u03b7. This iterative process is repeated for\nN steps until \u02c6x = x(N) is achieved (Figure 2a).\nFor the objective function, semantic or texture losses are\nmostly used [15]. The semantic loss, LS(x) = LSD(x),\nis designed to disrupt the denoising process of LDM, mis-\nleading it to generate samples that deviate from the original\nimages. AdvDM [16] adopts this loss and Anti-DB [30] is\nimproved to be more robust against DreamBooth [25] by in-\ncorporating its update process within the optimization. The\ntexture loss, LT (x) = \u2212||E(x) \u2212E(y)||2\n2, aims to pushing\nthe latent code of x towards the target image\u2019s latent code.\nGlaze [27] and PhotoGuard [26] belong to this category.\nUpon these objective functions, We can now define a uni-\nversal loss as LP = \u03bbSLS + \u03bbT LT , where {\u03bbS, \u03bbT } \u22650\nare balancing factors. Mist [15] utilizes this universal loss\nfunction to effectively capture the characteristics of both\nobjectives. Diff-Protect [33], building on Mist, introduces\na score distillation trick to achieve more efficient protec-\ntion. Impasto [1] incorporates perceptual-oriented compo-\nnents designed for imperceptible protection.\nSince all the current protection frameworks leverage it-\nerative optimization during inference, they can yield image-\nspecific and high-performing perturbations [1, 15\u201317, 26,\n27, 30\u201333, 37]. Despite this advantage, a significant draw-\nback is the substantial time required to operate protection.\nFor instance, to protect a 5122-px image, even the fastest\nmodel (e.g. PhotoGuard [26]) requires around 7 seconds on\nan A100 GPU and 350 seconds on a CPU. Consequently,\nwe explore alternatives to this iterative process to enhance\nefficiency without compromising the protection.\nUniversal Adversarial Perturbation (UAP). This concept\nis introduced in adversarial attacks, through a data-driven\napproach to find an image-agnostic perturbation from the\ntraining dataset [20]. In our task, we can adapt UAP as\nin Figure 2b. Given the training dataset XD, we employ\na protection loss LP to derive an universal perturbation \u03b4,\nconstrained within \u03b7-ball to maintain perceptual invisibility.\n\u03b4 = arg max\n||\u03b4||\u221e\u2264\u03b7\nEx\u223cXD [LP(x + \u03b4)] .\n(3)\nUpon finalizing the universal perturbation \u03b4 by the train-\ning, it can be applied to an image x to produce a protected\nimage \u02c6x = x + \u03b4. This is notably practical as it requires\nno computation at inference. Nevertheless, we found that\ndirectly applying UAP to our task significantly compro-\nmises protection performance. In Table 1, we compare the\nprotection efficacy of iterative optimization (via PGD) and\nUAP. We use DISTS [4] to assess protected images\u2019 quality,\nwhile FID [7] evaluates generated image through mimicry\nby LoRA [9] (a higher FID is better protection). As also\nshown in the qualitative mimicry results by LoRA, UAP\nleads to a notable degradation in protection efficacy. Hence,\nTable 1. PGD vs. UAP (Top) Invisibility (DISTS) and protection\nefficacy (FID) comparison. (Bottom) Mimicry results via LoRA.\nMethod Invisibility (DISTS; \u2193)\nProtection (FID; \u2191)\nPGD\n0.221\n227.6\nUAP\n0.222\n207.6\nClean\nPGD\nUAP\nit is crucial to refine this pre-training approach, but with a\nminimal inference cost in a manner akin to UAP.\n3. Method\n3.1. Perturbation Pre-Training\nMixture-of-Perturbation (MoP). We hypothesize that the\nunderperformance of UAP is primarily due to two reasons:\n1) a single perturbation has a limited capacity to deceive\ndiffusion models, and 2) the image-agnostic nature of UAP\nfails to cover the diverse features (e.g. texture or structure)\nof images. To overcome these, we introduce a mixture-of-\nperturbations (MoP), consisting of multiple perturbations,\n\u2206= {\u03b41, . . . , \u03b4K}, where K represents the number of per-\nturbations. For any given input image, MoP dynamically\nassigns and applies the appropriate perturbation to create a\nprotected image (Figure 2c). Specifically, given an image\nx, MoP first encodes it using a VAE encoder E to obtain the\nlatent code z. This code is used to select the specific pertur-\nbation to be applied, as dictated by an assignment function\nA. Formally, the protected image \u02c6x is generated as:\n\u02c6x = x + \u03b4g + \u2206k, where k = A(E(x)),\n(4)\nwhere \u03b4g is a global perturbation. We observed that adding\n\u03b4g slightly improves the performance. In MoP, the assign-\nment function A plays a crucial role by guiding the se-\nlection of perturbations based on an input image, ensur-\ning that similar images are protected with the same pertur-\nbation. This capability enables MoP to not only increase\nthe capacity of UAP but also to offer a degree of image-\nspecific protection, which is notably absent in UAP. With-\nout the assignment function, a naive perturbation averaging,\ni.e. \u02c6x = x+\u03b4g +1/K \u2217\u03a3K\nk=1\u2206k, increases the capacity but\nfail to overcome the intrinsic limitations of image-agnostic,\nthus limiting performance benefits. In contrast, our MoP\nimplements a semi-image-specific protection, positioning it\nbetween the image-agnostic and the image-specific preci-\nsion of the iterative approach while removing the expensive\ninference cost, as demonstrated in the ablation study.\nWe adopt a simple assignment function A that can lever-\nage the highly representative latent codes of the VAE en-\n28803\n\n=== Page 4 ===\nFigure 2. Model overview. (a) Current iterative optimization approaches lack a training phase and perform optimization during inference,\nresulting in extremely slow protection. (b) UAP [20] introduces pre-training of perturbations, but their image-agnostic nature leads to\ndegraded protection efficacy. (c) Combining the advantages of both paradigms, FastProtect adopts a pre-training approach similar to UAP\nbut with a novel mixture-of-perturbation scheme and multi-layer protection loss to enhance protection efficacy. At inference, adaptive\ntargeted protection further boosts protection efficacy with minimal additional cost, and adaptive protection strength improves invisibility.\ncoder. For our training dataset XD, we extract the latent\ncodes Z = {z1, . . . , zD} using the VAE encoder E. These\ncodes are then clustered into K predefined groups with K-\nmeans++ [3]. Despite its simplicity, this process shows its\nrobustness empirically, thus we adopt it for our framework.\nMulti-Layer Protection Loss. FastProtect employs a tar-\ngeted protection approach based on the texture loss. Con-\nventionally, texture loss is computed upon the VAE latent\ncode z, i.e. LT (x) = \u2212||z \u2212zy||2\n2, where zy is the latent\ncode of the target image. However, we observed that of-\nten the z-space loss does not sufficiently push z towards\nzy, resulting in suboptimal protection performance. To ad-\ndress this, we incorporate an auxiliary loss using multi-\nlayer features extracted by the VAE encoder. Specifically,\ngiven an input image, we extract the intermediate features\nF = {f 1, . . . , f L}, where L is the number of feature lay-\ners extracted. Similarly, we obtain Fy for the target im-\nage. Then, our proposed multi-layer protection loss func-\ntion with the balanced factor \u03bb is computed as:\nLT (x) = \u2212||z \u2212zy||2\n2 \u2212\u03bb\nL\nL\nX\nl=1\n||Fl \u2212Fl\ny||2\n2.\n(5)\nThe auxiliary MLP loss improves overall protection ef-\nficacy as this leverages the intermediate feature spaces be-\nyond the z-space when pushing z toward zy. In addition,\nsince these modifications are implemented during the pre-\ntraining stage, it improves protection efficacy without extra\ncomputational expense at inference.\nTraining. The assignment function A is first trained on XD\nthen MoP is updated on XD using Eq. 5 by Adam opti-\nmizer [12]. All the perturbations (\u03b4g, \u2206) are initialized at\na resolution of 512 \u00d7 512. When applied to input images\nof different resolutions, we resize the perturbation through\nbilinear interpolation. Since two perturbations are used in\nMoP, \u03b4g, \u2206are constrained within a (\u03b7/2)-ball.\n3.2. Adaptive Inference\nAdaptive Targeted Protection. In targeted protection, tar-\nget image influences protection performance. Liang and Wu\n[15] noted that patterned image is effective, while Zheng\net al. [37] reported that the protection efficacy varies with\npattern repetitions. Though they provide valuable insights,\nthey do not consider the relationship between the target and\ninput images. Instead, we investigate such relationship and\npropose an adaptive targeted protection approach.\nIn our analysis, we use two target images characterized\nby either low or high pattern repetition (shown in Appendix)\nand apply protection to input images with either simple\nor complex textures. Figure 3 shows mimicry outputs by\nLoRA of these four protection scenarios. Interestingly, our\nobservations suggest a relationship between the pattern rep-\n28804\n\n=== Page 5 ===\nFigure 3. Relationship between target image\u2019s pattern repetition\nand input image\u2019s texture. Simple textured image is successfully\nprotected by a low repetition target, but fails when using a high\nrepetition target; vice versa for complex texture cases.\netition of the target image and the texture complexity of an\ninput image. Zheng et al. [37] noted that protection efficacy\nimproves as pattern repetition increases but declines when\nit becomes too high. We suspect that their results might be\nbiased by fixing the input image, thereby keeping the tex-\nture factor constant. In contrast, our findings indicate that\ntailoring the pattern image to match the texture complexity\nof an input image can improve protection efficacy.\nHence, we design FastProtect to adaptively select the tar-\nget image based on an input image. To this end, we pre-\npare target images with varying pattern repetitions and train\nMoPs for each. We utilize three pattern images\u2014low, mid,\nand high\u2014resulting in three MoPs: {\u03b4l\ng, \u2206l}, {\u03b4m\ng , \u2206m},\nand {\u03b4h\ng , \u2206h}. We then extract the latent codes for each\ntarget image: zl\ny = E(yl), zm\ny = E(ym), zh\ny = E(yh),\nwhere yl, ym, yh are the target images with low, mid, and\nhigh pattern repetitions, respectively. At inference, input\nimage\u2019s latent code z is compared with {zl\ny, zm\ny , zh\ny} and\nthe target image whose latent code is closest to z is selected.\nTo measure the similarity between the target images\u2019 pattern\nrepetition and the input image, L1 norm of entropy is used\nas a distance. Overall, integration of MoP (in Eq. 4) and the\nadaptive targeted protection are as follows:\n\u02c6x = x + \u03b4t\ng + \u2206t\nk, t = arg min\ni\u2208{l,m,h}\n||H(z) \u2212H(zi\ny)||1, (6)\nwhere H(z) = \u2212P\nz\u2208z p(z) log p(z). The major advan-\ntage of our adaptive targeted protection is its robust perfor-\nmance across various domains, which is critically important\nfor practical use cases where the specific domain (or tex-\nture) of incoming images might be unknown. If the range\nof potential domains is known beforehand, the performance\ndifference might not be significant. However, the flexibil-\nity to adapt to different input characteristics without prior\ndomain knowledge enhances FastProtect\u2019s utility in diverse\nand unpredictable environments of real applications.\nAdaptive Protection Strength.\nAccording to the We-\nber\u2013Fechner law, humans are more adept at detecting sub-\nFigure 4. Given the original and protected images, we obtain the\nLPIPS distance map, which remarkably aligns with human per-\nception. The brighter regions on the perceptual map indicate areas\nwhere subtle distortions are more noticeable.\ntle changes in regions with simple textures than in complex\nones. Upon this, Ahn et al. [1] adapted human visual system\nto the image protection task. They utilized multiple percep-\ntion maps and adjusted perturbation strength during opti-\nmization to effectively enhance invisibility. However, the\nperception maps used by Ahn et al. [1] are computed before\nthe injection of perturbations, leading us to hypothesize that\nthese may not perfectly align with the actual perceived per-\nturbations intended for protection. Moreover, their reliance\non a combination of traditional perceptual map algorithms\nresulted in slow processing and performance limitations. To\naddress these challenges, we first apply MoP to create a sur-\nrogate protection image and then generate a perceptual map,\nwhich helps resolve the identified issues.\nSpecifically, we generate a surrogate protection image \u02c6x\nthrough Eq. 6. Subsequently, we create a spatial perceptual\nmap using LPIPS [35], i.e. M = LPIPS(x, \u02c6x). As shown in\nFigure 4, the perceptual map M is remarkably aligned with\nhuman cognitive perspectives. Hence, we utilize this map\nto produce the final protected image as in below:\n\u02c6x = x + S(1 \u2212M) \u2217(\u03b4t\ng + \u2206t\nk).\n(7)\nHere, S(\u00b7) is a scaling function (more details are in Ap-\npendix) and M serves as a distance map, necessitating an\ninversion step. Since this process requires only the forward\npass of the LPIPS backbone, the additional computational\ncost at inference is minimal, thus economically enhancing\ninvisibility without significant overhead. In Appendix, we\nsummarize the perturbation pre-training and inference pro-\ncess in the form of an algorithm.\n4. Experiment\nImplementation Details. We set the number of perturba-\ntions K in MoP as four. Other hyperparameters and details\non training and inference are described in Appendix.\nDatasets. We utilize four domains when constructing both\nthe training and benchmark datasets: object, face, painting,\n28805\n\n=== Page 6 ===\nTable 2. Quantitative comparison. Latency is measured on 512\u00d7512 image. Comparisons of other metrics are shown in Appendix.\nMethod\nLatency\nObject\nFace\nPainting\nCartoon\nCPU / GPU\nInvisibility: DISTS (\u2193) / Efficacy: FID (\u2191)\nAdvDM [16]\n1210s / 35s\n0.197 / 220.0\n0.173 / 303.8\n0.153 / 357.6\n0.271 / 212.5\nPhotoGuard [26]\n370s /\n7s\n0.203 / 223.0\n0.189 / 308.7\n0.107 / 350.9\n0.209 / 219.1\nAnti-DB [30]\n7278s / 225s\n0.239 / 214.4\n0.162 / 301.4\n0.114 / 347.7\n0.294 / 225.4\nMist [15]\n1440s / 40s\n0.185 / 217.2\n0.154 / 307.5\n0.129 / 357.0\n0.223 / 223.7\nImpasto [1]\n830s / 19s\n0.201 / 213.8\n0.198 / 298.4\n0.123 / 352.4\n0.207 / 215.5\nSDST [33]\n1410s / 24s\n0.242 / 219.2\n0.244 / 302.1\n0.152 / 354.1\n0.237 / 222.7\nFastProtect\n2.9s / 0.04s\n0.155 / 223.0\n0.149 / 308.9\n0.110 / 356.1\n0.186 / 220.3\nFigure 5. Qualitative comparison of different protection frameworks. (Top) Protected image with a zoomed-in patch in the inset. (Bottom)\nTwo output images from the personalized LoRA.\nand cartoon. These multiple scenarios allow us to analyze\nthe models\u2019 protection performance more closely to real ap-\nplications, as images from various domains can be encoun-\ntered. The dataset details are provided in Appendix.\nBaselines. We compare with existing diffusion-based im-\nage protection frameworks: AdvDM [16], PhotoGuard [26],\nAnti-DB [30], Mist [15], Impasto [1], and Diff-Protect\n(SDST) [33]. Except ours, all other methods rely on iter-\native optimization, using texture or semantic losses.\nEvaluation. The protection efficacy is evaluated against\nLoRA [9] on Stable Diffusion v1.5. Latency is measured\nusing an M1 Max CPU and an A100 GPU. We employ\nDISTS [4] to evaluate invisibility, and FID [7] to quan-\ntify protection efficacy. In Appendix, we provide additional\nmetrics for a more comprehensive comparison. Note that\nprevious studies typically fix the protection strength across\nmodels for comparison, but we found that protection ef-\nficacy and invisibility vary significantly depending on the\nprotection loss used. For instance, PhotoGuard [26] and\nAdvDM [16] exhibit different trends since they use dis-\nparate objectives. Moreover, evaluating protection efficacy\nonly without invisibility leads to a narrow assessment that\nmight overlook real-world needs. To address these, we ad-\njust the protection strength to match the protection level\nacross methods to ensure a fairer comparison and better re-\nflection of practical requirements. However, one might also\nbe curious about how the performance appears when the\nprotection strength is equalized across all methods. Hence,\nwe provide evaluation with a fixed strength in Appendix.\n4.1. Model Comparison\nAs shown in Figure 1a and Table 2, FastProtect is ultra-fast;\n125\u00d7 faster on CPU and 175\u00d7 faster on GPU compared to\nthe second fastest model, PhotoGuard [26]. Remarkably,\nFastProtect maintains consistent latency even as the input\nimage size increases, achieving near real-time performance\neven for 20482-px images. In contrast, all other frameworks\nshow exponential increases in latency (Figure 1a), which\nposes a significant issue in the real world where many recent\nartworks are high-resolution. In addition, FastProtect occu-\npies only 1.7GB of VRAM during inference, unlike other\nmethods that require more than 8GB. The unprecedented\nspeed of our model makes it highly user-friendly.\nFastProtect also demonstrates a superior trade-off be-\ntween protection efficacy and invisibility (Figure 1b). In\ncomparisons where the protection strength is adjusted to\nyield similar protection efficacy (Table 2), FastProtect con-\nsistently achieves the best invisibility across most protection\n28806\n\n=== Page 7 ===\nTable 3. Ablation study.\nConfiguration\nFID (\u2191)\nPhotoGuard\n227.6\nUAP [20]\n207.6\nMoP (w/o A)\n214.5\nMoP\n225.9\n+ MLP Loss\n234.6\n+ Adapt. Target\n238.8\nTable 4. Analysis on protection robustness scenarios.\nDomain\nMethod\nInvisibility\nCountermeasure\nArbitrary Size\nNoise\nJPEG\nSubject\nPhotoGuard\n0.203\n193.3\n193.2\n193.5\nFastProtect\n0.155\n214.4\n191.3\n219.1\nCartoon\nPhotoGuard\n0.209\n192.9\n193.1\n190.7\nFastProtect\n0.186\n191.8\n199.9\n204.0\nTable 5. Analysis of black-box protection scenarios (unknown diffusion models and personalization methods).\nDomain\nMethod\nInvisibility\nUnknown Model\nUnknown Personalization\n\u2192SD-v2.1 [24]\n\u2192SD-XL [22]\nTI [5]\nDreamStyler [2]\nSubject\nPhotoGuard\n0.203\n176.6\n190.9\n290.2\n224.9\nFastProtect\n0.155\n177.5\n218.4\n305.6\n231.3\nCartoon\nPhotoGuard\n0.209\n179.3\n195.1\n306.4\n193.7\nFastProtect\n0.186\n188.3\n207.3\n305.7\n209.9\ndomains, with the second-best performance in the painting\ndomain. In our analysis, FastProtect shows the strong ad-\nvantage in invisibility within areas that have flat textures.\nThese features result in significant improvements in the ob-\nject and cartoon domains. For example, Figure 5 shows that\nin the first row, the sky region is rendered nearly invisible\nby FastProtect, and the same applies to the cartoon in the\nsecond row. As demonstrated in Appendix, even when we\nfix the protection strength of all methods to the same value,\nour method achieves a favorable balance in this trade-off.\n4.2. Model Analysis\nAblation Study. In Table 3, we depict the component anal-\nysis of FastProtect. It is notable that UAP [20] significantly\ndeteriorates protection efficacy compared to the iterative op-\ntimization baseline, PhotoGuard [26]. On the other hand,\nwhen we introduce the proposed MoP, it recovers the lost\nperformance. However, if the assignment function A is not\nused, the improvement is limited, highlighting the impor-\ntance of achieving a semi-image-specific nature. As we dis-\ncussed, the limitations of UAP arise not only from insuffi-\ncient capacity but also from its image-agnostic nature. Sim-\nply increasing the number of perturbations (i.e. MoP w/o A)\nresolves the first limitation and improves performance over\nUAP. However, this still does not overcome the second one,\nwhich causes it to underperform compared to PhotoGuard.\nWhen we introduce the assignment function into MoP, it can\ndynamically account for image features. This allows MoP\nto match the performance of iterative optimization methods\nwhile remaining significantly faster.\nWhen we attach the multi-layer protection (MLP) loss,\nit significantly increases the protection efficacy. Similarly,\nadaptive targeted protection also enhances performance.\nOverall, by incorporating novel modules in both the pre-\ntraining and inference stages, FastProtect can achieve better\nprotection efficacy with a much faster inference.\nRobustness.\nHere, we analyze the vulnerability of pro-\ntected images to countermeasures (Table 4). We use Pho-\ntoGuard [26] as our baseline due to its similarity to our ap-\nproach. To assess robustness, we evaluate against counter-\nmeasures such as Gaussian noise and JPEG compression.\nAdditionally, we account for a more realistic scenario where\nthe input image can have arbitrary dimensions. Most previ-\nous evaluations in image protection tasks assume a fixed\nimage resolution, typically 5122, which represents an ide-\nalized scenario. In practice, image sizes and aspect ratios\nvary.\nOur model demonstrates comparable performance\nagainst these countermeasures, and notably, in the arbitrary\nsize scenario, FastProtect performs even better. This sug-\ngests that simply using interpolation to adapt MoP to differ-\nent sizes is both straightforward and effective.\nBlack-Box Scenario. Table 5 presents the model analy-\nsis in black-box scenarios. We examine two cases: the un-\nknown model scenario, where we transfer protection meth-\nods to other Stable Diffusion (SD) backbones, specifically\nSD v2.1 [24] and SD-XL [22]. We also assess image pro-\ntection against unknown personalization by applying Tex-\ntual Inversion (TI) [5] and DreamStyler [2] to replicate the\nimage. In these scenarios, FastProtect demonstrates supe-\nrior invisibility compared to the baseline while achieving\ncomparable or better performance in black-box settings.\nMixture-of-Perturbation. In Figure 6a, we vary the train-\ning dataset XD and measure the protection efficacy. Inter-\nestingly, even when the training domain is limited, there is\nno significant gap in the protection performance. We con-\njecture that MoP effectively handles unseen domains thanks\nto its adaptive perturbation selection mechanism. However,\nusing all domains is slightly better, so we train with this\nstrategy. Additionally, Figure 6b compares FastProtect by\nchanging K and excluding the assignment function A in\nMoP. Without the assignment function, performance im-\nprovement is limited, and in the full MoP case, it achieves\n28807\n\n=== Page 8 ===\nObject\nFace\nPainting\nCartoon\n200\n220\n240\n260\n280\n300\n320\n340\nFID\n= Object\n= Face\n= Painting\n= Cartoon\n= All\n(a) Domain Generalization of MoP\n1\n2\n4\n6\n8\n210\n215\n220\n225\nFID\nMoP\nMoP w/o \n(b) Effect of Assignment Function\n(c) Examples of MoP Group\nFigure 6. Analysis of the proposed modules in the pre-training phase.\nObject\nFace\nPainting\nCartoon\n200\n220\n240\n260\n280\n300\n320\n340\nFID\nLow pattern rep.\nMid pattern rep.\nHigh pattern rep.\nAdaptive target\n(a) Adapt. Targeted Protection\n0.20\n0.22\n0.24\n0.26\n220\n230\n240\n250\n260\nFID\nDISTS\nw/ Adaptive Protection Strength\nw/o Adaptive Protection Strength\n(b) Adapt. Protection Strength\n1\n2\n5\n10\n25\n50\n100\n185\n195\n205\n215\n225\n235\n245\nFID\nBaseline\nFastProtect\nFastProtect + PGD\n(c) FastProtect + PGD Refine\nFigure 7. Analysis of the proposed modules in the inference phase.\nsubstantially better efficacy with convergence at K = 4. In\nFigure 6c, we visualize the representative images assigned\nto each perturbation when K = 4. The images are grouped\nby their certain distinguishing features (e.g., texture, scene).\nAdaptive Targeted Protection. Figure 7a analyzes the re-\nlationship between protection efficacy and the target image.\nWhen a target image with low pattern repetition is used,\nthe protection is effective in the Face and Cartoon domains\nbut less so for Objects. In contrast, a high-repetition tar-\nget image leads to strong performance in the Object do-\nmain. The proposed adaptive targeted protection demon-\nstrates near-optimal performance across all scenarios. Since\nit is difficult to predict which domain will be encountered\nin real-world use cases, relying on a single domain with a\nfixed target image is not practical. Therefore, incorporating\nadaptive protection into FastProtect is essential for creating\na more robust protection solution in real-world applications.\nAdaptive Protection Strength. We analyze the impact of\nusing the proposed adaptive protection strength in Figure 7b\nby adjusting the perturbation budget. Without this module,\nthe trade-off between protection efficacy and invisibility is\nworse than that of the full model. Note that when the bud-\nget is small, the perturbation is inherently minimal, so the\ndifference could be marginal. However, with stronger pro-\ntection, the difference becomes significant.\nFastProtect + Iterative Optimization. FastProtect is ex-\ntremely fast, so users with sufficient computing resources\nmight want to invest additional computation to achieve even\nmore effective protection. Considering this scenario, in Fig-\nure 7c, we use the results of FastProtect as the initial per-\nturbation and further refine them using PGD (e.g. Photo-\nGuard [26]). Note that we display the baseline, PhotoGuard,\nwith a slightly larger budget since applying PGD to Fast-\nProtect tends to decrease invisibility. Hence, the baseline\nbudget is adjusted to match the invisibility level of FastPro-\ntect + PGD. Surprisingly, our method serves as a superior\ninitial checkpoint for the iterative optimization techniques.\nFor example, the baseline, requires 100 steps to match our\ninitial results, whereas FastProtect + PGD converges in just\n25 steps with a significantly higher protection efficacy.\n5. Conclusion\nIn this work, we propose FastProtect, which leverages pre-\ntrained mixture-of-perturbations for low latency. We also\npropose adaptive inference to compensate the loss of the\nprotection efficacy and to improve the invisibility of the\nperturbation. Our experiments demonstrate that FastProtect\noffers a more practical solution with comparable protection\nperformance to existing methods, with improved invisibility\nand substantially reduced inference time.\nLimitations. FastProtect still produces some visible distor-\ntion and this is an unavoidable drawback when using adver-\nsarial perturbations. Future research should focus on find-\ning new paradigms that can maximize the quality.\n28808\n\n=== Page 9 ===\nAcknowledgment.\nThis work was partly supported by\nNAVER WEBTOON and by Institute of Information &\ncommunications Technology Planning & Evaluation (IITP)\ngrant funded by the Korea government(MSIT) under\nthe Artificial Intelligence Convergence Innovation Human\nResources Development, Inha University (No.RS-2022-\n00155915) and under the Leading Generative AI Human\nResources Development (IITP-2025-RS-2024-00360227).\nReferences\n[1] Namhyuk Ahn, Wonhyuk Ahn, KiYoon Yoo, Daesik Kim,\nand Seung-Hun Nam.\nImperceptible protection against\nstyle imitation from diffusion models.\narXiv preprint\narXiv:2403.19254, 2024. 2, 3, 5, 6\n[2] Namhyuk Ahn, Junsoo Lee, Chunggi Lee, Kunhee Kim,\nDaesik Kim, Seung-Hun Nam, and Kibeom Hong. Dream-\nstyler: Paint by style inversion with text-to-image diffusion\nmodels. In Proceedings of the AAAI Conference on Artificial\nIntelligence, pages 674\u2013681, 2024. 1, 7\n[3] David Arthur, Sergei Vassilvitskii, et al. k-means++: The\nadvantages of careful seeding. In Soda, pages 1027\u20131035,\n2007. 4\n[4] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Simoncelli.\nImage quality assessment: Unifying structure and texture\nsimilarity. IEEE transactions on pattern analysis and ma-\nchine intelligence, 44(5):2567\u20132581, 2020. 3, 6\n[5] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 1, 2, 7\n[6] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.\nExplaining and harnessing adversarial examples.\narXiv\npreprint arXiv:1412.6572, 2014. 1\n[7] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 3, 6\n[8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 1\n[9] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 3, 6\n[10] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Composer: Creative and controllable im-\nage synthesis with composable conditions. arXiv preprint\narXiv:2302.09778, 2023. 1\n[11] Sungnyun Kim, Junsoo Lee, Kibeom Hong, Daesik Kim,\nand Namhyuk Ahn. Diffblender: Scalable and composable\nmultimodal text-to-image diffusion models. arXiv preprint\narXiv:2305.15194, 2023. 1\n[12] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 4\n[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. Advances in neural information processing systems,\n25, 2012. 2\n[14] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 1\n[15] Chumeng Liang and Xiaoyu Wu. Mist: Towards improved\nadversarial examples for diffusion models. arXiv preprint\narXiv:2305.12683, 2023. 3, 4, 6\n[16] Chumeng Liang, Xiaoyu Wu, Yang Hua, Jiaru Zhang, Yim-\ning Xue, Tao Song, Zhengui Xue, Ruhui Ma, and Haibing\nGuan.\nAdversarial example does good: preventing paint-\ning imitation from diffusion models via adversarial exam-\nples. In Proceedings of the 40th International Conference on\nMachine Learning, pages 20763\u201320786, 2023. 1, 3, 6\n[17] Yixin Liu, Chenrui Fan, Yutong Dai, Xun Chen, Pan Zhou,\nand Lichao Sun.\nMetacloak:\nPreventing unauthorized\nsubject-driven text-to-image diffusion-based synthesis via\nmeta-learning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 24219\u2013\n24228, 2024. 3\n[18] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,\nDimitris Tsipras, and Adrian Vladu. Towards deep learn-\ning models resistant to adversarial attacks. arXiv preprint\narXiv:1706.06083, 2017. 1, 2\n[19] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. arXiv preprint arXiv:2108.01073, 2021. 1\n[20] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar\nFawzi, and Pascal Frossard. Universal adversarial perturba-\ntions. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 1765\u20131773, 2017. 2,\n3, 4, 7\n[21] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 1\n[22] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 1, 7\n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2\n[24] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1, 2, 7\n28809\n\n=== Page 10 ===\n[25] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 1, 2, 3\n[26] Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew\nIlyas, and Aleksander Madry. Raising the cost of malicious\nai-powered image editing. arXiv preprint arXiv:2302.06588,\n2023. 1, 3, 6, 7, 8\n[27] Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng,\nRana Hanocka, and Ben Y Zhao. Glaze: Protecting artists\nfrom style mimicry by text-to-image models. arXiv preprint\narXiv:2302.04222, 2023. 1, 3\n[28] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro\nChin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,\nGlenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image\ngeneration in any style. arXiv preprint arXiv:2306.00983,\n2023. 1\n[29] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 1\n[30] Thanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan\nDao, Ngoc N Tran, and Anh Tran. Anti-dreambooth: Pro-\ntecting users from personalized text-to-image synthesis. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 2116\u20132127, 2023. 1, 3, 6\n[31] Cong Wan, Yuhang He, Xiang Song, and Yihong Gong.\nPrompt-agnostic adversarial perturbation for customized dif-\nfusion models. Advances in Neural Information Processing\nSystems, 37:136576\u2013136619, 2024.\n[32] Jingyao Xu, Yuetong Lu, Yandong Li, Siyang Lu, Dongdong\nWang, and Xiang Wei. Perturbing attention gives you more\nbang for the buck: Subtle imaging perturbations that effi-\nciently fool customized diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 24534\u201324543, 2024.\n[33] Haotian Xue, Chumeng Liang, Xiaoyu Wu, and Yongxin\nChen. Toward effective protection against diffusion-based\nmimicry through score distillation. In The Twelfth Interna-\ntional Conference on Learning Representations, 2023. 2, 3,\n6\n[34] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 1\n[35] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 5\n[36] Zhengyue Zhao, Jinhao Duan, Xing Hu, Kaidi Xu, Chenan\nWang, Rui Zhang, Zidong Du, Qi Guo, and Yunji Chen. Un-\nlearnable examples for diffusion models: Protect data from\nunauthorized exploitation. arXiv preprint arXiv:2306.01902,\n2023. 2\n[37] Boyang Zheng, Chumeng Liang, Xiaoyu Wu, and Yan Liu.\nUnderstanding and improving adversarial attacks on latent\ndiffusion model. arXiv preprint arXiv:2310.04687, 2023. 2,\n3, 4, 5\n28810\n",
      "searchable_content": "page 1 nearly zero-cost protection against mimicry by personalized diffusion models namhyuk ahn 1 2 kiyoon yoo 3 wonhyuk ahn 2 daesik kim 2 seung-hun nam 2 1 inha university 2 naver webtoon ai 3 krafton 512 768 1024 1660 2048 0.01 0.1 1 10 50 100 500 1000 5000 time anti-db mist fastprotect advdm photoguard sdst impasto a inference latency log-scaled vs. image size 0.15 0.17 0.19 0.21 0.23 0.25 185 195 205 215 225 235 245 255 fid dists fastprotect photoguard mist impasto sdst advdm anti-db b protection efficacy vs. invisibility figure 1. a fastprotect shows unprecedented speed in protection against diffusion models. on an a100 gpu fastprotect achieves real-time latency even for processing 20482-px image while others require substantially longer time. b in terms of the trade-off between protection efficacy fid is better and invisibility dists is better fastprotect exhibits improvement over other protection methods. abstract recent advancements in diffusion models revolutionize image generation but pose risks of misuse such as replicat- ing artworks or generating deepfakes. existing image pro- tection methods though effective struggle to balance pro- tection efficacy invisibility and latency thus limiting prac- tical use. we introduce perturbation pre-training to reduce latency and propose a mixture-of-perturbations approach that dynamically adapts to input images to minimize perfor- mance degradation. our novel training strategy computes protection loss across multiple vae feature spaces while adaptive targeted protection at inference enhances robust- ness and invisibility. experiments show comparable protec- tion performance with improved invisibility and drastically reduced inference time. the code and demo are available at https webtoon.github.io impasto 1. introduction large-scale diffusion models 8 22 24 29 have achieved remarkable success in the realm of image synthesis task revolutionizing the way we create and manipulate digital imagery 10 11 19 21 34 . by adopting recently emerged personalization techniques it is now possible to develop diffusion models that can generate images in one s unique styles 2 5 14 25 28 . however the very capabilities that make these models so valuable also pose significant risks. diffusion models can allow malicious users to replicate an individual s artwork without consent easily stealing their creative output 27 . furthermore diffusion models are ex- ceedingly adept at mimicking faces and objects 14 25 . this proficiency is not limited to benign applications but can be extended to the making of deepfakes or fake news. the implications for societal harm are profound as these technologies could be used to undermine public trust dis- tort political discourse and violate personal privacy. current efforts to protect images against diffusion mod- els have evolved through adversarial perturbation 6 . by introducing perturbations to a given image they render the protected image resistant to diffusion-based personaliza- tion methods causing the diffusion model to generate dis- torted outputs. existing methods primarily rely on two ap- proaches minimizing the distance between the latent fea- tures of the protected and a pre-defined target image within a vae encoder e.g. photoguard 26 and glaze 27 or fooling the noise prediction network e.g. advdm 16 and anti-db 30 . despite numerous advancements all cur- rent frameworks fundamentally start from an approach that optimizes perturbations during inference 18 when protec- tion is requested casting doubt on the practicality of these methods to be used in many real applications. for a protection framework to become a practical so- lution the following conditions should be satisfied 1 protection efficacy obviously image protection method should effectively counteract against diffusion-based per- sonalization. many recent studies have aimed to maximize this aspect by analyzing the internal dynamics of diffusion this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 28801 page 2 models 33 37 . 2 invisibility although some studies strive to be as invisible as possible adversarial perturba- tions inevitably leave visible traces on images. however for commercial services minimizing these traces is crucial. moreover as analyzed by ahn et al. 1 the task of pre- venting mimicry tends to leave more noticeable traces par- ticularly in images with flat textures e.g. cartoon or illustra- tion . despite its importance only a handful of studies high- light this issue 1 . 3 latency all existing methods up- date perturbations during the inference phase through itera- tive optimization. however this process is inherently time- consuming e.g. protecting a 5122-px image can take up to 5-120 minutes on a cpu and 7-200 seconds even on a high- end a100 gpu. the extensive time required by current pro- tection methods poses a significant barrier that prevents or- dinary users from utilizing image protection thereby still exposing them to the risk of misuse of generative models. consequently in order to democratize this technology it is crucial to develop protection framework that can operate effectively on less powerful devices. unfortunately despite extensive progresses in the image protection task there is scant focus on reducing latency. in this work we propose a novel protection framework fastprotect which meets all the requirements with a par- ticular focus on latency. fastprotect diverges from preva- lent approaches by adopting a pre-trained perturbation. al- though universal adversarial perturbation uap 20 is pro- posed in adversarial attack we observed that it significantly reduces protection efficacy for our task. to counteract this we introduce a mixture-of-perturbations mop . unlike uap which relies on a single perturbation mop prepares multiple perturbations and selects one of them based on the input image s latent code. we also propose a multi-layer protection mlp loss that utilizes the intermediate fea- tures when calculating the protection loss. it intensifies the distortion when the protected images go through fine-tuning by personalization methods thus enhancing the protection efficacy without any additional cost at inference. for the inference we introduce an adaptive targeted protection to enhance protection efficacy with minimal computational overhead. our observations suggest that in targeted protection which minimizes the distance between the input image s latent code and that of a pre-defined tar- get selecting an appropriate target image significantly in- fluences protection performance. taking this into account we adaptively determine the best target image based on the input image s latent code and then apply the perturbation trained for that specific target. to enhance invisibility we propose an adaptive protection strength approach using the lpips 36 distance. unlike ahn et al. 1 that lever- ages multiple just-noticeable difference jnd maps our approach does not require ad-hoc modules nor does it sig- nificantly increase computational load it only involves an additional forward pass of lpips i.e. alexnet 13 . we conduct extensive experiments to verify the effec- tiveness and efficiency of our protection method. to sim- ulate various use-cases we test across diverse domains natural images faces paintings and cartoons under dif- ferent personalization and countermeasure scenarios. fast- protect achieves similar protection efficacy to other meth- ods but at nearly zero cost with a 200 to 3500 speedup figure 1a . it also shows an improvement in invisibility compared to most methods figure 1b proving its suitabil- ity for practical applications. our main contributions are we propose fastprotect which achieves real-time pro- tection against diffusion models. our work is the first to address the critical issue of latency in this task. fastprotect integrates perturbations pre-training with adaptive inference schemes meeting all requirements for a practical protection solution. we validate fastprotect in various scenarios showing that despite its speed and invisibility it retains protec- tion efficacy robustness and generalization. 2. background diffusion models. these have gained prominence due to their ability to generate high-quality images. latent dif- fusion model ldm 24 is particularly highligted for its exceptional quality and efficiency. within ldm a vae encoder e transforms an input image x into a latent code z e x . this code is then reconstructed back into the im- age domain by a decoder d x d z . the diffusion process makes a latent code z by incorporating external factors y such as textual prompts. the training of ldm is driven by a denoising loss function at each timestep t as lsd ez e x y \u03f5 n 0 1 t \u03f5 \u03f5\u03b8 zt t c y 2 2 1 where a denoising network \u03f5\u03b8 restores the noised latent code zt based on the timestep t and a conditioning vec- tor c y . recent advancements explore the personalization or few-shot fine-tuning of ldm with a few reference im- ages through two primary approaches textual inversion 5 which utilizes the embedding space of clip 23 while keeping the denoising network fixed and model optimiza- tion 25 which directly updates the denoising network. protection against diffusion-based mimicry. current protection frameworks apply adversarial perturbations \u03b4 to image x producing a protected image \u02c6x x \u03b4 using pro- jected gradient descent pgd 18 . with a protection loss lp current methods obtain the protected image of i-th opti- mization step by a signed gradient ascent with step function sgn and step length \u03b1 as given by x i \u03c0n\u03b7 x h x i 1 \u03b1sgn x i lp x i 1 i 2 28802 page 3 where \u03c0n\u03b7 x projects onto the l neighborhood of x with a radius of \u03b7. this iterative process is repeated for n steps until \u02c6x x n is achieved figure 2a . for the objective function semantic or texture losses are mostly used 15 . the semantic loss ls x lsd x is designed to disrupt the denoising process of ldm mis- leading it to generate samples that deviate from the original images. advdm 16 adopts this loss and anti-db 30 is improved to be more robust against dreambooth 25 by in- corporating its update process within the optimization. the texture loss lt x e x e y 2 2 aims to pushing the latent code of x towards the target image s latent code. glaze 27 and photoguard 26 belong to this category. upon these objective functions we can now define a uni- versal loss as lp \u03bbsls \u03bbt lt where \u03bbs \u03bbt 0 are balancing factors. mist 15 utilizes this universal loss function to effectively capture the characteristics of both objectives. diff-protect 33 building on mist introduces a score distillation trick to achieve more efficient protec- tion. impasto 1 incorporates perceptual-oriented compo- nents designed for imperceptible protection. since all the current protection frameworks leverage it- erative optimization during inference they can yield image- specific and high-performing perturbations 1 15 17 26 27 30 33 37 . despite this advantage a significant draw- back is the substantial time required to operate protection. for instance to protect a 5122-px image even the fastest model e.g. photoguard 26 requires around 7 seconds on an a100 gpu and 350 seconds on a cpu. consequently we explore alternatives to this iterative process to enhance efficiency without compromising the protection. universal adversarial perturbation uap . this concept is introduced in adversarial attacks through a data-driven approach to find an image-agnostic perturbation from the training dataset 20 . in our task we can adapt uap as in figure 2b. given the training dataset xd we employ a protection loss lp to derive an universal perturbation \u03b4 constrained within \u03b7-ball to maintain perceptual invisibility. \u03b4 arg max \u03b4 \u03b7 ex xd lp x \u03b4 . 3 upon finalizing the universal perturbation \u03b4 by the train- ing it can be applied to an image x to produce a protected image \u02c6x x \u03b4. this is notably practical as it requires no computation at inference. nevertheless we found that directly applying uap to our task significantly compro- mises protection performance. in table 1 we compare the protection efficacy of iterative optimization via pgd and uap. we use dists 4 to assess protected images quality while fid 7 evaluates generated image through mimicry by lora 9 a higher fid is better protection . as also shown in the qualitative mimicry results by lora uap leads to a notable degradation in protection efficacy. hence table 1. pgd vs. uap top invisibility dists and protection efficacy fid comparison. bottom mimicry results via lora. method invisibility dists protection fid pgd 0.221 227.6 uap 0.222 207.6 clean pgd uap it is crucial to refine this pre-training approach but with a minimal inference cost in a manner akin to uap. 3. method 3.1. perturbation pre-training mixture-of-perturbation mop . we hypothesize that the underperformance of uap is primarily due to two reasons 1 a single perturbation has a limited capacity to deceive diffusion models and 2 the image-agnostic nature of uap fails to cover the diverse features e.g. texture or structure of images. to overcome these we introduce a mixture-of- perturbations mop consisting of multiple perturbations \u03b41 . . . \u03b4k where k represents the number of per- turbations. for any given input image mop dynamically assigns and applies the appropriate perturbation to create a protected image figure 2c . specifically given an image x mop first encodes it using a vae encoder e to obtain the latent code z. this code is used to select the specific pertur- bation to be applied as dictated by an assignment function a. formally the protected image \u02c6x is generated as \u02c6x x \u03b4g k where k a e x 4 where \u03b4g is a global perturbation. we observed that adding \u03b4g slightly improves the performance. in mop the assign- ment function a plays a crucial role by guiding the se- lection of perturbations based on an input image ensur- ing that similar images are protected with the same pertur- bation. this capability enables mop to not only increase the capacity of uap but also to offer a degree of image- specific protection which is notably absent in uap. with- out the assignment function a naive perturbation averaging i.e. \u02c6x x \u03b4g 1 k \u03c3k k 1 k increases the capacity but fail to overcome the intrinsic limitations of image-agnostic thus limiting performance benefits. in contrast our mop implements a semi-image-specific protection positioning it between the image-agnostic and the image-specific preci- sion of the iterative approach while removing the expensive inference cost as demonstrated in the ablation study. we adopt a simple assignment function a that can lever- age the highly representative latent codes of the vae en- 28803 page 4 figure 2. model overview. a current iterative optimization approaches lack a training phase and perform optimization during inference resulting in extremely slow protection. b uap 20 introduces pre-training of perturbations but their image-agnostic nature leads to degraded protection efficacy. c combining the advantages of both paradigms fastprotect adopts a pre-training approach similar to uap but with a novel mixture-of-perturbation scheme and multi-layer protection loss to enhance protection efficacy. at inference adaptive targeted protection further boosts protection efficacy with minimal additional cost and adaptive protection strength improves invisibility. coder. for our training dataset xd we extract the latent codes z z1 . . . zd using the vae encoder e. these codes are then clustered into k predefined groups with k- means 3 . despite its simplicity this process shows its robustness empirically thus we adopt it for our framework. multi-layer protection loss. fastprotect employs a tar- geted protection approach based on the texture loss. con- ventionally texture loss is computed upon the vae latent code z i.e. lt x z zy 2 2 where zy is the latent code of the target image. however we observed that of- ten the z-space loss does not sufficiently push z towards zy resulting in suboptimal protection performance. to ad- dress this we incorporate an auxiliary loss using multi- layer features extracted by the vae encoder. specifically given an input image we extract the intermediate features f f 1 . . . f l where l is the number of feature lay- ers extracted. similarly we obtain fy for the target im- age. then our proposed multi-layer protection loss func- tion with the balanced factor \u03bb is computed as lt x z zy 2 2 \u03bb l l x l 1 fl fl y 2 2. 5 the auxiliary mlp loss improves overall protection ef- ficacy as this leverages the intermediate feature spaces be- yond the z-space when pushing z toward zy. in addition since these modifications are implemented during the pre- training stage it improves protection efficacy without extra computational expense at inference. training. the assignment function a is first trained on xd then mop is updated on xd using eq. 5 by adam opti- mizer 12 . all the perturbations \u03b4g are initialized at a resolution of 512 512. when applied to input images of different resolutions we resize the perturbation through bilinear interpolation. since two perturbations are used in mop \u03b4g are constrained within a \u03b7 2 -ball. 3.2. adaptive inference adaptive targeted protection. in targeted protection tar- get image influences protection performance. liang and wu 15 noted that patterned image is effective while zheng et al. 37 reported that the protection efficacy varies with pattern repetitions. though they provide valuable insights they do not consider the relationship between the target and input images. instead we investigate such relationship and propose an adaptive targeted protection approach. in our analysis we use two target images characterized by either low or high pattern repetition shown in appendix and apply protection to input images with either simple or complex textures. figure 3 shows mimicry outputs by lora of these four protection scenarios. interestingly our observations suggest a relationship between the pattern rep- 28804 page 5 figure 3. relationship between target image s pattern repetition and input image s texture. simple textured image is successfully protected by a low repetition target but fails when using a high repetition target vice versa for complex texture cases. etition of the target image and the texture complexity of an input image. zheng et al. 37 noted that protection efficacy improves as pattern repetition increases but declines when it becomes too high. we suspect that their results might be biased by fixing the input image thereby keeping the tex- ture factor constant. in contrast our findings indicate that tailoring the pattern image to match the texture complexity of an input image can improve protection efficacy. hence we design fastprotect to adaptively select the tar- get image based on an input image. to this end we pre- pare target images with varying pattern repetitions and train mops for each. we utilize three pattern images low mid and high resulting in three mops \u03b4l g l \u03b4m g m and \u03b4h g h . we then extract the latent codes for each target image zl y e yl zm y e ym zh y e yh where yl ym yh are the target images with low mid and high pattern repetitions respectively. at inference input image s latent code z is compared with zl y zm y zh y and the target image whose latent code is closest to z is selected. to measure the similarity between the target images pattern repetition and the input image l1 norm of entropy is used as a distance. overall integration of mop in eq. 4 and the adaptive targeted protection are as follows \u02c6x x \u03b4t g t k t arg min i l m h h z h zi y 1 6 where h z p z z p z log p z . the major advan- tage of our adaptive targeted protection is its robust perfor- mance across various domains which is critically important for practical use cases where the specific domain or tex- ture of incoming images might be unknown. if the range of potential domains is known beforehand the performance difference might not be significant. however the flexibil- ity to adapt to different input characteristics without prior domain knowledge enhances fastprotect s utility in diverse and unpredictable environments of real applications. adaptive protection strength. according to the we- ber fechner law humans are more adept at detecting sub- figure 4. given the original and protected images we obtain the lpips distance map which remarkably aligns with human per- ception. the brighter regions on the perceptual map indicate areas where subtle distortions are more noticeable. tle changes in regions with simple textures than in complex ones. upon this ahn et al. 1 adapted human visual system to the image protection task. they utilized multiple percep- tion maps and adjusted perturbation strength during opti- mization to effectively enhance invisibility. however the perception maps used by ahn et al. 1 are computed before the injection of perturbations leading us to hypothesize that these may not perfectly align with the actual perceived per- turbations intended for protection. moreover their reliance on a combination of traditional perceptual map algorithms resulted in slow processing and performance limitations. to address these challenges we first apply mop to create a sur- rogate protection image and then generate a perceptual map which helps resolve the identified issues. specifically we generate a surrogate protection image \u02c6x through eq. 6. subsequently we create a spatial perceptual map using lpips 35 i.e. m lpips x \u02c6x . as shown in figure 4 the perceptual map m is remarkably aligned with human cognitive perspectives. hence we utilize this map to produce the final protected image as in below \u02c6x x s 1 m \u03b4t g t k . 7 here s is a scaling function more details are in ap- pendix and m serves as a distance map necessitating an inversion step. since this process requires only the forward pass of the lpips backbone the additional computational cost at inference is minimal thus economically enhancing invisibility without significant overhead. in appendix we summarize the perturbation pre-training and inference pro- cess in the form of an algorithm. 4. experiment implementation details. we set the number of perturba- tions k in mop as four. other hyperparameters and details on training and inference are described in appendix. datasets. we utilize four domains when constructing both the training and benchmark datasets object face painting 28805 page 6 table 2. quantitative comparison. latency is measured on 512 512 image. comparisons of other metrics are shown in appendix. method latency object face painting cartoon cpu gpu invisibility dists efficacy fid advdm 16 1210s 35s 0.197 220.0 0.173 303.8 0.153 357.6 0.271 212.5 photoguard 26 370s 7s 0.203 223.0 0.189 308.7 0.107 350.9 0.209 219.1 anti-db 30 7278s 225s 0.239 214.4 0.162 301.4 0.114 347.7 0.294 225.4 mist 15 1440s 40s 0.185 217.2 0.154 307.5 0.129 357.0 0.223 223.7 impasto 1 830s 19s 0.201 213.8 0.198 298.4 0.123 352.4 0.207 215.5 sdst 33 1410s 24s 0.242 219.2 0.244 302.1 0.152 354.1 0.237 222.7 fastprotect 2.9s 0.04s 0.155 223.0 0.149 308.9 0.110 356.1 0.186 220.3 figure 5. qualitative comparison of different protection frameworks. top protected image with a zoomed-in patch in the inset. bottom two output images from the personalized lora. and cartoon. these multiple scenarios allow us to analyze the models protection performance more closely to real ap- plications as images from various domains can be encoun- tered. the dataset details are provided in appendix. baselines. we compare with existing diffusion-based im- age protection frameworks advdm 16 photoguard 26 anti-db 30 mist 15 impasto 1 and diff-protect sdst 33 . except ours all other methods rely on iter- ative optimization using texture or semantic losses. evaluation. the protection efficacy is evaluated against lora 9 on stable diffusion v1.5. latency is measured using an m1 max cpu and an a100 gpu. we employ dists 4 to evaluate invisibility and fid 7 to quan- tify protection efficacy. in appendix we provide additional metrics for a more comprehensive comparison. note that previous studies typically fix the protection strength across models for comparison but we found that protection ef- ficacy and invisibility vary significantly depending on the protection loss used. for instance photoguard 26 and advdm 16 exhibit different trends since they use dis- parate objectives. moreover evaluating protection efficacy only without invisibility leads to a narrow assessment that might overlook real-world needs. to address these we ad- just the protection strength to match the protection level across methods to ensure a fairer comparison and better re- flection of practical requirements. however one might also be curious about how the performance appears when the protection strength is equalized across all methods. hence we provide evaluation with a fixed strength in appendix. 4.1. model comparison as shown in figure 1a and table 2 fastprotect is ultra-fast 125 faster on cpu and 175 faster on gpu compared to the second fastest model photoguard 26 . remarkably fastprotect maintains consistent latency even as the input image size increases achieving near real-time performance even for 20482-px images. in contrast all other frameworks show exponential increases in latency figure 1a which poses a significant issue in the real world where many recent artworks are high-resolution. in addition fastprotect occu- pies only 1.7gb of vram during inference unlike other methods that require more than 8gb. the unprecedented speed of our model makes it highly user-friendly. fastprotect also demonstrates a superior trade-off be- tween protection efficacy and invisibility figure 1b . in comparisons where the protection strength is adjusted to yield similar protection efficacy table 2 fastprotect con- sistently achieves the best invisibility across most protection 28806 page 7 table 3. ablation study. configuration fid photoguard 227.6 uap 20 207.6 mop w o a 214.5 mop 225.9 mlp loss 234.6 adapt. target 238.8 table 4. analysis on protection robustness scenarios. domain method invisibility countermeasure arbitrary size noise jpeg subject photoguard 0.203 193.3 193.2 193.5 fastprotect 0.155 214.4 191.3 219.1 cartoon photoguard 0.209 192.9 193.1 190.7 fastprotect 0.186 191.8 199.9 204.0 table 5. analysis of black-box protection scenarios unknown diffusion models and personalization methods . domain method invisibility unknown model unknown personalization sd-v2.1 24 sd-xl 22 ti 5 dreamstyler 2 subject photoguard 0.203 176.6 190.9 290.2 224.9 fastprotect 0.155 177.5 218.4 305.6 231.3 cartoon photoguard 0.209 179.3 195.1 306.4 193.7 fastprotect 0.186 188.3 207.3 305.7 209.9 domains with the second-best performance in the painting domain. in our analysis fastprotect shows the strong ad- vantage in invisibility within areas that have flat textures. these features result in significant improvements in the ob- ject and cartoon domains. for example figure 5 shows that in the first row the sky region is rendered nearly invisible by fastprotect and the same applies to the cartoon in the second row. as demonstrated in appendix even when we fix the protection strength of all methods to the same value our method achieves a favorable balance in this trade-off. 4.2. model analysis ablation study. in table 3 we depict the component anal- ysis of fastprotect. it is notable that uap 20 significantly deteriorates protection efficacy compared to the iterative op- timization baseline photoguard 26 . on the other hand when we introduce the proposed mop it recovers the lost performance. however if the assignment function a is not used the improvement is limited highlighting the impor- tance of achieving a semi-image-specific nature. as we dis- cussed the limitations of uap arise not only from insuffi- cient capacity but also from its image-agnostic nature. sim- ply increasing the number of perturbations i.e. mop w o a resolves the first limitation and improves performance over uap. however this still does not overcome the second one which causes it to underperform compared to photoguard. when we introduce the assignment function into mop it can dynamically account for image features. this allows mop to match the performance of iterative optimization methods while remaining significantly faster. when we attach the multi-layer protection mlp loss it significantly increases the protection efficacy. similarly adaptive targeted protection also enhances performance. overall by incorporating novel modules in both the pre- training and inference stages fastprotect can achieve better protection efficacy with a much faster inference. robustness. here we analyze the vulnerability of pro- tected images to countermeasures table 4 . we use pho- toguard 26 as our baseline due to its similarity to our ap- proach. to assess robustness we evaluate against counter- measures such as gaussian noise and jpeg compression. additionally we account for a more realistic scenario where the input image can have arbitrary dimensions. most previ- ous evaluations in image protection tasks assume a fixed image resolution typically 5122 which represents an ide- alized scenario. in practice image sizes and aspect ratios vary. our model demonstrates comparable performance against these countermeasures and notably in the arbitrary size scenario fastprotect performs even better. this sug- gests that simply using interpolation to adapt mop to differ- ent sizes is both straightforward and effective. black-box scenario. table 5 presents the model analy- sis in black-box scenarios. we examine two cases the un- known model scenario where we transfer protection meth- ods to other stable diffusion sd backbones specifically sd v2.1 24 and sd-xl 22 . we also assess image pro- tection against unknown personalization by applying tex- tual inversion ti 5 and dreamstyler 2 to replicate the image. in these scenarios fastprotect demonstrates supe- rior invisibility compared to the baseline while achieving comparable or better performance in black-box settings. mixture-of-perturbation. in figure 6a we vary the train- ing dataset xd and measure the protection efficacy. inter- estingly even when the training domain is limited there is no significant gap in the protection performance. we con- jecture that mop effectively handles unseen domains thanks to its adaptive perturbation selection mechanism. however using all domains is slightly better so we train with this strategy. additionally figure 6b compares fastprotect by changing k and excluding the assignment function a in mop. without the assignment function performance im- provement is limited and in the full mop case it achieves 28807 page 8 object face painting cartoon 200 220 240 260 280 300 320 340 fid object face painting cartoon all a domain generalization of mop 1 2 4 6 8 210 215 220 225 fid mop mop w o b effect of assignment function c examples of mop group figure 6. analysis of the proposed modules in the pre-training phase. object face painting cartoon 200 220 240 260 280 300 320 340 fid low pattern rep. mid pattern rep. high pattern rep. adaptive target a adapt. targeted protection 0.20 0.22 0.24 0.26 220 230 240 250 260 fid dists w adaptive protection strength w o adaptive protection strength b adapt. protection strength 1 2 5 10 25 50 100 185 195 205 215 225 235 245 fid baseline fastprotect fastprotect pgd c fastprotect pgd refine figure 7. analysis of the proposed modules in the inference phase. substantially better efficacy with convergence at k 4. in figure 6c we visualize the representative images assigned to each perturbation when k 4. the images are grouped by their certain distinguishing features e.g. texture scene . adaptive targeted protection. figure 7a analyzes the re- lationship between protection efficacy and the target image. when a target image with low pattern repetition is used the protection is effective in the face and cartoon domains but less so for objects. in contrast a high-repetition tar- get image leads to strong performance in the object do- main. the proposed adaptive targeted protection demon- strates near-optimal performance across all scenarios. since it is difficult to predict which domain will be encountered in real-world use cases relying on a single domain with a fixed target image is not practical. therefore incorporating adaptive protection into fastprotect is essential for creating a more robust protection solution in real-world applications. adaptive protection strength. we analyze the impact of using the proposed adaptive protection strength in figure 7b by adjusting the perturbation budget. without this module the trade-off between protection efficacy and invisibility is worse than that of the full model. note that when the bud- get is small the perturbation is inherently minimal so the difference could be marginal. however with stronger pro- tection the difference becomes significant. fastprotect iterative optimization. fastprotect is ex- tremely fast so users with sufficient computing resources might want to invest additional computation to achieve even more effective protection. considering this scenario in fig- ure 7c we use the results of fastprotect as the initial per- turbation and further refine them using pgd e.g. photo- guard 26 . note that we display the baseline photoguard with a slightly larger budget since applying pgd to fast- protect tends to decrease invisibility. hence the baseline budget is adjusted to match the invisibility level of fastpro- tect pgd. surprisingly our method serves as a superior initial checkpoint for the iterative optimization techniques. for example the baseline requires 100 steps to match our initial results whereas fastprotect pgd converges in just 25 steps with a significantly higher protection efficacy. 5. conclusion in this work we propose fastprotect which leverages pre- trained mixture-of-perturbations for low latency. we also propose adaptive inference to compensate the loss of the protection efficacy and to improve the invisibility of the perturbation. our experiments demonstrate that fastprotect offers a more practical solution with comparable protection performance to existing methods with improved invisibility and substantially reduced inference time. limitations. fastprotect still produces some visible distor- tion and this is an unavoidable drawback when using adver- sarial perturbations. future research should focus on find- ing new paradigms that can maximize the quality. 28808 page 9 acknowledgment. this work was partly supported by naver webtoon and by institute of information communications technology planning evaluation iitp grant funded by the korea government msit under the artificial intelligence convergence innovation human resources development inha university no.rs-2022- 00155915 and under the leading generative ai human resources development iitp-2025-rs-2024-00360227 . references 1 namhyuk ahn wonhyuk ahn kiyoon yoo daesik kim and seung-hun nam. imperceptible protection against style imitation from diffusion models. arxiv preprint arxiv 2403.19254 2024. 2 3 5 6 2 namhyuk ahn junsoo lee chunggi lee kunhee kim daesik kim seung-hun nam and kibeom hong. dream- styler paint by style inversion with text-to-image diffusion models. in proceedings of the aaai conference on artificial intelligence pages 674 681 2024. 1 7 3 david arthur sergei vassilvitskii et al. k-means the advantages of careful seeding. in soda pages 1027 1035 2007. 4 4 keyan ding kede ma shiqi wang and eero p simoncelli. image quality assessment unifying structure and texture similarity. ieee transactions on pattern analysis and ma- chine intelligence 44 5 2567 2581 2020. 3 6 5 rinon gal yuval alaluf yuval atzmon or patash- nik amit h bermano gal chechik and daniel cohen- or. an image is worth one word personalizing text-to- image generation using textual inversion. arxiv preprint arxiv 2208.01618 2022. 1 2 7 6 ian j goodfellow jonathon shlens and christian szegedy. explaining and harnessing adversarial examples. arxiv preprint arxiv 1412.6572 2014. 1 7 martin heusel hubert ramsauer thomas unterthiner bernhard nessler and sepp hochreiter. gans trained by a two time-scale update rule converge to a local nash equilib- rium. advances in neural information processing systems 30 2017. 3 6 8 jonathan ho ajay jain and pieter abbeel. denoising dif- fusion probabilistic models. advances in neural information processing systems 33 6840 6851 2020. 1 9 edward j hu yelong shen phillip wallis zeyuan allen- zhu yuanzhi li shean wang lu wang and weizhu chen. lora low-rank adaptation of large language models. arxiv preprint arxiv 2106.09685 2021. 3 6 10 lianghua huang di chen yu liu yujun shen deli zhao and jingren zhou. composer creative and controllable im- age synthesis with composable conditions. arxiv preprint arxiv 2302.09778 2023. 1 11 sungnyun kim junsoo lee kibeom hong daesik kim and namhyuk ahn. diffblender scalable and composable multimodal text-to-image diffusion models. arxiv preprint arxiv 2305.15194 2023. 1 12 diederik p kingma and jimmy ba. adam a method for stochastic optimization. arxiv preprint arxiv 1412.6980 2014. 4 13 alex krizhevsky ilya sutskever and geoffrey e hinton. imagenet classification with deep convolutional neural net- works. advances in neural information processing systems 25 2012. 2 14 nupur kumari bingliang zhang richard zhang eli shechtman and jun-yan zhu. multi-concept customization of text-to-image diffusion. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 1931 1941 2023. 1 15 chumeng liang and xiaoyu wu. mist towards improved adversarial examples for diffusion models. arxiv preprint arxiv 2305.12683 2023. 3 4 6 16 chumeng liang xiaoyu wu yang hua jiaru zhang yim- ing xue tao song zhengui xue ruhui ma and haibing guan. adversarial example does good preventing paint- ing imitation from diffusion models via adversarial exam- ples. in proceedings of the 40th international conference on machine learning pages 20763 20786 2023. 1 3 6 17 yixin liu chenrui fan yutong dai xun chen pan zhou and lichao sun. metacloak preventing unauthorized subject-driven text-to-image diffusion-based synthesis via meta-learning. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 24219 24228 2024. 3 18 aleksander madry aleksandar makelov ludwig schmidt dimitris tsipras and adrian vladu. towards deep learn- ing models resistant to adversarial attacks. arxiv preprint arxiv 1706.06083 2017. 1 2 19 chenlin meng yutong he yang song jiaming song jia- jun wu jun-yan zhu and stefano ermon. sdedit guided image synthesis and editing with stochastic differential equa- tions. arxiv preprint arxiv 2108.01073 2021. 1 20 seyed-mohsen moosavi-dezfooli alhussein fawzi omar fawzi and pascal frossard. universal adversarial perturba- tions. in proceedings of the ieee conference on computer vision and pattern recognition pages 1765 1773 2017. 2 3 4 7 21 chong mou xintao wang liangbin xie jian zhang zhon- gang qi ying shan and xiaohu qie. t2i-adapter learning adapters to dig out more controllable ability for text-to-image diffusion models. arxiv preprint arxiv 2302.08453 2023. 1 22 dustin podell zion english kyle lacey andreas blattmann tim dockhorn jonas m uller joe penna and robin rombach. sdxl improving latent diffusion mod- els for high-resolution image synthesis. arxiv preprint arxiv 2307.01952 2023. 1 7 23 alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sastry amanda askell pamela mishkin jack clark et al. learning transferable visual models from natural language supervi- sion. in international conference on machine learning pages 8748 8763. pmlr 2021. 2 24 robin rombach andreas blattmann dominik lorenz patrick esser and bj orn ommer. high-resolution image synthesis with latent diffusion models. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 10684 10695 2022. 1 2 7 28809 page 10 25 nataniel ruiz yuanzhen li varun jampani yael pritch michael rubinstein and kfir aberman. dreambooth fine tuning text-to-image diffusion models for subject-driven generation. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 22500 22510 2023. 1 2 3 26 hadi salman alaa khaddaj guillaume leclerc andrew ilyas and aleksander madry. raising the cost of malicious ai-powered image editing. arxiv preprint arxiv 2302.06588 2023. 1 3 6 7 8 27 shawn shan jenna cryan emily wenger haitao zheng rana hanocka and ben y zhao. glaze protecting artists from style mimicry by text-to-image models. arxiv preprint arxiv 2302.04222 2023. 1 3 28 kihyuk sohn nataniel ruiz kimin lee daniel castro chin irina blok huiwen chang jarred barber lu jiang glenn entis yuanzhen li et al. styledrop text-to-image generation in any style. arxiv preprint arxiv 2306.00983 2023. 1 29 yang song jascha sohl-dickstein diederik p kingma ab- hishek kumar stefano ermon and ben poole. score-based generative modeling through stochastic differential equa- tions. arxiv preprint arxiv 2011.13456 2020. 1 30 thanh van le hao phung thuan hoang nguyen quan dao ngoc n tran and anh tran. anti-dreambooth pro- tecting users from personalized text-to-image synthesis. in proceedings of the ieee cvf international conference on computer vision pages 2116 2127 2023. 1 3 6 31 cong wan yuhang he xiang song and yihong gong. prompt-agnostic adversarial perturbation for customized dif- fusion models. advances in neural information processing systems 37 136576 136619 2024. 32 jingyao xu yuetong lu yandong li siyang lu dongdong wang and xiang wei. perturbing attention gives you more bang for the buck subtle imaging perturbations that effi- ciently fool customized diffusion models. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 24534 24543 2024. 33 haotian xue chumeng liang xiaoyu wu and yongxin chen. toward effective protection against diffusion-based mimicry through score distillation. in the twelfth interna- tional conference on learning representations 2023. 2 3 6 34 lvmin zhang anyi rao and maneesh agrawala. adding conditional control to text-to-image diffusion models. in proceedings of the ieee cvf international conference on computer vision pages 3836 3847 2023. 1 35 richard zhang phillip isola alexei a efros eli shecht- man and oliver wang. the unreasonable effectiveness of deep features as a perceptual metric. in proceedings of the ieee conference on computer vision and pattern recogni- tion pages 586 595 2018. 5 36 zhengyue zhao jinhao duan xing hu kaidi xu chenan wang rui zhang zidong du qi guo and yunji chen. un- learnable examples for diffusion models protect data from unauthorized exploitation. arxiv preprint arxiv 2306.01902 2023. 2 37 boyang zheng chumeng liang xiaoyu wu and yan liu. understanding and improving adversarial attacks on latent diffusion model. arxiv preprint arxiv 2310.04687 2023. 2 3 4 5 28810",
      "keywords": [
        "page",
        "nearly",
        "zero-cost",
        "protection",
        "against",
        "mimicry",
        "personalized",
        "diffusion",
        "models",
        "namhyuk",
        "ahn",
        "kiyoon",
        "yoo",
        "wonhyuk",
        "daesik",
        "kim",
        "seung-hun",
        "nam",
        "inha",
        "university",
        "naver",
        "webtoon",
        "krafton",
        "512",
        "768",
        "1024",
        "1660",
        "2048",
        "0.01",
        "0.1",
        "100",
        "500",
        "1000",
        "5000",
        "time",
        "anti-db",
        "mist",
        "fastprotect",
        "advdm",
        "photoguard",
        "sdst",
        "impasto",
        "inference",
        "latency",
        "log-scaled",
        "vs.",
        "image",
        "size",
        "0.15",
        "0.17",
        "0.19",
        "0.21",
        "0.23",
        "0.25",
        "185",
        "195",
        "205",
        "215",
        "225",
        "235",
        "245",
        "255",
        "fid",
        "dists",
        "efficacy",
        "invisibility",
        "figure",
        "shows",
        "unprecedented",
        "speed",
        "models.",
        "a100",
        "gpu",
        "achieves",
        "real-time",
        "even",
        "processing",
        "20482-px",
        "while",
        "others",
        "require",
        "substantially",
        "longer",
        "time.",
        "terms",
        "trade-off",
        "between",
        "better",
        "exhibits",
        "improvement",
        "over",
        "other",
        "methods.",
        "abstract",
        "recent",
        "advancements",
        "revolutionize",
        "generation",
        "pose",
        "risks"
      ],
      "metadata": {
        "filename": "Ahn_Nearly_Zero-Cost_Protection_Against_Mimicry_by_Personalized_Diffusion_Models_CVPR_2025_paper.pdf",
        "original_filename": "Ahn_Nearly_Zero-Cost_Protection_Against_Mimicry_by_Personalized_Diffusion_Models_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 7044542,
        "upload_file_id": "upload_1752160722215_c1mf4dxr3",
        "upload_timestamp": "2025-07-10T15:18:42.269034",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:18:42.614142+00:00",
      "content_length": 44718,
      "content_type": ".pdf",
      "filename": "Ahn_Nearly_Zero-Cost_Protection_Against_Mimicry_by_Personalized_Diffusion_Models_CVPR_2025_paper.pdf"
    },
    "10": {
      "file_id": "10",
      "content": "\n=== Page 1 ===\nGaussian Splatting for Ef\ufb01cient Satellite Image Photogrammetry\nLuca Savant Aira1\nGabriele Facciolo2\nThibaud Ehret3\nhttps://mezzelfo.github.io/EOGS/\n1 Politecnico di Torino, Corso Duca degli Abruzzi, 24, 10129 Torino TO, Italia\n2 Universite Paris-Saclay, CNRS, ENS Paris-Saclay, Centre Borelli, 91190, Gif-sur-Yvette, France\n3 AMIAD, P\u02c6ole Recherche, France\nAbstract\nRecently, Gaussian splatting has emerged as a strong al-\nternative to NeRF, demonstrating impressive 3D modeling\ncapabilities while requiring only a fraction of the training\nand rendering time. In this paper, we show how the stan-\ndard Gaussian splatting framework can be adapted for re-\nmote sensing, retaining its high ef\ufb01ciency. This enables us\nto achieve state-of-the-art performance in just a few min-\nutes, compared to the day-long optimization required by\nthe best-performing NeRF-based Earth observation meth-\nods. The proposed framework incorporates remote-sensing\nimprovements from EO-NeRF, such as radiometric correc-\ntion and shadow modeling, while introducing novel com-\nponents, including sparsity, view consistency, and opacity\nregularizations.\n1. Introduction\nSince the mid-20th century, the number of active satellites\nin orbit has increased exponentially. Today, hundreds of\nsatellites are dedicated to Earth observation, and this num-\nber is expected to continue growing [9]. These satellites\nfrequently acquire optical images of the same areas at reg-\nular intervals. As a result, the availability of these datasets\nwill continue to grow in the near future, requiring ef\ufb01cient\nalgorithms to handle this expansion.\nOne common task for these image datasets is photogram-\nmetry [13, 19, 27], which aims to recover the 3D geome-\ntry (e.g. Digital Surface Model - DSM), and the appearance\n(e.g. an albedo map) of the Earth\u2019s surface for this area us-\ning only the available 2D satellite images. Speci\ufb01cally, we\nfocus on performing digital surface modeling from remote\nsensing images.\nHistorically, binocular stereovision and tri-stereo meth-\nods have been used for this purpose. However, these meth-\nods rely on image acquisitions being nearly simultaneous\nand with speci\ufb01c relative positions, which is often imprac-\ntical, with limited acquisition opportunities, and/or costly.\nEOGS altitude\nEO-NeRF altitude\nPansharpened image\nGround-truth altitude\nFigure 1. Using a limited number of satellite images of a given\nscene, the proposed EOGS method estimates the appearance and\ngeometry of the scene. It achieves the same level of detail as EO-\nNeRF [32], such as the group of fans or the thin structures on top\nof the tall building on the left. However, EOGS requires only a\nfew minutes of optimization, compared to the day-long training\ntime required by EO-NeRF [32].\nTherefore, our goal is to develop a method capable of han-\ndling multi-date images captured from arbitrary satellite po-\nsitions.\nMore recently, multi-view stereo methods tailored for\nnovel-view synthesis (NVS) have been used to solve this\ntask as they naturally handle diverse camera positions. In-\ndeed, as the NVS task also requires understanding the ge-\nometry of the scene, these methods can be used to recover a\ndigital elevation model. These methods are usually based\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n5959\n\n=== Page 2 ===\non the concept of radiance \ufb01eld, i.e. a representation of\nthe scene modeling the radiance emitted by each point, of\nwhich NeRF [36] is the main representative. Among those,\nEO-NeRF [32] has established itself as a state-of-the-art ap-\nproach for digital surface modeling thanks to an improved\nshadow model. However, because it is NeRF-based, it also\ninherits NeRF\u2019s computational slowness. In recent years,\n3D Gaussian Splatting (3DGS) [23] has been proposed as an\nalternative to NeRF. This method offers much faster training\nand rendering while reaching comparable reconstruction ac-\ncuracy.\nIn this work, we introduce EOGS, the Earth-observation\nGaussian splatting, the \ufb01rst method for digital elevation\nmodeling based on 3DGS. EOGS achieves accuracy com-\nparable to previous state-of-the-art approaches while being\napproximately 300\u00d7 faster. Keys to the success of EOGS\nare the following contributions, all of which are compatible\nwith the original 3DGS framework\u2019s ef\ufb01ciency:\n\u2022 Approximating locally the pushbroom satellite sensors as\naf\ufb01ne cameras.\n\u2022 Introducing a shadow-mapping-based pipeline for render-\ning the shadows in a physically accurate manner.\n\u2022 Adding three new regularization terms that promote spar-\nsity in the Gaussians opacities, view consistency, and\ncompletely opaque objects. This reduces the training time\nand improves the quality of the results.\n2. Related Work\n2.1. Stereovision for Earth Observation\nStereovision is at the heart of many tools for 3D estimation\nfrom series of satellite images. Examples of such pipelines\nare Ames stereo pipeline [4], MicMac [43], CARS [35],\nS2P [11], or CATENA [25]. Traditionally, these multi-view\nstereo methods are applied to well-chosen (either manually\nor automatically) image pairs. Since they process each pair\nindependently (for example in the dense stereo matching\nstep), a crucial step is the fusion of all the generated pair-\nwise 3D models into a single one.\nThe recent trend has been to replace classic dense match-\ning methods, such as semi-global matching (SGM) [20]\nor more global matching (MGM) [14], with deep learning\nbased methods such as PSM [8], HSM [53] or GA-Net [54].\nA review of these methods and a comparative study for\nsatellite images is performed in [30].\n2.2. NeRF for Earth Observation\nRecently, Mildenhall et al. [36] have shown that it is pos-\nsible to learn a volumetric model of a scene, called neural\nradiance \ufb01elds (NeRF), using differentiable inverse render-\ning. Given a sparse set of views of the scene, NeRF learns\nin a self-supervised manner by maximizing the photocon-\nsistency across the predicted renderings corresponding to\nthe available viewpoints. After convergence, the volumet-\nric model can then be used to render realistic novel views\nof the scene. In practice, this volumetric model is repre-\nsented by an MLP that predicts, for each position p of the\nspace, the local density of the scene \u03c3(p) as well as its ap-\npearance (i.e. color) c(p). The rendering is performed using\nan approximation of the volumetric integral I from optical\nphysics estimated using ray casting,\nI(o, d) =\nZ tf\ntn\ne\u2212\nR t\ntn \u03c3(r(s))dsc (r(t), d) \u03c3 (r(t)) dt\n(1)\nwith o the camera center from which the ray r originates\nand d unit direction.\nNeRF-based methods were then extended to the remote\nsensing case, and in particular to perform multi-view and\nmulti-date satellite photogrammetry, namely S-NeRF [12],\nSat-NeRF [31], and EO-NeRF [32]. S-NeRF [12] exploits\nthe solar direction, information typically available in the\nmetadata of each observation or that can be easily retrieved\nknowing the location of the scene as well as the acquisi-\ntion hour and date, to predict the direct sun light reach-\ning each point in the scene. This is done by adding the\nsolar direction as an input to the MLP and predicting the\namount of sun light reaching a point as a new output. In\nthis way, the shadows cast by buildings can be learned by\nthe MLP and generated accordingly during the novel-view\nrendering step. Sat-NeRF [31] extends S-NeRF by mod-\neling the transient parts of the scenes (e.g. cars, construc-\ntion sites, or foliage) as done in NeRF-in-the-wild [33] and\nimproves the camera representation of S-NeRF, from pin-\nhole to RPC [1, 46]. EO-NeRF [32] improves the handling\nof shadows of S-NeRF and Sat-NeRF by de\ufb01ning physi-\ncally plausible shadows directly from the geometry. These\nshadows are then rendered by additional raycasting from\nthe surface in the direction of the sun. More recent work fo-\ncuses on modeling dif\ufb01cult seasonal effects [17], extending\nthe proposed volumetric models to surface models [40], us-\ning the raw pre-pansharpened data provided directly by the\nsatellite operators [38], and accelerating the training step by\ntaking advantage of faster NeRF versions [5].\n2.3. 3D Gaussian Splatting\nFollowing the growing interest in NVS, 3DGS [23] was pro-\nposed as an alternative to NeRF-based methods. While both\nNeRF and 3DGS use alpha compositing as their image for-\nmation model, 3DGS represents the scene using a set of\ndiscrete Gaussian-shaped primitives placed in the 3D space,\nas opposed to a continuous black-box MLP representation\nof NeRF. In the following, we use the notation of Bul`o et\nal. [7].\nA Gaussian primitive is a tuple \u03b3 = (\u00b5, \u03a3, \u03b1, f) rep-\nresenting a single Gaussian-shaped volume element in the\nscene, where \u00b5 \u2208R3 is the primitive center, \u03a3 \u2208M3\u00d73(R)\n5960\n\n=== Page 3 ===\nits 3D shape and orientation, \u03b1 \u2208[0, 1] its opacity, and\nf \u2208Rd its feature vector (e.g. color when d = 3).\n3DGS uses K independent Gaussian primitives to repre-\nsent the scene, so we will index them using k = 1, . . . , K\nas subscript. Each Gaussian primitive k has an associated\n3D Gaussian kernel Gk de\ufb01ned as\nGk(x) = exp\n\u001a\n\u22121\n2 (x \u2212\u00b5k)T \u03a3\u22121\nk (x \u2212\u00b5k)\n\u001b\n.\n(2)\nTo render a view (characterized by its associated camera),\n3DGS \u201csplats\u201d each 3D Gaussian kernel onto the camera\nimage plane. This process is called the splatting operation\nand, mathematically, it associates a Gaussian primitive \u03b3k\nand a camera model/projection A : R3 \u2192R2 to a 2D Gaus-\nsian kernel GA\nk\n: R2 \u2192R. The original 3DGS method\ndeals only with pinhole camera models. During the splat-\nting operation, each Gaussian is projected according to the\n\ufb01rst-order approximation of the perspective projection com-\nputed at \u00b5k, JA (\u00b5k). In this way, the mean vector and\ncovariance matrix of the 2D Gaussian kernel are:\n\u00b5A\nk = A(\u00b5k)\n\u03a3A\nk = JA (\u00b5k) \u03a3\n\u0000JA (\u00b5k)\n\u0001\u2032 .\n(3)\nOnce all the primitives are splatted, they are sorted front-\nto-back with respect to the camera reference. Then, they\nare aggregated using the traditional alpha compositing, ac-\ncounting also for the Gaussian kernel decay such that\nIA(u) =\nK\nX\nk=1\n\u02dcf k\u03c9A\nk (u),\n(4)\nwhere IA is the rendered image associated with camera A,\nu \u2208R2 is a point in the 2D image plane, \u02dcf k are features that\ndepend on f k and the view direction (for modeling view-\ndirection dependent color effect, such as shiny objects), and\nthe alpha-compositing coef\ufb01cient is\n\u03c9A\nk (u) = \u03b1kGA\nk (u)\nk\u22121\nY\nj=1\n\u00001 \u2212\u03b1jGA\nj (u)\n\u0001\n.\n(5)\nRecent literature expanded the original 3DGS [23] in\nmany directions. As 3DGS has focused only on pinhole\ncamera models and relied on a \ufb01rst-order local approxi-\nmation of the model, recent works propose different cam-\nera models (e.g. \ufb01sh-eye [29]) or more complex splatting\nstrategies [21]. Another line of work focused on improving\n3DGS in challenging input regimes, such as sparse-view in-\nput [52], and input without camera parameters [16]. More-\nover, many recent methods focus on controllable texture and\nlighting [18, 22, 28, 44]. Further references can be found in\nrecent surveys [2, 51].\n3. Method\nThe proposed Earth-observation Gaussian splatting method,\nreferred to as EOGS, specializes and adapts 3DGS for the\nsatellite photogrammetry task. Given N non-orthorecti\ufb01ed\nsatellite images and their corresponding RPC camera model\ncoef\ufb01cients, a set of Gaussian-shaped 3D primitives is opti-\nmized to recover both the 3D geometry and appearance of\nthe scene.\nThe general learning problem is to \ufb01nd the set of K\nGaussian primitives that best approximates the N satellite\nimages, with the rendering process of Eq. (4). This can be\nformulated as:\narg min\n(\u03b3k)J1,KK\nN\nX\ni=1\n\u2113(\u02c6Ii, Ii),\n(6)\nwhere Ii is the i-th input satellite observation, \u02c6Ii is the cor-\nresponding synthesized view (in the original 3DGS \u02c6Ii =\nIAi), and \u2113is the same photometric distance function used\nin 3DGS.\nIn the following sections we highlight the differences\nbetween EOGS and previous 3DGS and NeRF-based ap-\nproaches.\n3.1. Projections and Coordinate Systems\nWe de\ufb01ne the coordinate system in which the Gaussian\nprimitive centers and shapes are expressed as world-space\ncoordinates. This coordinate system is a uniformly rescaled\nand recentered version of the Universal Transverse Mer-\ncator (UTM) coordinate system [49], such that the center\nof the scenes coincides with the origin, the scene is con-\ntained in a unit cube, and it is east-north-up aligned simi-\nlarly to EO-NeRF [32]. At the other end of the transforma-\ntion pipeline lies the 2D NDC-space, where the Gaussian\nprimitives are splatted.\nThe correct mapping between these two spaces is a\ncomposition of transformations: world-space to UTM to\nlongitude-latitude-altitude. Using the RPC coef\ufb01cients (that\nmodel the satellite position and 3D attitude) associated with\neach observation, the latter coordinate system is mapped\nonto the image row-column coordinates. Finally, the co-\nordinates are normalized to range in [\u22121, 1] to get to NDC-\nspace. As shown in Fig. 2, we instead compute a per-scene\naf\ufb01ne approximation of the whole transformation, introduc-\ning a negligible mean error of \u22480.012 pixels while be-\ning more computationally ef\ufb01cient than previous works and\ncompatible with a Gaussian Splatting formulation. Speci\ufb01-\ncally, for an af\ufb01ne camera model A : x \u2208R3 \u2192Ax + a \u2208\nR2, the Eq. (3) simpli\ufb01es to \u00b5A\nk = A\u00b5k + a \u2208R2 and\n\u03a3A\nk = A\u03a3A\u2032 \u2208R2\u00d72. This simpli\ufb01cation eliminates the\nneed for the local \ufb01rst-order approximation used in the orig-\ninal 3DGS method, as we moved the approximation to the\ncamera models.\n5961\n\n=== Page 4 ===\n2D NDC Space\nRowColumn\nLongLatAlt\nUTM\nWorld Space\nRPC\n\u03b6(x)\ncx + b\n\u2248A : R3 \u2192R2 af\ufb01ne transformation\nFigure 2. Summary of the transformation from world-space to NDC-space and its af\ufb01ne approximation. The af\ufb01ne approximation is com-\nputationally ef\ufb01cient, compatible with the Gaussian splatting formulation, and well-suited for satellite images. The coordinate systems in\nthe right red box represent 3D world coordinates (camera-independent), while the left blue box shows 2D coordinates (camera-dependent).\n3.2. Shadow Mapping\nAs in EO-NeRF, we want to explicitly model the shadow\nphenomena in the images, as the solar direction is avail-\nable for each image in the scene. Unlike previous literature,\nour method uses a custom variant of shadow mapping to\ncast geometrically consistent shadows. Introduced in [50],\nShadow Mapping is a well-known technique in the \ufb01eld of\n3D graphics for adding shadows to a computer graphic ren-\ndering. It is particularly suited for EOGS, as it requires just\nthe ability to render the scene from different points of view,\nas opposed to the shadow casting technique used in EO-\nNeRF that requires ray marching (which is not de\ufb01ned in\nGaussian splatting).\nBefore introducing our variant of Shadow Mapping, we\nde\ufb01ne the elevation render, localization function, and the\nhomologous point function.\nGiven a camera model/projection A, the elevation ren-\nder is de\ufb01ned as the 3DGS rendering Eq. (4) using the real\nelevations instead of colors\nEA(u) =\nK\nX\nk=1\n[E\u00b5k] \u03c9A\nk (u),\n(7)\nwhere E : R3 \u2192R is an af\ufb01ne operation mapping 3D points\nexpressed in the \u201cnative\u201d world coordinates to the corre-\nsponding real altitude, expressed in meters. We remark that\nthis is not the depth nor the inverse depth, typically found\nin the MVS literature.\nGiven a camera model/projection A, the localization\nfunction that maps a pixel of the camera and a given ab-\nsolute altitude to its associated point in the native 3D world\nis de\ufb01ned as\nlocA :\n\u0000R2 \u00d7 R\n\u0001\n\u2192R3,\n(u, h) 7\u2192x\ns.t.\nA(x) = u\nand\nE(x) = h.\n(8)\nGiven two cameras A and B, the homologous point func-\ntion maps a pixel of the \ufb01rst camera to the corresponding\npixel of the second camera, taking into consideration the\n3D geometry:\nhomA,B : R2 \u2192R2,\nu 7\u2192\u02dcu\ns.t.\n\u02dcu = B\n\u0000locA \u0000u, EA(u)\n\u0001\u0001\n.\n(9)\nIn our shadow mapping approach (depicted in Fig. 3),\nwe assume that the sun is the only light source present in\nthe scene. Moreover, since it is far from the scene, it can be\napproximated as a directional light. Following the classic\nshadow mapping approach, we construct a camera S, called\nsun camera, placed at and aligned with the light source (de-\ntailed in the supplementary). As the camera model corre-\nsponding to a directional light is the af\ufb01ne camera, we can\nhandle uniformly the sun cameras and the satellite cameras.\nThen, we consider a second camera, A, from which we\nwant to synthesize a novel view and apply shadows accord-\ning to the sun direction. Given a point u in the A NDC-\nspace, and its corresponding altitude EA(u), we \ufb01rst local-\nize it, obtaining a 3D point in world-space. We then project\nthis point according to S, obtaining the homologous point\nof u in S. We then resample the elevation rendering of S\nat this projected point and compare it with EA(u). Mathe-\nmatically, this corresponds to\n\u2206hA,S(u) = ES \u0000homA,S(u)\n\u0001\n\u2212EA(u).\n(10)\nIf these two elevations, EA(u) and ES(\u02dcu) are the same, it\nmeans that both the camera and the sun camera are imaging\nthe same 3D point, hence this point is in light. If the two\nelevations are not the same, then the sun camera is not able\nto \u201csee\u201d the 3D point, hence it is in shadows. To represent\nthis shading, the color of points in the shadows is multiplied\nby a darkening coef\ufb01cient computed from \u2206hA,S(u) as\nsA,S(u) = min\n\b\nexp\n\b\n\u2212\u03c1\u2206hA,S(u)\n\t\n, 1\n\t\n.\n(11)\nWe argue that this formulation is physically plausible as this\nwould be the correct equation for a homogeneous medium\nof density \u03c1, as shown in [34].\nFollowing [32], we also model a per-camera ambient\nlight \u03c8A so that in-shadow objects do not appear com-\npletely black. The shading to be applied to a given pixel\nu is given by the following lighting coef\ufb01cient\nlA,S(u) = sA,S(u) + (1 \u2212sA,S(u))\u03c8A.\n(12)\nFinally, EOGS image formation equation is:\nIA,S(u) = lA,S(u)\nK\nX\nk=1\n\u03c6A(f k)\u03c9A\nk (u),\n(13)\n5962\n\n=== Page 5 ===\nwhere \u03c6A(\u00b7) is a camera-speci\ufb01c af\ufb01ne color correction ap-\nplied to the intrinsic primitive colors f k. We remark that,\ndifferently from 3DGS, we drop the view-direction depen-\ndencies of the primitive colors and introduce a camera-\ndependent color correction.\nIt is useful to de\ufb01ne the albedo rendering, where we do\nnot use the shadows or the camera-speci\ufb01c color correction:\nIA(u) =\nK\nX\nk=1\nf k\u03c9A\nk (u).\n(14)\nWhile the image formation model de\ufb01ned in Eq. (13) is\nequivalent to EO-NeRF, the shadow de\ufb01nition is quite dif-\nferent. In EO-NeRF case, shadows are de\ufb01ned as the sun\nvisibility for all points on the surface. Because of possible\nocclusions, two points of the scene can correspond to the\nsame point seen from the sun direction. Therefore, it is not\npossible to de\ufb01ne the sun visibility as an \u201cimage\u201d that could\nbe estimated using a Gaussian splatting-like process. Trying\nto compute an irregularly sampled \u201cimage\u201d corresponding\nto these points would break the locality assumption used in\nGaussian splatting during the rasterization step and thus re-\nduce the computational ef\ufb01ciency. On the contrary, the pro-\nposed shadow mapping veri\ufb01es all the assumptions made\nby Gaussian splatting.\n3.3. Regularizers\nIt is well-known that deep neural networks are implicitly\nregularized [39, 41, 45, 48], meaning that despite being used\nin the overparametrized regime, they show generalization\ncapabilities.\nOn the other hand, we found out that primitives in 3DGS-\nbased methods are almost independently optimized one\nfrom the other. This is probably due to the fact that the\nprimitives in 3DGS are initialized as small spheres, spread\nout in the entire scene. This results in 3DGS being less regu-\nlarized than NeRF-based methods and \u201clacking\u201d constraints\nduring the optimization phase.\nHence we are free to add additional regularization con-\nstraints to the general optimization problem Eq. (6) that in-\nduce smoother and more regular solutions. In particular,\nwe introduce constraints that promote our solution to be\nsparse (i.e. we encourage solutions that require fewer Gaus-\nsian primitives), view consistent, and mostly composed of\ncompletely opaque objects.\nAs common ML pipelines are specialized for uncon-\nstrained optimization problems, we argue to use a La-\ngrangian relaxation approach and re-formulate each con-\nstraint as a new loss term, each with its own experimentally-\nfound Lagrangian multiplier.\nPromoting Sparsity. Training time is directly proportional\nto the number of Gaussian primitives considered during the\noptimization process. As we want to recover the geometry\nSun\nSatellite\nResampled\nSun\nSun\nResampled Sun\nSatellite\nFigure 3. Shadow mapping illustration. The point u in the satel-\nlite image (af\ufb01ne camera A) corresponds to a the the 3D point\nx = locA(u) on the vertical wall. Projecting x to the sun camera\n(af\ufb01ne camera S), \u02dcu = Sx is obtained. Then y = locS(\u02dcu) is\nobtained localizing \u02dcu. The point x and its pixel u are in shadow\nbecause the elevation of y is greater than the elevation of x. In-\ndeed, all and only the points where the satellite elevation and the\nresampled sun elevation do not match should be shaded. On the\nbottom of the illustration are shown examples of the sun elevation,\nthe resampled sun elevation, and the satellite elevation renderings,\nwith shadows highlighted in red.\nof the scene as fast as possible, we want as few Gaussian\nprimitives as possible, hence a sparse solution.\nInspired by the well-known LASSO regularization in lin-\near regression [47] that promotes a sparse solution, and by\nrecent works such as 3DGSMCMC [24], we consider a L1\nregularization of the opacities\nLo = 1\nK\nK\nX\nk=1\n\u03b1k.\n(15)\nThis regularization promotes sparsity in the primitive\nopacities distribution, hence only \u201cuseful\u201d primitives will\nbe visible at the end of the optimization. We pair this regu-\nlarization with a simple thresholding pruning procedure that\ndiscards any primitive with \u03b1 < \u03b1min. In this way, un-\nused primitives are actually discarded, yielding faster splat-\nting and overall faster training (speci\ufb01cally, we recorded\nspeedups of up to 2\u00d7 on the considered datasets).\nWe remark that many works [7, 24] have proposed re-\nplacements to the original 3DGS densi\ufb01cation/pruning pro-\ncedure. Here, instead, we aim only at lowering the num-\nber of primitives, so we do not need a densi\ufb01cation strategy\nas long as we instantiate enough of them at the beginning\nof the optimization. Moreover, we set \u03b1min = 0.0025 as\n5963\n\n=== Page 6 ===\nFigure 4.\nFrom top-left to bottom-right, shadow maps of EO-\nNeRF, EOGS without the Ls penalizer, EOGS with the Ls penal-\nizer, and the corresponding satellite image. Textures correspond-\ning to the image content can be observed in the shadow map of\nEO-NeRF and EOGS without the Ls penalizer, but not in EOGS.\nprimitives with lower opacities are already discarded in the\noriginal 3DGS implementation of the front-to-back splat-\nting procedure.\nPromoting View Consistency. Differently from the clas-\nsical NVS context, in remote sensing the available views\nare low-count and sparse, resulting in Eq. (6) being even\nless constrained. Paired with the fact that 3DGS does not\nbene\ufb01t from the implicit regularization of NeRF, we argue\nthat an additional constraint promoting view consistency is\nneeded.\nWe propose a \u201clocal view consistency\u201d loss based on the\nintuition that if the same 3D point is visible from two cam-\neras and the cameras are close to each other, then the color\nand elevation resampled at the corresponding pixels should\nbe the same. Otherwise, the object is occluded or outside\nthe camera boundaries.\nMathematically, given a camera A we randomly perturb\nit and obtain a camera B. Assuming that there is no view-\ndirection dependent color effect, this constraint reads:\n\u2206hA,B(u) < \u2206hmin \u21d2\n(\nIA(u) = IB \u0000homA,B(u)\n\u0001\nEA(u) = EB \u0000homA,B(u)\n\u0001\n,\n(16)\nwhere we reused the same notation of the shadow mapping\nexplanation.\nThis constraint results in two loss terms, the color\n(albedo) consistency and the altitude consistency:\nLcc =\nX\nu\nM A,B(u)\n\f\fIA(u) \u2212IB \u0000homA,B(u)\n\u0001\f\f\n(17)\nLac =\nX\nu\nM A,B(u)\n\f\fEA(u) \u2212EB \u0000homA,B(u)\n\u0001\f\f , (18)\nwhere M A,B(\u00b7) is a binary mask that selects all pixels u\nsuch that \u2206hA,B(u) < \u2206hmin and homA,B(u) is inside\nthe image boundaries. We remark that we always choose A\nfrom the input posed images and we set \u2206hmin = 30cm.\nMoreover, we obtain B by independently sampling q1, q2 \u2208\nR from a \u00b11-truncated standard distribution and de\ufb01ning\nB(x) = A(x) + 0.05 E(x)\n\u0012\nq1\nq2\n\u0013\n.\n(19)\nIn the generic NVS literature, many works [10, 37] pro-\nposed different methods for increasing the view consis-\ntency. RegNeRF [37] is the \ufb01rst work that deals with sparse\ncamera poses by introducing a loss term that maximizes\nthe likelihood of rendered RGB patches from virtual cam-\neras with a pre-trained deep normalizing-\ufb02ow model, while\nalso adding a total variation regularization on the rendered\ndepth. Furthermore, [10] introduces a reprojection mecha-\nnism such that only the geometry needs to be learned from\nthe NeRF, as the colors are resampled from the input im-\nages.\nNote that EOGS differs from [37] as we ask for\nconsistency (RGB and depth) between two cameras (one\nreal and one virtual), hence we do not need any pre-trained\nmodel for the RGB renders nor prior on the elevation ren-\nders. EOGS also differs from [10] as we learn the colors and\ndo not resample input images that may contain transients or\ncolor shifts.\nPromoting Opaqueness. Looking at the output from Sat-\nNeRF and EO-NeRF (see Fig. 4), we can see that much of\nthe texture of the scene is embedded in the geometric shad-\nows. This geometry misuse is caused by semi-transparent\nobjects casting semi-transparent shadows. In order to lessen\nthis effect, we propose to add an entropy-based penalty Ls\nfor incorrect use of the shadows. This penalty is de\ufb01ned as\nLs =\nX\nu\nH\n\u0000sA,S(u)\n\u0001\n,\n(20)\nwhere H(x) = \u2212(x log2(x) + (1 \u2212x) log2(1 \u2212x)). In-\ndeed, the shadow map sA,S should contain only 0 or 1 val-\nues. This is the case for \u03c1 \u2192+\u221ein Eq. (11), as a build-\ning should not cast a semi-transparent shadow. Hence we\nadd this entropy-based penalizer to discourage the use of\nsemi-transparent shadows, which in turn encourage objects\nto be either completely transparent or fully opaque. Note\nthat choosing a large \u03c1 during training is not an option since\nit would make the training unstable as Eq. (11) would be\nclose to a non-differentiable step function.\nWe acknowledge that the problem of promoting hard sur-\nfaces in NeRF has been studied in previous works, such\nas [3, 42].\n3.4. Implementation Details\nThe implementation of EOGS is based on the original\n3DGS code base.\nOther than the aforementioned novel\n5964\n\n=== Page 7 ===\nJAX\nIARPA\n004\n068\n214\n260\nMean \u2193\n001\n002\n003\nMean \u2193\nTime \u2193\nNumber of views\n8\n16\n20\n14\n-\n24\n20\n21\n-\n-\nNo mask\nEO-NeRF [32]\n1.37\n1.05\n1.61\n1.37\n1.35\n1.43\n1.79\n1.31\n1.51\n15 hours\nSAT-NGP [5]\n1.63\n1.27\n2.18\n1.79\n1.72\n1.54\n2.11\n1.69\n1.78\n25 minutes\nSat-Mesh [40]\n1.55\n1.15\n2.02\n1.36\n1.52\nN.A.\nN.A.\nN.A.\nN.A.\n8 minutes\nS2P [15]\n1.45\n1.19\n1.82\n1.66\n1.53\n1.48\n2.48\n1.38\n1.78\n20 minutes\nEOGS (ours)\n1.45\n1.10\n1.73\n1.55\n1.46\n1.58\n2.00\n1.27\n1.62\n3 minutes\nFoliage\nmask\nEO-NeRF [32]\n1.02\n1.03\n1.55\n1.24\n1.21\n1.32\n1.63\n1.18\n1.38\n15 hours\nSAT-NGP [5]\n1.03\n1.26\n2.17\n1.43\n1.47\n1.34\n1.85\n1.62\n1.60\n25 minutes\nEOGS (ours)\n0.89\n1.01\n1.63\n1.24\n1.19\n1.38\n1.70\n1.03\n1.37\n3 minutes\nTable 1. Mean absolute error on the elevation [meters] and the corresponding training time for various baseline methods, when considering\nthe whole AOI (no mask) or when ignoring foliage areas (foliage mask). Results for Sat-Mesh are reported from the paper since the authors\ndid not share their code.\ncontributions, the main differences lie in disabling the per-\nGaussian view-direction color dependency and initializing\nall the Gaussians with white color and as low as possible\nopacity (1%). Moreover, we reduce the number of itera-\ntions to 5000 and enable the shadow mapping and all three\nregularizations at iteration 1000. Furthermore, the Gaus-\nsians centers are initialized uniformly in the 3D scene such\nthat the initial density is 0.13 Gaussians per m3.\nWe use the same optimizer and scheduler of 3DGS for\nthe primitives and use a second Adam scheduler with 10\u22122\nlearning rate for learning the camera-dependent parameters:\nthe af\ufb01ne color-correction \u03c6A and the ambient color \u03c8A.\nThe Lagrangian coef\ufb01cients of the regularization con-\nstraints have been found experimentally on a single scene,\nrounded to the nearest power of ten, and applied to all\nscenes. This highlights the robustness of EOGS to the spe-\nci\ufb01c values of these coef\ufb01cients. The \ufb01nal loss is:\nmin\nN\nX\ni=1\n\u2113(\u02c6Ii, Ii) + 0.1Lo + 0.1Lcc + 0.01Lac + 0.01Ls,\n(21)\nwhere \u02c6Ii is now, differently from 3DGS in Eq. (6), a short-\nhand notation for IAi,Si from Eq. (13), which also depends\non the sun camera Si.\n4. Experiments\nWe evaluate EOGS in the same experimental setting as the\nmost recent related work in the literature, EO-NeRF.\nWe are using datasets provided in the 2019 IEEE GRSS\nData Fusion Contest (DFC2019) [6, 26] and 2016 IARPA\nMulti-View Stereo 3D Mapping Challenge (IARPA2016).\nThese datasets, comprising a total of 7 areas of inter-\nest (AOI), contain cropped non-orthorecti\ufb01ed multidate\nWorldView-3 observations, along with metadata such as the\n3D satellite attitude (encoded in the RPC coef\ufb01cient) and\nthe local sun direction. We use the bundled-adjusted ver-\nsion of the RPC coef\ufb01cient used in EO-NeRF. Each image\ncovers approximately 256 \u00d7 256 meters squared of terrain\nwith a resolution of 30 \u223c50 cm per pixel, while each AOI\nis imagined by 10 \u223c20 crops.\n4.1. Main Experiment Results\nTab. 1 show the main experimental results of EOGS. To as-\nsess the accuracy of EOGS we report the mean absolute er-\nror (MAE) between a lidar scan included in the dataset and\nthe elevation render aligned to this nadir view. We argue\nthat the volume of these data will grow in the near future,\nso we are also interested in the time required to recover the\ngeometry from the input images. Hence, we also report the\ntraining time.\nIf the entire AOIs are considered, as reported in Tab. 1\n(top), EOGS performs slightly worse than the state of the\nart EO-NeRF but it is approximately 300\u00d7 faster. For ref-\nerence, we also report all available results of other meth-\nods from the literature (EO-NeRF [32], SAT-NGP [5], Sat-\nMesh [40], and S2P [11, 15]). We see that EOGS is Pareto-\noptimal with respect to elevation MAE and training time. If\ninstead we use available ground truth semantic maps to ig-\nnore prediction in the foliage areas, EOGS performance is\nequivalent to EO-NeRF, showing higher accuracy for struc-\ntural objects, as reported in Tab. 1 (bottom). We present\nvisual results in Fig. 5, more results, as well as details on\nthe number of Gaussians per scene and memory usage, are\nprovided in the supplement.\n4.2. Ablation and Parameter Studies\nImpact of the Different Losses. Tab. 2 reports an abla-\ntion study of the loss terms in EOGS. Each column corre-\nsponds to a different ablation experiment, while each row\ncorresponds to a different component of EOGS being ab-\nlated. The \ufb01rst row indicates whether the Shadow Mapping\ntechnique is enabled or not. The following three rows indi-\n5965\n\n=== Page 8 ===\nFigure 5. From left to right: visual results on JAX 214 comparing\nSAT-NGP [5], EOGS, EO-NeRF [32] and the ground truth.\nShadowmap\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nSparsity\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\nConsistency\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\nOpaqueness\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\nMAE [m] \u21935.03 1.86 1.83 1.69 1.79 1.57 1.76 1.64 1.54\nTrain [min] \u21934.18\n-\n-\n-\n-\n4.27\n-\n-\n2.85\nTable 2. Ablation study of each proposed component of EOGS.\ncate, respectively, the presence of the sparsity, consistency,\nand opaqueness regularizers. We remark that the \ufb01rst col-\numn is equivalent to 3DGS with af\ufb01ne cameras, learnable\nper-camera af\ufb01ne color correction, and different primitives\ninitialization. For each column, we report the grand mean\nelevation MAE of JAX and IARPA scenes.\nTo quantitatively measure the impact of each single com-\nponent, we linearly regress the MAE from the presence of\nthe components, obtaining a coef\ufb01cient for each component\nthat expresses an elevation MAE gain with respect to the\nbase case (reported in the \ufb01rst column of Tab. 2). The in-\ntroduction of shadow mapping is the most impactful com-\nponent, gaining 3.16 meters of accuracy. Then, the con-\nsistency regularizer and the opaqueness regularizer further\nimprove the accuracy of EOGS by 0.20 and 0.09 meters,\nrespectively. Lastly, the sparsity regularizer, while being\nnecessary for achieving ef\ufb01cient training as shown in the\nlast row of Tab. 2, also reports an improvement of 0.04 me-\nters. Hence, all components independently contribute to the\nquality of the geometry reconstruction.\nRegularization Parameters. Fig. 6 shows the results of a\ngrid search on the coef\ufb01cients of Lcc and Lac in Eq. (21). It\nshows that both altitude regularization and color regulariza-\ntion are necessary to achieve the best performance. We re-\nmark that, in order to reduce over\ufb01tting to a particular scene,\nMAE (m)\nFigure 6.\nView consistency regularization parameter ablation\nstudy. Selected parameter set is shown with the cyan dot. Esti-\nmation performed on the JAX 260 sequence.\n0\n20\n40\n60\n80\nDSM points covered by at least x% of the satellite images\n0.8\n1.0\n1.2\nMAE\nEONeRF\nEOGS\nFigure 7. Impact of the visibility on the performance (using foliage\nmask).\nwe choose the same \u201cround coef\ufb01cients\u201d for all scenes.\nImpact of visibility. Fig. 7 shows the impact of the visibil-\nity (i.e. the number of cameras that can see a given point of\nthe scene) on the performance. While EOGS and EO-NeRF\nare comparable on average, this test shows that EOGS per-\nforms better for regions that are visible in most images but\nstruggles in the regions observed in only a few images.\n5. Conclusion\nThis study presents EOGS, the \ufb01rst Gaussian-Spatting-\nbased framework for earth observation. By introducing re-\nmote sensing requirements in 3DGS, such as shadow mod-\neling and camera-speci\ufb01c corrections, EOGS achieves com-\nparable accuracy to the existing state-of-the-art method,\nEO-NeRF, while being over 300 times faster.\nHence,\nEOGS will be a practical solution for near-future large-scale\ndatasets.\nOur analysis shows that EOGS excels in high-coverage\nregions, where it produces \ufb01ne details at a fraction of the\ncomputational cost.\nAddressing regions with low image\ncoverage or with foliage, by adding additional regulariza-\ntion or employing better initialization schemes, could fur-\nther improve EOGS accuracy.\n5966\n\n=== Page 9 ===\nAcknowledgements\nThis publication is part of the project PNRR-NGEU which\nhas received funding from the MUR - DM 352/2022. This\nwork was partially supported by the European Union under\nthe Italian National Recovery and Resilience Plan (NRRP)\nof NextGenerationEU, partnership on \u201cTelecommunica-\ntions of the Future\u201d (PE00000001 - program \u201cRESTART\u201d).\nIt was performed using HPC resources from GENCI-IDRIS\n(grant 2024-AD011012453R3). Centre Borelli is also with\nUniversit\u00b4e Paris Cit\u00b4e, SSA and INSERM.\nReferences\n[1] Emmanuel P Baltsavias and Dirk Stallmann. Metric infor-\nmation extraction from spot images and the role of polyno-\nmial mapping functions. In XVII ISPRS Congress, Commis-\nsion IV. Swiss Federal Institute of Technology, Institute of\nGeodesy and Photogrammetry, 1992. 2\n[2] Yanqi Bao, Tianyu Ding, Jing Huo, Yaoli Liu, Yuxin Li,\nWenbin Li, Yang Gao, and Jiebo Luo. 3d gaussian splatting:\nSurvey, technologies, challenges, and opportunities. arXiv\npreprint arXiv:2407.17418, 2024. 3\n[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance \ufb01elds.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 5470\u20135479, 2022. 6\n[4] Ross A Beyer, Oleg Alexandrov, and Scott McMichael. The\nAmes Stereo Pipeline: NASA\u2019s open source software for de-\nriving and processing terrain data. Earth and Space Science,\n5(9):537\u2013548, 2018. 2\n[5] Camille Billouard, Dawa Derksen, Emmanuelle Sarrazin,\nand Bruno Vallet.\nSat-ngp : Unleashing neural graphics\nprimitives for fast relightable transient-free 3d reconstruction\nfrom satellite imagery. In IGARSS 2024 - 2024 IEEE Inter-\nnational Geoscience and Remote Sensing Symposium, pages\n8749\u20138753, 2024. 2, 7, 8\n[6] Marc Bosch, Kevin Foster, Gordon Christie, Sean Wang,\nGregory D Hager, and Myron Brown. Semantic stereo for\nincidental satellite images. In 2019 IEEE Winter Conference\non Applications of Computer Vision (WACV), pages 1524\u2013\n1532, 2019. 7\n[7] Samuel Rota Bul`o, Lorenzo Porzi, and Peter Kontschieder.\nRevising densi\ufb01cation in gaussian splatting. arXiv preprint\narXiv:2404.06109, 2024. 2, 5\n[8] Jia-Ren Chang and Yong-Sheng Chen.\nPyramid stereo\nmatching network. In 2018 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 5410\u2013\n5418, 2018. 2\n[9] Jeff Clauson, Simon Cantrell, Jim Vrabel, John Oeding,\nBhaskar Ranjitkar, Tom Rusten, Shankar Ramaseri, and\nKimberly Casey.\nEarth observing satellites online com-\npendium. U.S. Geological Survey, 2024. 1\n[10] Franc\u00b8ois Darmon, B\u00b4en\u00b4edicte Bascle, Jean-Cl\u00b4ement Devaux,\nPascal Monasse, and Mathieu Aubry. Improving neural im-\nplicit surfaces geometry with patch warping. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6260\u20136269, 2022. 6\n[11] Carlo de Franchis, Enric Meinhardt-Llopis, Julien Michel,\nJean-Michel Morel, and Gabriele Facciolo. An automatic\nand modular stereo pipeline for pushbroom images. ISPRS\nAnnals of Photogrammetry, Remote Sensing and Spatial In-\nformation Sciences, 2-3:49\u201356, 2014. 2, 7\n[12] Dawa Derksen and Dario Izzo.\nShadow neural radiance\n\ufb01elds for multi-view satellite photogrammetry. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 1152\u20131161, 2021. 2\n[13] eoPortal.\nWorldView\nLegion,\n2024.\nhttps :\n//www.eoportal.org/satellite- missions/\nworldview-legion. 1\n[14] Gabriele Facciolo, Carlo de Franchis, and Enric Meinhardt.\nMGM: A signi\ufb01cantly more global matching for stereovi-\nsion. In Proceedings of the British Machine Vision Confer-\nence (BMVC), number 90, pages 1\u201312, 2015. 2\n[15] Gabriele Facciolo, Carlo de Franchis, and Enric Meinhardt-\nLlopis. Automatic 3D reconstruction from multi-date satel-\nlite images. In 2017 IEEE Conference on Computer Vision\nand Pattern Recognition Workshops (CVPRW), pages 1542\u2013\n1551, 2017. 7\n[16] Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A.\nEfros, and Xiaolong Wang. Colmap-free 3d gaussian splat-\nting. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 20796\u2013\n20805, 2024. 3\n[17] Michael Gableman and Avinash Kak. Incorporating season\nand solar speci\ufb01city into renderings made by a nerf archi-\ntecture using satellite images. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2024. 2\n[18] Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li\nZhang, and Yao Yao. Relightable 3d gaussian: Real-time\npoint cloud relighting with brdf decomposition and ray trac-\ning. arXiv:2311.16043, 2023. 3\n[19] Richard I Hartley and Joseph L Mundy. Relationship be-\ntween photogrammmetry and computer vision. Integrating\nphotogrammetric techniques with scene analysis and ma-\nchine vision, 1944:92\u2013105, 1993. 1\n[20] Heiko Hirschmuller. Stereo processing by semiglobal match-\ning and mutual information. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 30(2):328\u2013341, 2007. 2\n[21] Letian Huang, Jiayang Bai, Jie Guo, Yuanqi Li, and Yan-\nwen Guo.\nOn the error analysis of 3d gaussian splat-\nting and an optimal projection strategy.\narXiv preprint\narXiv:2402.00752, 2024. 3\n[22] Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xi-\naoxiao Long, Wenping Wang, and Yuexin Ma. Gaussian-\nshader: 3d gaussian splatting with shading functions for re-\n\ufb02ective surfaces. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n5322\u20135332, 2024. 3\n[23] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance \ufb01eld rendering. ACM Transactions on Graphics, 42\n(4), 2023. 2, 3\n5967\n\n=== Page 10 ===\n[24] Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Wei-\nwei Sun, Jeff Tseng, Hossam Isack, Abhishek Kar, An-\ndrea Tagliasacchi, and Kwang Moo Yi.\n3d gaussian\nsplatting as markov chain monte carlo.\narXiv preprint\narXiv:2404.09591, 2024. 5\n[25] Thomas Krau\u00df, Pablo d\u2019Angelo, Mathias Schneider, and\nVeronika Gstaiger. The fully automatic optical processing\nsystem CATENA at DLR. ISPRS International Archives of\nthe Photogrammetry, Remote Sensing and Spatial Informa-\ntion Sciences, 40-1/W1:177\u2013181, 2013. 2\n[26] Bertrand Le Saux, Naoto Yokoya, Ronny Hansch, Myron\nBrown, and Greg Hager. 2019 data fusion contest [technical\ncommittees]. IEEE Geoscience and Remote Sensing Maga-\nzine, 7(1):103\u2013105, 2019. 7\n[27] Laurent Leb`egue, Eric Cazala-Hourcade, Florie Languille,\nSt\u00b4ephanie Artigues, and Olivier Melet. Co3d, a worldwide\none one-meter accuracy dem for 2025.\nThe International\nArchives of the Photogrammetry, Remote Sensing and Spa-\ntial Information Sciences, XLIII-B1-2020:299\u2013304, 2020. 1\n[28] Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, and Kui\nJia. Gs-ir: 3d gaussian splatting for inverse rendering. arXiv\npreprint arXiv:2311.16473, 2023. 3\n[29] Zimu Liao, Siyan Chen, Rong Fu, Yi Wang, Zhongling\nSu, Hao Luo, Linning Xu, Bo Dai, Hengjie Li, Zhilin\nPei, et al.\nFisheye-gs: Lightweight and extensible gaus-\nsian splatting module for \ufb01sheye cameras. arXiv preprint\narXiv:2409.04751, 2024. 3\n[30] Roger Mar\u00b4\u0131, Thibaud Ehret, and Gabriele Facciolo. Disparity\nestimation networks for aerial and high-resolution satellite\nimages: A review. Image Processing On Line, 12:501\u2013526,\n2022. 2\n[31] Roger Mar\u00b4\u0131, Gabriele Facciolo, and Thibaud Ehret. Sat-nerf:\nLearning multi-view satellite photogrammetry with transient\nobjects and shadow modeling using rpc cameras. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 1311\u20131321, 2022. 2\n[32] Roger Mar\u00b4\u0131, Gabriele Facciolo, and Thibaud Ehret. Multi-\ndate earth observation nerf: The detail is in the shadows. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 2035\u20132045, 2023. 1, 2,\n3, 4, 7, 8\n[33] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,\nJonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-\nworth. Nerf in the wild: Neural radiance \ufb01elds for uncon-\nstrained photo collections. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 7210\u20137219, 2021. 2\n[34] N. Max. Optical models for direct volume rendering. IEEE\nTransactions on Visualization and Computer Graphics, 1(2):\n99\u2013108, 1995. 4\n[35] Julien Michel, Emmanuelle Sarrazin, David Yousse\ufb01, Myr-\niam Cournet, Fabrice Buffe, Jean-Marc Delvit, Aur\u00b4elie Em-\nilien, Julien Bosman, Olivier Melet, and C\u00b4eline L\u2019Helguen.\nA new satellite imagery stereo pipeline designed for scal-\nability, robustness and performance. ISPRS Annals of the\nPhotogrammetry, Remote Sensing and Spatial Information\nSciences, 5-2-2020:171\u2013178, 2020. 2\n[36] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance \ufb01elds for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n2\n[37] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,\nMehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg-\nnerf: Regularizing neural radiance \ufb01elds for view synthesis\nfrom sparse inputs. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5480\u20135490, 2022. 6\n[38] Emilie Pic, Thibaud Ehret, Gabriele Facciolo, and Roger\nMar\u00b4\u0131. Pseudo pansharpening nerf for satellite image collec-\ntions. In IGARSS 2024-2024 IEEE International Geoscience\nand Remote Sensing Symposium, pages 2650\u20132655. IEEE,\n2024. 2\n[39] Tomaso Poggio, Qianli Liao, Brando Miranda, Andrzej Ban-\nburski, Xavier Boix, and Jack Hidary. Theory iiib: General-\nization in deep networks. arXiv preprint arXiv:1806.11379,\n2018. 5\n[40] Yingjie Qu and Fei Deng. Sat-mesh: Learning neural im-\nplicit surfaces for multi-view satellite reconstruction. Remote\nSensing, 15(17):4297, 2023. 2, 7\n[41] Sameera Ramasinghe, Lachlan E MacDonald, and Simon\nLucey.\nOn the frequency-bias of coordinate-mlps.\nAd-\nvances in Neural Information Processing Systems, 35:796\u2013\n809, 2022. 5\n[42] Daniel Rebain, Mark Matthews, Kwang Moo Yi, Dmitry La-\ngun, and Andrea Tagliasacchi. Lolnerf: Learn from one look.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1558\u20131567, 2022. 6\n[43] Ewelina\nRupnik,\nMehdi\nDaakir,\nand\nMarc\nPierrot-\nDeseilligny. MicMac\u2013a free, open-source solution for pho-\ntogrammetry.\nOpen Geospatial Data, Software and Stan-\ndards, 2(14), 2017. 2\n[44] Yahao Shi, Yanmin Wu, Chenming Wu, Xing Liu, Chen\nZhao, Haocheng Feng, Jingtuo Liu, Liangjun Zhang, Jian\nZhang, Bin Zhou, et al.\nGir: 3d gaussian inverse ren-\ndering for relightable scene factorization.\narXiv preprint\narXiv:2312.05133, 2023. 3\n[45] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan T. Barron, and Ren Ng. Fourier fea-\ntures let networks learn high frequency functions in low di-\nmensional domains. NeurIPS, 2020. 5\n[46] C Vincent Tao and Yong Hu.\nA comprehensive study of\nthe rational function model for photogrammetric processing.\nPhotogrammetric engineering and remote sensing, 67(12):\n1347\u20131358, 2001. 2\n[47] Robert Tibshirani. Regression shrinkage and selection via\nthe lasso. Journal of the Royal Statistical Society Series B:\nStatistical Methodology, 58(1):267\u2013288, 1996. 5\n[48] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.\nDeep image prior. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 9446\u20139454,\n2018. 5\n5968\n\n=== Page 11 ===\n[49] U.S. Geological Survey. The universal transverse mercator\n(utm) grid. Technical report, U.S. Geological Survey, Re-\nston, VA, 2001. Report. 3\n[50] Lance Williams. Casting curved shadows on curved surfaces.\nIn Proceedings of the 5th annual conference on Computer\ngraphics and interactive techniques, pages 270\u2013274, 1978.\n4\n[51] Tong Wu, Yu-Jie Yuan, Ling-Xiao Zhang, Jie Yang, Yan-\nPei Cao, Ling-Qi Yan, and Lin Gao. Recent advances in 3d\ngaussian splatting. Computational Visual Media, 10(4):613\u2013\n642, 2024. 3\n[52] Haolin Xiong, Sairisheek Muttukuru, Rishi Upadhyay,\nPradyumna Chari, and Achuta Kadambi. Sparsegs: Real-\ntime 360 {\\deg} sparse view synthesis using gaussian splat-\nting. arXiv preprint arXiv:2312.00206, 2023. 3\n[53] Gengshan Yang, Joshua Manela, Michael Happold, and\nDeva Ramanan. Hierarchical deep stereo matching on high-\nresolution images. In 2019 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 5510\u2013\n5519, 2019. 2\n[54] Feihu Zhang,\nVictor Prisacariu,\nRuigang Yang,\nand\nPhilip H.S. Torr. GA-Net: Guided aggregation net for end-\nto-end stereo matching. In 2019 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n185\u2013194, 2019. 2\n5969\n",
      "searchable_content": "page 1 gaussian splatting for ef\ufb01cient satellite image photogrammetry luca savant aira1 gabriele facciolo2 thibaud ehret3 https mezzelfo.github.io eogs 1 politecnico di torino corso duca degli abruzzi 24 10129 torino to italia 2 universite paris-saclay cnrs ens paris-saclay centre borelli 91190 gif-sur-yvette france 3 amiad p\u02c6ole recherche france abstract recently gaussian splatting has emerged as a strong al- ternative to nerf demonstrating impressive 3d modeling capabilities while requiring only a fraction of the training and rendering time. in this paper we show how the stan- dard gaussian splatting framework can be adapted for re- mote sensing retaining its high ef\ufb01ciency. this enables us to achieve state-of-the-art performance in just a few min- utes compared to the day-long optimization required by the best-performing nerf-based earth observation meth- ods. the proposed framework incorporates remote-sensing improvements from eo-nerf such as radiometric correc- tion and shadow modeling while introducing novel com- ponents including sparsity view consistency and opacity regularizations. 1. introduction since the mid-20th century the number of active satellites in orbit has increased exponentially. today hundreds of satellites are dedicated to earth observation and this num- ber is expected to continue growing 9 . these satellites frequently acquire optical images of the same areas at reg- ular intervals. as a result the availability of these datasets will continue to grow in the near future requiring ef\ufb01cient algorithms to handle this expansion. one common task for these image datasets is photogram- metry 13 19 27 which aims to recover the 3d geome- try e.g. digital surface model - dsm and the appearance e.g. an albedo map of the earth s surface for this area us- ing only the available 2d satellite images. speci\ufb01cally we focus on performing digital surface modeling from remote sensing images. historically binocular stereovision and tri-stereo meth- ods have been used for this purpose. however these meth- ods rely on image acquisitions being nearly simultaneous and with speci\ufb01c relative positions which is often imprac- tical with limited acquisition opportunities and or costly. eogs altitude eo-nerf altitude pansharpened image ground-truth altitude figure 1. using a limited number of satellite images of a given scene the proposed eogs method estimates the appearance and geometry of the scene. it achieves the same level of detail as eo- nerf 32 such as the group of fans or the thin structures on top of the tall building on the left. however eogs requires only a few minutes of optimization compared to the day-long training time required by eo-nerf 32 . therefore our goal is to develop a method capable of han- dling multi-date images captured from arbitrary satellite po- sitions. more recently multi-view stereo methods tailored for novel-view synthesis nvs have been used to solve this task as they naturally handle diverse camera positions. in- deed as the nvs task also requires understanding the ge- ometry of the scene these methods can be used to recover a digital elevation model. these methods are usually based this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 5959 page 2 on the concept of radiance \ufb01eld i.e. a representation of the scene modeling the radiance emitted by each point of which nerf 36 is the main representative. among those eo-nerf 32 has established itself as a state-of-the-art ap- proach for digital surface modeling thanks to an improved shadow model. however because it is nerf-based it also inherits nerf s computational slowness. in recent years 3d gaussian splatting 3dgs 23 has been proposed as an alternative to nerf. this method offers much faster training and rendering while reaching comparable reconstruction ac- curacy. in this work we introduce eogs the earth-observation gaussian splatting the \ufb01rst method for digital elevation modeling based on 3dgs. eogs achieves accuracy com- parable to previous state-of-the-art approaches while being approximately 300 faster. keys to the success of eogs are the following contributions all of which are compatible with the original 3dgs framework s ef\ufb01ciency approximating locally the pushbroom satellite sensors as af\ufb01ne cameras. introducing a shadow-mapping-based pipeline for render- ing the shadows in a physically accurate manner. adding three new regularization terms that promote spar- sity in the gaussians opacities view consistency and completely opaque objects. this reduces the training time and improves the quality of the results. 2. related work 2.1. stereovision for earth observation stereovision is at the heart of many tools for 3d estimation from series of satellite images. examples of such pipelines are ames stereo pipeline 4 micmac 43 cars 35 s2p 11 or catena 25 . traditionally these multi-view stereo methods are applied to well-chosen either manually or automatically image pairs. since they process each pair independently for example in the dense stereo matching step a crucial step is the fusion of all the generated pair- wise 3d models into a single one. the recent trend has been to replace classic dense match- ing methods such as semi-global matching sgm 20 or more global matching mgm 14 with deep learning based methods such as psm 8 hsm 53 or ga-net 54 . a review of these methods and a comparative study for satellite images is performed in 30 . 2.2. nerf for earth observation recently mildenhall et al. 36 have shown that it is pos- sible to learn a volumetric model of a scene called neural radiance \ufb01elds nerf using differentiable inverse render- ing. given a sparse set of views of the scene nerf learns in a self-supervised manner by maximizing the photocon- sistency across the predicted renderings corresponding to the available viewpoints. after convergence the volumet- ric model can then be used to render realistic novel views of the scene. in practice this volumetric model is repre- sented by an mlp that predicts for each position p of the space the local density of the scene \u03c3 p as well as its ap- pearance i.e. color c p . the rendering is performed using an approximation of the volumetric integral i from optical physics estimated using ray casting i o d z tf tn e r t tn \u03c3 r s dsc r t d \u03c3 r t dt 1 with o the camera center from which the ray r originates and d unit direction. nerf-based methods were then extended to the remote sensing case and in particular to perform multi-view and multi-date satellite photogrammetry namely s-nerf 12 sat-nerf 31 and eo-nerf 32 . s-nerf 12 exploits the solar direction information typically available in the metadata of each observation or that can be easily retrieved knowing the location of the scene as well as the acquisi- tion hour and date to predict the direct sun light reach- ing each point in the scene. this is done by adding the solar direction as an input to the mlp and predicting the amount of sun light reaching a point as a new output. in this way the shadows cast by buildings can be learned by the mlp and generated accordingly during the novel-view rendering step. sat-nerf 31 extends s-nerf by mod- eling the transient parts of the scenes e.g. cars construc- tion sites or foliage as done in nerf-in-the-wild 33 and improves the camera representation of s-nerf from pin- hole to rpc 1 46 . eo-nerf 32 improves the handling of shadows of s-nerf and sat-nerf by de\ufb01ning physi- cally plausible shadows directly from the geometry. these shadows are then rendered by additional raycasting from the surface in the direction of the sun. more recent work fo- cuses on modeling dif\ufb01cult seasonal effects 17 extending the proposed volumetric models to surface models 40 us- ing the raw pre-pansharpened data provided directly by the satellite operators 38 and accelerating the training step by taking advantage of faster nerf versions 5 . 2.3. 3d gaussian splatting following the growing interest in nvs 3dgs 23 was pro- posed as an alternative to nerf-based methods. while both nerf and 3dgs use alpha compositing as their image for- mation model 3dgs represents the scene using a set of discrete gaussian-shaped primitives placed in the 3d space as opposed to a continuous black-box mlp representation of nerf. in the following we use the notation of bul o et al. 7 . a gaussian primitive is a tuple \u03b3 \u00b5 \u03c3 \u03b1 f rep- resenting a single gaussian-shaped volume element in the scene where \u00b5 r3 is the primitive center \u03c3 m3 3 r 5960 page 3 its 3d shape and orientation \u03b1 0 1 its opacity and f rd its feature vector e.g. color when d 3 . 3dgs uses k independent gaussian primitives to repre- sent the scene so we will index them using k 1 . . . k as subscript. each gaussian primitive k has an associated 3d gaussian kernel gk de\ufb01ned as gk x exp 1 2 x \u00b5k t \u03c3 1 k x \u00b5k . 2 to render a view characterized by its associated camera 3dgs splats each 3d gaussian kernel onto the camera image plane. this process is called the splatting operation and mathematically it associates a gaussian primitive \u03b3k and a camera model projection a r3 r2 to a 2d gaus- sian kernel ga k r2 r. the original 3dgs method deals only with pinhole camera models. during the splat- ting operation each gaussian is projected according to the \ufb01rst-order approximation of the perspective projection com- puted at \u00b5k ja \u00b5k . in this way the mean vector and covariance matrix of the 2d gaussian kernel are \u00b5a k a \u00b5k \u03c3a k ja \u00b5k \u03c3 ja \u00b5k . 3 once all the primitives are splatted they are sorted front- to-back with respect to the camera reference. then they are aggregated using the traditional alpha compositing ac- counting also for the gaussian kernel decay such that ia u k x k 1 f k\u03c9a k u 4 where ia is the rendered image associated with camera a u r2 is a point in the 2d image plane f k are features that depend on f k and the view direction for modeling view- direction dependent color effect such as shiny objects and the alpha-compositing coef\ufb01cient is \u03c9a k u \u03b1kga k u k 1 y j 1 1 \u03b1jga j u . 5 recent literature expanded the original 3dgs 23 in many directions. as 3dgs has focused only on pinhole camera models and relied on a \ufb01rst-order local approxi- mation of the model recent works propose different cam- era models e.g. \ufb01sh-eye 29 or more complex splatting strategies 21 . another line of work focused on improving 3dgs in challenging input regimes such as sparse-view in- put 52 and input without camera parameters 16 . more- over many recent methods focus on controllable texture and lighting 18 22 28 44 . further references can be found in recent surveys 2 51 . 3. method the proposed earth-observation gaussian splatting method referred to as eogs specializes and adapts 3dgs for the satellite photogrammetry task. given n non-orthorecti\ufb01ed satellite images and their corresponding rpc camera model coef\ufb01cients a set of gaussian-shaped 3d primitives is opti- mized to recover both the 3d geometry and appearance of the scene. the general learning problem is to \ufb01nd the set of k gaussian primitives that best approximates the n satellite images with the rendering process of eq. 4 . this can be formulated as arg min \u03b3k j1 kk n x i 1 \u2113 \u02c6ii ii 6 where ii is the i-th input satellite observation \u02c6ii is the cor- responding synthesized view in the original 3dgs \u02c6ii iai and \u2113is the same photometric distance function used in 3dgs. in the following sections we highlight the differences between eogs and previous 3dgs and nerf-based ap- proaches. 3.1. projections and coordinate systems we de\ufb01ne the coordinate system in which the gaussian primitive centers and shapes are expressed as world-space coordinates. this coordinate system is a uniformly rescaled and recentered version of the universal transverse mer- cator utm coordinate system 49 such that the center of the scenes coincides with the origin the scene is con- tained in a unit cube and it is east-north-up aligned simi- larly to eo-nerf 32 . at the other end of the transforma- tion pipeline lies the 2d ndc-space where the gaussian primitives are splatted. the correct mapping between these two spaces is a composition of transformations world-space to utm to longitude-latitude-altitude. using the rpc coef\ufb01cients that model the satellite position and 3d attitude associated with each observation the latter coordinate system is mapped onto the image row-column coordinates. finally the co- ordinates are normalized to range in 1 1 to get to ndc- space. as shown in fig. 2 we instead compute a per-scene af\ufb01ne approximation of the whole transformation introduc- ing a negligible mean error of 0.012 pixels while be- ing more computationally ef\ufb01cient than previous works and compatible with a gaussian splatting formulation. speci\ufb01- cally for an af\ufb01ne camera model a x r3 ax a r2 the eq. 3 simpli\ufb01es to \u00b5a k a\u00b5k a r2 and \u03c3a k a\u03c3a r2 2. this simpli\ufb01cation eliminates the need for the local \ufb01rst-order approximation used in the orig- inal 3dgs method as we moved the approximation to the camera models. 5961 page 4 2d ndc space rowcolumn longlatalt utm world space rpc \u03b6 x cx b a r3 r2 af\ufb01ne transformation figure 2. summary of the transformation from world-space to ndc-space and its af\ufb01ne approximation. the af\ufb01ne approximation is com- putationally ef\ufb01cient compatible with the gaussian splatting formulation and well-suited for satellite images. the coordinate systems in the right red box represent 3d world coordinates camera-independent while the left blue box shows 2d coordinates camera-dependent . 3.2. shadow mapping as in eo-nerf we want to explicitly model the shadow phenomena in the images as the solar direction is avail- able for each image in the scene. unlike previous literature our method uses a custom variant of shadow mapping to cast geometrically consistent shadows. introduced in 50 shadow mapping is a well-known technique in the \ufb01eld of 3d graphics for adding shadows to a computer graphic ren- dering. it is particularly suited for eogs as it requires just the ability to render the scene from different points of view as opposed to the shadow casting technique used in eo- nerf that requires ray marching which is not de\ufb01ned in gaussian splatting . before introducing our variant of shadow mapping we de\ufb01ne the elevation render localization function and the homologous point function. given a camera model projection a the elevation ren- der is de\ufb01ned as the 3dgs rendering eq. 4 using the real elevations instead of colors ea u k x k 1 e\u00b5k \u03c9a k u 7 where e r3 r is an af\ufb01ne operation mapping 3d points expressed in the native world coordinates to the corre- sponding real altitude expressed in meters. we remark that this is not the depth nor the inverse depth typically found in the mvs literature. given a camera model projection a the localization function that maps a pixel of the camera and a given ab- solute altitude to its associated point in the native 3d world is de\ufb01ned as loca r2 r r3 u h 7 x s.t. a x u and e x h. 8 given two cameras a and b the homologous point func- tion maps a pixel of the \ufb01rst camera to the corresponding pixel of the second camera taking into consideration the 3d geometry homa b r2 r2 u 7 u s.t. u b loca u ea u . 9 in our shadow mapping approach depicted in fig. 3 we assume that the sun is the only light source present in the scene. moreover since it is far from the scene it can be approximated as a directional light. following the classic shadow mapping approach we construct a camera s called sun camera placed at and aligned with the light source de- tailed in the supplementary . as the camera model corre- sponding to a directional light is the af\ufb01ne camera we can handle uniformly the sun cameras and the satellite cameras. then we consider a second camera a from which we want to synthesize a novel view and apply shadows accord- ing to the sun direction. given a point u in the a ndc- space and its corresponding altitude ea u we \ufb01rst local- ize it obtaining a 3d point in world-space. we then project this point according to s obtaining the homologous point of u in s. we then resample the elevation rendering of s at this projected point and compare it with ea u . mathe- matically this corresponds to ha s u es homa s u ea u . 10 if these two elevations ea u and es u are the same it means that both the camera and the sun camera are imaging the same 3d point hence this point is in light. if the two elevations are not the same then the sun camera is not able to see the 3d point hence it is in shadows. to represent this shading the color of points in the shadows is multiplied by a darkening coef\ufb01cient computed from ha s u as sa s u min exp \u03c1 ha s u 1 . 11 we argue that this formulation is physically plausible as this would be the correct equation for a homogeneous medium of density \u03c1 as shown in 34 . following 32 we also model a per-camera ambient light \u03c8a so that in-shadow objects do not appear com- pletely black. the shading to be applied to a given pixel u is given by the following lighting coef\ufb01cient la s u sa s u 1 sa s u \u03c8a. 12 finally eogs image formation equation is ia s u la s u k x k 1 \u03c6a f k \u03c9a k u 13 5962 page 5 where \u03c6a is a camera-speci\ufb01c af\ufb01ne color correction ap- plied to the intrinsic primitive colors f k. we remark that differently from 3dgs we drop the view-direction depen- dencies of the primitive colors and introduce a camera- dependent color correction. it is useful to de\ufb01ne the albedo rendering where we do not use the shadows or the camera-speci\ufb01c color correction ia u k x k 1 f k\u03c9a k u . 14 while the image formation model de\ufb01ned in eq. 13 is equivalent to eo-nerf the shadow de\ufb01nition is quite dif- ferent. in eo-nerf case shadows are de\ufb01ned as the sun visibility for all points on the surface. because of possible occlusions two points of the scene can correspond to the same point seen from the sun direction. therefore it is not possible to de\ufb01ne the sun visibility as an image that could be estimated using a gaussian splatting-like process. trying to compute an irregularly sampled image corresponding to these points would break the locality assumption used in gaussian splatting during the rasterization step and thus re- duce the computational ef\ufb01ciency. on the contrary the pro- posed shadow mapping veri\ufb01es all the assumptions made by gaussian splatting. 3.3. regularizers it is well-known that deep neural networks are implicitly regularized 39 41 45 48 meaning that despite being used in the overparametrized regime they show generalization capabilities. on the other hand we found out that primitives in 3dgs- based methods are almost independently optimized one from the other. this is probably due to the fact that the primitives in 3dgs are initialized as small spheres spread out in the entire scene. this results in 3dgs being less regu- larized than nerf-based methods and lacking constraints during the optimization phase. hence we are free to add additional regularization con- straints to the general optimization problem eq. 6 that in- duce smoother and more regular solutions. in particular we introduce constraints that promote our solution to be sparse i.e. we encourage solutions that require fewer gaus- sian primitives view consistent and mostly composed of completely opaque objects. as common ml pipelines are specialized for uncon- strained optimization problems we argue to use a la- grangian relaxation approach and re-formulate each con- straint as a new loss term each with its own experimentally- found lagrangian multiplier. promoting sparsity. training time is directly proportional to the number of gaussian primitives considered during the optimization process. as we want to recover the geometry sun satellite resampled sun sun resampled sun satellite figure 3. shadow mapping illustration. the point u in the satel- lite image af\ufb01ne camera a corresponds to a the the 3d point x loca u on the vertical wall. projecting x to the sun camera af\ufb01ne camera s u sx is obtained. then y locs u is obtained localizing u. the point x and its pixel u are in shadow because the elevation of y is greater than the elevation of x. in- deed all and only the points where the satellite elevation and the resampled sun elevation do not match should be shaded. on the bottom of the illustration are shown examples of the sun elevation the resampled sun elevation and the satellite elevation renderings with shadows highlighted in red. of the scene as fast as possible we want as few gaussian primitives as possible hence a sparse solution. inspired by the well-known lasso regularization in lin- ear regression 47 that promotes a sparse solution and by recent works such as 3dgsmcmc 24 we consider a l1 regularization of the opacities lo 1 k k x k 1 \u03b1k. 15 this regularization promotes sparsity in the primitive opacities distribution hence only useful primitives will be visible at the end of the optimization. we pair this regu- larization with a simple thresholding pruning procedure that discards any primitive with \u03b1 \u03b1min. in this way un- used primitives are actually discarded yielding faster splat- ting and overall faster training speci\ufb01cally we recorded speedups of up to 2 on the considered datasets . we remark that many works 7 24 have proposed re- placements to the original 3dgs densi\ufb01cation pruning pro- cedure. here instead we aim only at lowering the num- ber of primitives so we do not need a densi\ufb01cation strategy as long as we instantiate enough of them at the beginning of the optimization. moreover we set \u03b1min 0.0025 as 5963 page 6 figure 4. from top-left to bottom-right shadow maps of eo- nerf eogs without the ls penalizer eogs with the ls penal- izer and the corresponding satellite image. textures correspond- ing to the image content can be observed in the shadow map of eo-nerf and eogs without the ls penalizer but not in eogs. primitives with lower opacities are already discarded in the original 3dgs implementation of the front-to-back splat- ting procedure. promoting view consistency. differently from the clas- sical nvs context in remote sensing the available views are low-count and sparse resulting in eq. 6 being even less constrained. paired with the fact that 3dgs does not bene\ufb01t from the implicit regularization of nerf we argue that an additional constraint promoting view consistency is needed. we propose a local view consistency loss based on the intuition that if the same 3d point is visible from two cam- eras and the cameras are close to each other then the color and elevation resampled at the corresponding pixels should be the same. otherwise the object is occluded or outside the camera boundaries. mathematically given a camera a we randomly perturb it and obtain a camera b. assuming that there is no view- direction dependent color effect this constraint reads ha b u hmin ia u ib homa b u ea u eb homa b u 16 where we reused the same notation of the shadow mapping explanation. this constraint results in two loss terms the color albedo consistency and the altitude consistency lcc x u m a b u ia u ib homa b u 17 lac x u m a b u ea u eb homa b u 18 where m a b is a binary mask that selects all pixels u such that ha b u hmin and homa b u is inside the image boundaries. we remark that we always choose a from the input posed images and we set hmin 30cm. moreover we obtain b by independently sampling q1 q2 r from a 1-truncated standard distribution and de\ufb01ning b x a x 0.05 e x q1 q2 . 19 in the generic nvs literature many works 10 37 pro- posed different methods for increasing the view consis- tency. regnerf 37 is the \ufb01rst work that deals with sparse camera poses by introducing a loss term that maximizes the likelihood of rendered rgb patches from virtual cam- eras with a pre-trained deep normalizing-\ufb02ow model while also adding a total variation regularization on the rendered depth. furthermore 10 introduces a reprojection mecha- nism such that only the geometry needs to be learned from the nerf as the colors are resampled from the input im- ages. note that eogs differs from 37 as we ask for consistency rgb and depth between two cameras one real and one virtual hence we do not need any pre-trained model for the rgb renders nor prior on the elevation ren- ders. eogs also differs from 10 as we learn the colors and do not resample input images that may contain transients or color shifts. promoting opaqueness. looking at the output from sat- nerf and eo-nerf see fig. 4 we can see that much of the texture of the scene is embedded in the geometric shad- ows. this geometry misuse is caused by semi-transparent objects casting semi-transparent shadows. in order to lessen this effect we propose to add an entropy-based penalty ls for incorrect use of the shadows. this penalty is de\ufb01ned as ls x u h sa s u 20 where h x x log2 x 1 x log2 1 x . in- deed the shadow map sa s should contain only 0 or 1 val- ues. this is the case for \u03c1 in eq. 11 as a build- ing should not cast a semi-transparent shadow. hence we add this entropy-based penalizer to discourage the use of semi-transparent shadows which in turn encourage objects to be either completely transparent or fully opaque. note that choosing a large \u03c1 during training is not an option since it would make the training unstable as eq. 11 would be close to a non-differentiable step function. we acknowledge that the problem of promoting hard sur- faces in nerf has been studied in previous works such as 3 42 . 3.4. implementation details the implementation of eogs is based on the original 3dgs code base. other than the aforementioned novel 5964 page 7 jax iarpa 004 068 214 260 mean 001 002 003 mean time number of views 8 16 20 14 - 24 20 21 - - no mask eo-nerf 32 1.37 1.05 1.61 1.37 1.35 1.43 1.79 1.31 1.51 15 hours sat-ngp 5 1.63 1.27 2.18 1.79 1.72 1.54 2.11 1.69 1.78 25 minutes sat-mesh 40 1.55 1.15 2.02 1.36 1.52 n.a. n.a. n.a. n.a. 8 minutes s2p 15 1.45 1.19 1.82 1.66 1.53 1.48 2.48 1.38 1.78 20 minutes eogs ours 1.45 1.10 1.73 1.55 1.46 1.58 2.00 1.27 1.62 3 minutes foliage mask eo-nerf 32 1.02 1.03 1.55 1.24 1.21 1.32 1.63 1.18 1.38 15 hours sat-ngp 5 1.03 1.26 2.17 1.43 1.47 1.34 1.85 1.62 1.60 25 minutes eogs ours 0.89 1.01 1.63 1.24 1.19 1.38 1.70 1.03 1.37 3 minutes table 1. mean absolute error on the elevation meters and the corresponding training time for various baseline methods when considering the whole aoi no mask or when ignoring foliage areas foliage mask . results for sat-mesh are reported from the paper since the authors did not share their code. contributions the main differences lie in disabling the per- gaussian view-direction color dependency and initializing all the gaussians with white color and as low as possible opacity 1 . moreover we reduce the number of itera- tions to 5000 and enable the shadow mapping and all three regularizations at iteration 1000. furthermore the gaus- sians centers are initialized uniformly in the 3d scene such that the initial density is 0.13 gaussians per m3. we use the same optimizer and scheduler of 3dgs for the primitives and use a second adam scheduler with 10 2 learning rate for learning the camera-dependent parameters the af\ufb01ne color-correction \u03c6a and the ambient color \u03c8a. the lagrangian coef\ufb01cients of the regularization con- straints have been found experimentally on a single scene rounded to the nearest power of ten and applied to all scenes. this highlights the robustness of eogs to the spe- ci\ufb01c values of these coef\ufb01cients. the \ufb01nal loss is min n x i 1 \u2113 \u02c6ii ii 0.1lo 0.1lcc 0.01lac 0.01ls 21 where \u02c6ii is now differently from 3dgs in eq. 6 a short- hand notation for iai si from eq. 13 which also depends on the sun camera si. 4. experiments we evaluate eogs in the same experimental setting as the most recent related work in the literature eo-nerf. we are using datasets provided in the 2019 ieee grss data fusion contest dfc2019 6 26 and 2016 iarpa multi-view stereo 3d mapping challenge iarpa2016 . these datasets comprising a total of 7 areas of inter- est aoi contain cropped non-orthorecti\ufb01ed multidate worldview-3 observations along with metadata such as the 3d satellite attitude encoded in the rpc coef\ufb01cient and the local sun direction. we use the bundled-adjusted ver- sion of the rpc coef\ufb01cient used in eo-nerf. each image covers approximately 256 256 meters squared of terrain with a resolution of 30 50 cm per pixel while each aoi is imagined by 10 20 crops. 4.1. main experiment results tab. 1 show the main experimental results of eogs. to as- sess the accuracy of eogs we report the mean absolute er- ror mae between a lidar scan included in the dataset and the elevation render aligned to this nadir view. we argue that the volume of these data will grow in the near future so we are also interested in the time required to recover the geometry from the input images. hence we also report the training time. if the entire aois are considered as reported in tab. 1 top eogs performs slightly worse than the state of the art eo-nerf but it is approximately 300 faster. for ref- erence we also report all available results of other meth- ods from the literature eo-nerf 32 sat-ngp 5 sat- mesh 40 and s2p 11 15 . we see that eogs is pareto- optimal with respect to elevation mae and training time. if instead we use available ground truth semantic maps to ig- nore prediction in the foliage areas eogs performance is equivalent to eo-nerf showing higher accuracy for struc- tural objects as reported in tab. 1 bottom . we present visual results in fig. 5 more results as well as details on the number of gaussians per scene and memory usage are provided in the supplement. 4.2. ablation and parameter studies impact of the different losses. tab. 2 reports an abla- tion study of the loss terms in eogs. each column corre- sponds to a different ablation experiment while each row corresponds to a different component of eogs being ab- lated. the \ufb01rst row indicates whether the shadow mapping technique is enabled or not. the following three rows indi- 5965 page 8 figure 5. from left to right visual results on jax 214 comparing sat-ngp 5 eogs eo-nerf 32 and the ground truth. shadowmap sparsity consistency opaqueness mae m 5.03 1.86 1.83 1.69 1.79 1.57 1.76 1.64 1.54 train min 4.18 - - - - 4.27 - - 2.85 table 2. ablation study of each proposed component of eogs. cate respectively the presence of the sparsity consistency and opaqueness regularizers. we remark that the \ufb01rst col- umn is equivalent to 3dgs with af\ufb01ne cameras learnable per-camera af\ufb01ne color correction and different primitives initialization. for each column we report the grand mean elevation mae of jax and iarpa scenes. to quantitatively measure the impact of each single com- ponent we linearly regress the mae from the presence of the components obtaining a coef\ufb01cient for each component that expresses an elevation mae gain with respect to the base case reported in the \ufb01rst column of tab. 2 . the in- troduction of shadow mapping is the most impactful com- ponent gaining 3.16 meters of accuracy. then the con- sistency regularizer and the opaqueness regularizer further improve the accuracy of eogs by 0.20 and 0.09 meters respectively. lastly the sparsity regularizer while being necessary for achieving ef\ufb01cient training as shown in the last row of tab. 2 also reports an improvement of 0.04 me- ters. hence all components independently contribute to the quality of the geometry reconstruction. regularization parameters. fig. 6 shows the results of a grid search on the coef\ufb01cients of lcc and lac in eq. 21 . it shows that both altitude regularization and color regulariza- tion are necessary to achieve the best performance. we re- mark that in order to reduce over\ufb01tting to a particular scene mae m figure 6. view consistency regularization parameter ablation study. selected parameter set is shown with the cyan dot. esti- mation performed on the jax 260 sequence. 0 20 40 60 80 dsm points covered by at least x of the satellite images 0.8 1.0 1.2 mae eonerf eogs figure 7. impact of the visibility on the performance using foliage mask . we choose the same round coef\ufb01cients for all scenes. impact of visibility. fig. 7 shows the impact of the visibil- ity i.e. the number of cameras that can see a given point of the scene on the performance. while eogs and eo-nerf are comparable on average this test shows that eogs per- forms better for regions that are visible in most images but struggles in the regions observed in only a few images. 5. conclusion this study presents eogs the \ufb01rst gaussian-spatting- based framework for earth observation. by introducing re- mote sensing requirements in 3dgs such as shadow mod- eling and camera-speci\ufb01c corrections eogs achieves com- parable accuracy to the existing state-of-the-art method eo-nerf while being over 300 times faster. hence eogs will be a practical solution for near-future large-scale datasets. our analysis shows that eogs excels in high-coverage regions where it produces \ufb01ne details at a fraction of the computational cost. addressing regions with low image coverage or with foliage by adding additional regulariza- tion or employing better initialization schemes could fur- ther improve eogs accuracy. 5966 page 9 acknowledgements this publication is part of the project pnrr-ngeu which has received funding from the mur - dm 352 2022. this work was partially supported by the european union under the italian national recovery and resilience plan nrrp of nextgenerationeu partnership on telecommunica- tions of the future pe00000001 - program restart . it was performed using hpc resources from genci-idris grant 2024-ad011012453r3 . centre borelli is also with universit e paris cit e ssa and inserm. references 1 emmanuel p baltsavias and dirk stallmann. metric infor- mation extraction from spot images and the role of polyno- mial mapping functions. in xvii isprs congress commis- sion iv. swiss federal institute of technology institute of geodesy and photogrammetry 1992. 2 2 yanqi bao tianyu ding jing huo yaoli liu yuxin li wenbin li yang gao and jiebo luo. 3d gaussian splatting survey technologies challenges and opportunities. arxiv preprint arxiv 2407.17418 2024. 3 3 jonathan t barron ben mildenhall dor verbin pratul p srinivasan and peter hedman. mip-nerf 360 unbounded anti-aliased neural radiance \ufb01elds. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 5470 5479 2022. 6 4 ross a beyer oleg alexandrov and scott mcmichael. the ames stereo pipeline nasa s open source software for de- riving and processing terrain data. earth and space science 5 9 537 548 2018. 2 5 camille billouard dawa derksen emmanuelle sarrazin and bruno vallet. sat-ngp unleashing neural graphics primitives for fast relightable transient-free 3d reconstruction from satellite imagery. in igarss 2024 - 2024 ieee inter- national geoscience and remote sensing symposium pages 8749 8753 2024. 2 7 8 6 marc bosch kevin foster gordon christie sean wang gregory d hager and myron brown. semantic stereo for incidental satellite images. in 2019 ieee winter conference on applications of computer vision wacv pages 1524 1532 2019. 7 7 samuel rota bul o lorenzo porzi and peter kontschieder. revising densi\ufb01cation in gaussian splatting. arxiv preprint arxiv 2404.06109 2024. 2 5 8 jia-ren chang and yong-sheng chen. pyramid stereo matching network. in 2018 ieee cvf conference on com- puter vision and pattern recognition cvpr pages 5410 5418 2018. 2 9 jeff clauson simon cantrell jim vrabel john oeding bhaskar ranjitkar tom rusten shankar ramaseri and kimberly casey. earth observing satellites online com- pendium. u.s. geological survey 2024. 1 10 franc ois darmon b en edicte bascle jean-cl ement devaux pascal monasse and mathieu aubry. improving neural im- plicit surfaces geometry with patch warping. in proceedings of the ieee cvf conference on computer vision and pat- tern recognition pages 6260 6269 2022. 6 11 carlo de franchis enric meinhardt-llopis julien michel jean-michel morel and gabriele facciolo. an automatic and modular stereo pipeline for pushbroom images. isprs annals of photogrammetry remote sensing and spatial in- formation sciences 2-3 49 56 2014. 2 7 12 dawa derksen and dario izzo. shadow neural radiance \ufb01elds for multi-view satellite photogrammetry. in proceed- ings of the ieee cvf conference on computer vision and pattern recognition pages 1152 1161 2021. 2 13 eoportal. worldview legion 2024. https www.eoportal.org satellite- missions worldview-legion. 1 14 gabriele facciolo carlo de franchis and enric meinhardt. mgm a signi\ufb01cantly more global matching for stereovi- sion. in proceedings of the british machine vision confer- ence bmvc number 90 pages 1 12 2015. 2 15 gabriele facciolo carlo de franchis and enric meinhardt- llopis. automatic 3d reconstruction from multi-date satel- lite images. in 2017 ieee conference on computer vision and pattern recognition workshops cvprw pages 1542 1551 2017. 7 16 yang fu sifei liu amey kulkarni jan kautz alexei a. efros and xiaolong wang. colmap-free 3d gaussian splat- ting. in proceedings of the ieee cvf conference on com- puter vision and pattern recognition cvpr pages 20796 20805 2024. 3 17 michael gableman and avinash kak. incorporating season and solar speci\ufb01city into renderings made by a nerf archi- tecture using satellite images. ieee transactions on pattern analysis and machine intelligence 2024. 2 18 jian gao chun gu youtian lin hao zhu xun cao li zhang and yao yao. relightable 3d gaussian real-time point cloud relighting with brdf decomposition and ray trac- ing. arxiv 2311.16043 2023. 3 19 richard i hartley and joseph l mundy. relationship be- tween photogrammmetry and computer vision. integrating photogrammetric techniques with scene analysis and ma- chine vision 1944 92 105 1993. 1 20 heiko hirschmuller. stereo processing by semiglobal match- ing and mutual information. ieee transactions on pattern analysis and machine intelligence 30 2 328 341 2007. 2 21 letian huang jiayang bai jie guo yuanqi li and yan- wen guo. on the error analysis of 3d gaussian splat- ting and an optimal projection strategy. arxiv preprint arxiv 2402.00752 2024. 3 22 yingwenqi jiang jiadong tu yuan liu xifeng gao xi- aoxiao long wenping wang and yuexin ma. gaussian- shader 3d gaussian splatting with shading functions for re- \ufb02ective surfaces. in proceedings of the ieee cvf confer- ence on computer vision and pattern recognition pages 5322 5332 2024. 3 23 bernhard kerbl georgios kopanas thomas leimk uhler and george drettakis. 3d gaussian splatting for real-time radiance \ufb01eld rendering. acm transactions on graphics 42 4 2023. 2 3 5967 page 10 24 shakiba kheradmand daniel rebain gopal sharma wei- wei sun jeff tseng hossam isack abhishek kar an- drea tagliasacchi and kwang moo yi. 3d gaussian splatting as markov chain monte carlo. arxiv preprint arxiv 2404.09591 2024. 5 25 thomas krau\u00df pablo d angelo mathias schneider and veronika gstaiger. the fully automatic optical processing system catena at dlr. isprs international archives of the photogrammetry remote sensing and spatial informa- tion sciences 40-1 w1 177 181 2013. 2 26 bertrand le saux naoto yokoya ronny hansch myron brown and greg hager. 2019 data fusion contest technical committees . ieee geoscience and remote sensing maga- zine 7 1 103 105 2019. 7 27 laurent leb egue eric cazala-hourcade florie languille st ephanie artigues and olivier melet. co3d a worldwide one one-meter accuracy dem for 2025. the international archives of the photogrammetry remote sensing and spa- tial information sciences xliii-b1-2020 299 304 2020. 1 28 zhihao liang qi zhang ying feng ying shan and kui jia. gs-ir 3d gaussian splatting for inverse rendering. arxiv preprint arxiv 2311.16473 2023. 3 29 zimu liao siyan chen rong fu yi wang zhongling su hao luo linning xu bo dai hengjie li zhilin pei et al. fisheye-gs lightweight and extensible gaus- sian splatting module for \ufb01sheye cameras. arxiv preprint arxiv 2409.04751 2024. 3 30 roger mar \u0131 thibaud ehret and gabriele facciolo. disparity estimation networks for aerial and high-resolution satellite images a review. image processing on line 12 501 526 2022. 2 31 roger mar \u0131 gabriele facciolo and thibaud ehret. sat-nerf learning multi-view satellite photogrammetry with transient objects and shadow modeling using rpc cameras. in pro- ceedings of the ieee cvf conference on computer vision and pattern recognition pages 1311 1321 2022. 2 32 roger mar \u0131 gabriele facciolo and thibaud ehret. multi- date earth observation nerf the detail is in the shadows. in proceedings of the ieee cvf conference on computer vi- sion and pattern recognition pages 2035 2045 2023. 1 2 3 4 7 8 33 ricardo martin-brualla noha radwan mehdi sm sajjadi jonathan t barron alexey dosovitskiy and daniel duck- worth. nerf in the wild neural radiance \ufb01elds for uncon- strained photo collections. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 7210 7219 2021. 2 34 n. max. optical models for direct volume rendering. ieee transactions on visualization and computer graphics 1 2 99 108 1995. 4 35 julien michel emmanuelle sarrazin david yousse\ufb01 myr- iam cournet fabrice buffe jean-marc delvit aur elie em- ilien julien bosman olivier melet and c eline l helguen. a new satellite imagery stereo pipeline designed for scal- ability robustness and performance. isprs annals of the photogrammetry remote sensing and spatial information sciences 5-2-2020 171 178 2020. 2 36 ben mildenhall pratul p srinivasan matthew tancik jonathan t barron ravi ramamoorthi and ren ng. nerf representing scenes as neural radiance \ufb01elds for view syn- thesis. communications of the acm 65 1 99 106 2021. 2 37 michael niemeyer jonathan t barron ben mildenhall mehdi sm sajjadi andreas geiger and noha radwan. reg- nerf regularizing neural radiance \ufb01elds for view synthesis from sparse inputs. in proceedings of the ieee cvf con- ference on computer vision and pattern recognition pages 5480 5490 2022. 6 38 emilie pic thibaud ehret gabriele facciolo and roger mar \u0131. pseudo pansharpening nerf for satellite image collec- tions. in igarss 2024-2024 ieee international geoscience and remote sensing symposium pages 2650 2655. ieee 2024. 2 39 tomaso poggio qianli liao brando miranda andrzej ban- burski xavier boix and jack hidary. theory iiib general- ization in deep networks. arxiv preprint arxiv 1806.11379 2018. 5 40 yingjie qu and fei deng. sat-mesh learning neural im- plicit surfaces for multi-view satellite reconstruction. remote sensing 15 17 4297 2023. 2 7 41 sameera ramasinghe lachlan e macdonald and simon lucey. on the frequency-bias of coordinate-mlps. ad- vances in neural information processing systems 35 796 809 2022. 5 42 daniel rebain mark matthews kwang moo yi dmitry la- gun and andrea tagliasacchi. lolnerf learn from one look. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 1558 1567 2022. 6 43 ewelina rupnik mehdi daakir and marc pierrot- deseilligny. micmac a free open-source solution for pho- togrammetry. open geospatial data software and stan- dards 2 14 2017. 2 44 yahao shi yanmin wu chenming wu xing liu chen zhao haocheng feng jingtuo liu liangjun zhang jian zhang bin zhou et al. gir 3d gaussian inverse ren- dering for relightable scene factorization. arxiv preprint arxiv 2312.05133 2023. 3 45 matthew tancik pratul p. srinivasan ben mildenhall sara fridovich-keil nithin raghavan utkarsh singhal ravi ra- mamoorthi jonathan t. barron and ren ng. fourier fea- tures let networks learn high frequency functions in low di- mensional domains. neurips 2020. 5 46 c vincent tao and yong hu. a comprehensive study of the rational function model for photogrammetric processing. photogrammetric engineering and remote sensing 67 12 1347 1358 2001. 2 47 robert tibshirani. regression shrinkage and selection via the lasso. journal of the royal statistical society series b statistical methodology 58 1 267 288 1996. 5 48 dmitry ulyanov andrea vedaldi and victor lempitsky. deep image prior. in proceedings of the ieee conference on computer vision and pattern recognition pages 9446 9454 2018. 5 5968 page 11 49 u.s. geological survey. the universal transverse mercator utm grid. technical report u.s. geological survey re- ston va 2001. report. 3 50 lance williams. casting curved shadows on curved surfaces. in proceedings of the 5th annual conference on computer graphics and interactive techniques pages 270 274 1978. 4 51 tong wu yu-jie yuan ling-xiao zhang jie yang yan- pei cao ling-qi yan and lin gao. recent advances in 3d gaussian splatting. computational visual media 10 4 613 642 2024. 3 52 haolin xiong sairisheek muttukuru rishi upadhyay pradyumna chari and achuta kadambi. sparsegs real- time 360 deg sparse view synthesis using gaussian splat- ting. arxiv preprint arxiv 2312.00206 2023. 3 53 gengshan yang joshua manela michael happold and deva ramanan. hierarchical deep stereo matching on high- resolution images. in 2019 ieee cvf conference on com- puter vision and pattern recognition cvpr pages 5510 5519 2019. 2 54 feihu zhang victor prisacariu ruigang yang and philip h.s. torr. ga-net guided aggregation net for end- to-end stereo matching. in 2019 ieee cvf conference on computer vision and pattern recognition cvpr pages 185 194 2019. 2 5969",
      "keywords": [
        "page",
        "gaussian",
        "splatting",
        "ef\ufb01cient",
        "satellite",
        "image",
        "photogrammetry",
        "luca",
        "savant",
        "aira1",
        "gabriele",
        "facciolo2",
        "thibaud",
        "ehret3",
        "https",
        "mezzelfo.github.io",
        "eogs",
        "politecnico",
        "torino",
        "corso",
        "duca",
        "degli",
        "abruzzi",
        "10129",
        "italia",
        "universite",
        "paris-saclay",
        "cnrs",
        "ens",
        "centre",
        "borelli",
        "91190",
        "gif-sur-yvette",
        "france",
        "amiad",
        "p\u02c6ole",
        "recherche",
        "abstract",
        "recently",
        "emerged",
        "strong",
        "al-",
        "ternative",
        "nerf",
        "demonstrating",
        "impressive",
        "modeling",
        "capabilities",
        "while",
        "requiring",
        "only",
        "fraction",
        "training",
        "rendering",
        "time.",
        "paper",
        "show",
        "how",
        "stan-",
        "dard",
        "framework",
        "adapted",
        "re-",
        "mote",
        "sensing",
        "retaining",
        "high",
        "ef\ufb01ciency.",
        "enables",
        "achieve",
        "state-of-the-art",
        "performance",
        "just",
        "few",
        "min-",
        "utes",
        "compared",
        "day-long",
        "optimization",
        "required",
        "best-performing",
        "nerf-based",
        "earth",
        "observation",
        "meth-",
        "ods.",
        "proposed",
        "incorporates",
        "remote-sensing",
        "improvements",
        "from",
        "eo-nerf",
        "such",
        "radiometric",
        "correc-",
        "tion",
        "shadow",
        "introducing",
        "novel",
        "com-"
      ],
      "metadata": {
        "filename": "Aira_Gaussian_Splatting_for_Efficient_Satellite_Image_Photogrammetry_CVPR_2025_paper.pdf",
        "original_filename": "Aira_Gaussian_Splatting_for_Efficient_Satellite_Image_Photogrammetry_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 2797968,
        "upload_file_id": "upload_1752161051154_qkokqurww",
        "upload_timestamp": "2025-07-10T15:24:11.199203",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:24:11.391035+00:00",
      "content_length": 47136,
      "content_type": ".pdf",
      "filename": "Aira_Gaussian_Splatting_for_Efficient_Satellite_Image_Photogrammetry_CVPR_2025_paper.pdf"
    },
    "11": {
      "file_id": "11",
      "content": "\n=== Page 1 ===\nBenchmarking Object Detectors under Real-World Distribution Shifts in\nSatellite Imagery\nSara A. Al-Emadi1,2\nYin Yang2\nFerda Ofli1\n1 Qatar Computing Research Institute, HBKU & 2 College of Science and Engineering, HBKU\n{salemadi, yyang, fofli}@hbku.edu.qa\nAbstract\nObject detectors have achieved remarkable performance in\nmany applications; however, these deep learning models\nare typically designed under the i.i.d. assumption, meaning\nthey are trained and evaluated on data sampled from the\nsame (source) distribution. In real-world deployment, how-\never, target distributions often differ from source data, lead-\ning to substantial performance degradation. Domain Gen-\neralisation (DG) seeks to bridge this gap by enabling mod-\nels to generalise to Out-Of-Distribution (OOD) data with-\nout access to target distributions during training, enhancing\nrobustness to unseen conditions. In this work, we examine\nthe generalisability and robustness of state-of-the-art ob-\nject detectors under real-world distribution shifts, focusing\nparticularly on spatial domain shifts. Despite the need, a\nstandardised benchmark dataset specifically designed for\nassessing object detection under realistic DG scenarios\nis currently lacking. To address this, we introduce Real-\nWorld Distribution Shifts (RWDS), a suite of three novel DG\nbenchmarking datasets that focus on humanitarian and cli-\nmate change applications. These datasets enable the inves-\ntigation of domain shifts across (i) climate zones and (ii)\nvarious disasters and geographic regions. To our knowl-\nedge, these are the first DG benchmarking datasets tailored\nfor object detection in real-world, high-impact contexts. We\naim for these datasets to serve as valuable resources for\nevaluating the robustness and generalisation of future ob-\nject detection models. Our datasets and code are available\nat https://github.com/RWGAI/RWDS.\n1. Introduction\nDeep learning has achieved remarkable success in various\napplications, including flood mapping [32, 51], medical di-\nagnostics [4, 35], and self-driving cars [19, 60]. However,\nthese machine learning models are typically developed un-\nder the i.i.d. assumption, where they are trained and eval-\nuated on data samples drawn from the same source distri-\nbution. Consequently, when deployed in real-world envi-\nFigure 1. Example images from different climate zones\nronments with differing target distributions, these models\nexperience significant performance degradation, hindering\ntheir large-scale deployment. This phenomenon is known\nas distribution or domain shift. In this paper, we focus a spe-\ncific type of domain shift, referred to as spatial domain shift\n(i.e., covariate shift) on a global scale, which is driven by vi-\nsual variations in land cover and built structures influenced\nby factors such as natural landscapes, climate zones, archi-\ntectural styles, economic and financial development, social\nand cultural attributes, and human settlement patterns.\nIn satellite imagery-based object detection, spatial do-\nmain shift poses a significant challenge, especially when en-\nvironmental conditions vary unpredictably, as demonstrated\nin Figure 1. To examine this, we use the K\u00a8oppen climate\nclassification system [61] and consider a scenario where\nan object detector is trained on images from a tropical cli-\nmate zone but evaluated on Out-of-Distribution (OOD) tar-\nget domains, specifically dry and temperate climate zones.\nSince these target domains exhibit distinct visual character-\nistics compared to the source domain, a performance drop\nis expected, highlighting the impact of spatial domain shift\nwhen applying object detection models across diverse cli-\nmatic contexts.\nSeveral studies have sought to mitigate the issue of do-\nmain shift through data augmentation [59], transfer learn-\ning [40], and domain adaptation, in which the model has ac-\ncess to unlabelled samples from the target distribution dur-\ning training [43, 65]. However, in real-world applications,\nmodels often encounter distributions that cannot be fore-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8299\n\n=== Page 2 ===\nseen before deployment. To tackle this challenge, recent\nresearch has focused on domain shift under this constraint,\na problem known as Domain Generalisation (DG).\nDG datasets are essential for assessing a model\u2019s ability\nto generalise to unseen target distributions. In image classi-\nfication, a substantial body of research have been devoted to\ncurating DG datasets for broad use cases, such as PACS [29]\nand DomainNet [44], as well as datasets introducing syn-\nthetic domain shifts, like RotatedMNIST [15], or real-world\ndistribution shifts, as seen in WILDS [26]. However, the\nstudy of domain shift in object detection remains relatively\nunderexplored. To bridge this gap, Mao et al. introduced\nCOCO-O [41], a DG benchmark for object detection with\nsix domains including Sketch, Weather, Cartoon, Painting,\nTattoo, and Handmade, to evaluate both in-domain (ID) and\nOOD performance. While the domain shifts in COCO-O\nare evident, such as the differences between sketches and\npaintings, further investigation of the practical motivation\nfor training a model on sketches and testing it on paintings\ncould provide valuable insights. To our knowledge, a DG\nbenchmark does not currently exist for evaluating the be-\nhaviours of object detectors on OOD test data in a common,\nreal-world application setting.\nMotivated by this, we introduce Real-World Distribution\nShifts (RWDS), a suite of three realistic DG benchmark-\ning datasets, namely, RWDS-CZ, RWDS-FR and RWDS-\nHE, which focus on humanitarian and climate change ap-\nplications and investigate domain shifts across (i) climate\nzones and (ii) different disasters and geographic regions,\nrespectively. Moreover, we benchmark and analyse the per-\nformance of several state-of-the-art (SOTA) object detec-\ntion algorithms on RWDS under two setups: single-source,\nwhere an object detector is trained on only one source do-\nmain, and multi-source, where training incorporates multi-\nple source domains. We then evaluate these models on the\nunseen target domains to provide comprehensive insights\ninto their generalisation performance. We trained around\n100 object detector models and conducted over 200 experi-\nments. Our contributions are summarised as follows:\n\u2022 We propose RWDS, a suite of novel, realistic and chal-\nlenging DG datasets designed to evaluate spatial domain\nshifts in real-world object detection tasks.\n\u2022 We provide the community with in-depth benchmarking\nanalyses on the performance of the SOTA object detectors\non RWDS datasets.\n\u2022 We analyse the impact of single-source versus multi-\nsource training in DG for spatial domain shifts in satellite\nimagery, concluding that multi-source training enhances\ngeneralisability of object detectors.\nThe rest of the paper is organised as follows: Section 2\nreviews literature on object detection and robustness bench-\nmarks. Section 3 introduces the RWDS datasets with details\non data cleaning and preprocessing. Section 4 describes the\nevaluation metrics, experimental setups, and selected ob-\nject detectors. Section 5 presents the results and provides a\ncomprehensive analysis and Section 6 concludes the paper.\n2. Related Work\n2.1. Object Detection\nEarly attempts in deep-learning-based object detection used\na set of bounding boxes and masked regions as input\nto the CNN architecture to incorporate shape information\ninto the classification process to perform object localisa-\ntion [12, 16, 54, 55].\nLater on, end-to-end techniques\nwere proposed based on shared computation of convolu-\ntions for simultaneous detection and localization of the ob-\njects [9, 17, 22, 24, 37, 46, 48, 52]. These methods can\nbe generally divided into two categories: one-stage de-\ntectors [11, 13, 28, 37, 45, 46, 50, 56, 57, 70] and two-\nstage detectors [5, 16, 17, 23, 24, 33, 47, 53].\nMore\nrecently, transformer-based object detection models have\nproved more efficient and accurate, thanks to their ability\nto not require anchor boxes and non-maximum suppression\nprocedure [6, 67, 71]. Besides, with the advances in foun-\ndation models (large vision models or vision-language mod-\nels), open-set and open-world object detection has become\npopular [31, 36, 62]. Following these trends, remote sens-\ning community has also integrated deep learning-based ob-\nject detection models into their research [1, 10, 21, 30, 38,\n39, 66]. However, accurate object detection from satellite\nimagery at scale remains a challenging task.\n2.2. Robustness Benchmarks\nVarious benchmark studies have been developed to assess\nthe robustness of object detection models under distribution\nshifts. For instance, COCO-C [42] evaluates model per-\nformance by applying synthetic corruptions, such as JPEG\ncompression and Gaussian noise, to the COCO test set.\nSimilarly, OOD-CV [68] and its extended version, OOD-\nCV-v2 [69], include OOD examples across 10 object cate-\ngories from PASCAL VOC and ImageNet, spanning vari-\nations in pose, shape, texture, context, and weather con-\nditions. These datasets enable benchmarking across mul-\ntiple tasks like image classification, object detection, and\n3D pose estimation. COCO-O [41] introduces natural dis-\ntribution shifts in COCO-based datasets, spanning six do-\nmains\u2014weather, painting, handmade, cartoon, tattoo, and\nsketch. Their study has shown that there is a significant per-\nformance gap of 55.7% between ID and OOD performance,\nhighlighting the domain generalisation challenges under\nsuch shifts.\nHowever, despite their contributions, these\ndatasets still lack the complexity of real-world distribution\nshifts. More realistic benchmarks include those reflecting\nenvironmental changes in autonomous driving [25] and ob-\nject variations in aerial imagery [63], which better capture\n8300\n\n=== Page 3 ===\nthe dynamic and unpredictable conditions faced in practi-\ncal applications. However, they remain limited in scope,\nas they do not comprehensively account for geographic and\ntemporal variability, environmental and weather conditions,\nocclusion, clutter, and object appearance changes within a\nunified framework. In contrast, our RWDS datasets aim to\nbridge this gap by providing a diverse and realistic evalua-\ntion setting that encapsulates these real-world domain shifts\nmore holistically.\n3. Our RWDS Datasets\n3.1. RWDS Across Climate Zones\nWhile there are increasing efforts to mitigate and reduce\nthe negative and potential impact of climate change on the\nglobal ecosystem including natural resources, weather and\nthe natural landscapes [49], there is a need to develop ro-\nbust models to support computer vision tasks under these\ncircumstances, more particularly, object detection task. In\norder to investigate their robustness and generalisability\nacross different climate zones, we propose RWDS across\nClimate Zones (RWDS-CZ) dataset where we focus on\nK\u00a8oppen\u2019s climate zone classification [3, 61].\nGiven the\nscarcity of global satellite imagery that covers all climate\nzones, we use the raw satellite imagery from the xView\ndataset [27], an open-source object detection dataset fea-\nturing high-resolution (0.3m) images captured at a global\nscale across 60 object classes.\nFor this study, we focus\non three distinct climate zones: Zone A (CZ A)\u2014tropical\nor equatorial, Zone B (CZ B)\u2014arid or dry, and Zone C\n(CZ C)\u2014warm/mild temperate. These serve as our distinct\ndomains for studying spatial domain shifts in satellite-based\nobject detection.\nTo create the domains, we first map the geo-coordinates\nof each image to its respective climate zone and proceed\nwith splitting the overall dataset into domains. However,\nthis results in a mismatch between the classes available\nacross the domains. To resolve this, we retain only those\nclasses that appear in all climate zones. Additionally, we\nset a threshold of 30 samples per class to ensure sufficient\ndata for training. Any class with fewer than 30 samples\nis excluded from all domains. This process yields a total\nof 16 classes. To maintain consistent distribution of ob-\nject instances across the training, validation, and test sets\nwithin each domain, we follow the procedure outlined in\nAlgorithm 1. This process is repeated for each domain, re-\nsulting in the final RWDS-CZ dataset. Table 1 summarises\nthe dataset statistics, while Figure 2 shows the distribution\nof training samples across classes in all domains.\nTo visualize the domain shift in RWDS-CZ, we extract\nimage embeddings using RemoteCLIP [34] and project\nthem into 2D using t-SNE [58]. Figure 3-A showcases the\nshift between images from CZ A and CZ B.\nAlgorithm 1 Dataset Split Procedure\n1: Input: Set of images I, class labels C\n2: Initialise: Training set T \u2190\u2205, Validation set V \u2190\u2205, Testing\nset S \u2190\u2205\n3: function ALLOCIMAGES(I, N)\n4:\nfor each class c \u2208C do\n5:\nI\u2217\u2190argmax\nI\u2208I\ncount(I, c)\n\u25b7Select image with most\ninstances of c\n6:\nN \u2190N \u222a{I\u2217}\n\u25b7Append to designated set\n7:\nI \u2190I \\ {I\u2217}\n\u25b7Remove allocated image\n8:\nend for\n9:\nreturn (N, I)\n\u25b7Return final dataset splits\n10: end function\n11: while I \u0338= \u2205do\n12:\nfor i = 1 to 3 do\n\u25b7Repeat 3 times for training set\n13:\nT , I \u2190ALLOCIMAGES(I, T ) \u25b7Update training set\n14:\nend for\n15:\nV, I \u2190ALLOCIMAGES(I, V)\n\u25b7Update validation set\n16:\nS, I \u2190ALLOCIMAGES(I, S)\n\u25b7Update test set\n17: end while\nSplit\nCZ A\nCZ B\nCZ C\nTraining\n117, 265\n43, 272\n124, 717\nValidation\n58, 997\n13, 423\n47, 362\nTest\n56, 954\n24, 745\n60, 310\nTable 1. RWDS-CZ overall object instances per partition\n3.2. RWDS in Disaster Damage Assessment\nA notable consequence of climate change is the increasing\nfrequency and severity of natural disasters such as hurri-\ncanes, storms, floods, wildfires, earthquakes, tsunamis, etc.\nDamage assessment is essential during and after disasters\nto support aid delivery, guide building reconstruction ef-\nforts, and provide governments and humanitarian agencies\nwith an estimate of the disaster\u2019s impact. Generally, large\namount of satellite imagery is captured around the disaster-\nhit locations.\nHowever, given the sheer volume of data,\nthe cleaning, preprocessing and re-training of object detec-\ntors for each disaster on the spot is time consuming and\nmight not be feasible due to lack of annotations, highlight-\ning the crucial need for robust models that can generalise\nwell to unseen distributions beyond those they were trained\non. Hence, we investigate the robustness of SOTA object\ndetectors in this application under two different scenarios.\nIn the first use-case, we examine the shift of the same\ndisaster type across distant geographic regions with differ-\nent socioeconomical characteristics. Specifically, we de-\nfine domains in terms of collection of events that caused\nfloods across the United States and India, respectively. We\nrefer to this use-case as RWDS across Flooded Regions\n(RWDS-FR). Whereas, in the second use-case, we focus\non understanding the shift in the behaviour of these models\n8301\n\n=== Page 4 ===\n(a) Climate Zone A\n(b) Climate Zone B\n(c) Climate Zone C\nFigure 2. Class-wise distribution of training data for each domain as well as the overall data distribution across domains in RWDS-CZ\nFigure 3. Embedding space representations of the RWDS datasets\nacross different disaster events of the same type, namely,\nhurricanes, in North America. We refer to this use-case as\nRWDS across Hurricane Events (RWDS-HE). Similar to\nthe discussion related to the scarcity of open-source satel-\nlite imagery, we utilise the raw satellite images released in\nthe xDB building damage assessment dataset [20] for both\nRWDS-FR and RWDS-HE.\n3.2.1\nRWDS Across Flooded Regions (RWDS-FR)\nWe start by creating the metadata for the raw images. xDB\ndataset provides disaster event, damage type, and polygons\nof buildings for segmentation application. However, given\nthat we are interested in object detection, we convert poly-\ngons of buildings into bounding boxes. Furthermore, simi-\nlar to Section 3.1, we map the latitude and longitude coor-\ndinates of the polygons to find the corresponding location\nof each object instance in terms of country, region, conti-\nnent, etc. We then extract the flooded objects in India and\nUS. Figure 4 shows example images illustrating the shift be-\ntween the domains, with a close-up visualization of image\nembeddings in Figure 3-B. Unlike the original data, where\nthe instances are categorised into four classes, namely, no\ndamage, minor damage, major damage and destroyed, when\nextracting the flooded instances in the US and India, we ob-\nserve a class imbalance between those classes. Therefore,\nwe transform the task into a binary categorisation, leaving\nFigure 4. Comparison of flood scenes between the US and India\nIndia\nUS\nSplit\nD\nND\nD\nND\nTraining\n5, 023\n14, 841\n10, 680\n20, 055\nValidation\n2, 532\n8, 320\n5, 470\n9, 834\nTest\n2, 802\n8, 064\n5, 452\n10, 034\nTable 2. RWDS-FR object instances per partition.\nus with two classes, namely, damaged (D) and no dam-\nage (ND). We then follow the same logic as that discussed\nin Section 3.1 to create the training, validation and testing\nsplits. This yields the RWDS-FR dataset. Table 2 represents\nthe resulting domain and class distributions per split.\n3.2.2\nRWDS Across Hurricane Events (RWDS-HE)\nIn contrast to RWDS-FR dataset, where domains are de-\nfined by diverse geographic regions, this dataset focuses\non hurricane events across North America, which are ge-\nographically closer in proximity. As a result, the dataset\nconsists of four hurricane events as domains: Florence,\nMichael, Harvey, and Matthew, as shown in Figure 5. Fig-\nure 3-C presents the image embeddings for hurricanes Flo-\nrence and Matthew as an example. We adhere to the pre-\nprocessing, metadata creation, binary class categorisation,\nand data splitting procedures outlined in Section 3.2.1. This\n8302\n\n=== Page 5 ===\nFigure 5. Comparison of hurricane scenes from different events\nFlorence\nMichael\nHarvey\nMatthew\nSplit\nD\nND\nD\nND\nD\nND\nD\nND\nTraining\n1,102 4,196 6,132 11,347 9,270 9,223 8,919 1,938\nValidation\n578 2,112 3,075 5,455 4,670 4,594 4,910 1,042\nTest\n582 2,158 3,229 5,890 4,796 4,821 4,743 1,078\nTable 3. RWDS-HE object instances per partition.\ndefines the RWDS-HE dataset. Table 3 presents the final\nper-class and per-split distribution for each domain.\n4. Experiments\n4.1. Single-Source and Multi-Source Setup\nWe investigate DG in two setups. The first involves training\nan object detector on a single ID source training set, then\nassessing its performance on both the held-out OOD target\ndomains and the ID test set. This setup reflects scenarios\nwith a limited diversity of data distributions. Whereas, in\nthe second setup, we incorporate training an object detec-\ntor on a collection of source domains, mirroring real-world\nscenarios, where data from a variety of distributions may\nbe available. For quantitative comparison, we evaluate the\ntrained object detector on each OOD target domain sepa-\nrately, as well as on the average performance across ID do-\nmains.1\n4.2. DG Evaluation Metrics\nMethods for evaluating DG models remain an active and\nopen area of investigation.\nResearchers have, however,\nadapted existing approaches to assess the performance of\ndeep learning models on OOD datasets for classification\ntasks. Among these, the leave-one-domain-out evaluation\nstrategy [18] is widely regarded. In this setup, one domain\nis excluded from training, enabling it to serve as an inde-\npendent test domain to rigorously evaluate model perfor-\nmance without any additional tuning. Inspired by this, we\nadapt this evaluation technique for object detection under\nthe single- and multi-source setups.\nWe assess the performance of the object detectors using\nthe standard mean Average Precision (mAP) metric which is\ncommonly used in object detection applications [72]. More\n1The single- and multi-source setups are formally defined in Supple-\nmentary A.\nObject Detector\nBackbone\nFaster R-CNN [47]\nResNeXt-101-64x4d\nMask R-CNN [24]\nResNeXt-101-64x4d and FPN\nTOOD [13]\nResNeXt-101-64x4d, DCNv2 and FPN\nDINO (5scale) [67]\nSwin-L\nGrounding DINO [36]\nBERT and Swin-B\nGLIP (L) [31]\nBERT and Swin-L\nTable 4. Object detectors and their backbone architectures.\nspecifically, we use the MS-COCO AP metric, which is cal-\nculated as the average over multiple IoU thresholds ranging\nfrom 0.50 to 0.95 with a stepsize of 0.05.2\nPerformance Drop (PD). A metric frequently used in the\nDG community for assessing the generalisability of clas-\nsification tasks is the Performance Drop, which quantifies\nthe percentage of performance degradation observed in the\nmodel when subjected to OOD data from a target domain.\nDrawing inspiration from this approach, we apply it in the\ncontext of DG for object detection. This is formulated as\nfollows:\nPD = 100 \u00d7 mAPID \u2212mAPOOD\nmAPID\n(1)\nwhere mAPID and mAPOOD represent the average mAP\nof the combination of detectors tested on a specific domain\u2019s\nID and OOD test sets, respectively.\nHarmonic Mean (H). To compare the ID and OOD perfor-\nmance of object detectors based on their mAP, we adopt the\nwidely recognised Harmonic Mean as another evaluation\nmetric. This choice is motivated by its use in recent gen-\neralised open-set zero-shot learning studies [8, 14, 64] to\ncompute a joint score reflecting model performance across\nin-domain and out-of-domain test sets. This is defined as:\nH = 2 \u00d7 mAPOOD \u00d7 mAPID\nmAPOOD + mAPID\n(2)\n4.3. Object Detectors and Hyperparameters\nWe conduct all the experiments using the MMDetection\ntoolbox [7].\nWe selected object detectors across clas-\nsical (Faster R-CNN [47], Mask R-CNN [24]), recent\n(DINO [67], TOOD [13]), and foundation model-based ap-\nproaches (Grounding DINO [36], GLIP [31]).\nTable 4\npresents the top-performing backbone architecture selected\nfor each detector, as evaluated on standard object detection\ndatasets by MMDetection.\nTo train the object detectors, we perform preprocessing\nto unify the image sizes of the raw images. We start by\ncropping all the images into 512\u00d7512 tiles with an overlap-\nping ratio of 0.2 using SAHI [1] while preserving the orig-\ninal resolution of the images. To ensure a fair comparison\n2mAP50 & mAP75 results are included in Supplementary B.\n8303\n\n=== Page 6 ===\nTarget\nCZ A\nCZ B\nCZ C\nMethods\nmAPID\nmAPOOD\nPD \u2193\nH \u2191\nmAPID\nmAPOOD\nPD \u2193\nH \u2191\nmAPID\nmAPOOD\nPD \u2193\nH \u2191\nFaster R-CNN\n7.2\n3.9\n47\n5.0\n7.5\n6.0\n20\n6.7\n7.7\n3.4\n56\n4.7\nMask R-CNN\n7.3\n3.7\n49\n4.9\n7.7\n5.8\n25\n6.6\n7.8\n3.5\n55\n4.8\nTOOD\n7.8\n4.0\n49\n5.2\n7.8\n6.1\n22\n6.8\n8.2\n4.0\n52\n5.3\nDINO\n11.0\n5.6\n49\n7.4\n9.6\n8.0\n17\n8.7\n11.0\n5.6\n49\n7.4\nGrounding DINO\n12.9\n7.5\n42\n9.5\n10.8\n10.0\n7\n10.4\n13.1\n7.1\n46\n9.2\nGLIP\n9.8\n6.3\n36\n7.6\n8.8\n8.2\n7\n8.5\n9.2\n5.4\n41\n6.8\nTable 5. Single-source DG analysis of SOTA detectors on RWDS-CZ.\nTarget\nCZ A\nCZ B\nCZ C\nMethods\nmAPID\nmAPOOD\nPD \u2193\nH \u2191\nmAPID\nmAPOOD\nPD \u2193\nH \u2191\nmAPID\nmAPOOD\nPD \u2193\nH \u2191\nFaster R-CNN\n7.7\n4.9\n36\n6.0\n8.2\n7.1\n13\n7.6\n7.7\n4.1\n47\n5.4\nMask R-CNN\n7.5\n4.7\n37\n5.8\n8.1\n6.9\n15\n7.5\n7.9\n4.3\n46\n5.6\nTOOD\n8.2\n5.0\n39\n6.2\n8.7\n7.0\n19\n7.7\n8.3\n4.8\n42\n6.1\nDINO\n11.6\n7.2\n38\n8.9\n11.5\n9.6\n16\n10.4\n11.8\n7.0\n40\n8.8\nGrounding DINO\n13.1\n8.8\n33\n10.5\n12.5\n11.0\n12\n11.7\n13.1\n9.3\n29\n10.9\nGLIP\n10.6\n8.0\n24\n9.1\n9.8\n9.2\n6\n9.5\n9.8\n6.8\n31\n8.0\nTable 6. Multi-source DG analysis of SOTA detectors on RWDS-CZ.\nof model performances and to mimic real-world conditions\nwhere hyperparameter optimisation may be impractical, we\nuse the default hyperparameters specified for each model.\n5. Results and Analyses\nIn the single-source experiment, we evaluate OOD perfor-\nmance by calculating the average performance across mod-\nels tested on OOD domains, while the ID performance is\nassessed on the test set of the ID source domain. In contrast,\nfor the multi-source setup, we calculate ID performance as\nthe average performance of all object detectors trained on\nsource domains, while OOD performance is evaluated on\nthe test set of the left-out OOD target domain.\n5.1. RWDS across Climate Zones (RWDS-CZ)\n5.1.1\nSingle-Source DG Experiment\nIn the single-source setup, we train all six detectors on the\nthree climate zones, namely, CZ A, CZ B and CZ C, indi-\nvidually. This results in a total of 18 trained object detec-\ntors. We then proceed to evaluate the performance of the\ntrained detectors on the different ID and target OOD test\nsets, yielding a total of an additional 54 inference experi-\nments. Table 5 summarises the performance of the detectors\non each climate zone.3\nWhen comparing the performance on OOD climate\nzones to the ID test sets, it can be observed from Table 5\n3The detectors\u2019 cross-domain results on RWDS-CZ under single-\nsource setup are in Supplementary B.1.1.\nthat all object detectors exhibit a significant performance\ndrop of above 35% for CZ A, 7% for CZ B and 40% for\nCZ C, highlighting the challenges posed by domain shift\nacross different climate zones and the limitations of current\nmodels in handling OOD data efficiently.\nWhile GLIP experiences the lowest drop between ID\nand OOD performance for all the climate zones, Ground-\ning DINO achieves the highest overall tradeoff, balancing\nboth ID and OOD performance most effectively. Moreover,\nhighlighted by H-scores, among the SOTA object detectors\nevaluated, Grounding DINO outperforms other detectors,\nboth in terms of ID and OOD performance. A plausible ex-\nplanation to this observation could be that Grounding DINO\nwas designed to generalise to unseen classes in an open-set\nsetting and such capabilities not only boost the performance\nin an open-set setting but also under a DG setting. The qual-\nitative performance across domains are analysed in Supple-\nmentary B.1.2.\n5.1.2\nMulti-Source DG Experiment\nSimilar to the single-source experiment, we evaluate the\nperformance of the trained detectors on the different ID and\nOOD test sets, yielding a total of an additional 54 experi-\nments. Table 6 summarises the performance of each detec-\ntor across the various combinations of the climate zones.4\nWhen comparing the performance of the object detec-\ntors trained on a single-source to those trained under the\n4The detectors\u2019 cross-domain results on RWDS-CZ under multi-source\nsetup are in Supplementary B.1.3.\n8304\n\n=== Page 7 ===\nTarget\nIndia\nUS\nMethods\nmAPID\nmAPOOD\nPD \u2193\nH \u2191\nmAPID\nmAPOOD\nPD \u2193\nH \u2191\nFaster R-CNN\n4.5\n1.3\n71\n2.0\n25.5\n1.8\n93\n3.4\nMask R-CNN\n4.3\n1.2\n72\n1.9\n25.9\n2.0\n92\n3.7\nTOOD\n5.1\n1.6\n69\n2.4\n27.6\n2.4\n91\n4.4\nDINO\n7.0\n2.2\n69\n3.3\n30.8\n4.3\n86\n7.5\nGrounding DINO\n6.7\n3.3\n51\n4.4\n31.3\n10.8\n65\n16.1\nGLIP\n6.7\n3.3\n51\n4.4\n30.7\n11.9\n61\n17.2\nTable 7. Single-source DG analysis of SOTA detectors on RWDS-FR.\nTarget\nFlorence\nMichael\nHarvey\nMatthew\nMethods\nmAPID mAPOOD PD \u2193H \u2191mAPID mAPOOD PD \u2193H \u2191mAPID mAPOOD PD \u2193H \u2191mAPID mAPOOD PD \u2193H \u2191\nFaster R-CNN\n34.5\n8.6\n75\n13.8\n18.6\n6.5\n65\n9.7\n25.1\n3.7\n85\n6.4\n1.5\n0.3\n78\n0.5\nMask R-CNN\n34.0\n8.3\n76\n13.3\n19.1\n6.9\n64\n10.1\n25.6\n3.7\n86\n6.4\n1.7\n0.4\n78\n0.6\nTOOD\n35.7\n10.4\n71\n16.1\n21.0\n7.1\n66\n10.6\n27.5\n4.4\n84\n7.5\n2.4\n0.5\n78\n0.9\nDINO\n36.5\n12.0\n67\n18.0\n20.6\n7.6\n63\n11.1\n31.4\n4.9\n84\n8.5\n2.5\n0.8\n69\n1.2\nGrounding DINO\n39.3\n17.4\n56\n24.2\n24.2\n9.3\n62\n13.4\n31.0\n7.7\n75 12.4\n3.3\n1.2\n65\n1.7\nGLIP\n40.8\n19.0\n53 25.9\n23.9\n10.2\n57 14.3\n29.2\n7.0\n76\n11.3\n3.7\n1.3\n64 2.0\nTable 8. Single-source DG analysis of SOTA detectors on RWDS-HE.\nmulti-source setup, it can be observed that training on mul-\ntiple source domains enhances not only the object detector\u2019s\nperformance on the ID test set but also provides a more sub-\nstantial boost to the OOD performance.\nFurthermore, consistent with the findings of the single-\nsource experiment, all object detectors evaluated under the\nmulti-source setting experience a notable performance drop\nwhen tested on OOD target test set compared to the ID test\nset. Moreover, across the object detectors benchmarked,\nGrounding DINO demonstrates the highest performance on\nthe ID and OOD test sets, as evidenced by its H-score.\n5.2. RWDS across Flooded Regions (RWDS-FR)\nSince RWDS-FR includes only India and the US as do-\nmains, this dataset inherently fits within a single-source set-\nting. We train a total of 12 object detectors and conduct a\ntotal of 24 experiments to evaluate their ID and OOD perfor-\nmances, respectively. Table 7 summarises the ID and OOD\nperformance of all the object detectors.5\nA notable decline in performance can be observed across\nboth India and the US on the OOD test sets with drops ex-\nceeding 52% and 62%, respectively, indicating a significant\nimpact of the domain-specific variations between the two\nregions. Grounding DINO and GLIP exhibit comparable\nperformance, both outperforming other object detectors in\nterms of ID and OOD performance. Their superior OOD\nperformance highlights their slight robustness in handling\ndomain shifts, making them more effective than the other\n5The detectors\u2019 cross-domain results on RWDS-FR are in Supplemen-\ntary B.2.1.\nobject detection models when the domain shift is defined\nin terms of disparate geographic regions. We present quali-\ntative performance analyses across domains in Supplemen-\ntary B.2.2.\n5.3. RWDS across Hurricane Events (RWDS-HE)\n5.3.1\nSingle-Source DG Experiment\nA significant decline in performance can be observed, in\nTable 8, between hurricane events, suggesting that varia-\ntions in the nature and characteristics of these events con-\ntribute to discrepancies in detection precision.6 This drop\nhighlights the challenge of generalising disaster object de-\ntection across different types of extreme weather events due\nto domain-specific variations, underscoring the need for fur-\nther research on developing more robust and generalisable\ndetectors to account for such discrepancies.\nConsistent\nwith\nthe\nRWDS-FR\nexperiment,\nboth\nGrounding DINO and GLIP achieve the highest perfor-\nmance, with GLIP performing slightly better and excelling\nin OOD detection compared to the other object detectors.\nIt is also evident that the object detectors have the weakest\nID performance when evaluated on the test set of Hurricane\nMatthew, which suggests that the underlying data is difficult\nand might suffer from factors such as label noise and class\nimbalance, as discussed in Section 3.2.2, influencing the\nperformance of the models negatively. Qualitative domain\nperformance analyses are provided in Supplementary B.2.2.\n6The detectors\u2019 cross-domain results on RWDS-HE under single-\nsource setup are in Supplementary B.3.1.\n8305\n\n=== Page 8 ===\nTarget\nFlorence\nMichael\nHarvey\nMatthew\nMethods\nmAPID mAPOOD PD \u2193H \u2191mAPID mAPOOD PD \u2193H \u2191mAPID mAPOOD PD \u2193H \u2191mAPID mAPOOD PD \u2193H \u2191\nFaster R-CNN\n32.8\n12.7\n61\n18.3\n19.0\n8.9\n53\n12.1\n25.0\n5.2\n79\n8.6\n1.7\n0.4\n76\n0.6\nMask R-CNN\n33.6\n13.3\n60\n19.1\n19.3\n9.1\n53\n12.4\n25.8\n5.4\n79\n8.9\n1.6\n0.7\n56\n1.0\nTOOD\n34.2\n14.0\n59\n19.9\n19.7\n9.6\n51\n12.9\n27.2\n5.3\n81\n8.9\n2.2\n0.5\n77\n0.8\nDINO\n37.3\n17.0\n54\n23.3\n21.4\n10.6\n50\n14.2\n31.3\n7.7\n75\n12.4\n2.8\n0.8\n71\n1.2\nGrounding DINO\n39.6\n28.2\n29\n32.9\n24.3\n12.8\n47 16.8 32.2\n9.4\n71 14.5\n3.1\n1.5\n52 2.0\nGLIP\n40.8\n30.7\n25 35.0 24.3\n11.4\n53\n15.5\n30.9\n7.8\n75\n12.5\n3.2\n1.1\n66\n1.6\nTable 9. Multi-source DG analysis of SOTA detectors on RWDS-HE.\n5.3.2\nMulti-Source DG Experiment\nIn our experimental results illustrated in Table 9, we observe\na slight performance drop in the ID performance of the ob-\nject detectors when comparing the multi-source setup to the\nsingle-source.7\nThis decline in ID performance suggests\nthat training on multiple source domains introduces addi-\ntional complexity, which can marginally reduce the model\u2019s\nability to generalise effectively to the ID domain. There-\nfore, the inclusion of multiple domains likely causes the\nmodel to adapt to a broader set of domain features, which\nmay dilute its focus on optimising the performance specifi-\ncally on the ID test set. While the multi-source setup gener-\nally enhances OOD performance, this comes at the cost of\nslightly decreased ID accuracy.\nHowever, a notable improvement in OOD performance is\nobserved across nearly all models and domains when com-\npared to the single-source setup. This improvement indi-\ncates that the model benefits significantly from exposure to\na diverse set of source domains, enhancing its ability to gen-\neralise to unseen, OOD domain. Furthermore, consistent\nwith the previous experiments, Grounding DINO and GLIP\nhad a comparably high ID and OOD performances in com-\nparison to the other evaluated object detectors.\n5.4. Error Analysis of Object Detectors\nWe analyse detection errors using the TIDE [2] toolbox, as\nshown in Figures 6. Evaluating on the OOD data of RWDS-\nCZ as a use-case, where the model is trained on CZ A and\ntested on CZ B, we find that classification errors are the\nmain factor for performance drop, followed by background\nerrors for both GLIP and Grounding DINO, and missed\nclassifications for Faster R-CNN. Moreover, Faster R-CNN\nhas the highest classification and missed groundtruth er-\nrors, which aligns with the findings in Section 5.1, where\nit consistently had the weakest performance. While GLIP\nmakes less errors than Faster R-CNN, its higher rate of\nbackground errors explains its weaker performance com-\npared to Grounding DINO. A similar trend appears when\nevaluating the performance of the model trained of CZ B\n7The detectors\u2019 cross-domain results on RWDS-HE under multi-source\nsetup are in Supplementary B.3.3.\nFigure 6. Object detection errors of detectors trained on CZ A and\nevaluated on ID (top-row) and OOD (bottom-row) data of CZ B.\nand evaluated on its ID test set, where Faster R-CNN and\nGrounding DINO exhibit higher classification errors, while\nGLIP has the highest background error rate.\n6. Conclusion\nObject detectors typically perform well under the assump-\ntion that training and evaluation data come from the same\ndistribution.\nHowever, real-world target distributions of-\nten differ, causing performance drops due to the distribution\nshift. DG aims to address this by enabling models to gen-\neralise to OOD data without access to target distributions\nduring training. This study examines the generalisability of\nSOTA object detectors under spatial domain shifts in real\nworld applications and introduces three novel DG bench-\nmark datasets focused on humanitarian and climate change\napplications. Supported by our findings under single-source\nand multi-source setups, these datasets, covering domain\nshifts across climate zones, regions and disaster events, are\nthe first to assess object detection in high-impact real-world\ncontexts and aim to provide valuable resources for evaluat-\ning future models\u2019 robustness and generalisation.\n8306\n\n=== Page 9 ===\nReferences\n[1] Fatih Cagatay Akyon, Sinan Onur Altinuc, and Alptekin\nTemizel. Slicing aided hyper inference and fine-tuning for\nsmall object detection. 2022 IEEE International Conference\non Image Processing (ICIP), pages 966\u2013970, 2022. 2, 5\n[2] Daniel Bolya, Sean Foley, James Hays, and Judy Hoffman.\nTide: A general toolbox for identifying object detection er-\nrors. In ECCV, 2020. 8\n[3] Tyson Brown. K\u00a8oppen Climate Classification System - Na-\ntional Geographic, 2024. 3\n[4] P\u00b4eter B\u00b4andi, Oscar Geessink, Quirine Manson, Marcory\nVan Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak\nEhteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng,\nAoxiao Zhong, Quanzheng Li, Farhad Ghazvinian Zanjani,\nSvitlana Zinger, Keisuke Fukuta, Daisuke Komura, Vlado\nOvtcharov, Shenghua Cheng, Shaoqun Zeng, Jeppe Tha-\ngaard, Anders B. Dahl, Huangjing Lin, Hao Chen, Lud-\nwig Jacobsson, Martin Hedlund, Melih C\u00b8 etin, Eren Hal\u0131c\u0131,\nHunter Jackson, Richard Chen, Fabian Both, J\u00a8org Franke,\nHeidi K\u00a8usters-Vandevelde, Willem Vreuls, Peter Bult, Bram\nvan Ginneken, Jeroen van der Laak, and Geert Litjens.\nFrom detection of individual metastases to classification of\nlymph node status at the patient level: The camelyon17 chal-\nlenge. IEEE Transactions on Medical Imaging, 38(2):550\u2013\n560, 2019. 1\n[5] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: High\nquality object detection and instance segmentation.\nIEEE\ntransactions on pattern analysis and machine intelligence,\n43(5):1483\u20131498, 2019. 2\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European confer-\nence on computer vision, pages 213\u2013229. Springer, 2020. 2\n[7] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-\nheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,\nJifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,\nChen Change Loy, and Dahua Lin.\nMMDetection: Open\nmmlab detection toolbox and benchmark.\narXiv preprint\narXiv:1906.07155, 2019. 5\n[8] Xingyu Chen, Xuguang Lan, Fuchun Sun, and Nanning\nZheng. A boundary based out-of-distribution classifier for\ngeneralized zero-shot learning. In European conference on\ncomputer vision, pages 572\u2013588. Springer, 2020. 5\n[9] J. Dai, K. He, and J. Sun. Convolutional feature masking\nfor joint object and stuff segmentation. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n3992\u20134000, Boston, MA, USA, 2015. IEEE. 2\n[10] Jian Ding, Nan Xue, Gui-Song Xia, Xiang Bai, Wen Yang,\nMichael Ying Yang, Serge Belongie, Jiebo Luo, Mihai\nDatcu, Marcello Pelillo, et al. Object detection in aerial im-\nages: A large-scale benchmark and challenges. IEEE trans-\nactions on pattern analysis and machine intelligence, 44(11):\n7778\u20137796, 2021. 2\n[11] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qing-\nming Huang, and Qi Tian. Centernet: Keypoint triplets for\nobject detection.\nIn Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 6569\u20136578,\n2019. 2\n[12] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable\nobject detection using deep neural networks. In IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n2155\u20132162, Columbus, OH, USA, 2014. IEEE. 2\n[13] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R. Scott,\nand Weilin Huang. TOOD: Task-aligned One-stage Object\nDetection. In 2021 IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 3490\u20133499, Montreal, QC,\nCanada, 2021. IEEE. 2, 5\n[14] Yanwei Fu, Xiaomei Wang, Hanze Dong, Yu-Gang Jiang,\nMeng Wang, Xiangyang Xue, and Leonid Sigal. Vocabulary-\ninformed zero-shot and open-set learning. IEEE transactions\non pattern analysis and machine intelligence, 42(12):3136\u2013\n3152, 2019. 5\n[15] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang,\nand David Balduzzi. Domain generalization for object recog-\nnition with multi-task autoencoders. In Proceedings of the\nIEEE international conference on computer vision, pages\n2551\u20132559, 2015. 2\n[16] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detec-\ntion and semantic segmentation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), Columbus, OH, USA, 2014. IEEE. 2\n[17] Ross B Girshick. Fast R-CNN. In International Conference\non Computer Vision, pages 1440\u20131448, Boston, MA, USA,\n2015. IEEE. 2\n[18] Ishaan Gulrajani and David Lopez-Paz. In search of lost do-\nmain generalization. International Conference on Learning\nRepresentations, 2021. 5\n[19] Abhishek Gupta, Alagan Anpalagan, Ling Guan, and\nAhmed Shaharyar Khwaja. Deep learning for object detec-\ntion and scene perception in self-driving cars: Survey, chal-\nlenges, and open issues. Array, 10:100057, 2021. 1\n[20] Ritwik Gupta, Richard Hosfelt, Sandra Sajeev, Nirav Patel,\nBryce Goodman, Jigar Doshi, Eric Heim, Howie Choset, and\nMatthew Gaston. xBD: A Dataset for Assessing Building\nDamage from Satellite Imagery, 2019.\narXiv:1911.09296\n[cs]. 4\n[21] Jiaming Han, Jian Ding, Nan Xue, and Gui-Song Xia. Redet:\nA rotation-equivariant detector for aerial object detection.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 2786\u20132795, Online,\n2021. IEEE. 2\n[22] Bharath Hariharan, Pablo Arbel\u00b4aez, Ross Girshick, and Ji-\ntendra Malik. Simultaneous detection and segmentation. In\nEuropean Conference on Computer Vision (ECCV), pages\n297\u2013312, Zurich, Switzerland, 2014. Springer. 2\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nSpatial pyramid pooling in deep convolutional networks for\nvisual recognition. IEEE transactions on pattern analysis\nand machine intelligence, 37(9):1904\u20131916, 2015. 2\n[24] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\n8307\n\n=== Page 10 ===\nconference on computer vision, pages 2961\u20132969, Venice,\nItaly, 2017. IEEE. 2, 5\n[25] Matthew Johnson-Roberson, Charles Barto, Rounak Mehta,\nSharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan.\nDriving in the matrix: Can virtual worlds replace human-\ngenerated annotations for real world tasks? In IEEE Confer-\nence on Robotics and Automation, 2017. 2\n[26] Pang\nWei\nKoh,\nShiori\nSagawa,\nHenrik\nMarklund,\nSang Michael Xie,\nMarvin Zhang,\nAkshay Balsubra-\nmani, Weihua Hu, Michihiro Yasunaga, Richard Lanas\nPhillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness,\nWei Guo, Berton Earnshaw, Imran Haque, Sara M Beery,\nJure Leskovec, Anshul Kundaje, Emma Pierson, Sergey\nLevine, Chelsea Finn, and Percy Liang. Wilds: A bench-\nmark of in-the-wild distribution shifts. In Proceedings of the\n38th International Conference on Machine Learning, pages\n5637\u20135664. PMLR, 2021. 2\n[27] Darius Lam, Richard Kuzma, Kevin McGee, Samuel Doo-\nley, Michael Laielli, Matthew Klaric, Yaroslav Bulatov, and\nBrendan McCord. xView: Objects in Context in Overhead\nImagery, 2018. arXiv:1802.07856 [cs]. 3\n[28] Hei Law and Jia Deng.\nCornernet: Detecting objects as\npaired keypoints. In Proceedings of the European confer-\nence on computer vision (ECCV), pages 734\u2013750, 2018. 2\n[29] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M\nHospedales. Deeper, broader and artier domain generaliza-\ntion. In Proceedings of the IEEE international conference on\ncomputer vision, pages 5542\u20135550, 2017. 2\n[30] Ke Li, Gang Wan, Gong Cheng, Liqiu Meng, and Junwei\nHan. Object detection in optical remote sensing images: A\nsurvey and a new benchmark. ISPRS journal of photogram-\nmetry and remote sensing, 159:296\u2013307, 2020. 2\n[31] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and\nJianfeng Gao.\nGrounded Language-Image Pre-training,\n2022. arXiv:2112.03857. 2, 5\n[32] Lin Lin, Chaoqing Tang, Qiuhua Liang, Zening Wu, Xin-\nling Wang, and Shan Zhao.\nRapid urban flood risk map-\nping for data-scarce environments using social sensing and\nregion-stable deep neural network. Journal of Hydrology,\n617:128758, 2023. 1\n[33] Tsung-Yi Lin, Piotr Doll\u00b4ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie.\nFeature pyra-\nmid networks for object detection.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 2117\u20132125, 2017. 2\n[34] Fan Liu, Delong Chen, Zhangqingyun Guan, Xiaocong\nZhou, Jiale Zhu, Qiaolin Ye, Liyong Fu, and Jun Zhou. Re-\nmoteclip: A vision language foundation model for remote\nsensing.\nIEEE Transactions on Geoscience and Remote\nSensing, 62:1\u201316, 2024. 3\n[35] Quande Liu, Qi Dou, and Pheng-Ann Heng. Shape-aware\nmeta-learning for generalizing prostate mri segmentation to\nunseen domains. In Medical Image Computing and Com-\nputer Assisted Intervention\u2013MICCAI 2020: 23rd Interna-\ntional Conference, Lima, Peru, October 4\u20138, 2020, Proceed-\nings, Part II 23, pages 475\u2013485. Springer, 2020. 1\n[36] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang,\nHang Su, Jun Zhu, and Lei Zhang. Grounding DINO: Marry-\ning DINO with Grounded Pre-Training for Open-Set Object\nDetection, 2024. arXiv:2303.05499. 2, 5\n[37] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\nSzegedy, Scott Reed, Cheng-Yang Fu, and Alexander C.\nBerg. Ssd: Single shot multibox detector. In European Con-\nference on Computer Vision (ECCV), pages 21\u201337, Amster-\ndam, The Netherlands, 2016. Springer. 2\n[38] Wenchao Liu, Long Ma, Jue Wang, et al. Detection of mul-\nticlass objects in optical remote sensing images. IEEE Geo-\nscience and Remote Sensing Letters, 16(5):791\u2013795, 2018.\n2\n[39] Yang Long, Yiping Gong, Zhifeng Xiao, and Qing Liu. Ac-\ncurate object localization in remote sensing images based on\nconvolutional neural networks. IEEE Transactions on Geo-\nscience and Remote Sensing, 55(5):2486\u20132498, 2017. 2\n[40] Yuchi Ma, Shuo Chen, Stefano Ermon, and David B. Lobell.\nTransfer learning in environmental remote sensing. Remote\nSensing of Environment, 301:113924, 2024. 1\n[41] Xiaofeng Mao, Yuefeng Chen, Yao Zhu, Da Chen, Hang\nSu, Rong Zhang, and Hui Xue. COCO-O: A Benchmark\nfor Object Detectors under Natural Distribution Shifts. In\n2023 IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 6316\u20136327, Paris, France, 2023. IEEE.\n2\n[42] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos,\nEvgenia Rusak, Oliver Bringmann, Alexander S Ecker,\nMatthias Bethge, and Wieland Brendel. Benchmarking ro-\nbustness in object detection: Autonomous driving when win-\nter is coming. arXiv preprint arXiv:1907.07484, 2019. 2\n[43] Daifeng Peng, Haiyan Guan, Yufu Zang, and Lorenzo Bruz-\nzone. Full-level domain adaptation for building extraction in\nvery-high-resolution optical remote-sensing images. IEEE\nTransactions on Geoscience and Remote Sensing, 60, 2022.\n1\n[44] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\nSaenko, and Bo Wang. Moment matching for multi-source\ndomain adaptation. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 1406\u20131415,\n2019. 2\n[45] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster,\nstronger. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7263\u20137271, 2017. 2\n[46] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\nFarhadi. You only look once: Unified, real-time object de-\ntection. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 779\u2013788, Las Vegas,\nNV, USA, 2016. IEEE. 2\n[47] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R-CNN: Towards Real-Time Object Detection with\nRegion Proposal Networks. In Advances in Neural Informa-\ntion Processing Systems. Curran Associates, Inc., 2015. 2,\n5\n[48] Shaoqing Ren, Kaiming He, Ross Girshick, Xiangyu Zhang,\nand Jian Sun. Object detection networks on convolutional\nfeature maps. arXiv:1504.06066 (v2), 2016. 2\n8308\n\n=== Page 11 ===\n[49] David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly\nKochanski,\nAlexandre\nLacoste,\nKris\nSankaran,\nAn-\ndrew Slavin Ross,\nNikola Milojevic-Dupont,\nNatasha\nJaques, Anna Waldman-Brown, Alexandra Sasha Luccioni,\nTegan Maharaj, Evan D. Sherwin, S. Karthik Mukkavilli,\nKonrad P. Kording, Carla P. Gomes, Andrew Y. Ng, Demis\nHassabis, John C. Platt, Felix Creutzig, Jennifer Chayes, and\nYoshua Bengio.\nTackling Climate Change with Machine\nLearning. ACM Computing Surveys, 55(2):1\u201396, 2023. 3\n[50] T-YLPG Ross and GKHP Doll\u00b4ar. Focal loss for dense ob-\nject detection.\nIn proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2980\u20132988,\n2017. 2\n[51] Rizwan Sadiq, Zainab Akhtar, Muhammad Imran, and Ferda\nOfli. Integrating remote sensing and social sensing for flood\nmapping. Remote Sensing Applications: Society and Envi-\nronment, 25:100697, 2022. 1\n[52] Pierre Sermanet, David Eigen, Xiang Zhang, Michael Math-\nieu, Rob Fergus, and Yann LeCun.\nOverfeat: Integrated\nrecognition, localization and detection using convolutional\nnetworks. In International Conference on Learning Repre-\nsentations (ICLR 2014), Banff, AB, Canada, 2014. CBLS.\n2\n[53] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen-\nfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan\nYuan, Changhu Wang, et al. Sparse r-cnn: End-to-end ob-\nject detection with learnable proposals. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 14454\u201314463, 2021. 2\n[54] Christian Szegedy, Alexander Toshev, and Dumitru Erhan.\nDeep neural networks for object detection.\nIn Advances\nin Neural Information Processing Systems 26, pages 2553\u2013\n2561, Lake Tahoe, NV, USA, 2013. Curran Associates, Inc.\n2\n[55] Christian Szegedy, Scott E. Reed, Dumitru Erhan, Dragomir\nAnguelov, and Sergey Ioffe. Scalable, high-quality object\ndetection. arXiv:1405.0312 (v3), 2015. 2\n[56] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet:\nScalable and efficient object detection. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10781\u201310790, 2020. 2\n[57] Z Tian, C Shen, H Chen, and T He. Fcos: Fully convolu-\ntional one-stage object detection. In Proceedings of the IEEE\ninternational conference on computer vision, 2019. 2\n[58] Laurens van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-sne. Journal of Machine Learning Research, 9\n(86):2579\u20132605, 2008. 3\n[59] Riccardo Volpi and Vittorio Murino. Addressing Model Vul-\nnerability to Distributional Shifts Over Image Transforma-\ntion Sets. In 2019 IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 7979\u20137988, Seoul, Korea\n(South), 2019. IEEE. 1\n[60] Li-Hua Wen and Kang-Hyun Jo. Deep learning-based per-\nception systems for autonomous driving: A comprehensive\nsurvey. Neurocomputing, 2022. 1\n[61] Wikipedia. K\u00a8oppen climate classification, 2024. Page Ver-\nsion ID: 1256836310. 1, 3\n[62] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan,\nZicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A gener-\native region-to-text transformer for object understanding. In\nEuropean Conference on Computer Vision, pages 207\u2013224.\nSpringer, 2024. 2\n[63] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-\nlongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-\npei Zhang. Dota: A large-scale dataset for object detection\nin aerial images.\nIn Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 3974\u2013\n3983, Salt Lake City, UT, USA, 2018. IEEE. 2\n[64] Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot\nlearning-the good, the bad and the ugly. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 4582\u20134591, 2017. 5\n[65] Tao Xu, Xian Sun, Wenhui Diao, Liangjin Zhao, Kun Fu, and\nHongqi Wang. Fada: Feature aligned domain adaptive object\ndetection in remote sensing imagery. IEEE Transactions on\nGeoscience and Remote Sensing, 60:1\u201316, 2022. 1\n[66] Xue Yang, Jirui Yang, Junchi Yan, Yue Zhang, Tengfei\nZhang, Zhi Guo, Xian Sun, and Kun Fu. Scrdet: Towards\nmore robust detection for small, cluttered and rotated ob-\njects. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 8232\u20138241, Long Beach,\nCA, USA, 2019. IEEE. 2\n[67] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su,\nJun Zhu, Lionel M. Ni, and Heung-Yeung Shum. DINO:\nDETR with Improved DeNoising Anchor Boxes for End-to-\nEnd Object Detection, 2022. arXiv:2203.03605 [cs]. 2, 5\n[68] Bingchen Zhao, Shaozuo Yu, Wufei Ma, Mingxin Yu,\nShenxiao Mei, Angtian Wang, Ju He, Alan Yuille, and Adam\nKortylewski. Ood-cv: A benchmark for robustness to out-of-\ndistribution shifts of individual nuisances in natural images.\nIn European conference on computer vision, pages 163\u2013180.\nSpringer, 2022. 2\n[69] Bingchen Zhao, Jiahao Wang, Wufei Ma, Artur Jesslen, Si-\nwei Yang, Shaozuo Yu, Oliver Zendel, Christian Theobalt,\nAlan Yuille, and Adam Kortylewski.\nOod-cv-v2: An ex-\ntended benchmark for robustness to out-of-distribution shifts\nof individual nuisances in natural images. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 2024. 2\n[70] Xingyi Zhou, Jiacheng Zhuo, and Philipp Krahenbuhl.\nBottom-up object detection by grouping extreme and center\npoints. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 850\u2013859, 2019.\n2\n[71] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. In International Conference\non Learning Representations, 2021. 2\n[72] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and\nJieping Ye. Object Detection in 20 Years: A Survey. Pro-\nceedings of the IEEE, 111(3):257\u2013276, 2023. Conference\nName: Proceedings of the IEEE. 5\n8309\n",
      "searchable_content": "page 1 benchmarking object detectors under real-world distribution shifts in satellite imagery sara a. al-emadi1 2 yin yang2 ferda ofli1 1 qatar computing research institute hbku 2 college of science and engineering hbku salemadi yyang fofli hbku.edu.qa abstract object detectors have achieved remarkable performance in many applications however these deep learning models are typically designed under the i.i.d. assumption meaning they are trained and evaluated on data sampled from the same source distribution. in real-world deployment how- ever target distributions often differ from source data lead- ing to substantial performance degradation. domain gen- eralisation dg seeks to bridge this gap by enabling mod- els to generalise to out-of-distribution ood data with- out access to target distributions during training enhancing robustness to unseen conditions. in this work we examine the generalisability and robustness of state-of-the-art ob- ject detectors under real-world distribution shifts focusing particularly on spatial domain shifts. despite the need a standardised benchmark dataset specifically designed for assessing object detection under realistic dg scenarios is currently lacking. to address this we introduce real- world distribution shifts rwds a suite of three novel dg benchmarking datasets that focus on humanitarian and cli- mate change applications. these datasets enable the inves- tigation of domain shifts across i climate zones and ii various disasters and geographic regions. to our knowl- edge these are the first dg benchmarking datasets tailored for object detection in real-world high-impact contexts. we aim for these datasets to serve as valuable resources for evaluating the robustness and generalisation of future ob- ject detection models. our datasets and code are available at https github.com rwgai rwds. 1. introduction deep learning has achieved remarkable success in various applications including flood mapping 32 51 medical di- agnostics 4 35 and self-driving cars 19 60 . however these machine learning models are typically developed un- der the i.i.d. assumption where they are trained and eval- uated on data samples drawn from the same source distri- bution. consequently when deployed in real-world envi- figure 1. example images from different climate zones ronments with differing target distributions these models experience significant performance degradation hindering their large-scale deployment. this phenomenon is known as distribution or domain shift. in this paper we focus a spe- cific type of domain shift referred to as spatial domain shift i.e. covariate shift on a global scale which is driven by vi- sual variations in land cover and built structures influenced by factors such as natural landscapes climate zones archi- tectural styles economic and financial development social and cultural attributes and human settlement patterns. in satellite imagery-based object detection spatial do- main shift poses a significant challenge especially when en- vironmental conditions vary unpredictably as demonstrated in figure 1. to examine this we use the k oppen climate classification system 61 and consider a scenario where an object detector is trained on images from a tropical cli- mate zone but evaluated on out-of-distribution ood tar- get domains specifically dry and temperate climate zones. since these target domains exhibit distinct visual character- istics compared to the source domain a performance drop is expected highlighting the impact of spatial domain shift when applying object detection models across diverse cli- matic contexts. several studies have sought to mitigate the issue of do- main shift through data augmentation 59 transfer learn- ing 40 and domain adaptation in which the model has ac- cess to unlabelled samples from the target distribution dur- ing training 43 65 . however in real-world applications models often encounter distributions that cannot be fore- this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 8299 page 2 seen before deployment. to tackle this challenge recent research has focused on domain shift under this constraint a problem known as domain generalisation dg . dg datasets are essential for assessing a model s ability to generalise to unseen target distributions. in image classi- fication a substantial body of research have been devoted to curating dg datasets for broad use cases such as pacs 29 and domainnet 44 as well as datasets introducing syn- thetic domain shifts like rotatedmnist 15 or real-world distribution shifts as seen in wilds 26 . however the study of domain shift in object detection remains relatively underexplored. to bridge this gap mao et al. introduced coco-o 41 a dg benchmark for object detection with six domains including sketch weather cartoon painting tattoo and handmade to evaluate both in-domain id and ood performance. while the domain shifts in coco-o are evident such as the differences between sketches and paintings further investigation of the practical motivation for training a model on sketches and testing it on paintings could provide valuable insights. to our knowledge a dg benchmark does not currently exist for evaluating the be- haviours of object detectors on ood test data in a common real-world application setting. motivated by this we introduce real-world distribution shifts rwds a suite of three realistic dg benchmark- ing datasets namely rwds-cz rwds-fr and rwds- he which focus on humanitarian and climate change ap- plications and investigate domain shifts across i climate zones and ii different disasters and geographic regions respectively. moreover we benchmark and analyse the per- formance of several state-of-the-art sota object detec- tion algorithms on rwds under two setups single-source where an object detector is trained on only one source do- main and multi-source where training incorporates multi- ple source domains. we then evaluate these models on the unseen target domains to provide comprehensive insights into their generalisation performance. we trained around 100 object detector models and conducted over 200 experi- ments. our contributions are summarised as follows we propose rwds a suite of novel realistic and chal- lenging dg datasets designed to evaluate spatial domain shifts in real-world object detection tasks. we provide the community with in-depth benchmarking analyses on the performance of the sota object detectors on rwds datasets. we analyse the impact of single-source versus multi- source training in dg for spatial domain shifts in satellite imagery concluding that multi-source training enhances generalisability of object detectors. the rest of the paper is organised as follows section 2 reviews literature on object detection and robustness bench- marks. section 3 introduces the rwds datasets with details on data cleaning and preprocessing. section 4 describes the evaluation metrics experimental setups and selected ob- ject detectors. section 5 presents the results and provides a comprehensive analysis and section 6 concludes the paper. 2. related work 2.1. object detection early attempts in deep-learning-based object detection used a set of bounding boxes and masked regions as input to the cnn architecture to incorporate shape information into the classification process to perform object localisa- tion 12 16 54 55 . later on end-to-end techniques were proposed based on shared computation of convolu- tions for simultaneous detection and localization of the ob- jects 9 17 22 24 37 46 48 52 . these methods can be generally divided into two categories one-stage de- tectors 11 13 28 37 45 46 50 56 57 70 and two- stage detectors 5 16 17 23 24 33 47 53 . more recently transformer-based object detection models have proved more efficient and accurate thanks to their ability to not require anchor boxes and non-maximum suppression procedure 6 67 71 . besides with the advances in foun- dation models large vision models or vision-language mod- els open-set and open-world object detection has become popular 31 36 62 . following these trends remote sens- ing community has also integrated deep learning-based ob- ject detection models into their research 1 10 21 30 38 39 66 . however accurate object detection from satellite imagery at scale remains a challenging task. 2.2. robustness benchmarks various benchmark studies have been developed to assess the robustness of object detection models under distribution shifts. for instance coco-c 42 evaluates model per- formance by applying synthetic corruptions such as jpeg compression and gaussian noise to the coco test set. similarly ood-cv 68 and its extended version ood- cv-v2 69 include ood examples across 10 object cate- gories from pascal voc and imagenet spanning vari- ations in pose shape texture context and weather con- ditions. these datasets enable benchmarking across mul- tiple tasks like image classification object detection and 3d pose estimation. coco-o 41 introduces natural dis- tribution shifts in coco-based datasets spanning six do- mains weather painting handmade cartoon tattoo and sketch. their study has shown that there is a significant per- formance gap of 55.7 between id and ood performance highlighting the domain generalisation challenges under such shifts. however despite their contributions these datasets still lack the complexity of real-world distribution shifts. more realistic benchmarks include those reflecting environmental changes in autonomous driving 25 and ob- ject variations in aerial imagery 63 which better capture 8300 page 3 the dynamic and unpredictable conditions faced in practi- cal applications. however they remain limited in scope as they do not comprehensively account for geographic and temporal variability environmental and weather conditions occlusion clutter and object appearance changes within a unified framework. in contrast our rwds datasets aim to bridge this gap by providing a diverse and realistic evalua- tion setting that encapsulates these real-world domain shifts more holistically. 3. our rwds datasets 3.1. rwds across climate zones while there are increasing efforts to mitigate and reduce the negative and potential impact of climate change on the global ecosystem including natural resources weather and the natural landscapes 49 there is a need to develop ro- bust models to support computer vision tasks under these circumstances more particularly object detection task. in order to investigate their robustness and generalisability across different climate zones we propose rwds across climate zones rwds-cz dataset where we focus on k oppen s climate zone classification 3 61 . given the scarcity of global satellite imagery that covers all climate zones we use the raw satellite imagery from the xview dataset 27 an open-source object detection dataset fea- turing high-resolution 0.3m images captured at a global scale across 60 object classes. for this study we focus on three distinct climate zones zone a cz a tropical or equatorial zone b cz b arid or dry and zone c cz c warm mild temperate. these serve as our distinct domains for studying spatial domain shifts in satellite-based object detection. to create the domains we first map the geo-coordinates of each image to its respective climate zone and proceed with splitting the overall dataset into domains. however this results in a mismatch between the classes available across the domains. to resolve this we retain only those classes that appear in all climate zones. additionally we set a threshold of 30 samples per class to ensure sufficient data for training. any class with fewer than 30 samples is excluded from all domains. this process yields a total of 16 classes. to maintain consistent distribution of ob- ject instances across the training validation and test sets within each domain we follow the procedure outlined in algorithm 1. this process is repeated for each domain re- sulting in the final rwds-cz dataset. table 1 summarises the dataset statistics while figure 2 shows the distribution of training samples across classes in all domains. to visualize the domain shift in rwds-cz we extract image embeddings using remoteclip 34 and project them into 2d using t-sne 58 . figure 3-a showcases the shift between images from cz a and cz b. algorithm 1 dataset split procedure 1 input set of images i class labels c 2 initialise training set t validation set v testing set s 3 function allocimages i n 4 for each class c c do 5 i argmax i i count i c select image with most instances of c 6 n n i append to designated set 7 i i i remove allocated image 8 end for 9 return n i return final dataset splits 10 end function 11 while i do 12 for i 1 to 3 do repeat 3 times for training set 13 t i allocimages i t update training set 14 end for 15 v i allocimages i v update validation set 16 s i allocimages i s update test set 17 end while split cz a cz b cz c training 117 265 43 272 124 717 validation 58 997 13 423 47 362 test 56 954 24 745 60 310 table 1. rwds-cz overall object instances per partition 3.2. rwds in disaster damage assessment a notable consequence of climate change is the increasing frequency and severity of natural disasters such as hurri- canes storms floods wildfires earthquakes tsunamis etc. damage assessment is essential during and after disasters to support aid delivery guide building reconstruction ef- forts and provide governments and humanitarian agencies with an estimate of the disaster s impact. generally large amount of satellite imagery is captured around the disaster- hit locations. however given the sheer volume of data the cleaning preprocessing and re-training of object detec- tors for each disaster on the spot is time consuming and might not be feasible due to lack of annotations highlight- ing the crucial need for robust models that can generalise well to unseen distributions beyond those they were trained on. hence we investigate the robustness of sota object detectors in this application under two different scenarios. in the first use-case we examine the shift of the same disaster type across distant geographic regions with differ- ent socioeconomical characteristics. specifically we de- fine domains in terms of collection of events that caused floods across the united states and india respectively. we refer to this use-case as rwds across flooded regions rwds-fr . whereas in the second use-case we focus on understanding the shift in the behaviour of these models 8301 page 4 a climate zone a b climate zone b c climate zone c figure 2. class-wise distribution of training data for each domain as well as the overall data distribution across domains in rwds-cz figure 3. embedding space representations of the rwds datasets across different disaster events of the same type namely hurricanes in north america. we refer to this use-case as rwds across hurricane events rwds-he . similar to the discussion related to the scarcity of open-source satel- lite imagery we utilise the raw satellite images released in the xdb building damage assessment dataset 20 for both rwds-fr and rwds-he. 3.2.1 rwds across flooded regions rwds-fr we start by creating the metadata for the raw images. xdb dataset provides disaster event damage type and polygons of buildings for segmentation application. however given that we are interested in object detection we convert poly- gons of buildings into bounding boxes. furthermore simi- lar to section 3.1 we map the latitude and longitude coor- dinates of the polygons to find the corresponding location of each object instance in terms of country region conti- nent etc. we then extract the flooded objects in india and us. figure 4 shows example images illustrating the shift be- tween the domains with a close-up visualization of image embeddings in figure 3-b. unlike the original data where the instances are categorised into four classes namely no damage minor damage major damage and destroyed when extracting the flooded instances in the us and india we ob- serve a class imbalance between those classes. therefore we transform the task into a binary categorisation leaving figure 4. comparison of flood scenes between the us and india india us split d nd d nd training 5 023 14 841 10 680 20 055 validation 2 532 8 320 5 470 9 834 test 2 802 8 064 5 452 10 034 table 2. rwds-fr object instances per partition. us with two classes namely damaged d and no dam- age nd . we then follow the same logic as that discussed in section 3.1 to create the training validation and testing splits. this yields the rwds-fr dataset. table 2 represents the resulting domain and class distributions per split. 3.2.2 rwds across hurricane events rwds-he in contrast to rwds-fr dataset where domains are de- fined by diverse geographic regions this dataset focuses on hurricane events across north america which are ge- ographically closer in proximity. as a result the dataset consists of four hurricane events as domains florence michael harvey and matthew as shown in figure 5. fig- ure 3-c presents the image embeddings for hurricanes flo- rence and matthew as an example. we adhere to the pre- processing metadata creation binary class categorisation and data splitting procedures outlined in section 3.2.1. this 8302 page 5 figure 5. comparison of hurricane scenes from different events florence michael harvey matthew split d nd d nd d nd d nd training 1 102 4 196 6 132 11 347 9 270 9 223 8 919 1 938 validation 578 2 112 3 075 5 455 4 670 4 594 4 910 1 042 test 582 2 158 3 229 5 890 4 796 4 821 4 743 1 078 table 3. rwds-he object instances per partition. defines the rwds-he dataset. table 3 presents the final per-class and per-split distribution for each domain. 4. experiments 4.1. single-source and multi-source setup we investigate dg in two setups. the first involves training an object detector on a single id source training set then assessing its performance on both the held-out ood target domains and the id test set. this setup reflects scenarios with a limited diversity of data distributions. whereas in the second setup we incorporate training an object detec- tor on a collection of source domains mirroring real-world scenarios where data from a variety of distributions may be available. for quantitative comparison we evaluate the trained object detector on each ood target domain sepa- rately as well as on the average performance across id do- mains.1 4.2. dg evaluation metrics methods for evaluating dg models remain an active and open area of investigation. researchers have however adapted existing approaches to assess the performance of deep learning models on ood datasets for classification tasks. among these the leave-one-domain-out evaluation strategy 18 is widely regarded. in this setup one domain is excluded from training enabling it to serve as an inde- pendent test domain to rigorously evaluate model perfor- mance without any additional tuning. inspired by this we adapt this evaluation technique for object detection under the single- and multi-source setups. we assess the performance of the object detectors using the standard mean average precision map metric which is commonly used in object detection applications 72 . more 1the single- and multi-source setups are formally defined in supple- mentary a. object detector backbone faster r-cnn 47 resnext-101-64x4d mask r-cnn 24 resnext-101-64x4d and fpn tood 13 resnext-101-64x4d dcnv2 and fpn dino 5scale 67 swin-l grounding dino 36 bert and swin-b glip l 31 bert and swin-l table 4. object detectors and their backbone architectures. specifically we use the ms-coco ap metric which is cal- culated as the average over multiple iou thresholds ranging from 0.50 to 0.95 with a stepsize of 0.05.2 performance drop pd . a metric frequently used in the dg community for assessing the generalisability of clas- sification tasks is the performance drop which quantifies the percentage of performance degradation observed in the model when subjected to ood data from a target domain. drawing inspiration from this approach we apply it in the context of dg for object detection. this is formulated as follows pd 100 mapid mapood mapid 1 where mapid and mapood represent the average map of the combination of detectors tested on a specific domain s id and ood test sets respectively. harmonic mean h . to compare the id and ood perfor- mance of object detectors based on their map we adopt the widely recognised harmonic mean as another evaluation metric. this choice is motivated by its use in recent gen- eralised open-set zero-shot learning studies 8 14 64 to compute a joint score reflecting model performance across in-domain and out-of-domain test sets. this is defined as h 2 mapood mapid mapood mapid 2 4.3. object detectors and hyperparameters we conduct all the experiments using the mmdetection toolbox 7 . we selected object detectors across clas- sical faster r-cnn 47 mask r-cnn 24 recent dino 67 tood 13 and foundation model-based ap- proaches grounding dino 36 glip 31 . table 4 presents the top-performing backbone architecture selected for each detector as evaluated on standard object detection datasets by mmdetection. to train the object detectors we perform preprocessing to unify the image sizes of the raw images. we start by cropping all the images into 512 512 tiles with an overlap- ping ratio of 0.2 using sahi 1 while preserving the orig- inal resolution of the images. to ensure a fair comparison 2map50 map75 results are included in supplementary b. 8303 page 6 target cz a cz b cz c methods mapid mapood pd h mapid mapood pd h mapid mapood pd h faster r-cnn 7.2 3.9 47 5.0 7.5 6.0 20 6.7 7.7 3.4 56 4.7 mask r-cnn 7.3 3.7 49 4.9 7.7 5.8 25 6.6 7.8 3.5 55 4.8 tood 7.8 4.0 49 5.2 7.8 6.1 22 6.8 8.2 4.0 52 5.3 dino 11.0 5.6 49 7.4 9.6 8.0 17 8.7 11.0 5.6 49 7.4 grounding dino 12.9 7.5 42 9.5 10.8 10.0 7 10.4 13.1 7.1 46 9.2 glip 9.8 6.3 36 7.6 8.8 8.2 7 8.5 9.2 5.4 41 6.8 table 5. single-source dg analysis of sota detectors on rwds-cz. target cz a cz b cz c methods mapid mapood pd h mapid mapood pd h mapid mapood pd h faster r-cnn 7.7 4.9 36 6.0 8.2 7.1 13 7.6 7.7 4.1 47 5.4 mask r-cnn 7.5 4.7 37 5.8 8.1 6.9 15 7.5 7.9 4.3 46 5.6 tood 8.2 5.0 39 6.2 8.7 7.0 19 7.7 8.3 4.8 42 6.1 dino 11.6 7.2 38 8.9 11.5 9.6 16 10.4 11.8 7.0 40 8.8 grounding dino 13.1 8.8 33 10.5 12.5 11.0 12 11.7 13.1 9.3 29 10.9 glip 10.6 8.0 24 9.1 9.8 9.2 6 9.5 9.8 6.8 31 8.0 table 6. multi-source dg analysis of sota detectors on rwds-cz. of model performances and to mimic real-world conditions where hyperparameter optimisation may be impractical we use the default hyperparameters specified for each model. 5. results and analyses in the single-source experiment we evaluate ood perfor- mance by calculating the average performance across mod- els tested on ood domains while the id performance is assessed on the test set of the id source domain. in contrast for the multi-source setup we calculate id performance as the average performance of all object detectors trained on source domains while ood performance is evaluated on the test set of the left-out ood target domain. 5.1. rwds across climate zones rwds-cz 5.1.1 single-source dg experiment in the single-source setup we train all six detectors on the three climate zones namely cz a cz b and cz c indi- vidually. this results in a total of 18 trained object detec- tors. we then proceed to evaluate the performance of the trained detectors on the different id and target ood test sets yielding a total of an additional 54 inference experi- ments. table 5 summarises the performance of the detectors on each climate zone.3 when comparing the performance on ood climate zones to the id test sets it can be observed from table 5 3the detectors cross-domain results on rwds-cz under single- source setup are in supplementary b.1.1. that all object detectors exhibit a significant performance drop of above 35 for cz a 7 for cz b and 40 for cz c highlighting the challenges posed by domain shift across different climate zones and the limitations of current models in handling ood data efficiently. while glip experiences the lowest drop between id and ood performance for all the climate zones ground- ing dino achieves the highest overall tradeoff balancing both id and ood performance most effectively. moreover highlighted by h-scores among the sota object detectors evaluated grounding dino outperforms other detectors both in terms of id and ood performance. a plausible ex- planation to this observation could be that grounding dino was designed to generalise to unseen classes in an open-set setting and such capabilities not only boost the performance in an open-set setting but also under a dg setting. the qual- itative performance across domains are analysed in supple- mentary b.1.2. 5.1.2 multi-source dg experiment similar to the single-source experiment we evaluate the performance of the trained detectors on the different id and ood test sets yielding a total of an additional 54 experi- ments. table 6 summarises the performance of each detec- tor across the various combinations of the climate zones.4 when comparing the performance of the object detec- tors trained on a single-source to those trained under the 4the detectors cross-domain results on rwds-cz under multi-source setup are in supplementary b.1.3. 8304 page 7 target india us methods mapid mapood pd h mapid mapood pd h faster r-cnn 4.5 1.3 71 2.0 25.5 1.8 93 3.4 mask r-cnn 4.3 1.2 72 1.9 25.9 2.0 92 3.7 tood 5.1 1.6 69 2.4 27.6 2.4 91 4.4 dino 7.0 2.2 69 3.3 30.8 4.3 86 7.5 grounding dino 6.7 3.3 51 4.4 31.3 10.8 65 16.1 glip 6.7 3.3 51 4.4 30.7 11.9 61 17.2 table 7. single-source dg analysis of sota detectors on rwds-fr. target florence michael harvey matthew methods mapid mapood pd h mapid mapood pd h mapid mapood pd h mapid mapood pd h faster r-cnn 34.5 8.6 75 13.8 18.6 6.5 65 9.7 25.1 3.7 85 6.4 1.5 0.3 78 0.5 mask r-cnn 34.0 8.3 76 13.3 19.1 6.9 64 10.1 25.6 3.7 86 6.4 1.7 0.4 78 0.6 tood 35.7 10.4 71 16.1 21.0 7.1 66 10.6 27.5 4.4 84 7.5 2.4 0.5 78 0.9 dino 36.5 12.0 67 18.0 20.6 7.6 63 11.1 31.4 4.9 84 8.5 2.5 0.8 69 1.2 grounding dino 39.3 17.4 56 24.2 24.2 9.3 62 13.4 31.0 7.7 75 12.4 3.3 1.2 65 1.7 glip 40.8 19.0 53 25.9 23.9 10.2 57 14.3 29.2 7.0 76 11.3 3.7 1.3 64 2.0 table 8. single-source dg analysis of sota detectors on rwds-he. multi-source setup it can be observed that training on mul- tiple source domains enhances not only the object detector s performance on the id test set but also provides a more sub- stantial boost to the ood performance. furthermore consistent with the findings of the single- source experiment all object detectors evaluated under the multi-source setting experience a notable performance drop when tested on ood target test set compared to the id test set. moreover across the object detectors benchmarked grounding dino demonstrates the highest performance on the id and ood test sets as evidenced by its h-score. 5.2. rwds across flooded regions rwds-fr since rwds-fr includes only india and the us as do- mains this dataset inherently fits within a single-source set- ting. we train a total of 12 object detectors and conduct a total of 24 experiments to evaluate their id and ood perfor- mances respectively. table 7 summarises the id and ood performance of all the object detectors.5 a notable decline in performance can be observed across both india and the us on the ood test sets with drops ex- ceeding 52 and 62 respectively indicating a significant impact of the domain-specific variations between the two regions. grounding dino and glip exhibit comparable performance both outperforming other object detectors in terms of id and ood performance. their superior ood performance highlights their slight robustness in handling domain shifts making them more effective than the other 5the detectors cross-domain results on rwds-fr are in supplemen- tary b.2.1. object detection models when the domain shift is defined in terms of disparate geographic regions. we present quali- tative performance analyses across domains in supplemen- tary b.2.2. 5.3. rwds across hurricane events rwds-he 5.3.1 single-source dg experiment a significant decline in performance can be observed in table 8 between hurricane events suggesting that varia- tions in the nature and characteristics of these events con- tribute to discrepancies in detection precision.6 this drop highlights the challenge of generalising disaster object de- tection across different types of extreme weather events due to domain-specific variations underscoring the need for fur- ther research on developing more robust and generalisable detectors to account for such discrepancies. consistent with the rwds-fr experiment both grounding dino and glip achieve the highest perfor- mance with glip performing slightly better and excelling in ood detection compared to the other object detectors. it is also evident that the object detectors have the weakest id performance when evaluated on the test set of hurricane matthew which suggests that the underlying data is difficult and might suffer from factors such as label noise and class imbalance as discussed in section 3.2.2 influencing the performance of the models negatively. qualitative domain performance analyses are provided in supplementary b.2.2. 6the detectors cross-domain results on rwds-he under single- source setup are in supplementary b.3.1. 8305 page 8 target florence michael harvey matthew methods mapid mapood pd h mapid mapood pd h mapid mapood pd h mapid mapood pd h faster r-cnn 32.8 12.7 61 18.3 19.0 8.9 53 12.1 25.0 5.2 79 8.6 1.7 0.4 76 0.6 mask r-cnn 33.6 13.3 60 19.1 19.3 9.1 53 12.4 25.8 5.4 79 8.9 1.6 0.7 56 1.0 tood 34.2 14.0 59 19.9 19.7 9.6 51 12.9 27.2 5.3 81 8.9 2.2 0.5 77 0.8 dino 37.3 17.0 54 23.3 21.4 10.6 50 14.2 31.3 7.7 75 12.4 2.8 0.8 71 1.2 grounding dino 39.6 28.2 29 32.9 24.3 12.8 47 16.8 32.2 9.4 71 14.5 3.1 1.5 52 2.0 glip 40.8 30.7 25 35.0 24.3 11.4 53 15.5 30.9 7.8 75 12.5 3.2 1.1 66 1.6 table 9. multi-source dg analysis of sota detectors on rwds-he. 5.3.2 multi-source dg experiment in our experimental results illustrated in table 9 we observe a slight performance drop in the id performance of the ob- ject detectors when comparing the multi-source setup to the single-source.7 this decline in id performance suggests that training on multiple source domains introduces addi- tional complexity which can marginally reduce the model s ability to generalise effectively to the id domain. there- fore the inclusion of multiple domains likely causes the model to adapt to a broader set of domain features which may dilute its focus on optimising the performance specifi- cally on the id test set. while the multi-source setup gener- ally enhances ood performance this comes at the cost of slightly decreased id accuracy. however a notable improvement in ood performance is observed across nearly all models and domains when com- pared to the single-source setup. this improvement indi- cates that the model benefits significantly from exposure to a diverse set of source domains enhancing its ability to gen- eralise to unseen ood domain. furthermore consistent with the previous experiments grounding dino and glip had a comparably high id and ood performances in com- parison to the other evaluated object detectors. 5.4. error analysis of object detectors we analyse detection errors using the tide 2 toolbox as shown in figures 6. evaluating on the ood data of rwds- cz as a use-case where the model is trained on cz a and tested on cz b we find that classification errors are the main factor for performance drop followed by background errors for both glip and grounding dino and missed classifications for faster r-cnn. moreover faster r-cnn has the highest classification and missed groundtruth er- rors which aligns with the findings in section 5.1 where it consistently had the weakest performance. while glip makes less errors than faster r-cnn its higher rate of background errors explains its weaker performance com- pared to grounding dino. a similar trend appears when evaluating the performance of the model trained of cz b 7the detectors cross-domain results on rwds-he under multi-source setup are in supplementary b.3.3. figure 6. object detection errors of detectors trained on cz a and evaluated on id top-row and ood bottom-row data of cz b. and evaluated on its id test set where faster r-cnn and grounding dino exhibit higher classification errors while glip has the highest background error rate. 6. conclusion object detectors typically perform well under the assump- tion that training and evaluation data come from the same distribution. however real-world target distributions of- ten differ causing performance drops due to the distribution shift. dg aims to address this by enabling models to gen- eralise to ood data without access to target distributions during training. this study examines the generalisability of sota object detectors under spatial domain shifts in real world applications and introduces three novel dg bench- mark datasets focused on humanitarian and climate change applications. supported by our findings under single-source and multi-source setups these datasets covering domain shifts across climate zones regions and disaster events are the first to assess object detection in high-impact real-world contexts and aim to provide valuable resources for evaluat- ing future models robustness and generalisation. 8306 page 9 references 1 fatih cagatay akyon sinan onur altinuc and alptekin temizel. slicing aided hyper inference and fine-tuning for small object detection. 2022 ieee international conference on image processing icip pages 966 970 2022. 2 5 2 daniel bolya sean foley james hays and judy hoffman. tide a general toolbox for identifying object detection er- rors. in eccv 2020. 8 3 tyson brown. k oppen climate classification system - na- tional geographic 2024. 3 4 p eter b andi oscar geessink quirine manson marcory van dijk maschenka balkenhol meyke hermsen babak ehteshami bejnordi byungjae lee kyunghyun paeng aoxiao zhong quanzheng li farhad ghazvinian zanjani svitlana zinger keisuke fukuta daisuke komura vlado ovtcharov shenghua cheng shaoqun zeng jeppe tha- gaard anders b. dahl huangjing lin hao chen lud- wig jacobsson martin hedlund melih c etin eren hal\u0131c\u0131 hunter jackson richard chen fabian both j org franke heidi k usters-vandevelde willem vreuls peter bult bram van ginneken jeroen van der laak and geert litjens. from detection of individual metastases to classification of lymph node status at the patient level the camelyon17 chal- lenge. ieee transactions on medical imaging 38 2 550 560 2019. 1 5 zhaowei cai and nuno vasconcelos. cascade r-cnn high quality object detection and instance segmentation. ieee transactions on pattern analysis and machine intelligence 43 5 1483 1498 2019. 2 6 nicolas carion francisco massa gabriel synnaeve nicolas usunier alexander kirillov and sergey zagoruyko. end-to- end object detection with transformers. in european confer- ence on computer vision pages 213 229. springer 2020. 2 7 kai chen jiaqi wang jiangmiao pang yuhang cao yu xiong xiaoxiao li shuyang sun wansen feng ziwei liu jiarui xu zheng zhang dazhi cheng chenchen zhu tian- heng cheng qijie zhao buyu li xin lu rui zhu yue wu jifeng dai jingdong wang jianping shi wanli ouyang chen change loy and dahua lin. mmdetection open mmlab detection toolbox and benchmark. arxiv preprint arxiv 1906.07155 2019. 5 8 xingyu chen xuguang lan fuchun sun and nanning zheng. a boundary based out-of-distribution classifier for generalized zero-shot learning. in european conference on computer vision pages 572 588. springer 2020. 5 9 j. dai k. he and j. sun. convolutional feature masking for joint object and stuff segmentation. in ieee conference on computer vision and pattern recognition cvpr pages 3992 4000 boston ma usa 2015. ieee. 2 10 jian ding nan xue gui-song xia xiang bai wen yang michael ying yang serge belongie jiebo luo mihai datcu marcello pelillo et al. object detection in aerial im- ages a large-scale benchmark and challenges. ieee trans- actions on pattern analysis and machine intelligence 44 11 7778 7796 2021. 2 11 kaiwen duan song bai lingxi xie honggang qi qing- ming huang and qi tian. centernet keypoint triplets for object detection. in proceedings of the ieee cvf inter- national conference on computer vision pages 6569 6578 2019. 2 12 d. erhan c. szegedy a. toshev and d. anguelov. scalable object detection using deep neural networks. in ieee con- ference on computer vision and pattern recognition pages 2155 2162 columbus oh usa 2014. ieee. 2 13 chengjian feng yujie zhong yu gao matthew r. scott and weilin huang. tood task-aligned one-stage object detection. in 2021 ieee cvf international conference on computer vision iccv pages 3490 3499 montreal qc canada 2021. ieee. 2 5 14 yanwei fu xiaomei wang hanze dong yu-gang jiang meng wang xiangyang xue and leonid sigal. vocabulary- informed zero-shot and open-set learning. ieee transactions on pattern analysis and machine intelligence 42 12 3136 3152 2019. 5 15 muhammad ghifary w bastiaan kleijn mengjie zhang and david balduzzi. domain generalization for object recog- nition with multi-task autoencoders. in proceedings of the ieee international conference on computer vision pages 2551 2559 2015. 2 16 ross girshick jeff donahue trevor darrell and jitendra malik. rich feature hierarchies for accurate object detec- tion and semantic segmentation. in proceedings of the ieee conference on computer vision and pattern recognition cvpr columbus oh usa 2014. ieee. 2 17 ross b girshick. fast r-cnn. in international conference on computer vision pages 1440 1448 boston ma usa 2015. ieee. 2 18 ishaan gulrajani and david lopez-paz. in search of lost do- main generalization. international conference on learning representations 2021. 5 19 abhishek gupta alagan anpalagan ling guan and ahmed shaharyar khwaja. deep learning for object detec- tion and scene perception in self-driving cars survey chal- lenges and open issues. array 10 100057 2021. 1 20 ritwik gupta richard hosfelt sandra sajeev nirav patel bryce goodman jigar doshi eric heim howie choset and matthew gaston. xbd a dataset for assessing building damage from satellite imagery 2019. arxiv 1911.09296 cs . 4 21 jiaming han jian ding nan xue and gui-song xia. redet a rotation-equivariant detector for aerial object detection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 2786 2795 online 2021. ieee. 2 22 bharath hariharan pablo arbel aez ross girshick and ji- tendra malik. simultaneous detection and segmentation. in european conference on computer vision eccv pages 297 312 zurich switzerland 2014. springer. 2 23 kaiming he xiangyu zhang shaoqing ren and jian sun. spatial pyramid pooling in deep convolutional networks for visual recognition. ieee transactions on pattern analysis and machine intelligence 37 9 1904 1916 2015. 2 24 kaiming he georgia gkioxari piotr doll ar and ross gir- shick. mask r-cnn. in proceedings of the ieee international 8307 page 10 conference on computer vision pages 2961 2969 venice italy 2017. ieee. 2 5 25 matthew johnson-roberson charles barto rounak mehta sharath nittur sridhar karl rosaen and ram vasudevan. driving in the matrix can virtual worlds replace human- generated annotations for real world tasks in ieee confer- ence on robotics and automation 2017. 2 26 pang wei koh shiori sagawa henrik marklund sang michael xie marvin zhang akshay balsubra- mani weihua hu michihiro yasunaga richard lanas phillips irena gao tony lee etienne david ian stavness wei guo berton earnshaw imran haque sara m beery jure leskovec anshul kundaje emma pierson sergey levine chelsea finn and percy liang. wilds a bench- mark of in-the-wild distribution shifts. in proceedings of the 38th international conference on machine learning pages 5637 5664. pmlr 2021. 2 27 darius lam richard kuzma kevin mcgee samuel doo- ley michael laielli matthew klaric yaroslav bulatov and brendan mccord. xview objects in context in overhead imagery 2018. arxiv 1802.07856 cs . 3 28 hei law and jia deng. cornernet detecting objects as paired keypoints. in proceedings of the european confer- ence on computer vision eccv pages 734 750 2018. 2 29 da li yongxin yang yi-zhe song and timothy m hospedales. deeper broader and artier domain generaliza- tion. in proceedings of the ieee international conference on computer vision pages 5542 5550 2017. 2 30 ke li gang wan gong cheng liqiu meng and junwei han. object detection in optical remote sensing images a survey and a new benchmark. isprs journal of photogram- metry and remote sensing 159 296 307 2020. 2 31 liunian harold li pengchuan zhang haotian zhang jian- wei yang chunyuan li yiwu zhong lijuan wang lu yuan lei zhang jenq-neng hwang kai-wei chang and jianfeng gao. grounded language-image pre-training 2022. arxiv 2112.03857. 2 5 32 lin lin chaoqing tang qiuhua liang zening wu xin- ling wang and shan zhao. rapid urban flood risk map- ping for data-scarce environments using social sensing and region-stable deep neural network. journal of hydrology 617 128758 2023. 1 33 tsung-yi lin piotr doll ar ross girshick kaiming he bharath hariharan and serge belongie. feature pyra- mid networks for object detection. in proceedings of the ieee conference on computer vision and pattern recogni- tion pages 2117 2125 2017. 2 34 fan liu delong chen zhangqingyun guan xiaocong zhou jiale zhu qiaolin ye liyong fu and jun zhou. re- moteclip a vision language foundation model for remote sensing. ieee transactions on geoscience and remote sensing 62 1 16 2024. 3 35 quande liu qi dou and pheng-ann heng. shape-aware meta-learning for generalizing prostate mri segmentation to unseen domains. in medical image computing and com- puter assisted intervention miccai 2020 23rd interna- tional conference lima peru october 4 8 2020 proceed- ings part ii 23 pages 475 485. springer 2020. 1 36 shilong liu zhaoyang zeng tianhe ren feng li hao zhang jie yang qing jiang chunyuan li jianwei yang hang su jun zhu and lei zhang. grounding dino marry- ing dino with grounded pre-training for open-set object detection 2024. arxiv 2303.05499. 2 5 37 wei liu dragomir anguelov dumitru erhan christian szegedy scott reed cheng-yang fu and alexander c. berg. ssd single shot multibox detector. in european con- ference on computer vision eccv pages 21 37 amster- dam the netherlands 2016. springer. 2 38 wenchao liu long ma jue wang et al. detection of mul- ticlass objects in optical remote sensing images. ieee geo- science and remote sensing letters 16 5 791 795 2018. 2 39 yang long yiping gong zhifeng xiao and qing liu. ac- curate object localization in remote sensing images based on convolutional neural networks. ieee transactions on geo- science and remote sensing 55 5 2486 2498 2017. 2 40 yuchi ma shuo chen stefano ermon and david b. lobell. transfer learning in environmental remote sensing. remote sensing of environment 301 113924 2024. 1 41 xiaofeng mao yuefeng chen yao zhu da chen hang su rong zhang and hui xue. coco-o a benchmark for object detectors under natural distribution shifts. in 2023 ieee cvf international conference on computer vi- sion iccv pages 6316 6327 paris france 2023. ieee. 2 42 claudio michaelis benjamin mitzkus robert geirhos evgenia rusak oliver bringmann alexander s ecker matthias bethge and wieland brendel. benchmarking ro- bustness in object detection autonomous driving when win- ter is coming. arxiv preprint arxiv 1907.07484 2019. 2 43 daifeng peng haiyan guan yufu zang and lorenzo bruz- zone. full-level domain adaptation for building extraction in very-high-resolution optical remote-sensing images. ieee transactions on geoscience and remote sensing 60 2022. 1 44 xingchao peng qinxun bai xide xia zijun huang kate saenko and bo wang. moment matching for multi-source domain adaptation. in proceedings of the ieee cvf inter- national conference on computer vision pages 1406 1415 2019. 2 45 joseph redmon and ali farhadi. yolo9000 better faster stronger. in proceedings of the ieee conference on computer vision and pattern recognition pages 7263 7271 2017. 2 46 joseph redmon santosh divvala ross girshick and ali farhadi. you only look once unified real-time object de- tection. in proceedings of the ieee conference on computer vision and pattern recognition pages 779 788 las vegas nv usa 2016. ieee. 2 47 shaoqing ren kaiming he ross girshick and jian sun. faster r-cnn towards real-time object detection with region proposal networks. in advances in neural informa- tion processing systems. curran associates inc. 2015. 2 5 48 shaoqing ren kaiming he ross girshick xiangyu zhang and jian sun. object detection networks on convolutional feature maps. arxiv 1504.06066 v2 2016. 2 8308 page 11 49 david rolnick priya l. donti lynn h. kaack kelly kochanski alexandre lacoste kris sankaran an- drew slavin ross nikola milojevic-dupont natasha jaques anna waldman-brown alexandra sasha luccioni tegan maharaj evan d. sherwin s. karthik mukkavilli konrad p. kording carla p. gomes andrew y. ng demis hassabis john c. platt felix creutzig jennifer chayes and yoshua bengio. tackling climate change with machine learning. acm computing surveys 55 2 1 96 2023. 3 50 t-ylpg ross and gkhp doll ar. focal loss for dense ob- ject detection. in proceedings of the ieee conference on computer vision and pattern recognition pages 2980 2988 2017. 2 51 rizwan sadiq zainab akhtar muhammad imran and ferda ofli. integrating remote sensing and social sensing for flood mapping. remote sensing applications society and envi- ronment 25 100697 2022. 1 52 pierre sermanet david eigen xiang zhang michael math- ieu rob fergus and yann lecun. overfeat integrated recognition localization and detection using convolutional networks. in international conference on learning repre- sentations iclr 2014 banff ab canada 2014. cbls. 2 53 peize sun rufeng zhang yi jiang tao kong chen- feng xu wei zhan masayoshi tomizuka lei li zehuan yuan changhu wang et al. sparse r-cnn end-to-end ob- ject detection with learnable proposals. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 14454 14463 2021. 2 54 christian szegedy alexander toshev and dumitru erhan. deep neural networks for object detection. in advances in neural information processing systems 26 pages 2553 2561 lake tahoe nv usa 2013. curran associates inc. 2 55 christian szegedy scott e. reed dumitru erhan dragomir anguelov and sergey ioffe. scalable high-quality object detection. arxiv 1405.0312 v3 2015. 2 56 mingxing tan ruoming pang and quoc v le. efficientdet scalable and efficient object detection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 10781 10790 2020. 2 57 z tian c shen h chen and t he. fcos fully convolu- tional one-stage object detection. in proceedings of the ieee international conference on computer vision 2019. 2 58 laurens van der maaten and geoffrey hinton. visualizing data using t-sne. journal of machine learning research 9 86 2579 2605 2008. 3 59 riccardo volpi and vittorio murino. addressing model vul- nerability to distributional shifts over image transforma- tion sets. in 2019 ieee cvf international conference on computer vision iccv pages 7979 7988 seoul korea south 2019. ieee. 1 60 li-hua wen and kang-hyun jo. deep learning-based per- ception systems for autonomous driving a comprehensive survey. neurocomputing 2022. 1 61 wikipedia. k oppen climate classification 2024. page ver- sion id 1256836310. 1 3 62 jialian wu jianfeng wang zhengyuan yang zhe gan zicheng liu junsong yuan and lijuan wang. grit a gener- ative region-to-text transformer for object understanding. in european conference on computer vision pages 207 224. springer 2024. 2 63 gui-song xia xiang bai jian ding zhen zhu serge be- longie jiebo luo mihai datcu marcello pelillo and liang- pei zhang. dota a large-scale dataset for object detection in aerial images. in proceedings of the ieee conference on computer vision and pattern recognition pages 3974 3983 salt lake city ut usa 2018. ieee. 2 64 yongqin xian bernt schiele and zeynep akata. zero-shot learning-the good the bad and the ugly. in proceedings of the ieee conference on computer vision and pattern recog- nition pages 4582 4591 2017. 5 65 tao xu xian sun wenhui diao liangjin zhao kun fu and hongqi wang. fada feature aligned domain adaptive object detection in remote sensing imagery. ieee transactions on geoscience and remote sensing 60 1 16 2022. 1 66 xue yang jirui yang junchi yan yue zhang tengfei zhang zhi guo xian sun and kun fu. scrdet towards more robust detection for small cluttered and rotated ob- jects. in proceedings of the ieee cvf international con- ference on computer vision pages 8232 8241 long beach ca usa 2019. ieee. 2 67 hao zhang feng li shilong liu lei zhang hang su jun zhu lionel m. ni and heung-yeung shum. dino detr with improved denoising anchor boxes for end-to- end object detection 2022. arxiv 2203.03605 cs . 2 5 68 bingchen zhao shaozuo yu wufei ma mingxin yu shenxiao mei angtian wang ju he alan yuille and adam kortylewski. ood-cv a benchmark for robustness to out-of- distribution shifts of individual nuisances in natural images. in european conference on computer vision pages 163 180. springer 2022. 2 69 bingchen zhao jiahao wang wufei ma artur jesslen si- wei yang shaozuo yu oliver zendel christian theobalt alan yuille and adam kortylewski. ood-cv-v2 an ex- tended benchmark for robustness to out-of-distribution shifts of individual nuisances in natural images. ieee transactions on pattern analysis and machine intelligence 2024. 2 70 xingyi zhou jiacheng zhuo and philipp krahenbuhl. bottom-up object detection by grouping extreme and center points. in proceedings of the ieee cvf conference on com- puter vision and pattern recognition pages 850 859 2019. 2 71 xizhou zhu weijie su lewei lu bin li xiaogang wang and jifeng dai. deformable detr deformable transformers for end-to-end object detection. in international conference on learning representations 2021. 2 72 zhengxia zou keyan chen zhenwei shi yuhong guo and jieping ye. object detection in 20 years a survey. pro- ceedings of the ieee 111 3 257 276 2023. conference name proceedings of the ieee. 5 8309",
      "keywords": [
        "page",
        "benchmarking",
        "object",
        "detectors",
        "under",
        "real-world",
        "distribution",
        "shifts",
        "satellite",
        "imagery",
        "sara",
        "al-emadi1",
        "yin",
        "yang2",
        "ferda",
        "ofli1",
        "qatar",
        "computing",
        "research",
        "institute",
        "hbku",
        "college",
        "science",
        "engineering",
        "salemadi",
        "yyang",
        "fofli",
        "hbku.edu.qa",
        "abstract",
        "achieved",
        "remarkable",
        "performance",
        "many",
        "applications",
        "however",
        "deep",
        "learning",
        "models",
        "typically",
        "designed",
        "i.i.d.",
        "assumption",
        "meaning",
        "trained",
        "evaluated",
        "data",
        "sampled",
        "from",
        "same",
        "source",
        "distribution.",
        "deployment",
        "how-",
        "ever",
        "target",
        "distributions",
        "often",
        "differ",
        "lead-",
        "ing",
        "substantial",
        "degradation.",
        "domain",
        "gen-",
        "eralisation",
        "seeks",
        "bridge",
        "gap",
        "enabling",
        "mod-",
        "els",
        "generalise",
        "out-of-distribution",
        "ood",
        "with-",
        "out",
        "access",
        "during",
        "training",
        "enhancing",
        "robustness",
        "unseen",
        "conditions.",
        "work",
        "examine",
        "generalisability",
        "state-of-the-art",
        "ob-",
        "ject",
        "focusing",
        "particularly",
        "spatial",
        "shifts.",
        "despite",
        "need",
        "standardised",
        "benchmark",
        "dataset",
        "specifically",
        "assessing"
      ],
      "metadata": {
        "filename": "Al-Emadi_Benchmarking_Object_Detectors_under_Real-World_Distribution_Shifts_in_Satellite_Imagery_CVPR_2025_paper.pdf",
        "original_filename": "Al-Emadi_Benchmarking_Object_Detectors_under_Real-World_Distribution_Shifts_in_Satellite_Imagery_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 4620890,
        "upload_file_id": "upload_1752160776629_ccxhzz5od",
        "upload_timestamp": "2025-07-10T15:19:36.690836",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:19:37.029253+00:00",
      "content_length": 52246,
      "content_type": ".pdf",
      "filename": "Al-Emadi_Benchmarking_Object_Detectors_under_Real-World_Distribution_Shifts_in_Satellite_Imagery_CVPR_2025_paper.pdf"
    },
    "12": {
      "file_id": "12",
      "content": "\n=== Page 1 ===\nInterDyn: Controllable Interactive Dynamics with Video Diffusion Models\nRick Akkerman1,2\u2192\nHaiwen Feng1\u2192\u2020\nMichael J. Black1\nDimitrios Tzionas2\nVictoria Fern\u00b4andez Abrevaya1\n1Max Planck Institute for Intelligent Systems, T\u00a8ubingen, Germany\n2University of Amsterdam, the Netherlands\n{rick.akkerman, haiwen.feng, black, victoria.abrevaya}@tuebingen.mpg.de\nd.tzionas@uva.nl\nInput image\nVideo generation by InterDyn using only the hand mask sequence as control signal\nForce propagation\nCounterfactual dynamics\nFuture #1\nFuture #2\nDenotes a driving object motion |         Tracks indicate an object with generated uncontrolled dynamics\nt=3\nt=13\nt=0\nt=0\nFigure 1. We present InterDyn, a framework for synthesizing realistic interactive dynamics without 3D reconstruction and physics simu-\nlation. Our core principle is to rely on the implicit physics knowledge embedded in large-scale video generative models. Given an image\nand a \u201cdriving motion\u201d, our model generates the consequential scene dynamics. We investigate the generated interactive dynamics in a\nsimple object collision scenario (bottom) and complex in-the-wild human-object interaction (top).\nAbstract\nPredicting the dynamics of interacting objects is essential\nfor both humans and intelligent systems. However, existing\napproaches are limited to simpli\ufb01ed, toy settings and\nlack generalizability to complex, real-world environments.\nRecent advances in generative models have enabled the\nprediction of state transitions based on interventions, but\nfocus on generating a single future state which neglects\nthe continuous dynamics resulting from the interaction. To\naddress this gap, we propose InterDyn, a novel framework\nthat generates videos of interactive dynamics given an\ninitial frame and a control signal encoding the motion of a\ndriving object or actor. Our key insight is that large video\n*Equal contribution\n\u2020Project lead\ngeneration models can act as both neural renderers and\nimplicit physics \u201csimulators\u201d, having learned interactive\ndynamics from large-scale video data.\nTo effectively\nharness this capability, we introduce an interactive control\nmechanism that conditions the video generation process\non the motion of the driving entity.\nQualitative results\ndemonstrate that InterDyn generates plausible, tempo-\nrally consistent videos of complex object interactions\nwhile generalizing to unseen objects.\nQuantitative eval-\nuations show that InterDyn outperforms baselines that\nfocus on static state transitions.\nThis work highlights\nthe potential of leveraging video generative models as\nimplicit physics engines. Code and trained models will be\nreleased at: https://interdyn.is.tue.mpg.de/.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n12467\n\n=== Page 2 ===\n1. Introduction\nHumans have the remarkable ability to predict the future\ndynamics of observed systems intuitively. With just a sin-\ngle image, we can anticipate and imagine how objects will\nmove over time \u2013 not only their motion but also their in-\nteractions with the environment and other elements in the\nscene. Inferring this requires an advanced form of scene-\nlevel reasoning beyond merely recognizing the semantics\nand geometry of static elements; it involves a deep physical\nand causal understanding of how each object will interact\ngiven the environment, object properties, and forces.\nThere has been a growing interest in developing machine\nlearning systems that emulate similar levels of dynamic un-\nderstanding given visual observations, such as images or\nvideos. Early work [84] addressed this by \ufb01rst reconstruct-\ning a 3D representation from the image, then predicting fu-\nture states with a physics simulator and \ufb01nally generating\nthe output video with a rendering engine. This relies heav-\nily on explicit reconstruction and simulation, which is com-\nputationally intensive, prone to errors, and may not gener-\nalize well. More recent methods [2, 24, 39, 46, 49] leverage\nkeypoint or latent representations within graph relational\nframeworks; however, they have only been trained and val-\nidated in over-simpli\ufb01ed, synthetic environments, showing\nlimited generalizability to complex real-world scenarios.\nInstead, the advent of powerful generative models [1, 5,\n16, 54, 66] opens new avenues for synthesizing interactions\nunder complex scenarios. For example, Sudhakar et al. [70]\nrecently proposed CosHand, a controllable image-to-image\nmodel based on Stable Diffusion [66] that infers state tran-\nsitions of an object. The task here is de\ufb01ned as follows:\ngiven an image of a hand interacting with an object, along-\nside a hand mask of the current frame and a mask of the\nhand at a future frame, generate a modi\ufb01ed input image that\nsatis\ufb01es the mask, with realistic interactions. The challenge,\nas in early intuitive physics works, lies in accurately mod-\neling how the objects will change after forces are applied.\nHowever, we argue that static state transitions are insuf\ufb01-\ncient for this task, as they fail to capture the continuous dy-\nnamic processes inherent to the problem, e.g. see Fig. 2. In-\nvestigating interactive dynamics within a two-state setting\nis highly limiting, since dynamics can extend beyond the\nperiod of direct contact \u2013 for example, predicting the mo-\ntion occurring while a person pours water requires a physi-\ncal understanding that goes beyond the state of the hand at\na future frame. The driving force, in this case the hand, may\ninteract with the system only brie\ufb02y, but the system\u2019s sub-\nsequent dynamics continue according to physical laws and\nmay even in\ufb02uence other parts via force propagation.\nIn this paper, we explore controllable synthesis of inter-\nactive dynamics\u2013generating a video from an input image\nand a dynamic control signal (e.g. a moving hand mask) to\nmodel realistic object dynamics. In particular, we propose\nCosHand\nOurs\nFigure 2.\nState transition vs. dynamics.\nMethods that gen-\nerate static state transitions (i.e. predict a future image) such as\nCosHand [70] struggle to capture the inherent dynamic processes\ninvolved in human-object interactions. Here, we show a video se-\nquence where the motion continues beyond the interaction.\nInterDyn, a novel framework for synthesizing controllable\ndynamic interactions that leverages the physical and dy-\nnamics \u201cknowledge\u201d of a large video generation model [5].\nUnlike prior approaches that rely on explicit physical simu-\nlation [84] or are constrained to static state transitions [70],\nwe leverage video generation models to generate dynamic\nprocesses implicitly, see Fig. 1.\nSpeci\ufb01cally, we extend\nStable Video Diffusion (SVD) [5] with a dynamic control\nbranch and \ufb01ne-tune it on diverse scenes, enabling synthe-\nsis of complex interactions aligned with the control signal.\nWe start our investigation by \ufb01ne-tuning InterDyn on a\nsimple synthetic scenario of cubes, cylinders, and spheres:\nthe CLEVRER dataset [97]. To control the motion we add a\nmask driving signal that manipulates the movement of some\n(but not all) of the objects in the scene. We then evalu-\nate how the synthesized trajectories of uncontrolled objects\nchange under various interactions, including multiple ob-\njects colliding with each other. This multi-object collision\nsetting allows us to \u201cprobe\u201d the physical understanding and\ncausal effects of the video diffusion model, and our qualita-\ntive experiments show InterDyn\u2019s ability for counterfactual\nfuture prediction and physical force propagation.\nNext, we evaluate how the system performs in a dif\ufb01-\ncult real-world scenario, such as Human-Object Interaction\n(HOI). Here, the dexterity of hand motions and the diver-\nsity of objects vastly increase the complexity of the prob-\nlem. We \ufb01ne-tune the model on a commonly used HOI\nvideo dataset [21] and compare with the state-of-the-art\nbaseline CosHand [70], as well as two text-control based\ninteractive dynamics generation methods: Seer [23] and\nDynamiCrafter [89]. We quantify our investigations using\nstandard image and video metrics, as well as a motion \ufb01-\ndelity metric based on point tracking. InterDyn surpasses\nthe previous SOTA over 37.5% on LPIPS and 77% on FVD\non the Something-Something-v2 (SSV2) dataset [21]. Our\nexperiments also demonstrate diverse and physically plausi-\nble generations of interactive dynamics, probing into SVD\u2019s\n\u201cunderstanding\u201d of physics and dynamics.\n12468\n\n=== Page 3 ===\nIn summary, we present InterDyn, a framework that em-\nploys video generative models to simulate object dynam-\nics without explicit 3D reconstruction or physical simu-\nlation.\nWe demonstrate how the inherent \u201cknowledge\u201d\nwithin video generation models can be leveraged to predict\ncomplex object interactions and movements over time, im-\nplicitly modeling physical and causal dynamics. We per-\nform comprehensive experiments on multi-object collision\ndatasets and hand-object manipulation datasets, demon-\nstrating the effectiveness of our approach.\n2. Related Work\nModeling human-object interactions (HOI).\nHuman-\nobject interaction has been widely studied within the con-\ntext of 3D reconstruction [17, 18, 27, 28, 73, 88, 94],\nwhere the goal is to recover realistic geometry of hands\nand objects. The \ufb01eld of 3D HOI synthesis has also re-\nceived increasing attention, including the generation of\nstatic [40, 44, 71, 106] or dynamic [63, 72, 103, 105]\nhand poses conditioned on 3D objects, whole-body inter-\nactions [90], or more recently, hand-object meshes given\ntextual descriptions [10, 15, 57, 86, 96]. Few works address\nHOI synthesis in the 2D domain. GANHand [14] predicts\n3D hand shape and pose given an RGB image of an object,\nwhile AffordanceDiffusion [95] estimates a 2D hand using\na diffusion model. Kulal et al. [47] take as input an image\nof a human and a scene separately and generate a compos-\nite image that positions the human with correct affordances.\nAlso relevant is HOIDiffusion [101], in which a texture-less\nrendering of a 3D hand and object is converted to a real-\nistic image using a text description. Most closely related\nto us is CosHand [70], which takes as input an RGB im-\nage of a hand-object interaction, a hand mask at the current\nstate, and the hand mask of the future state, and generates an\nRGB image of the future state. Unlike us, they cannot gen-\nerate post-interaction object dynamics. Importantly, none\nof these works study dynamics, generating instead discrete\nstate transitions that fail to capture the nuanced, temporally\ncoherent behaviors observed in interactions.\nSynthesizing causal physical relations from visual input.\nA growing body of work aims to model and predict physical\ncausal effects from visual inputs such as images or videos.\nFor example, research in intuitive physics seeks to replicate\nthe human-like, non-mathematical understanding of physi-\ncal events, e.g. by predicting future frames given an input\nvideo. Early works like [22, 48] train neural networks to as-\nsess the stability of block towers, while [24] leverage prior\nphysical knowledge formalized through partial differential\nequations (PDEs). Other approaches investigate counterfac-\ntual reasoning by leveraging graph neural networks [2, 39].\nWu et al. [82\u201384] explore the use of an inverse rendering\napproach, extracting geometry and physical properties from\nthe video which are then coupled with a physics simulator\nand a rendering engine to generate the future frames. Other\nworks [81] incorporate Interaction Networks [3] to approx-\nimate physical systems from video data. These approaches\nare often limited to simpli\ufb01ed, synthetic datasets and strug-\ngle to generalize to real-world scenarios.\nRecent methods have started to combine language mod-\nels with physical engines. Liu et al. [51] ground a large lan-\nguage model using a computational physics engine while\nGao et al. [19] show that \ufb01ne-tuning a vision-language\nmodel (VLM) on annotated datasets of physical concepts\nimproves its understanding of physical interactions. Closely\nrelated to our work is PhysGen [53], which trains an image-\nto-video model that conditions the video generation on\nphysics parameters (e.g., force or torque). However, the\nmodel relies on a dynamics simulator to generate motion,\nand its application is limited to rigid objects. A related but\ntangential line of work focuses on identifying and gener-\nating the effects of objects on their surroundings. For ex-\nample, Omnimatte [56] introduces the problem of identi-\nfying all parts of a scene in\ufb02uenced by an object, given a\nvideo and a mask of the object. Similarly, Lu et al. [55] pro-\npose to re-time the motion of different subjects in a scene\nwhile maintaining realistic interactions with the environ-\nment. ActAnywhere [62] generates videos with plausible\nhuman-scene interactions, taking a masked video of a per-\nson and a background image as input. These works address\nthe problem of synthesizing realistic interactions within a\nscene, however, lack \ufb01ne-grained control.\nControllable video generation. Video generation has ad-\nvanced signi\ufb01cantly in recent years, with diffusion models\nleading to substantial improvements in unconditional [33,\n99], text-based [1, 5, 6, 12, 20, 23, 26, 32, 37, 68, 76,\n85, 89, 92, 104] and image-based [1, 5, 20, 25, 78, 89]\ngeneration.\nThese advances have raised the question of\nhow to incorporate more nuanced control into video gen-\neration. Some text-to-video approaches are trained by \u201cin-\n\ufb02ating\u201d text-to-image (T2V) models [8, 13, 25, 26, 38, 85],\nand can thus be integrated with conditional T2V mod-\nels such as ControlNet [100] or T2V-Adapter [60]. Con-\ntrol can also be achieved by conditioning on trajecto-\nries [58, 87, 98] or bounding-boxes [77], \ufb01ne-tuning on ap-\npropriate datasets. VideoComposer [78] incorporates multi-\nple condition types, including text, depth, style, and tempo-\nral conditions via motion vectors. Camera motion control\nhas also been explored, with AnimateDiff [79] employing\nLoRA [34] modules to control camera movement, while\nMotionCtrl [80] and CameraCtrl [29] directly embed the\ncamera information for more precise control. Additionally,\nseveral works target human animation from a pose con-\ntrol signal, such as DreamPose [42], MagicPose [91], and\nAnimateAnyone [35], but do not account for interactions.\n12469\n\n=== Page 4 ===\n3. Controllable Interactive Dynamics\nVideo diffusion models such as [5, 54] have demonstrated\nimpressive performance in generating videos from text or\nimages, and have even shown potential in tasks that re-\nquire 3D understanding when properly \ufb01ne-tuned [36, 75].\nTrained on millions of videos, we hypothesize that these\nmodels also possess implicit knowledge of complex inter-\nactive dynamics, such as those that appear when humans\ninteract with objects. Out of the box, however, they lack a\nprecise control mechanism, often relying solely on textual\ninputs or requiring careful selection of the starting frame.\nTask. Given an input image, x \u2192R1\u2191H\u2191W \u21913, and a driv-\ning motion in the form of a pixel-wise corresponding control\nsignal c \u2192RN\u2191H\u2191W \u21913, we task InterDyn with generating\na video sequence, y \u2192RN\u2191H\u2191W \u21913, depicting plausible\nobject dynamics. Through this task, we aim to learn the\nconditional distribution between a driving motion, such as\nthat of a human hand, and the consequent motion of manip-\nulated objects. In other words, the model needs to synthe-\nsize plausible object movement and appearance without any\nindication other than the driving motion, while maintaining\nphysical and visual consistency with the input image.\nStable Video Diffusion. We extend Stable Video Diffu-\nsion [5] (SVD) to enable controllable interactive dynam-\nics and explore the versatility of this model across a range\nof scenarios. SVD is a publicly available U-Net-based la-\ntent diffusion model [66] that extends Stable Diffusion 2.1\nto video generation by interleaving the network with tem-\nporal layers. Given a static input image of a scene, SVD\ndenoises a sequence of N frames y \u2192RN\u2191H\u2191W \u21913 to gen-\nerate a video that follows the initial frame. The input image\nis fed into the denoising U-Net by concatenating its latent\nto each of the frames in the noised input, and by supplying\nits CLIP [64] embedding to the U-Net\u2019s cross-attention lay-\ners. In addition, SVD is conditioned on the video\u2019s FPS and\nmotion ID, where the motion ID represents the amount of\nmotion in the video. We found a motion ID of 40 to align\nwell with our frozen SVD prior.\nControl. InterDyn extends SVD with an additional con-\ntrol signal c \u2192RN\u2191H\u2191W \u21913 by integrating a ControlNet-\nlike branch [100].\nAn overview of our pipeline is pre-\nsented in Fig. 3. The SVD weights remain frozen to pre-\nserve its learned dynamics prior. Following [100], we in-\ntroduce a trainable copy of the SVD encoder E, connected\nto SVD\u2019s frozen decoder via skip connections, and mod-\nulated by zero-initialized convolutions.\nWe use a small\nCNN, E(\u00b7), to encode the control signal c into the latent\nspace, which is then added to the noisy input latent that is\npassed to the ControlNet encoder. Similar to SVD, the con-\ntrol branch interleaves convolutional, spatial, and temporal\nblocks, enabling InterDyn to process the control signal in a\ntemporal-aware manner. This helps InterDyn to be robust to\nControl\nEncode\nEncode & repeat\nchannel concat\nSVD\nDecode\n= Frozen\n= Fine-tuned\nFigure 3. Overview of InterDyn. Given an input image depicting\na scene, such as a hand holding a remote, and a \u201cdriving motion,\u201d\nsuch as a sequence of binary hand masks, InterDyn generates a\nvideo depicting plausible hand and object dynamics. Crucially,\nInterDyn receives no control signal for the object. Through this\nsetup, we probe and assess the implicit knowledge of large video\ngeneration models on complex interactive dynamics. We use Sta-\nble Video Diffusion (SVD) as our frozen backbone and \ufb01ne-tune\na separate control signal encoder. Videos are iteratively denoised\nover T timesteps, starting from Gaussian noise \u03c9 \u2192N(0, I).\nnoisy control signals, see Fig. 6. We opt for binary masks\nas conditioning signal due to their accessibility. However,\nour method can be extended to incorporate diverse types of\nsignals. We \ufb01nd that for hand-object interactions, the type\nof conditioning signal does not signi\ufb01cantly impact perfor-\nmance, see Appendix A and Tab. S1.\nInference. During inference, we start from an input image,\ncontrol signal sequence, and randomly sampled Gaussian\nnoise \u03c9 \u2191N(0, I). Through iteratively applying InterDyn\nover T denoising timesteps, we generate a video y depict-\ning plausible hand and object dynamics, aligned with the\ncontrol signal.\n4. Experiments\nThe primary goal of this work is to synthesize scene-\nlevel interactive dynamics by leveraging the implicit phys-\nical understanding of a pre-trained video generative model.\nWe begin by probing the model\u2019s ability to predict phys-\nically plausible outcomes within simulated environments,\nspeci\ufb01cally using the CLEVRER dataset [97].\nWe test\nforce propagation amongst uncontrolled objects and exam-\nine counterfactual future prediction by generating videos\nfor one input image with different control signals. Moti-\nvated by promising results, we extend our investigation to\ncomplex, real-world hand-object interaction scenarios us-\ning the Something-Something-v2 (SSV2) dataset [21], con-\nducting comprehensive comparisons with existing baselines\nthat pursue similar objectives. Additionally, we showcase\ndiverse physical examples to demonstrate the capabilities\nof InterDyn in generating realistic interactive dynamics.\n12470\n\n=== Page 5 ===\nInput image\n(a) Force Propagation. InterDyn generates force propagation to and amongst uncontrolled objects (cyan cylinder, pink cylinder, and pink cube).\nw/o brown\ncylinder\nw/ brown\ncylinder\nInput image\nGeneration 1\nGeneration 2\n(b) Counterfactual Dynamics. Two generations of InterDyn on the same input image: (1) w/ brown-cylinder control and (2) w/o brown-cylinder control.\nFigure 4. Qualitative investigation on the CLEVRER dataset. Given an input image and the \u201cdriving\u201d motion of one or two objects,\nour model predicts the future interactive dynamics of multiple elements in the scene. The driving motion is given in the form of semantic\nmask sequences. The generated object motions are highlighted with a red-line trajectory. Note that our model can generate videos with\nforce propagation across multiple uncontrolled objects (top) and can generate multiple futures (bottom). ! Zoom in for details.\n4.1. Implementation\nWe initialize InterDyn with the 14-frame image-to-video\nweights of SVD [5]. During training, we use the Adam\noptimizer [45] with a learning rate of 1 \u219310\u21935. We use\nthe EDM framework [43] with noise distribution log \u03c9 \u2191\nN(0.7, 1.62).\nWe train on two 80GB H100 GPUs, with\na per-GPU batch size of 4. Video length and FPS de\ufb01ne\nthe temporal resolution of dynamics; to balance short- and\nlong-range events we subsample videos to 7 FPS. To fa-\ncilitate classi\ufb01er-free guidance [31], we randomly drop the\ninput image with a probability of 5%. At inference, we ap-\nply the Euler scheduler [43] over 50 denoising timesteps.\n4.2. Metrics\nWe evaluate InterDyn on image quality, spatio-temporal\nsimilarity, and motion \ufb01delity. Image quality metrics are\ncomputed frame-wise. All metrics are reported excluding\nthe \ufb01rst frame, as it serves as input conditioning.\nImage quality. We report the Structural Similarity Index\nMeasure (SSIM) [79], Peak Signal-to-Noise Ratio (PSNR),\nLearned Perceptual Image Patch Similarity (LPIPS) [102],\nFr\u00b4echet Inception Distance (FID) [30] and unbiased Kernel\nInception Distance (KID) [4].\nSpatio-temporal similarity. To assess the spatio-temporal\nperceptual similarity between the ground truth and the gen-\nerated video distributions, we use the Fr\u00b4echet Video Dis-\ntance (FVD) and unbiased Kernel Video Distance (KVD)\nproposed in [74]. We use the implementation of [69].\nMotion Fidelity. Through InterDyn, we do not have ex-\nplicit control over object dynamics, which means that the\npixel alignment of an object in the generated and ground\ntruth video is only guaranteed in the starting frame. In this\ncase, comparing generated object motion to the ground truth\nnaively might misrepresent the true quality of object motion\nover time. Therefore, we adapt the Motion Fidelity (MF)\nmetric proposed by Yatim et al. [93], which measures the\nsimilarity between point-tracking trajectories.\nTo compute the metric for any video, we obtain a mask\nof the object in the starting frame, sample 100 points on the\nobject, and track these throughout both the ground truth and\ngenerated video using CoTracker3 [41]. Given the resulting\ntwo sets of tracklets T = {\u03b51, . . . , \u03b5n}, \u02dcT = {\u02dc\u03b51, . . . , \u02dc\u03b5m}\nthe motion \ufb01delity metric is de\ufb01ned as:\n1\nm\n!\n\u02dc\u03c9\u2194\u02dcT\nmax\n\u03c9\u2194T corr(\u03b5, \u02dc\u03b5) + 1\nn\n!\n\u03c9\u2194T\nmax\n\u02dc\u03c9\u2194\u02dcT\ncorr(\u03b5, \u02dc\u03b5),\n(1)\nwith the correlation between two tracklets corr(\u03b5, \u02dc\u03b5) [50]:\ncorr(\u03b5, \u02dc\u03b5) 1\nF\nF\n!\nk=1\nvx\nka \u00b7 \u02dcvx\nk + vy\nk \u00b7 \u02dcvy\nk\n\"\n(vx\nk)2 + (vy\nk)2 \u00b7\n\"\n(\u02dcvx\nk)2 + (\u02dcvy\nk)2 , (2)\nwhere (vx\nk, vy\nk), (\u02dcvx\nk, \u02dcvy\nk) are the kth frame displacement of\ntracklets \u03b5, \u02dc\u03b5 respectively. If there are less than 100 points\nto query on the object due to it being too small, we do not\nconsider the video for the motion \ufb01delity metric.\n12471\n\n=== Page 6 ===\n4.3. Probing Dynamics with Object Collision Events\nHere, we \ufb01ne-tune InterDyn on an object collision dataset\nto probe its ability to generate realistic object interactions.\nQualitatively, we review whether InterDyn can produce\nplausible object motion for uncontrolled objects, given the\nmotion of objects entering the scene. In addition, we ex-\namine whether InterDyn can generate counterfactual videos\nfor the same input image, but different control signals.\nDataset. We use CLEVRER [97], which provides 20,000\nvideos of colliding objects with annotated segmentation\nmasks and metadata on collision events. We construct a\ncontrol signal for objects entering the scene and aim to use\nInterDyn to generate the motion of the objects that are al-\nready present, upon collision. Stationary objects do not re-\nceive any form of control signal. Colored masks help the\nmodel distinguish unique objects. The frames are cropped\nand scaled to 320 \u2193448, and we only sample input frames\nbefore collisions between objects in the scene, to maximize\nInterDyn\u2019s exposure to interactive dynamics.\nForce propagation. InterDyn can generate force propaga-\ntion dynamics between a controlled object and an uncon-\ntrolled object, as well as amongst uncontrolled objects, as\nillustrated in Fig. 4a. Here, the red cylinder at the top left\nis the driving force. It collides with the uncontrolled blue\ncylinder, which then collides with the uncontrolled pur-\nple cylinder, in turn striking the uncontrolled purple cube\non the far right.\nPoint-tracking trajectories display how\ncollisions alter each object\u2019s position. This suggests that\nInterDyn possesses an implicit understanding of physical\ninteractions, enabling it to generate plausible dynamics.\nCounterfactual dynamics. By altering the control signal,\nInterDyn can simulate counterfactual scenarios for the same\ninput image, as shown in Fig. 4b. In \u201cGeneration 1\u201d, the\ngray cylinder (controlled) collides with the stationary red\nsphere (uncontrolled), causing it to move; it is later struck\nby the brown cylinder (controlled), altering its path once\nagain. In \u201cGeneration 2\u201d, removing the brown cylinder lets\nthe red sphere continue along its original trajectory, consis-\ntent with expectations. Crucially, there is no control signal\nfor the red sphere throughout the sequence; its motion and\ntrajectory are entirely generated by InterDyn.\nProbing InterDyn on CLEVRER highlights its ability to\ngenerate interactive dynamics for objects within a simple\nsynthetic environment.\nWe provide additional results in\nvideo format on our webpage.\n4.4. Generating Human-Object Interactions\nFor this experiment, we \ufb01ne-tune InterDyn on a human-\naction dataset, focused on hand-object interaction. We en-\ncode human movement as a sequence of pixel-aligned bi-\nnary hand masks and task InterDyn to generate a video with\nhand movements and corresponding object dynamics.\nDataset. We use Something-Something-v2 (SSV2), which\nprovides 220,847 videos of humans performing basic ac-\ntions with everyday objects.\nIt contains actions such\nas \u201cpushing [something] from left to right\u201d, \u201csqueezing\n[something]\u201d and \u201clifting [something] with [something] on\nit\u201d. This dataset allows us to train InterDyn at a larger scale\nand compare with our baseline CosHand [70]. We train\none version of InterDyn at the same resolution as CosHand,\n256\u2193256, and a second version at 256\u2193384, which aligns\nbetter with the dynamic prior of SVD.\nWe generate binary hand masks by prompting Segment\nAnything 2 (SAM2) [65] with hand bounding boxes, pro-\nvided by the Something-Else dataset [59].\nSimilar to\nour baseline CosHand, we exclude the \u201cpretending\u201d class.\nAdditionally, we remove all \u201c[something] is done behind\n[something]\u201d classes since the object would be out of view\nof the camera. We remove videos smaller than the target\nresolution/length, ensure the hand and objects are continu-\nously visible, and crop videos larger than the target reso-\nlution while centering the object. We include videos with-\nout obvious motion and state transition, such as \u201cholding\n[something]\u201d.\nSince bounding box annotations are only\nprovided for the train split (79,043 samples for 256\u2193384\nand 104,260 for 256\u2193256) and validation split (8,667 sam-\nples for 256\u2193384 and 11,229 for 256\u2193256), we report all\nevaluation metrics on the validation split.\nCosHand\nFrame-wise\nCosHand\nAuto-regressive\nInterDyn\nOurs\nFigure 5. Qualitative comparison. A two-state approach such as\nCosHand [70] struggles with post-interaction object dynamics.\nGround\ntruth\nInterDyn\nOurs\nFigure 6. Robustness to noise. SAM2 outputs noisy/coarse masks\nfor frames with considerable motion blur (orange/red). Despite\nthis, InterDyn can generate plausible hand and object dynamics.\n12472\n\n=== Page 7 ===\nFigure 7. Diverse generation of interactive dynamics. We show multiple challenging examples, such as (from top to bottom): interacting\nwith articulated objects, pouring liquid, letting an object fall, squeezing a highly deformable or \u201ccollapsible\u201d object, interacting with\nre\ufb02ective objects, tilting a ridged object, or stacking objects. ! Zoom in for details.\n12473\n\n=== Page 8 ===\nMethod\nSSIM \u2194\nPSNR \u2194\nLPIPS \u2197\nFID \u2197\nKID \u2197\nFVD \u2197\nKVD \u2197\nMotion Fidelity \u2194[93]\nSeer [23]\n0.418\n10.71\n0.588\n33.35\n0.030\n287.46\n81.31\n\u2014\nDynamiCrafter [89]\u2020\n\u2014\n\u2014\n\u2014\n17.48\n0.014\n204.11\n31.81\n\u2014\nCosHand-Independent [70]\n0.615\n16.87\n0.313\n2.95\n0.002\n91.18\n19.24\n0.432\nCosHand-Autoregressive [70]\n0.531\n14.92\n0.408\n12.66\n0.012\n90.30\n13.68\n0.570\nOurs 256\u2193256\n0.664\n18.60\n0.260\n4.95\n0.004\n19.27\n1.99\n0.633\nOurs 256\u2193384\n0.680\n19.04\n0.252\n5.07\n0.004\n22.22\n2.09\n0.641\nTable 1. Quantitative comparison on Something-Something-v2. We compare against two language-instructed video generation methods,\nSeer [23] and DynamiCrafter [89] and two video extensions of our baseline CosHand [70]. We report results for InterDyn at two resolutions:\n256\u2191256 (matching CosHand) and 256\u2191384 (matching SVD\u2019s prior). Methods denoted with \u2020 do not use SSV2 as their training dataset.\nBaselines. To generate videos using CosHand [70] rather\nthan state transitions, we run CosHand in a frame-by-frame\napproach (CosHand-Independent) and an auto-regressive\napproach (CosHand-Autoregressive).\nIn the frame-by-\nframe variant, each future frame is independently predicted\nfrom the initial frame and its corresponding mask:\n\u02c6xt+1 = CosHand(x0, h0, ht+1),\n\u2198t \u2192[0, 13],\nwhere h0 and ht+1 denote masks, and x0 the initial frame.\nIn the auto-regressive variant, we use CosHand to generate\nvideo frames sequentially:\n\u02c6xt+1 = CosHand(\u02c6xt, ht, ht+1),\n\u2198t \u2192[0, 13],\nwith \u02c6xt being the previously generated frame and \u02c6x0 = x0.\nWe also compare against two language-instructed video\ngeneration methods: Seer [23] and DynamiCrafter [89],\nwhich we prompt with a \ufb01rst frame and its corresponding\nSSV2 class label as instruction. Note that DynamiCrafter\nwas trained with a random video frame as conditioning,\ni.e. not always the \ufb01rst frame. At inference, its generated\nvideos are thus not strictly a continuation of the condition-\ning, which precludes frame-aligned metrics. We compare\nInterDyn against these baselines in Tab. 1.\nQuantitative analysis.\nThe frame-by-frame variant of\nCosHand achieves high image quality but struggles with\ntemporal coherence and motion \ufb01delity. Qualitatively, we\nnotice that object locations are inconsistent across frames.\nIn contrast, the auto-regressive variant improves object mo-\ntion \ufb01delity but suffers from lower frame-wise image qual-\nity due to error propagation. Both variants fail in scenarios\nrequiring accurate post-interaction dynamics, such as when\nobjects continue moving after being released from direct\nhand contact, as shown in Fig. 5.\nOur method, InterDyn (256\u2193384), achieves the best\noverall performance, surpassing our baseline CosHand in\nspatio-temporal dynamics, motion \ufb01delity, and all but two\nimage quality metrics.\nWe hypothesize that this might\nbe due to two reasons (1) we use a frozen U-Net, while\nCosHand \ufb01ne-tunes its model on SSV2, so CosHand might\ngenerate frames closer to the SSV2 distribution, and 2)\nwhen SVD was trained, it was initialized with SD weights\nas spatial layers, and then \ufb01ne-tuned over multiple stages;\nthis might have degraded its spatial prior, and by extension\nthe quality of produced images compared to CosHand.\nFine-tuned on noisy masks and leveraging its temporal-\naware control branch, InterDyn can interpret a noisy control\nsequence; e.g. when SAM2 produces a coarse and noisy\nhand mask sequence, InterDyn generates detailed hands in-\ncluding individual \ufb01ngers, see Fig. 6. Though not always\nconsistent, InterDyn is capable of depicting post-interaction\ndynamics, such as rolling or sliding objects.\nQualitative analysis.\nWe present diverse qualitative re-\nsults generated by InterDyn in Fig. 7. Row 1 shows how\nInterDyn generates the articulated motion of an object. Row\n2 showcases pouring water into a glass; note how the water\nlevel increases over time. Row 3 demonstrates an object be-\ning dropped, moving out of frame when falling, and rolling\nback in frame once hitting the \ufb02oor, featuring realistic mo-\ntion blur synthesis. Rows 4 and 5 illustrate how InterDyn\nhandles squeezing interactions\u2014the rubber and the spring\nare compressed and restored accordingly. Row 6 demon-\nstrates an understanding of physical size and distance to the\ncamera, as the phone moves closer to the viewer. These\nresults highlight the complexity that InterDyn can handle,\nimplying its generalization ability and future potential as an\nimplicit, yet generalized physical simulator and renderer.\n5. Conclusion\nWe introduced InterDyn, a framework that generates videos\nof interactive dynamics using large video generation models\nas implicit physics simulators. By incorporating an interac-\ntive control mechanism, InterDyn produces plausible, tem-\nporally consistent videos of object interactions\u2014including\ncomplex human-object interactions\u2014while generalizing to\nunseen objects. Our evaluations demonstrate that InterDyn\neffectively captures continuous motion and subsequent dy-\nnamics, outperforming baselines that focus on single future\nstates. This work highlights the potential of using video\ngenerative models as physics simulators without explicit re-\nconstruction, opening new avenues for future research.\n12474\n\n=== Page 9 ===\nAcknowledgements: We thank Benjamin Pellkofer for IT sup-\nport, Peter Kulits for discussions, Tomasz Niewiadomski for cod-\ning support, and Allan D. Jepson, Artur Grigorev, Soubhik Sanyal,\nOmid Taheri & Angjoo Kanazawa for feedback. DT is supported\nby the ERC Starting Grant (project STRIPES, 101165317).\nDisclosure: https://\ufb01les.is.tue.mpg.de/black/CoI CVPR 2025.txt\nDT has received a research gift fund from Google.\nReferences\n[1] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Her-\nrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur,\nGuanghui Liu, Amit Raj, et al. Lumiere: A space-time dif-\nfusion model for video generation. In International Con-\nference on Computer Graphics and Interactive Techniques\n(SIGGRAPH), pages 1\u201311, 2024. 2, 3\n[2] Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori,\nand Christian Wolf.\nCoPhy: Counterfactual learning of\nphysical dynamics. In International Conference on Learn-\ning Representations (ICLR), 2020. 2, 3\n[3] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo\nJimenez Rezende, et al. Interaction networks for learning\nabout objects, relations and physics. In Conference on Neu-\nral Information Processing Systems (NeurIPS), 2016. 3\n[4] Miko!aj Bi\u00b4nkowski, Danica J. Sutherland, Michael Arbel,\nand Arthur Gretton. Demystifying MMD GANs. In Inter-\nnational Conference on Learning Representations (ICLR),\n2018. 5\n[5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al.\nStable\nVideo Diffusion: Scaling latent video diffusion models to\nlarge datasets. arXiv:2311.15127, 2023. 2, 3, 4, 5\n[6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim\nDockhorn, Seung Wook Kim, Sanja Fidler, and Karsten\nKreis. Align your latents: High-resolution video synthe-\nsis with latent diffusion models. In Computer Vision and\nPattern Recognition (CVPR), pages 22563\u201322575, 2023. 3\n[7] Tim Brooks, Aleksander Holynski, and Alexei A. Efros.\nInstructPix2Pix: Learning to follow image editing instruc-\ntions. In Computer Vision and Pattern Recognition (CVPR),\npages 18392\u201318402, 2023. 14\n[8] Shengqu Cai, Duygu Ceylan, Matheus Gadelha, Chun-\nHao Paul Huang, Tuanfeng Yang Wang, and Gordon Wet-\nzstein. Generative rendering: Controllable 4D-guided video\ngeneration with 2D diffusion models. In Computer Vision\nand Pattern Recognition (CVPR), pages 7611\u20137620, 2024.\n3\n[9] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.\nRealtime multi-person 2D pose estimation using part af\ufb01n-\nity \ufb01elds.\nIn Computer Vision and Pattern Recognition\n(CVPR), pages 7291\u20137299, 2017. 14\n[10] Junuk Cha, Jihyeon Kim, Jae Shin Yoon, and Seungryul\nBaek. Text2HOI: Text-guided 3D motion generation for\nhand-object interaction. In Computer Vision and Pattern\nRecognition (CVPR), pages 1577\u20131585, 2024. 3\n[11] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov,\nAnkur Handa, Jonathan Tremblay, Yashraj S. Narang, Karl\nVan Wyk, Umar Iqbal, Stan Birch\ufb01eld, et al.\nDexYCB:\nA benchmark for capturing hand grasping of objects. In\nComputer Vision and Pattern Recognition (CVPR), pages\n9044\u20139053, 2021. 14\n[12] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang\nLiu, Qifeng Chen, Xintao Wang, et al.\nVideoCrafter1:\nOpen diffusion models for high-quality video generation.\narXiv:2310.19512, 2023. 3\n[13] Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Ji-\nashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-A-\nVideo: Controllable text-to-video generation with diffusion\nmodels. arXiv:2305.13840, 2023. 3\n[14] Enric Corona, Albert Pumarola, Guillem Alenya, Francesc\nMoreno-Noguer, and Gr\u00b4egory Rogez. GanHand: Predicting\nhuman grasp affordances in multi-object scenes. In Com-\nputer Vision and Pattern Recognition (CVPR), pages 5031\u2013\n5041, 2020. 3\n[15] Christian Diller and Angela Dai. CG-HOI: Contact-guided\n3D human-object interaction generation. In Computer Vi-\nsion and Pattern Recognition (CVPR), pages 19888\u201319901,\n2024. 3\n[16] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M\u00a8uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti-\n\ufb01ed \ufb02ow transformers for high-resolution image synthesis.\nIn International Conference on Machine Learning (ICML),\n2024. 2\n[17] Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed\nKocabas, Manuel Kaufmann, Michael J. Black, and Ot-\nmar Hilliges. ARCTIC: A dataset for dexterous bimanual\nhand-object manipulation. In Computer Vision and Pattern\nRecognition (CVPR), pages 12943\u201312954, 2023. 3\n[18] Zicong Fan, Maria Parelli, Maria Eleni Kadoglou, Xu\nChen, Muhammed Kocabas, Michael J. Black, and Otmar\nHilliges. HOLD: Category-agnostic 3D reconstruction of\ninteracting hands and objects from video.\nIn Computer\nVision and Pattern Recognition (CVPR), pages 494\u2013504,\n2024. 3\n[19] Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu,\nBrian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Phys-\nically grounded vision-language models for robotic manip-\nulation. In International Conference on Robotics and Au-\ntomation (ICRA), pages 12462\u201312469, 2024. 3\n[20] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Du-\nval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah,\nXi Yin, Devi Parikh, and Ishan Misra. Emu Video: Factor-\nizing text-to-video generation by explicit image condition-\ning. In European Conference on Computer Vision (ECCV),\n2024. 3\n[21] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-\nski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\nValentin Haenel, Ingo Fruend, Peter Yianilos, Moritz\nMueller-Freitag, et al.\nThe something something video\ndatabase for learning and evaluating visual common sense.\nIn International Conference on Computer Vision (ICCV),\npages 5842\u20135850, 2017. 2, 4\n12475\n\n=== Page 10 ===\n[22] Oliver Groth, Fabian B. Fuchs, Ingmar Posner, and Andrea\nVedaldi. ShapeStacks: Learning vision-based physical in-\ntuition for generalised object stacking. In European Con-\nference on Computer Vision (ECCV), pages 702\u2013717, 2018.\n3\n[23] Xianfan Gu, Chuan Wen, Weirui Ye, Jiaming Song, and\nYang Gao. Seer: Language instructed video prediction with\nlatent diffusion models.\nIn International Conference on\nLearning Representations (ICLR), 2024. 2, 3, 8, 17\n[24] Vincent Le Guen and Nicolas Thome. Disentangling physi-\ncal dynamics from unknown factors for unsupervised video\nprediction.\nIn Computer Vision and Pattern Recognition\n(CVPR), pages 11471\u201311481, 2020. 2, 3\n[25] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan\nDeng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu,\nZhengjun Zha, et al.\nI2V-Adapter: A general image-to-\nvideo adapter for diffusion models. In International Con-\nference on Computer Graphics and Interactive Techniques\n(SIGGRAPH), 2024. 3\n[26] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang,\nYaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin,\nand Bo Dai.\nAnimateDiff: Animate your personalized\ntext-to-image diffusion models without speci\ufb01c tuning.\narXiv:2307.04725, 2023. 3\n[27] Yana Hasson, G\u00a8ul Varol, Dimitrios Tzionas, Igor Kale-\nvatykh, Michael J. Black, Ivan Laptev, and Cordelia\nSchmid. Learning joint reconstruction of hands and manip-\nulated objects. In Computer Vision and Pattern Recognition\n(CVPR), pages 11807\u201311816, 2019. 3\n[28] Yana Hasson, G\u00a8ul Varol, Cordelia Schmid, and Ivan Laptev.\nTowards unconstrained joint hand-object reconstruction\nfrom RGB videos. In International Conference on 3D Vi-\nsion (3DV), pages 659\u2013668, 2021. 3\n[29] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein,\nBo Dai, Hongsheng Li, and Ceyuan Yang.\nCameraC-\ntrl: Enabling camera control for text-to-video generation.\narXiv:2404.02101, 2024. 3\n[30] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs trained by\na two time-scale update rule converge to a local nash equi-\nlibrium. In Conference on Neural Information Processing\nSystems (NeurIPS), pages 6629\u20136640, 2017. 5\n[31] Jonathan Ho and Tim Salimans. Classi\ufb01er-free diffusion\nguidance. arXiv:2207.12598, 2022. 5\n[32] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben\nPoole, Mohammad Norouzi, David J. Fleet, et al.\nIma-\ngen Video: High de\ufb01nition video generation with diffusion\nmodels. arXiv:2210.02303, 2022. 3\n[33] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J. Fleet. Video dif-\nfusion models. In Conference on Neural Information Pro-\ncessing Systems (NeurIPS), pages 8633\u20138646, 2022. 3\n[34] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. LoRA: Low-rank adaptation of large language mod-\nels. In International Conference on Learning Representa-\ntions (ICLR), 2022. 3\n[35] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and\nLiefeng Bo. Animate Anyone: Consistent and controllable\nimage-to-video synthesis for character animation. In Com-\nputer Vision and Pattern Recognition (CVPR), pages 8153\u2013\n8163, 2024. 3\n[36] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xi-\naodong Cun, Yong Zhang, Long Quan, and Ying Shan.\nDepthCrafter: Generating consistent long depth sequences\nfor open-world videos. arXiv:2409.02095, 2024. 4\n[37] Yaosi Hu, Chong Luo, and Zhenzhong Chen.\nMake It\nMove: Controllable image-to-video generation with text\ndescriptions. In Computer Vision and Pattern Recognition\n(CVPR), pages 18219\u201318228, 2022. 3\n[38] Zhihao Hu and Dong Xu.\nVideoControlNet: A motion-\nguided video-to-video translation framework by using dif-\nfusion model with ControlNet. arXiv:2307.14073, 2023.\n3\n[39] Steeven Janny, Fabien Baradel, Natalia Neverova, Madiha\nNadri, Greg Mori, and Christian Wolf.\nFiltered-CoPhy:\nUnsupervised learning of counterfactual physics in pixel\nspace. In International Conference on Learning Represen-\ntations (ICLR), 2022. 2, 3\n[40] Hanwen Jiang, Shaowei Liu, Jiashun Wang, and Xiaolong\nWang. Hand-object contact consistency reasoning for hu-\nman grasps generation.\nIn International Conference on\nComputer Vision (ICCV), pages 11107\u201311116, 2021. 3\n[41] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia\nNeverova, Andrea Vedaldi, and Christian Rupprecht. Co-\nTracker3: Simpler and better point tracking by pseudo-\nlabelling real videos. In European Conference on Computer\nVision (ECCV), pages 18\u201335, 2024. 5\n[42] Johanna Karras, Aleksander Holynski, Ting-Chun Wang,\nand Ira Kemelmacher-Shlizerman.\nDreamPose: Fashion\nimage-to-video synthesis via stable diffusion.\nIn Inter-\nnational Conference on Computer Vision (ICCV), pages\n22623\u201322633, 2023. 3\n[43] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. Conference on Neural Information Processing Sys-\ntems (NeurIPS), 35:26565\u201326577, 2022. 5\n[44] Hyeonwoo Kim, Sookwan Han, Patrick Kwon, et al. Be-\nyond the Contact: Discovering comprehensive affordance\nfor 3D objects from pre-trained 2D diffusion models. In\nEuropean Conference on Computer Vision (ECCV), pages\n400\u2013419, 2024. 3\n[45] Diederik P. Kingma and Jimmy Ba.\nAdam: A method\nfor stochastic optimization. In International Conference on\nLearning Representations (ICLR), 2015. 5\n[46] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max\nWelling, and Richard Zemel. Neural relational inference\nfor interacting systems. In International Conference on Ma-\nchine Learning (ICML), pages 2693\u20132702, 2018. 2\n[47] Sumith Kulal, Tim Brooks, Alex Aiken, Jiajun Wu, Jimei\nYang, Jingwan Lu, Alexei A. Efros, and Krishna Kumar\nSingh. Putting people in their place: Affordance-aware hu-\nman insertion into scenes. In Computer Vision and Pattern\nRecognition (CVPR), pages 17089\u201317099, 2023. 3\n12476\n\n=== Page 11 ===\n[48] Adam Lerer, Sam Gross, and Rob Fergus. Learning phys-\nical intuition of block towers by example. In International\nConference on Machine Learning (ICML), pages 430\u2013438,\n2016. 3\n[49] Yunzhu Li, Antonio Torralba, Anima Anandkumar, Dieter\nFox, and Animesh Garg. Causal discovery in physical sys-\ntems from videos. Conference on Neural Information Pro-\ncessing Systems (NeurIPS), 2020. 2\n[50] Ce Liu, Antonio Torralba, William T. Freeman, Fr\u00b4edo Du-\nrand, and Edward H. Adelson.\nMotion magni\ufb01cation.\nTransactions on Graphics (TOG), 24(3):519\u2013526, 2005. 5\n[51] Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu,\nSoroush Vosoughi, Claire Cui, Denny Zhou, and Andrew M\nDai.\nMind\u2019s Eye: Grounded language model reasoning\nthrough simulation. In International Conference on Learn-\ning Representations (ICLR), 2023. 3\n[52] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang\nSu, Jun Zhu, et al.\nGrounding DINO: Marrying dino\nwith grounded pre-training for open-set object detection.\narXiv:2303.05499, 2023. 14\n[53] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shen-\nlong Wang.\nPhysGen:\nRigid-body physics-grounded\nimage-to-video generation.\nIn European Conference on\nComputer Vision (ECCV), pages 360\u2013378, 2024. 3\n[54] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao,\nRuoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun,\nJianfeng Gao, et al. Sora: A review on background, technol-\nogy, limitations, and opportunities of large vision models.\narXiv:2402.17177, 2024. 2, 4\n[55] Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, An-\ndrew Zisserman, David Salesin, William T. Freeman, and\nMichael Rubinstein. Layered neural rendering for retiming\npeople in video. Transactions on Graphics (TOG), 39(6):\n1\u201314, 2020. 3\n[56] Erika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman,\nWilliam T. Freeman, and Michael Rubinstein. Omnimatte:\nAssociating objects and their effects in video. In Computer\nVision and Pattern Recognition (CVPR), pages 4507\u20134515,\n2021. 3\n[57] Jiaxin Lu, Hao Kang, Haoxiang Li, Bo Liu, Yiding Yang,\nQixing Huang, and Gang Hua. UGG: Uni\ufb01ed generative\ngrasping.\nIn European Conference on Computer Vision\n(ECCV), pages 414\u2013433, 2024. 3\n[58] Wan-Duo Kurt Ma, John P Lewis, and W. Bastiaan Kleijn.\nTrailBlazer: Trajectory control for diffusion-based video\ngeneration.\nIn International Conference on Computer\nGraphics and Interactive Techniques (SIGGRAPH), pages\n1\u201311, 2023. 3\n[59] Joanna Materzynska, Tete Xiao, Roei Herzig, Huijuan Xu,\nXiaolong Wang, and Trevor Darrell.\nSomething-Else:\nCompositional action recognition with spatial-temporal in-\nteraction networks. In Computer Vision and Pattern Recog-\nnition (CVPR), pages 1049\u20131059, 2020. 6\n[60] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian\nZhang, Zhongang Qi, and Ying Shan. T2I-Adapter: Learn-\ning adapters to dig out more controllable ability for text-to-\nimage diffusion models. In AAAI Conference on Arti\ufb01cial\nIntelligence, pages 4296\u20134304, 2024. 3\n[61] Franziska Mueller, Micah Davis, Florian Bernard, Olek-\nsandr Sotnychenko, Mickeal Verschoor, Miguel A. Otaduy,\nDan Casas, and Christian Theobalt.\nReal-time pose and\nshape reconstruction of two interacting hands with a sin-\ngle depth camera. Transactions on Graphics (TOG), 38(4):\n1\u201313, 2019. 14\n[62] Boxiao Pan, Zhan Xu, Chun-Hao Paul Huang, Krishna Ku-\nmar Singh, Yang Zhou, Leonidas J. Guibas, and Jimei Yang.\nActAnywhere: Subject-aware video background genera-\ntion. In Conference on Neural Information Processing Sys-\ntems (NeurIPS), 2024. 3\n[63] Georgios Paschalidis, Romana Wilschut, Dimitrije Anti\u00b4c,\nOmid Taheri, and Dimitrios Tzionas. 3D whole-body grasp\nsynthesis with directional controllability. In International\nConference on 3D Vision (3DV), 2025. 3\n[64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning\n(ICML), pages 8748\u20138763, 2021. 4\n[65] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\nHu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\nR\u00a8adle, Chlo\u00b4e Rolland, Laura Gustafson, Eric Mintun, Junt-\ning Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-\nYuan Wu, Ross B. Girshick, Piotr Doll\u00b4ar, and Christoph\nFeichtenhofer. SAM 2: Segment anything in images and\nvideos. arXiv:2408.00714, 2024. 6, 14\n[66] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models.\nIn Computer Vi-\nsion and Pattern Recognition (CVPR), pages 10684\u201310695,\n2022. 2, 4, 14\n[67] Javier Romero, Dimitrios Tzionas, and Michael J. Black.\nEmbodied hands: Modeling and capturing hands and bod-\nies together. Transactions on Graphics (TOG), 36(6), 2022.\n14\n[68] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-A-Video: Text-to-video genera-\ntion without text-video data. In International Conference\non Learning Representations (ICLR), 2023. 3\n[69] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny. StyleGAN-V: A continuous video generator with the\nprice, image quality and perks of StyleGAN2. In Computer\nVision and Pattern Recognition (CVPR), pages 3626\u20133636,\n2022. 5\n[70] Sruthi Sudhakar, Ruoshi Liu, Basile Van Hoorick, Carl\nVondrick, and Richard Zemel. Controlling the world by\nsleight of hand. In European Conference on Computer Vi-\nsion (ECCV), pages 414\u2013430, 2024. 2, 3, 6, 8, 14, 17\n[71] Omid Taheri, Nima Ghorbani, Michael J. Black, and Dim-\nitrios Tzionas.\nGRAB: A dataset of whole-body human\ngrasping of objects. In European Conference on Computer\nVision (ECCV), pages 581\u2013600, 2020. 3\n12477\n\n=== Page 12 ===\n[72] Omid Taheri, Yi Zhou, Dimitrios Tzionas, Yang Zhou,\nDuygu Ceylan, Soren Pirk, and Michael J. Black. GRIP:\nGenerating interaction poses using latent consistency and\nspatial cues.\nIn International Conference on 3D Vision\n(3DV), pages 933\u2013943, 2024. 3\n[73] Tze Ho Elden Tse, Kwang In Kim, Ales Leonardis, and\nHyung Jin Chang.\nCollaborative learning for hand and\nobject reconstruction with attention-guided graph convolu-\ntion. In Computer Vision and Pattern Recognition (CVPR),\npages 1664\u20131674, 2022. 3\n[74] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRapha\u00a8el Marinier, Marcin Michalski, and Sylvain Gelly.\nFVD: A new metric for video generation.\nIn Interna-\ntional Conference on Learning Representations Workshops\n(ICLRw), 2019. 5\n[75] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts,\nDavid Pankratz, Dmitry Tochilkin, Christian Laforte, Robin\nRombach, and Varun Jampani. SV3D: Novel multi-view\nsynthesis and 3D generation from a single image using la-\ntent video diffusion. In European Conference on Computer\nVision (ECCV), pages 439\u2013457, 2025. 4, 14\n[76] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv:2308.06571, 2023. 3\n[77] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guo-\nqiang Wei, Liping Yuan, and Hang Li. Boximator: Gen-\nerating rich and controllable motions for video synthesis.\nIn International Conference on Machine Learning (ICML),\n2024. 3\n[78] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and\nJingren Zhou. VideoComposer: Compositional video syn-\nthesis with motion controllability. In Conference on Neural\nInformation Processing Systems (NeurIPS), 2024. 3\n[79] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P.\nSimoncelli. Image Quality Assessment: From error visibil-\nity to structural similarity. Transactions on Image Process-\ning (TIP), 13(4):600\u2013612, 2004. 3, 5\n[80] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li,\nTianshui Chen, Menghan Xia, Ping Luo, and Ying Shan.\nMotionCtrl: A uni\ufb01ed and \ufb02exible motion controller for\nvideo generation.\nIn Transactions on Graphics (TOG),\npages 1\u201311, 2024. 3\n[81] Nicholas Watters, Daniel Zoran, Theophane Weber, Peter\nBattaglia, Razvan Pascanu, and Andrea Tacchetti. Visual\nInteraction Networks: Learning a physics simulator from\nvideo.\nIn Conference on Neural Information Processing\nSystems (NeurIPS), pages 4539\u20134547, 2017. 3\n[82] Jiajun Wu, Ilker Yildirim, Joseph J. Lim, Bill Freeman, and\nJoshua B. Tenenbaum.\nGalileo: Perceiving physical ob-\nject properties by integrating a physics engine with deep\nlearning. In Conference on Neural Information Processing\nSystems (NeurIPS), pages 127\u2013135, 2015. 3\n[83] Jiajun Wu, Joseph J. Lim, Hongyi Zhang, Joshua B. Tenen-\nbaum, and William T. Freeman.\nPhysics 101: Learning\nphysical object properties from unlabeled videos. In British\nMachine Vision Conference (BMVC), 2016.\n[84] Jiajun Wu, Erika Lu, Pushmeet Kohli, Bill Freeman, and\nJosh Tenenbaum. Learning to see physics via visual de-\nanimation. In Conference on Neural Information Process-\ning Systems (NeurIPS), pages 153\u2013164, 2017. 2, 3\n[85] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-A-Video: One-shot tun-\ning of image diffusion models for text-to-video generation.\nIn International Conference on Computer Vision (ICCV),\npages 7623\u20137633, 2023. 3\n[86] Qianyang Wu, Ye Shi, Xiaoshui Huang, Jingyi Yu, Lan Xu,\nand Jingya Wang. THOR: Text to human-object interac-\ntion diffusion via relation intervention. arXiv:2403.11208,\n2024. 3\n[87] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He,\nDavid Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting\nGao, and Di Zhang. DragAnything: Motion control for any-\nthing using entity representation. In European Conference\non Computer Vision (ECCV), pages 331\u2013348, 2025. 3\n[88] Xianghui Xie, Bharat Lal Bhatnagar, Jan Eric Lenssen, and\nGerard Pons-Moll. Template free reconstruction of human-\nobject interaction with procedural interaction generation. In\nComputer Vision and Pattern Recognition (CVPR), pages\n10003\u201310015, 2024. 3\n[89] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen,\nWangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying\nShan, and Tien-Tsin Wong.\nDynamiCrafter: Animating\nopen-domain images with video diffusion priors. In Euro-\npean Conference on Computer Vision (ECCV), pages 399\u2013\n417, 2025. 2, 3, 8, 17\n[90] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan\nGui. InterDiff: Generating 3D human-object interactions\nwith physics-informed diffusion.\nIn International Con-\nference on Computer Vision (ICCV), pages 14928\u201314940,\n2023. 3\n[91] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Han-\nshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and\nMike Zheng Shou. MagicAnimate: Temporally consistent\nhuman image animation using diffusion model. In Com-\nputer Vision and Pattern Recognition (CVPR), pages 1481\u2013\n1490, 2024. 3\n[92] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding,\nShiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong,\nXiaohan Zhang, Guanyu Feng, et al. CogVideoX: Text-\nto-video diffusion models with an expert transformer.\narXiv:2408.06072, 2024. 3\n[93] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten,\nand Tali Dekel. Space-time diffusion features for zero-shot\ntext-driven motion transfer. In Computer Vision and Pattern\nRecognition (CVPR), pages 8466\u20138476, 2024. 5, 8, 14, 17\n[94] Yufei Ye, Poorvi Hebbar, Abhinav Gupta, and Shubham\nTulsiani.\nDiffusion-guided reconstruction of everyday\nhand-object interaction clips. In International Conference\non Computer Vision (ICCV), pages 19717\u201319728, 2023. 3\n[95] Yufei Ye, Xueting Li, Abhinav Gupta, Shalini De Mello,\nStan Birch\ufb01eld, Jiaming Song, Shubham Tulsiani, and Sifei\nLiu. Affordance Diffusion: Synthesizing hand-object in-\nteractions.\nIn Computer Vision and Pattern Recognition\n(CVPR), pages 22479\u201322489, 2023. 3\n12478\n\n=== Page 13 ===\n[96] Yufei Ye, Abhinav Gupta, Kris Kitani, and Shubham Tul-\nsiani. G-HOP: Generative hand-object prior for interaction\nreconstruction and grasp synthesis. In Computer Vision and\nPattern Recognition (CVPR), pages 1911\u20131920, 2024. 3\n[97] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Ji-\najun Wu, Antonio Torralba, and Joshua B. Tenenbaum.\nCLEVRER: Collision events for video representation and\nreasoning. In International Conference on Learning Repre-\nsentations (ICLR), 2020. 2, 4, 6\n[98] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang\nLi, Gong Ming, and Nan Duan. DragNUWA: Fine-grained\ncontrol in video generation by integrating text, image, and\ntrajectory. arXiv:2308.08089, 2023. 3\n[99] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin.\nVideo probabilistic diffusion models in projected latent\nspace.\nIn Computer Vision and Pattern Recognition\n(CVPR), pages 18456\u201318466, 2023. 3\n[100] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In In-\nternational Conference on Computer Vision (ICCV), pages\n3836\u20133847, 2023. 3, 4\n[101] Mengqi Zhang, Yang Fu, Zheng Ding, Sifei Liu, Zhuowen\nTu, and Xiaolong Wang. HOIDiffusion: Generating real-\nistic 3D hand-object interaction data. In Computer Vision\nand Pattern Recognition (CVPR), pages 8521\u20138531, 2024.\n3\n[102] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Computer Vision\nand Pattern Recognition (CVPR), pages 586\u2013595, 2018. 5\n[103] Juntian Zheng, Qingyuan Zheng, Lixing Fang, Yun Liu,\nand Li Yi. CAMS: Canonicalized manipulation spaces for\ncategory-level functional hand-object manipulation synthe-\nsis. In Computer Vision and Pattern Recognition (CVPR),\npages 585\u2013594, 2023. 3\n[104] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. MagicVideo: Ef\ufb01cient video\ngeneration with latent diffusion models. arXiv:2211.11018,\n2022. 3\n[105] Keyang Zhou, Bharat Lal Bhatnagar, Jan Eric Lenssen, and\nGerard Pons-Moll. GEARS: Local geometry-aware hand-\nobject interaction synthesis. In Computer Vision and Pat-\ntern Recognition (CVPR), pages 20634\u201320643, 2024. 3\n[106] Tianqiang Zhu, Rina Wu, Xiangbo Lin, and Yi Sun. To-\nward human-like grasp: Dexterous grasping via semantic\nrepresentation of object-hand. In International Conference\non Computer Vision (ICCV), pages 15741\u201315751, 2021. 3\n12479\n",
      "searchable_content": "page 1 interdyn controllable interactive dynamics with video diffusion models rick akkerman1 2 haiwen feng1 michael j. black1 dimitrios tzionas2 victoria fern andez abrevaya1 1max planck institute for intelligent systems t ubingen germany 2university of amsterdam the netherlands rick.akkerman haiwen.feng black victoria.abrevaya tuebingen.mpg.de d.tzionas uva.nl input image video generation by interdyn using only the hand mask sequence as control signal force propagation counterfactual dynamics future 1 future 2 denotes a driving object motion tracks indicate an object with generated uncontrolled dynamics t 3 t 13 t 0 t 0 figure 1. we present interdyn a framework for synthesizing realistic interactive dynamics without 3d reconstruction and physics simu- lation. our core principle is to rely on the implicit physics knowledge embedded in large-scale video generative models. given an image and a driving motion our model generates the consequential scene dynamics. we investigate the generated interactive dynamics in a simple object collision scenario bottom and complex in-the-wild human-object interaction top . abstract predicting the dynamics of interacting objects is essential for both humans and intelligent systems. however existing approaches are limited to simpli\ufb01ed toy settings and lack generalizability to complex real-world environments. recent advances in generative models have enabled the prediction of state transitions based on interventions but focus on generating a single future state which neglects the continuous dynamics resulting from the interaction. to address this gap we propose interdyn a novel framework that generates videos of interactive dynamics given an initial frame and a control signal encoding the motion of a driving object or actor. our key insight is that large video equal contribution project lead generation models can act as both neural renderers and implicit physics simulators having learned interactive dynamics from large-scale video data. to effectively harness this capability we introduce an interactive control mechanism that conditions the video generation process on the motion of the driving entity. qualitative results demonstrate that interdyn generates plausible tempo- rally consistent videos of complex object interactions while generalizing to unseen objects. quantitative eval- uations show that interdyn outperforms baselines that focus on static state transitions. this work highlights the potential of leveraging video generative models as implicit physics engines. code and trained models will be released at https interdyn.is.tue.mpg.de . this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 12467 page 2 1. introduction humans have the remarkable ability to predict the future dynamics of observed systems intuitively. with just a sin- gle image we can anticipate and imagine how objects will move over time not only their motion but also their in- teractions with the environment and other elements in the scene. inferring this requires an advanced form of scene- level reasoning beyond merely recognizing the semantics and geometry of static elements it involves a deep physical and causal understanding of how each object will interact given the environment object properties and forces. there has been a growing interest in developing machine learning systems that emulate similar levels of dynamic un- derstanding given visual observations such as images or videos. early work 84 addressed this by \ufb01rst reconstruct- ing a 3d representation from the image then predicting fu- ture states with a physics simulator and \ufb01nally generating the output video with a rendering engine. this relies heav- ily on explicit reconstruction and simulation which is com- putationally intensive prone to errors and may not gener- alize well. more recent methods 2 24 39 46 49 leverage keypoint or latent representations within graph relational frameworks however they have only been trained and val- idated in over-simpli\ufb01ed synthetic environments showing limited generalizability to complex real-world scenarios. instead the advent of powerful generative models 1 5 16 54 66 opens new avenues for synthesizing interactions under complex scenarios. for example sudhakar et al. 70 recently proposed coshand a controllable image-to-image model based on stable diffusion 66 that infers state tran- sitions of an object. the task here is de\ufb01ned as follows given an image of a hand interacting with an object along- side a hand mask of the current frame and a mask of the hand at a future frame generate a modi\ufb01ed input image that satis\ufb01es the mask with realistic interactions. the challenge as in early intuitive physics works lies in accurately mod- eling how the objects will change after forces are applied. however we argue that static state transitions are insuf\ufb01- cient for this task as they fail to capture the continuous dy- namic processes inherent to the problem e.g. see fig. 2. in- vestigating interactive dynamics within a two-state setting is highly limiting since dynamics can extend beyond the period of direct contact for example predicting the mo- tion occurring while a person pours water requires a physi- cal understanding that goes beyond the state of the hand at a future frame. the driving force in this case the hand may interact with the system only brie\ufb02y but the system s sub- sequent dynamics continue according to physical laws and may even in\ufb02uence other parts via force propagation. in this paper we explore controllable synthesis of inter- active dynamics generating a video from an input image and a dynamic control signal e.g. a moving hand mask to model realistic object dynamics. in particular we propose coshand ours figure 2. state transition vs. dynamics. methods that gen- erate static state transitions i.e. predict a future image such as coshand 70 struggle to capture the inherent dynamic processes involved in human-object interactions. here we show a video se- quence where the motion continues beyond the interaction. interdyn a novel framework for synthesizing controllable dynamic interactions that leverages the physical and dy- namics knowledge of a large video generation model 5 . unlike prior approaches that rely on explicit physical simu- lation 84 or are constrained to static state transitions 70 we leverage video generation models to generate dynamic processes implicitly see fig. 1. speci\ufb01cally we extend stable video diffusion svd 5 with a dynamic control branch and \ufb01ne-tune it on diverse scenes enabling synthe- sis of complex interactions aligned with the control signal. we start our investigation by \ufb01ne-tuning interdyn on a simple synthetic scenario of cubes cylinders and spheres the clevrer dataset 97 . to control the motion we add a mask driving signal that manipulates the movement of some but not all of the objects in the scene. we then evalu- ate how the synthesized trajectories of uncontrolled objects change under various interactions including multiple ob- jects colliding with each other. this multi-object collision setting allows us to probe the physical understanding and causal effects of the video diffusion model and our qualita- tive experiments show interdyn s ability for counterfactual future prediction and physical force propagation. next we evaluate how the system performs in a dif\ufb01- cult real-world scenario such as human-object interaction hoi . here the dexterity of hand motions and the diver- sity of objects vastly increase the complexity of the prob- lem. we \ufb01ne-tune the model on a commonly used hoi video dataset 21 and compare with the state-of-the-art baseline coshand 70 as well as two text-control based interactive dynamics generation methods seer 23 and dynamicrafter 89 . we quantify our investigations using standard image and video metrics as well as a motion \ufb01- delity metric based on point tracking. interdyn surpasses the previous sota over 37.5 on lpips and 77 on fvd on the something-something-v2 ssv2 dataset 21 . our experiments also demonstrate diverse and physically plausi- ble generations of interactive dynamics probing into svd s understanding of physics and dynamics. 12468 page 3 in summary we present interdyn a framework that em- ploys video generative models to simulate object dynam- ics without explicit 3d reconstruction or physical simu- lation. we demonstrate how the inherent knowledge within video generation models can be leveraged to predict complex object interactions and movements over time im- plicitly modeling physical and causal dynamics. we per- form comprehensive experiments on multi-object collision datasets and hand-object manipulation datasets demon- strating the effectiveness of our approach. 2. related work modeling human-object interactions hoi . human- object interaction has been widely studied within the con- text of 3d reconstruction 17 18 27 28 73 88 94 where the goal is to recover realistic geometry of hands and objects. the \ufb01eld of 3d hoi synthesis has also re- ceived increasing attention including the generation of static 40 44 71 106 or dynamic 63 72 103 105 hand poses conditioned on 3d objects whole-body inter- actions 90 or more recently hand-object meshes given textual descriptions 10 15 57 86 96 . few works address hoi synthesis in the 2d domain. ganhand 14 predicts 3d hand shape and pose given an rgb image of an object while affordancediffusion 95 estimates a 2d hand using a diffusion model. kulal et al. 47 take as input an image of a human and a scene separately and generate a compos- ite image that positions the human with correct affordances. also relevant is hoidiffusion 101 in which a texture-less rendering of a 3d hand and object is converted to a real- istic image using a text description. most closely related to us is coshand 70 which takes as input an rgb im- age of a hand-object interaction a hand mask at the current state and the hand mask of the future state and generates an rgb image of the future state. unlike us they cannot gen- erate post-interaction object dynamics. importantly none of these works study dynamics generating instead discrete state transitions that fail to capture the nuanced temporally coherent behaviors observed in interactions. synthesizing causal physical relations from visual input. a growing body of work aims to model and predict physical causal effects from visual inputs such as images or videos. for example research in intuitive physics seeks to replicate the human-like non-mathematical understanding of physi- cal events e.g. by predicting future frames given an input video. early works like 22 48 train neural networks to as- sess the stability of block towers while 24 leverage prior physical knowledge formalized through partial differential equations pdes . other approaches investigate counterfac- tual reasoning by leveraging graph neural networks 2 39 . wu et al. 82 84 explore the use of an inverse rendering approach extracting geometry and physical properties from the video which are then coupled with a physics simulator and a rendering engine to generate the future frames. other works 81 incorporate interaction networks 3 to approx- imate physical systems from video data. these approaches are often limited to simpli\ufb01ed synthetic datasets and strug- gle to generalize to real-world scenarios. recent methods have started to combine language mod- els with physical engines. liu et al. 51 ground a large lan- guage model using a computational physics engine while gao et al. 19 show that \ufb01ne-tuning a vision-language model vlm on annotated datasets of physical concepts improves its understanding of physical interactions. closely related to our work is physgen 53 which trains an image- to-video model that conditions the video generation on physics parameters e.g. force or torque . however the model relies on a dynamics simulator to generate motion and its application is limited to rigid objects. a related but tangential line of work focuses on identifying and gener- ating the effects of objects on their surroundings. for ex- ample omnimatte 56 introduces the problem of identi- fying all parts of a scene in\ufb02uenced by an object given a video and a mask of the object. similarly lu et al. 55 pro- pose to re-time the motion of different subjects in a scene while maintaining realistic interactions with the environ- ment. actanywhere 62 generates videos with plausible human-scene interactions taking a masked video of a per- son and a background image as input. these works address the problem of synthesizing realistic interactions within a scene however lack \ufb01ne-grained control. controllable video generation. video generation has ad- vanced signi\ufb01cantly in recent years with diffusion models leading to substantial improvements in unconditional 33 99 text-based 1 5 6 12 20 23 26 32 37 68 76 85 89 92 104 and image-based 1 5 20 25 78 89 generation. these advances have raised the question of how to incorporate more nuanced control into video gen- eration. some text-to-video approaches are trained by in- \ufb02ating text-to-image t2v models 8 13 25 26 38 85 and can thus be integrated with conditional t2v mod- els such as controlnet 100 or t2v-adapter 60 . con- trol can also be achieved by conditioning on trajecto- ries 58 87 98 or bounding-boxes 77 \ufb01ne-tuning on ap- propriate datasets. videocomposer 78 incorporates multi- ple condition types including text depth style and tempo- ral conditions via motion vectors. camera motion control has also been explored with animatediff 79 employing lora 34 modules to control camera movement while motionctrl 80 and cameractrl 29 directly embed the camera information for more precise control. additionally several works target human animation from a pose con- trol signal such as dreampose 42 magicpose 91 and animateanyone 35 but do not account for interactions. 12469 page 4 3. controllable interactive dynamics video diffusion models such as 5 54 have demonstrated impressive performance in generating videos from text or images and have even shown potential in tasks that re- quire 3d understanding when properly \ufb01ne-tuned 36 75 . trained on millions of videos we hypothesize that these models also possess implicit knowledge of complex inter- active dynamics such as those that appear when humans interact with objects. out of the box however they lack a precise control mechanism often relying solely on textual inputs or requiring careful selection of the starting frame. task. given an input image x r1 h w 3 and a driv- ing motion in the form of a pixel-wise corresponding control signal c rn h w 3 we task interdyn with generating a video sequence y rn h w 3 depicting plausible object dynamics. through this task we aim to learn the conditional distribution between a driving motion such as that of a human hand and the consequent motion of manip- ulated objects. in other words the model needs to synthe- size plausible object movement and appearance without any indication other than the driving motion while maintaining physical and visual consistency with the input image. stable video diffusion. we extend stable video diffu- sion 5 svd to enable controllable interactive dynam- ics and explore the versatility of this model across a range of scenarios. svd is a publicly available u-net-based la- tent diffusion model 66 that extends stable diffusion 2.1 to video generation by interleaving the network with tem- poral layers. given a static input image of a scene svd denoises a sequence of n frames y rn h w 3 to gen- erate a video that follows the initial frame. the input image is fed into the denoising u-net by concatenating its latent to each of the frames in the noised input and by supplying its clip 64 embedding to the u-net s cross-attention lay- ers. in addition svd is conditioned on the video s fps and motion id where the motion id represents the amount of motion in the video. we found a motion id of 40 to align well with our frozen svd prior. control. interdyn extends svd with an additional con- trol signal c rn h w 3 by integrating a controlnet- like branch 100 . an overview of our pipeline is pre- sented in fig. 3. the svd weights remain frozen to pre- serve its learned dynamics prior. following 100 we in- troduce a trainable copy of the svd encoder e connected to svd s frozen decoder via skip connections and mod- ulated by zero-initialized convolutions. we use a small cnn e to encode the control signal c into the latent space which is then added to the noisy input latent that is passed to the controlnet encoder. similar to svd the con- trol branch interleaves convolutional spatial and temporal blocks enabling interdyn to process the control signal in a temporal-aware manner. this helps interdyn to be robust to control encode encode repeat channel concat svd decode frozen fine-tuned figure 3. overview of interdyn. given an input image depicting a scene such as a hand holding a remote and a driving motion such as a sequence of binary hand masks interdyn generates a video depicting plausible hand and object dynamics. crucially interdyn receives no control signal for the object. through this setup we probe and assess the implicit knowledge of large video generation models on complex interactive dynamics. we use sta- ble video diffusion svd as our frozen backbone and \ufb01ne-tune a separate control signal encoder. videos are iteratively denoised over t timesteps starting from gaussian noise \u03c9 n 0 i . noisy control signals see fig. 6. we opt for binary masks as conditioning signal due to their accessibility. however our method can be extended to incorporate diverse types of signals. we \ufb01nd that for hand-object interactions the type of conditioning signal does not signi\ufb01cantly impact perfor- mance see appendix a and tab. s1. inference. during inference we start from an input image control signal sequence and randomly sampled gaussian noise \u03c9 n 0 i . through iteratively applying interdyn over t denoising timesteps we generate a video y depict- ing plausible hand and object dynamics aligned with the control signal. 4. experiments the primary goal of this work is to synthesize scene- level interactive dynamics by leveraging the implicit phys- ical understanding of a pre-trained video generative model. we begin by probing the model s ability to predict phys- ically plausible outcomes within simulated environments speci\ufb01cally using the clevrer dataset 97 . we test force propagation amongst uncontrolled objects and exam- ine counterfactual future prediction by generating videos for one input image with different control signals. moti- vated by promising results we extend our investigation to complex real-world hand-object interaction scenarios us- ing the something-something-v2 ssv2 dataset 21 con- ducting comprehensive comparisons with existing baselines that pursue similar objectives. additionally we showcase diverse physical examples to demonstrate the capabilities of interdyn in generating realistic interactive dynamics. 12470 page 5 input image a force propagation. interdyn generates force propagation to and amongst uncontrolled objects cyan cylinder pink cylinder and pink cube . w o brown cylinder w brown cylinder input image generation 1 generation 2 b counterfactual dynamics. two generations of interdyn on the same input image 1 w brown-cylinder control and 2 w o brown-cylinder control. figure 4. qualitative investigation on the clevrer dataset. given an input image and the driving motion of one or two objects our model predicts the future interactive dynamics of multiple elements in the scene. the driving motion is given in the form of semantic mask sequences. the generated object motions are highlighted with a red-line trajectory. note that our model can generate videos with force propagation across multiple uncontrolled objects top and can generate multiple futures bottom . zoom in for details. 4.1. implementation we initialize interdyn with the 14-frame image-to-video weights of svd 5 . during training we use the adam optimizer 45 with a learning rate of 1 10 5. we use the edm framework 43 with noise distribution log \u03c9 n 0.7 1.62 . we train on two 80gb h100 gpus with a per-gpu batch size of 4. video length and fps de\ufb01ne the temporal resolution of dynamics to balance short- and long-range events we subsample videos to 7 fps. to fa- cilitate classi\ufb01er-free guidance 31 we randomly drop the input image with a probability of 5 . at inference we ap- ply the euler scheduler 43 over 50 denoising timesteps. 4.2. metrics we evaluate interdyn on image quality spatio-temporal similarity and motion \ufb01delity. image quality metrics are computed frame-wise. all metrics are reported excluding the \ufb01rst frame as it serves as input conditioning. image quality. we report the structural similarity index measure ssim 79 peak signal-to-noise ratio psnr learned perceptual image patch similarity lpips 102 fr echet inception distance fid 30 and unbiased kernel inception distance kid 4 . spatio-temporal similarity. to assess the spatio-temporal perceptual similarity between the ground truth and the gen- erated video distributions we use the fr echet video dis- tance fvd and unbiased kernel video distance kvd proposed in 74 . we use the implementation of 69 . motion fidelity. through interdyn we do not have ex- plicit control over object dynamics which means that the pixel alignment of an object in the generated and ground truth video is only guaranteed in the starting frame. in this case comparing generated object motion to the ground truth naively might misrepresent the true quality of object motion over time. therefore we adapt the motion fidelity mf metric proposed by yatim et al. 93 which measures the similarity between point-tracking trajectories. to compute the metric for any video we obtain a mask of the object in the starting frame sample 100 points on the object and track these throughout both the ground truth and generated video using cotracker3 41 . given the resulting two sets of tracklets t \u03b51 . . . \u03b5n t \u03b51 . . . \u03b5m the motion \ufb01delity metric is de\ufb01ned as 1 m \u03c9 t max \u03c9 t corr \u03b5 \u03b5 1 n \u03c9 t max \u03c9 t corr \u03b5 \u03b5 1 with the correlation between two tracklets corr \u03b5 \u03b5 50 corr \u03b5 \u03b5 1 f f k 1 vx ka vx k vy k vy k vx k 2 vy k 2 vx k 2 vy k 2 2 where vx k vy k vx k vy k are the kth frame displacement of tracklets \u03b5 \u03b5 respectively. if there are less than 100 points to query on the object due to it being too small we do not consider the video for the motion \ufb01delity metric. 12471 page 6 4.3. probing dynamics with object collision events here we \ufb01ne-tune interdyn on an object collision dataset to probe its ability to generate realistic object interactions. qualitatively we review whether interdyn can produce plausible object motion for uncontrolled objects given the motion of objects entering the scene. in addition we ex- amine whether interdyn can generate counterfactual videos for the same input image but different control signals. dataset. we use clevrer 97 which provides 20 000 videos of colliding objects with annotated segmentation masks and metadata on collision events. we construct a control signal for objects entering the scene and aim to use interdyn to generate the motion of the objects that are al- ready present upon collision. stationary objects do not re- ceive any form of control signal. colored masks help the model distinguish unique objects. the frames are cropped and scaled to 320 448 and we only sample input frames before collisions between objects in the scene to maximize interdyn s exposure to interactive dynamics. force propagation. interdyn can generate force propaga- tion dynamics between a controlled object and an uncon- trolled object as well as amongst uncontrolled objects as illustrated in fig. 4a. here the red cylinder at the top left is the driving force. it collides with the uncontrolled blue cylinder which then collides with the uncontrolled pur- ple cylinder in turn striking the uncontrolled purple cube on the far right. point-tracking trajectories display how collisions alter each object s position. this suggests that interdyn possesses an implicit understanding of physical interactions enabling it to generate plausible dynamics. counterfactual dynamics. by altering the control signal interdyn can simulate counterfactual scenarios for the same input image as shown in fig. 4b. in generation 1 the gray cylinder controlled collides with the stationary red sphere uncontrolled causing it to move it is later struck by the brown cylinder controlled altering its path once again. in generation 2 removing the brown cylinder lets the red sphere continue along its original trajectory consis- tent with expectations. crucially there is no control signal for the red sphere throughout the sequence its motion and trajectory are entirely generated by interdyn. probing interdyn on clevrer highlights its ability to generate interactive dynamics for objects within a simple synthetic environment. we provide additional results in video format on our webpage. 4.4. generating human-object interactions for this experiment we \ufb01ne-tune interdyn on a human- action dataset focused on hand-object interaction. we en- code human movement as a sequence of pixel-aligned bi- nary hand masks and task interdyn to generate a video with hand movements and corresponding object dynamics. dataset. we use something-something-v2 ssv2 which provides 220 847 videos of humans performing basic ac- tions with everyday objects. it contains actions such as pushing something from left to right squeezing something and lifting something with something on it . this dataset allows us to train interdyn at a larger scale and compare with our baseline coshand 70 . we train one version of interdyn at the same resolution as coshand 256 256 and a second version at 256 384 which aligns better with the dynamic prior of svd. we generate binary hand masks by prompting segment anything 2 sam2 65 with hand bounding boxes pro- vided by the something-else dataset 59 . similar to our baseline coshand we exclude the pretending class. additionally we remove all something is done behind something classes since the object would be out of view of the camera. we remove videos smaller than the target resolution length ensure the hand and objects are continu- ously visible and crop videos larger than the target reso- lution while centering the object. we include videos with- out obvious motion and state transition such as holding something . since bounding box annotations are only provided for the train split 79 043 samples for 256 384 and 104 260 for 256 256 and validation split 8 667 sam- ples for 256 384 and 11 229 for 256 256 we report all evaluation metrics on the validation split. coshand frame-wise coshand auto-regressive interdyn ours figure 5. qualitative comparison. a two-state approach such as coshand 70 struggles with post-interaction object dynamics. ground truth interdyn ours figure 6. robustness to noise. sam2 outputs noisy coarse masks for frames with considerable motion blur orange red . despite this interdyn can generate plausible hand and object dynamics. 12472 page 7 figure 7. diverse generation of interactive dynamics. we show multiple challenging examples such as from top to bottom interacting with articulated objects pouring liquid letting an object fall squeezing a highly deformable or collapsible object interacting with re\ufb02ective objects tilting a ridged object or stacking objects. zoom in for details. 12473 page 8 method ssim psnr lpips fid kid fvd kvd motion fidelity 93 seer 23 0.418 10.71 0.588 33.35 0.030 287.46 81.31 dynamicrafter 89 17.48 0.014 204.11 31.81 coshand-independent 70 0.615 16.87 0.313 2.95 0.002 91.18 19.24 0.432 coshand-autoregressive 70 0.531 14.92 0.408 12.66 0.012 90.30 13.68 0.570 ours 256 256 0.664 18.60 0.260 4.95 0.004 19.27 1.99 0.633 ours 256 384 0.680 19.04 0.252 5.07 0.004 22.22 2.09 0.641 table 1. quantitative comparison on something-something-v2. we compare against two language-instructed video generation methods seer 23 and dynamicrafter 89 and two video extensions of our baseline coshand 70 . we report results for interdyn at two resolutions 256 256 matching coshand and 256 384 matching svd s prior . methods denoted with do not use ssv2 as their training dataset. baselines. to generate videos using coshand 70 rather than state transitions we run coshand in a frame-by-frame approach coshand-independent and an auto-regressive approach coshand-autoregressive . in the frame-by- frame variant each future frame is independently predicted from the initial frame and its corresponding mask \u02c6xt 1 coshand x0 h0 ht 1 t 0 13 where h0 and ht 1 denote masks and x0 the initial frame. in the auto-regressive variant we use coshand to generate video frames sequentially \u02c6xt 1 coshand \u02c6xt ht ht 1 t 0 13 with \u02c6xt being the previously generated frame and \u02c6x0 x0. we also compare against two language-instructed video generation methods seer 23 and dynamicrafter 89 which we prompt with a \ufb01rst frame and its corresponding ssv2 class label as instruction. note that dynamicrafter was trained with a random video frame as conditioning i.e. not always the \ufb01rst frame. at inference its generated videos are thus not strictly a continuation of the condition- ing which precludes frame-aligned metrics. we compare interdyn against these baselines in tab. 1. quantitative analysis. the frame-by-frame variant of coshand achieves high image quality but struggles with temporal coherence and motion \ufb01delity. qualitatively we notice that object locations are inconsistent across frames. in contrast the auto-regressive variant improves object mo- tion \ufb01delity but suffers from lower frame-wise image qual- ity due to error propagation. both variants fail in scenarios requiring accurate post-interaction dynamics such as when objects continue moving after being released from direct hand contact as shown in fig. 5. our method interdyn 256 384 achieves the best overall performance surpassing our baseline coshand in spatio-temporal dynamics motion \ufb01delity and all but two image quality metrics. we hypothesize that this might be due to two reasons 1 we use a frozen u-net while coshand \ufb01ne-tunes its model on ssv2 so coshand might generate frames closer to the ssv2 distribution and 2 when svd was trained it was initialized with sd weights as spatial layers and then \ufb01ne-tuned over multiple stages this might have degraded its spatial prior and by extension the quality of produced images compared to coshand. fine-tuned on noisy masks and leveraging its temporal- aware control branch interdyn can interpret a noisy control sequence e.g. when sam2 produces a coarse and noisy hand mask sequence interdyn generates detailed hands in- cluding individual \ufb01ngers see fig. 6. though not always consistent interdyn is capable of depicting post-interaction dynamics such as rolling or sliding objects. qualitative analysis. we present diverse qualitative re- sults generated by interdyn in fig. 7. row 1 shows how interdyn generates the articulated motion of an object. row 2 showcases pouring water into a glass note how the water level increases over time. row 3 demonstrates an object be- ing dropped moving out of frame when falling and rolling back in frame once hitting the \ufb02oor featuring realistic mo- tion blur synthesis. rows 4 and 5 illustrate how interdyn handles squeezing interactions the rubber and the spring are compressed and restored accordingly. row 6 demon- strates an understanding of physical size and distance to the camera as the phone moves closer to the viewer. these results highlight the complexity that interdyn can handle implying its generalization ability and future potential as an implicit yet generalized physical simulator and renderer. 5. conclusion we introduced interdyn a framework that generates videos of interactive dynamics using large video generation models as implicit physics simulators. by incorporating an interac- tive control mechanism interdyn produces plausible tem- porally consistent videos of object interactions including complex human-object interactions while generalizing to unseen objects. our evaluations demonstrate that interdyn effectively captures continuous motion and subsequent dy- namics outperforming baselines that focus on single future states. this work highlights the potential of using video generative models as physics simulators without explicit re- construction opening new avenues for future research. 12474 page 9 acknowledgements we thank benjamin pellkofer for it sup- port peter kulits for discussions tomasz niewiadomski for cod- ing support and allan d. jepson artur grigorev soubhik sanyal omid taheri angjoo kanazawa for feedback. dt is supported by the erc starting grant project stripes 101165317 . disclosure https \ufb01les.is.tue.mpg.de black coi cvpr 2025.txt dt has received a research gift fund from google. references 1 omer bar-tal hila chefer omer tov charles her- rmann roni paiss shiran zada ariel ephrat junhwa hur guanghui liu amit raj et al. lumiere a space-time dif- fusion model for video generation. in international con- ference on computer graphics and interactive techniques siggraph pages 1 11 2024. 2 3 2 fabien baradel natalia neverova julien mille greg mori and christian wolf. cophy counterfactual learning of physical dynamics. in international conference on learn- ing representations iclr 2020. 2 3 3 peter battaglia razvan pascanu matthew lai danilo jimenez rezende et al. interaction networks for learning about objects relations and physics. in conference on neu- ral information processing systems neurips 2016. 3 4 miko aj bi nkowski danica j. sutherland michael arbel and arthur gretton. demystifying mmd gans. in inter- national conference on learning representations iclr 2018. 5 5 andreas blattmann tim dockhorn sumith kulal daniel mendelevitch maciej kilian dominik lorenz yam levi zion english vikram voleti adam letts et al. stable video diffusion scaling latent video diffusion models to large datasets. arxiv 2311.15127 2023. 2 3 4 5 6 andreas blattmann robin rombach huan ling tim dockhorn seung wook kim sanja fidler and karsten kreis. align your latents high-resolution video synthe- sis with latent diffusion models. in computer vision and pattern recognition cvpr pages 22563 22575 2023. 3 7 tim brooks aleksander holynski and alexei a. efros. instructpix2pix learning to follow image editing instruc- tions. in computer vision and pattern recognition cvpr pages 18392 18402 2023. 14 8 shengqu cai duygu ceylan matheus gadelha chun- hao paul huang tuanfeng yang wang and gordon wet- zstein. generative rendering controllable 4d-guided video generation with 2d diffusion models. in computer vision and pattern recognition cvpr pages 7611 7620 2024. 3 9 zhe cao tomas simon shih-en wei and yaser sheikh. realtime multi-person 2d pose estimation using part af\ufb01n- ity \ufb01elds. in computer vision and pattern recognition cvpr pages 7291 7299 2017. 14 10 junuk cha jihyeon kim jae shin yoon and seungryul baek. text2hoi text-guided 3d motion generation for hand-object interaction. in computer vision and pattern recognition cvpr pages 1577 1585 2024. 3 11 yu-wei chao wei yang yu xiang pavlo molchanov ankur handa jonathan tremblay yashraj s. narang karl van wyk umar iqbal stan birch\ufb01eld et al. dexycb a benchmark for capturing hand grasping of objects. in computer vision and pattern recognition cvpr pages 9044 9053 2021. 14 12 haoxin chen menghan xia yingqing he yong zhang xiaodong cun shaoshu yang jinbo xing yaofang liu qifeng chen xintao wang et al. videocrafter1 open diffusion models for high-quality video generation. arxiv 2310.19512 2023. 3 13 weifeng chen yatai ji jie wu hefeng wu pan xie ji- ashi li xin xia xuefeng xiao and liang lin. control-a- video controllable text-to-video generation with diffusion models. arxiv 2305.13840 2023. 3 14 enric corona albert pumarola guillem alenya francesc moreno-noguer and gr egory rogez. ganhand predicting human grasp affordances in multi-object scenes. in com- puter vision and pattern recognition cvpr pages 5031 5041 2020. 3 15 christian diller and angela dai. cg-hoi contact-guided 3d human-object interaction generation. in computer vi- sion and pattern recognition cvpr pages 19888 19901 2024. 3 16 patrick esser sumith kulal andreas blattmann rahim entezari jonas m uller harry saini yam levi dominik lorenz axel sauer frederic boesel et al. scaling recti- \ufb01ed \ufb02ow transformers for high-resolution image synthesis. in international conference on machine learning icml 2024. 2 17 zicong fan omid taheri dimitrios tzionas muhammed kocabas manuel kaufmann michael j. black and ot- mar hilliges. arctic a dataset for dexterous bimanual hand-object manipulation. in computer vision and pattern recognition cvpr pages 12943 12954 2023. 3 18 zicong fan maria parelli maria eleni kadoglou xu chen muhammed kocabas michael j. black and otmar hilliges. hold category-agnostic 3d reconstruction of interacting hands and objects from video. in computer vision and pattern recognition cvpr pages 494 504 2024. 3 19 jensen gao bidipta sarkar fei xia ted xiao jiajun wu brian ichter anirudha majumdar and dorsa sadigh. phys- ically grounded vision-language models for robotic manip- ulation. in international conference on robotics and au- tomation icra pages 12462 12469 2024. 3 20 rohit girdhar mannat singh andrew brown quentin du- val samaneh azadi sai saketh rambhatla akbar shah xi yin devi parikh and ishan misra. emu video factor- izing text-to-video generation by explicit image condition- ing. in european conference on computer vision eccv 2024. 3 21 raghav goyal samira ebrahimi kahou vincent michal- ski joanna materzynska susanne westphal heuna kim valentin haenel ingo fruend peter yianilos moritz mueller-freitag et al. the something something video database for learning and evaluating visual common sense. in international conference on computer vision iccv pages 5842 5850 2017. 2 4 12475 page 10 22 oliver groth fabian b. fuchs ingmar posner and andrea vedaldi. shapestacks learning vision-based physical in- tuition for generalised object stacking. in european con- ference on computer vision eccv pages 702 717 2018. 3 23 xianfan gu chuan wen weirui ye jiaming song and yang gao. seer language instructed video prediction with latent diffusion models. in international conference on learning representations iclr 2024. 2 3 8 17 24 vincent le guen and nicolas thome. disentangling physi- cal dynamics from unknown factors for unsupervised video prediction. in computer vision and pattern recognition cvpr pages 11471 11481 2020. 2 3 25 xun guo mingwu zheng liang hou yuan gao yufan deng pengfei wan di zhang yufan liu weiming hu zhengjun zha et al. i2v-adapter a general image-to- video adapter for diffusion models. in international con- ference on computer graphics and interactive techniques siggraph 2024. 3 26 yuwei guo ceyuan yang anyi rao zhengyang liang yaohui wang yu qiao maneesh agrawala dahua lin and bo dai. animatediff animate your personalized text-to-image diffusion models without speci\ufb01c tuning. arxiv 2307.04725 2023. 3 27 yana hasson g ul varol dimitrios tzionas igor kale- vatykh michael j. black ivan laptev and cordelia schmid. learning joint reconstruction of hands and manip- ulated objects. in computer vision and pattern recognition cvpr pages 11807 11816 2019. 3 28 yana hasson g ul varol cordelia schmid and ivan laptev. towards unconstrained joint hand-object reconstruction from rgb videos. in international conference on 3d vi- sion 3dv pages 659 668 2021. 3 29 hao he yinghao xu yuwei guo gordon wetzstein bo dai hongsheng li and ceyuan yang. camerac- trl enabling camera control for text-to-video generation. arxiv 2404.02101 2024. 3 30 martin heusel hubert ramsauer thomas unterthiner bernhard nessler and sepp hochreiter. gans trained by a two time-scale update rule converge to a local nash equi- librium. in conference on neural information processing systems neurips pages 6629 6640 2017. 5 31 jonathan ho and tim salimans. classi\ufb01er-free diffusion guidance. arxiv 2207.12598 2022. 5 32 jonathan ho william chan chitwan saharia jay whang ruiqi gao alexey gritsenko diederik p. kingma ben poole mohammad norouzi david j. fleet et al. ima- gen video high de\ufb01nition video generation with diffusion models. arxiv 2210.02303 2022. 3 33 jonathan ho tim salimans alexey gritsenko william chan mohammad norouzi and david j. fleet. video dif- fusion models. in conference on neural information pro- cessing systems neurips pages 8633 8646 2022. 3 34 edward j. hu yelong shen phillip wallis zeyuan allen- zhu yuanzhi li shean wang lu wang and weizhu chen. lora low-rank adaptation of large language mod- els. in international conference on learning representa- tions iclr 2022. 3 35 li hu xin gao peng zhang ke sun bang zhang and liefeng bo. animate anyone consistent and controllable image-to-video synthesis for character animation. in com- puter vision and pattern recognition cvpr pages 8153 8163 2024. 3 36 wenbo hu xiangjun gao xiaoyu li sijie zhao xi- aodong cun yong zhang long quan and ying shan. depthcrafter generating consistent long depth sequences for open-world videos. arxiv 2409.02095 2024. 4 37 yaosi hu chong luo and zhenzhong chen. make it move controllable image-to-video generation with text descriptions. in computer vision and pattern recognition cvpr pages 18219 18228 2022. 3 38 zhihao hu and dong xu. videocontrolnet a motion- guided video-to-video translation framework by using dif- fusion model with controlnet. arxiv 2307.14073 2023. 3 39 steeven janny fabien baradel natalia neverova madiha nadri greg mori and christian wolf. filtered-cophy unsupervised learning of counterfactual physics in pixel space. in international conference on learning represen- tations iclr 2022. 2 3 40 hanwen jiang shaowei liu jiashun wang and xiaolong wang. hand-object contact consistency reasoning for hu- man grasps generation. in international conference on computer vision iccv pages 11107 11116 2021. 3 41 nikita karaev iurii makarov jianyuan wang natalia neverova andrea vedaldi and christian rupprecht. co- tracker3 simpler and better point tracking by pseudo- labelling real videos. in european conference on computer vision eccv pages 18 35 2024. 5 42 johanna karras aleksander holynski ting-chun wang and ira kemelmacher-shlizerman. dreampose fashion image-to-video synthesis via stable diffusion. in inter- national conference on computer vision iccv pages 22623 22633 2023. 3 43 tero karras miika aittala timo aila and samuli laine. elucidating the design space of diffusion-based generative models. conference on neural information processing sys- tems neurips 35 26565 26577 2022. 5 44 hyeonwoo kim sookwan han patrick kwon et al. be- yond the contact discovering comprehensive affordance for 3d objects from pre-trained 2d diffusion models. in european conference on computer vision eccv pages 400 419 2024. 3 45 diederik p. kingma and jimmy ba. adam a method for stochastic optimization. in international conference on learning representations iclr 2015. 5 46 thomas kipf ethan fetaya kuan-chieh wang max welling and richard zemel. neural relational inference for interacting systems. in international conference on ma- chine learning icml pages 2693 2702 2018. 2 47 sumith kulal tim brooks alex aiken jiajun wu jimei yang jingwan lu alexei a. efros and krishna kumar singh. putting people in their place affordance-aware hu- man insertion into scenes. in computer vision and pattern recognition cvpr pages 17089 17099 2023. 3 12476 page 11 48 adam lerer sam gross and rob fergus. learning phys- ical intuition of block towers by example. in international conference on machine learning icml pages 430 438 2016. 3 49 yunzhu li antonio torralba anima anandkumar dieter fox and animesh garg. causal discovery in physical sys- tems from videos. conference on neural information pro- cessing systems neurips 2020. 2 50 ce liu antonio torralba william t. freeman fr edo du- rand and edward h. adelson. motion magni\ufb01cation. transactions on graphics tog 24 3 519 526 2005. 5 51 ruibo liu jason wei shixiang shane gu te-yen wu soroush vosoughi claire cui denny zhou and andrew m dai. mind s eye grounded language model reasoning through simulation. in international conference on learn- ing representations iclr 2023. 3 52 shilong liu zhaoyang zeng tianhe ren feng li hao zhang jie yang chunyuan li jianwei yang hang su jun zhu et al. grounding dino marrying dino with grounded pre-training for open-set object detection. arxiv 2303.05499 2023. 14 53 shaowei liu zhongzheng ren saurabh gupta and shen- long wang. physgen rigid-body physics-grounded image-to-video generation. in european conference on computer vision eccv pages 360 378 2024. 3 54 yixin liu kai zhang yuan li zhiling yan chujie gao ruoxi chen zhengqing yuan yue huang hanchi sun jianfeng gao et al. sora a review on background technol- ogy limitations and opportunities of large vision models. arxiv 2402.17177 2024. 2 4 55 erika lu forrester cole tali dekel weidi xie an- drew zisserman david salesin william t. freeman and michael rubinstein. layered neural rendering for retiming people in video. transactions on graphics tog 39 6 1 14 2020. 3 56 erika lu forrester cole tali dekel andrew zisserman william t. freeman and michael rubinstein. omnimatte associating objects and their effects in video. in computer vision and pattern recognition cvpr pages 4507 4515 2021. 3 57 jiaxin lu hao kang haoxiang li bo liu yiding yang qixing huang and gang hua. ugg uni\ufb01ed generative grasping. in european conference on computer vision eccv pages 414 433 2024. 3 58 wan-duo kurt ma john p lewis and w. bastiaan kleijn. trailblazer trajectory control for diffusion-based video generation. in international conference on computer graphics and interactive techniques siggraph pages 1 11 2023. 3 59 joanna materzynska tete xiao roei herzig huijuan xu xiaolong wang and trevor darrell. something-else compositional action recognition with spatial-temporal in- teraction networks. in computer vision and pattern recog- nition cvpr pages 1049 1059 2020. 6 60 chong mou xintao wang liangbin xie yanze wu jian zhang zhongang qi and ying shan. t2i-adapter learn- ing adapters to dig out more controllable ability for text-to- image diffusion models. in aaai conference on arti\ufb01cial intelligence pages 4296 4304 2024. 3 61 franziska mueller micah davis florian bernard olek- sandr sotnychenko mickeal verschoor miguel a. otaduy dan casas and christian theobalt. real-time pose and shape reconstruction of two interacting hands with a sin- gle depth camera. transactions on graphics tog 38 4 1 13 2019. 14 62 boxiao pan zhan xu chun-hao paul huang krishna ku- mar singh yang zhou leonidas j. guibas and jimei yang. actanywhere subject-aware video background genera- tion. in conference on neural information processing sys- tems neurips 2024. 3 63 georgios paschalidis romana wilschut dimitrije anti c omid taheri and dimitrios tzionas. 3d whole-body grasp synthesis with directional controllability. in international conference on 3d vision 3dv 2025. 3 64 alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sastry amanda askell pamela mishkin jack clark et al. learn- ing transferable visual models from natural language super- vision. in international conference on machine learning icml pages 8748 8763 2021. 4 65 nikhila ravi valentin gabeur yuan-ting hu ronghang hu chaitanya ryali tengyu ma haitham khedr roman r adle chlo e rolland laura gustafson eric mintun junt- ing pan kalyan vasudev alwala nicolas carion chao- yuan wu ross b. girshick piotr doll ar and christoph feichtenhofer. sam 2 segment anything in images and videos. arxiv 2408.00714 2024. 6 14 66 robin rombach andreas blattmann dominik lorenz patrick esser and bj orn ommer. high-resolution image synthesis with latent diffusion models. in computer vi- sion and pattern recognition cvpr pages 10684 10695 2022. 2 4 14 67 javier romero dimitrios tzionas and michael j. black. embodied hands modeling and capturing hands and bod- ies together. transactions on graphics tog 36 6 2022. 14 68 uriel singer adam polyak thomas hayes xi yin jie an songyang zhang qiyuan hu harry yang oron ashual oran gafni et al. make-a-video text-to-video genera- tion without text-video data. in international conference on learning representations iclr 2023. 3 69 ivan skorokhodov sergey tulyakov and mohamed elho- seiny. stylegan-v a continuous video generator with the price image quality and perks of stylegan2. in computer vision and pattern recognition cvpr pages 3626 3636 2022. 5 70 sruthi sudhakar ruoshi liu basile van hoorick carl vondrick and richard zemel. controlling the world by sleight of hand. in european conference on computer vi- sion eccv pages 414 430 2024. 2 3 6 8 14 17 71 omid taheri nima ghorbani michael j. black and dim- itrios tzionas. grab a dataset of whole-body human grasping of objects. in european conference on computer vision eccv pages 581 600 2020. 3 12477 page 12 72 omid taheri yi zhou dimitrios tzionas yang zhou duygu ceylan soren pirk and michael j. black. grip generating interaction poses using latent consistency and spatial cues. in international conference on 3d vision 3dv pages 933 943 2024. 3 73 tze ho elden tse kwang in kim ales leonardis and hyung jin chang. collaborative learning for hand and object reconstruction with attention-guided graph convolu- tion. in computer vision and pattern recognition cvpr pages 1664 1674 2022. 3 74 thomas unterthiner sjoerd van steenkiste karol kurach rapha el marinier marcin michalski and sylvain gelly. fvd a new metric for video generation. in interna- tional conference on learning representations workshops iclrw 2019. 5 75 vikram voleti chun-han yao mark boss adam letts david pankratz dmitry tochilkin christian laforte robin rombach and varun jampani. sv3d novel multi-view synthesis and 3d generation from a single image using la- tent video diffusion. in european conference on computer vision eccv pages 439 457 2025. 4 14 76 jiuniu wang hangjie yuan dayou chen yingya zhang xiang wang and shiwei zhang. modelscope text-to-video technical report. arxiv 2308.06571 2023. 3 77 jiawei wang yuchen zhang jiaxin zou yan zeng guo- qiang wei liping yuan and hang li. boximator gen- erating rich and controllable motions for video synthesis. in international conference on machine learning icml 2024. 3 78 xiang wang hangjie yuan shiwei zhang dayou chen jiuniu wang yingya zhang yujun shen deli zhao and jingren zhou. videocomposer compositional video syn- thesis with motion controllability. in conference on neural information processing systems neurips 2024. 3 79 zhou wang alan c. bovik hamid r. sheikh and eero p. simoncelli. image quality assessment from error visibil- ity to structural similarity. transactions on image process- ing tip 13 4 600 612 2004. 3 5 80 zhouxia wang ziyang yuan xintao wang yaowei li tianshui chen menghan xia ping luo and ying shan. motionctrl a uni\ufb01ed and \ufb02exible motion controller for video generation. in transactions on graphics tog pages 1 11 2024. 3 81 nicholas watters daniel zoran theophane weber peter battaglia razvan pascanu and andrea tacchetti. visual interaction networks learning a physics simulator from video. in conference on neural information processing systems neurips pages 4539 4547 2017. 3 82 jiajun wu ilker yildirim joseph j. lim bill freeman and joshua b. tenenbaum. galileo perceiving physical ob- ject properties by integrating a physics engine with deep learning. in conference on neural information processing systems neurips pages 127 135 2015. 3 83 jiajun wu joseph j. lim hongyi zhang joshua b. tenen- baum and william t. freeman. physics 101 learning physical object properties from unlabeled videos. in british machine vision conference bmvc 2016. 84 jiajun wu erika lu pushmeet kohli bill freeman and josh tenenbaum. learning to see physics via visual de- animation. in conference on neural information process- ing systems neurips pages 153 164 2017. 2 3 85 jay zhangjie wu yixiao ge xintao wang stan weixian lei yuchao gu yufei shi wynne hsu ying shan xiaohu qie and mike zheng shou. tune-a-video one-shot tun- ing of image diffusion models for text-to-video generation. in international conference on computer vision iccv pages 7623 7633 2023. 3 86 qianyang wu ye shi xiaoshui huang jingyi yu lan xu and jingya wang. thor text to human-object interac- tion diffusion via relation intervention. arxiv 2403.11208 2024. 3 87 weijia wu zhuang li yuchao gu rui zhao yefei he david junhao zhang mike zheng shou yan li tingting gao and di zhang. draganything motion control for any- thing using entity representation. in european conference on computer vision eccv pages 331 348 2025. 3 88 xianghui xie bharat lal bhatnagar jan eric lenssen and gerard pons-moll. template free reconstruction of human- object interaction with procedural interaction generation. in computer vision and pattern recognition cvpr pages 10003 10015 2024. 3 89 jinbo xing menghan xia yong zhang haoxin chen wangbo yu hanyuan liu gongye liu xintao wang ying shan and tien-tsin wong. dynamicrafter animating open-domain images with video diffusion priors. in euro- pean conference on computer vision eccv pages 399 417 2025. 2 3 8 17 90 sirui xu zhengyuan li yu-xiong wang and liang-yan gui. interdiff generating 3d human-object interactions with physics-informed diffusion. in international con- ference on computer vision iccv pages 14928 14940 2023. 3 91 zhongcong xu jianfeng zhang jun hao liew han- shu yan jia-wei liu chenxu zhang jiashi feng and mike zheng shou. magicanimate temporally consistent human image animation using diffusion model. in com- puter vision and pattern recognition cvpr pages 1481 1490 2024. 3 92 zhuoyi yang jiayan teng wendi zheng ming ding shiyu huang jiazheng xu yuanming yang wenyi hong xiaohan zhang guanyu feng et al. cogvideox text- to-video diffusion models with an expert transformer. arxiv 2408.06072 2024. 3 93 danah yatim rafail fridman omer bar-tal yoni kasten and tali dekel. space-time diffusion features for zero-shot text-driven motion transfer. in computer vision and pattern recognition cvpr pages 8466 8476 2024. 5 8 14 17 94 yufei ye poorvi hebbar abhinav gupta and shubham tulsiani. diffusion-guided reconstruction of everyday hand-object interaction clips. in international conference on computer vision iccv pages 19717 19728 2023. 3 95 yufei ye xueting li abhinav gupta shalini de mello stan birch\ufb01eld jiaming song shubham tulsiani and sifei liu. affordance diffusion synthesizing hand-object in- teractions. in computer vision and pattern recognition cvpr pages 22479 22489 2023. 3 12478 page 13 96 yufei ye abhinav gupta kris kitani and shubham tul- siani. g-hop generative hand-object prior for interaction reconstruction and grasp synthesis. in computer vision and pattern recognition cvpr pages 1911 1920 2024. 3 97 kexin yi chuang gan yunzhu li pushmeet kohli ji- ajun wu antonio torralba and joshua b. tenenbaum. clevrer collision events for video representation and reasoning. in international conference on learning repre- sentations iclr 2020. 2 4 6 98 shengming yin chenfei wu jian liang jie shi houqiang li gong ming and nan duan. dragnuwa fine-grained control in video generation by integrating text image and trajectory. arxiv 2308.08089 2023. 3 99 sihyun yu kihyuk sohn subin kim and jinwoo shin. video probabilistic diffusion models in projected latent space. in computer vision and pattern recognition cvpr pages 18456 18466 2023. 3 100 lvmin zhang anyi rao and maneesh agrawala. adding conditional control to text-to-image diffusion models. in in- ternational conference on computer vision iccv pages 3836 3847 2023. 3 4 101 mengqi zhang yang fu zheng ding sifei liu zhuowen tu and xiaolong wang. hoidiffusion generating real- istic 3d hand-object interaction data. in computer vision and pattern recognition cvpr pages 8521 8531 2024. 3 102 richard zhang phillip isola alexei a. efros eli shecht- man and oliver wang. the unreasonable effectiveness of deep features as a perceptual metric. in computer vision and pattern recognition cvpr pages 586 595 2018. 5 103 juntian zheng qingyuan zheng lixing fang yun liu and li yi. cams canonicalized manipulation spaces for category-level functional hand-object manipulation synthe- sis. in computer vision and pattern recognition cvpr pages 585 594 2023. 3 104 daquan zhou weimin wang hanshu yan weiwei lv yizhe zhu and jiashi feng. magicvideo ef\ufb01cient video generation with latent diffusion models. arxiv 2211.11018 2022. 3 105 keyang zhou bharat lal bhatnagar jan eric lenssen and gerard pons-moll. gears local geometry-aware hand- object interaction synthesis. in computer vision and pat- tern recognition cvpr pages 20634 20643 2024. 3 106 tianqiang zhu rina wu xiangbo lin and yi sun. to- ward human-like grasp dexterous grasping via semantic representation of object-hand. in international conference on computer vision iccv pages 15741 15751 2021. 3 12479",
      "keywords": [
        "page",
        "interdyn",
        "controllable",
        "interactive",
        "dynamics",
        "video",
        "diffusion",
        "models",
        "rick",
        "akkerman1",
        "haiwen",
        "feng1",
        "michael",
        "black1",
        "dimitrios",
        "tzionas2",
        "victoria",
        "fern",
        "andez",
        "abrevaya1",
        "1max",
        "planck",
        "institute",
        "intelligent",
        "systems",
        "ubingen",
        "germany",
        "2university",
        "amsterdam",
        "netherlands",
        "rick.akkerman",
        "haiwen.feng",
        "black",
        "victoria.abrevaya",
        "tuebingen.mpg.de",
        "d.tzionas",
        "uva.nl",
        "input",
        "image",
        "generation",
        "using",
        "only",
        "hand",
        "mask",
        "sequence",
        "control",
        "signal",
        "force",
        "propagation",
        "counterfactual",
        "future",
        "denotes",
        "driving",
        "object",
        "motion",
        "tracks",
        "indicate",
        "generated",
        "uncontrolled",
        "figure",
        "present",
        "framework",
        "synthesizing",
        "realistic",
        "without",
        "reconstruction",
        "physics",
        "simu-",
        "lation.",
        "core",
        "principle",
        "rely",
        "implicit",
        "knowledge",
        "embedded",
        "large-scale",
        "generative",
        "models.",
        "given",
        "model",
        "generates",
        "consequential",
        "scene",
        "dynamics.",
        "investigate",
        "simple",
        "collision",
        "scenario",
        "bottom",
        "complex",
        "in-the-wild",
        "human-object",
        "interaction",
        "top",
        "abstract",
        "predicting",
        "interacting",
        "objects",
        "essential",
        "both"
      ],
      "metadata": {
        "filename": "Akkerman_InterDyn_Controllable_Interactive_Dynamics_with_Video_Diffusion_Models_CVPR_2025_paper.pdf",
        "original_filename": "Akkerman_InterDyn_Controllable_Interactive_Dynamics_with_Video_Diffusion_Models_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 3040457,
        "upload_file_id": "upload_1752161088904_7g8zo3xhr",
        "upload_timestamp": "2025-07-10T15:24:48.952723",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:24:49.149233+00:00",
      "content_length": 59523,
      "content_type": ".pdf",
      "filename": "Akkerman_InterDyn_Controllable_Interactive_Dynamics_with_Video_Diffusion_Models_CVPR_2025_paper.pdf"
    },
    "13": {
      "file_id": "13",
      "content": "\n=== Page 1 ===\nMulti-Scale Neighborhood Occupancy Masked Autoencoder for\nSelf-Supervised Learning in LiDAR Point Clouds\nMohamed Abdelsamad1,2\nMichael Ulrich1\nClaudius Gl\u00a8aser1\nAbhinav Valada2\n1Bosch Center for AI\n2University of Freiburg\nAbstract\nMasked autoencoders (MAE) have shown tremendous\npotential for self-supervised learning (SSL) in vision and\nbeyond. However, point clouds from LiDARs used in auto-\nmated driving are particularly challenging for MAEs since\nlarge areas of the 3D volume are empty. Consequently, ex-\nisting work suffers from leaking occupancy information into\nthe decoder and has significant computational complexity,\nthereby limiting the SSL pre-training to only 2D bird\u2019s eye\nview encoders in practice. In this work, we propose the\nnovel neighborhood occupancy MAE (NOMAE) that over-\ncomes the aforementioned challenges by employing masked\noccupancy reconstruction only in the neighborhood of non-\nmasked voxels. We incorporate voxel masking and occu-\npancy reconstruction at multiple scales with our proposed\nhierarchical mask generation technique to capture features\nof objects of different sizes in the point cloud. NOMAEs are\nextremely flexible and can be directly employed for SSL in\nexisting 3D architectures. We perform extensive evaluations\non the nuScenes and Waymo Open datasets for the down-\nstream perception tasks of semantic segmentation and 3D\nobject detection, comparing with both discriminative and\ngenerative SSL methods. The results demonstrate that NO-\nMAE sets the new state-of-the-art on multiple benchmarks\nfor multiple point cloud perception tasks.\n1. Introduction\nSensors that generate point clouds, such as LiDARs or radars,\nhave become a cornerstone in automated driving as they pro-\nvide high-resolution three-dimensional representations of\nthe environment [29]. The rich spatial information repre-\nsented in point clouds enables vehicles to accurately detect\nand classify objects, navigate complex environments, and en-\nhance safety through real-time situational awareness. How-\never, annotated point cloud datasets are significantly smaller\nthan their image-based counterparts, which makes learn-\ning large-scale perception models extremely challenging.\nSelf-supervised learning (SSL) [2, 5\u20137, 14\u201317, 21, 22, 32],\nthrough contrastive learning or masked modeling, provides\nData-Efficient\nNuscenes\nObjDet\nNuscenes\nSemSeg\n82\n80\n78\n76\nNoMAE(Ours)\u00a0\nPTv3\u00a0\nUniPAD\u00a0\n81.8\n80.4\n79.4\nmIoU\n73\n71\n69\n67\nmIoU\nNoMAE(Ours)\u00a0\nPTv3\u00a0\n72.3\n71.3\nWaymo\nSemSeg\n60\n50\n40\n30\nmAP\nNoMAE(Ours)\u00a0 54.4\n48.1\nUVTR-L 39.0\nSphereFormer\u00a0 69.9\nUniPAD\u00a0\nFigure 1. NOMAE enables masking and reconstructing occupancy\nas a self-supervised pretext task for large-scale point clouds. It lim-\nits the reconstruction of masked voxels\nto the neighborhood\nof visible\nvoxels and reconstructs the masked occupancy at mul-\ntiple scales. NOMAE\nachieves state-of-the-art performance on\nnuScenes semantic segmentation, Waymo semantic segmentation,\nand nuScenes object detection tasks, outperforming existing self-\nsupervised methods\nas well as transformer methods\n.\nan effective solution to this problem by learning meaningful\nrepresentations from vast amounts of unlabeled data. SSL\nalso reduces the reliance on arduous annotation processes\nwhile improving performance and generalization. Pioneer-\ning works [24, 27, 45, 47] have successfully employed SSL\nto small-scale indoor point clouds, with more recent efforts\nextending it to large-scale outdoor point clouds [18, 33, 40].\nOutdoor point clouds, however, pose a unique challenge\nfor masked modeling as most of the measured 3D volume\nis empty space. Current approaches resort to reconstructing\nprecise point locations within occupied voxels [18, 33, 40],\nbut this often leaks information to the decoder, signaling that\nthe queried voxel is occupied. Recent methods [26, 43] at-\ntempt to overcome this problem by reconstructing the entire\nscene, but the computational complexity and class imbal-\nance caused by the large number of empty voxels limit these\napproaches to either 2D bird\u2019s-eye-view representations or\ncoarse-grained 3D reconstructions.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n22234\n\n=== Page 2 ===\nIn this work, we propose Neighborhood Occupancy MAE\n(NOMAE), the first multi-scale sparse self-supervised learn-\ning framework for LiDAR point clouds that directly ad-\ndresses the problem of 3D point cloud sparsity in masked\nmodeling. The novelty in NOMAE lies in the concept that\nthe occupancy of fine-grained 3D voxels is only evaluated\n(loss) in the neighborhood of visible (not masked) occupied\nvoxels. This is motivated by the fact that LiDAR points are\ntypically clustered in the proximity of other LiDAR points\nin outdoor driving scenarios. Thereby, we avoid leaking in-\nformation about masked voxels to the decoder and eliminate\nthe need for dense feature spaces or reconstructing large\nunoccupied areas. This makes our approach lightweight\nand usable with more modern sensors that have finer resolu-\ntions as well as state-of-the-art 3D transformer architectures.\nOur framework employs self-supervision at multiple scales,\nmade feasible by the lightweight nature of our reconstruc-\ntion task. To facilitate this, we introduce a hierarchical mask\ngeneration module that is suitable for multi-scale SSL. We\nperform extensive experiments with NOMAE on the com-\npetitive nuScenes [4] and Waymo Open [30] datasets that\ndemonstrate state-of-art pretraining performance for mul-\ntiple downstream tasks (illustrated in Fig. 1). Our main\ncontributions are as follows:\n\u2022 The novel localized reconstruction self-supervised learn-\ning framework for point clouds, Neighborhood Occupancy\nMAE.\n\u2022 A multi-scale SSL strategy, where different feature levels\nare supervised at different scales.\n\u2022 A novel mask generating scheme suitable for multi-scale\nSSL.\n\u2022 Extensive benchmarking on two standard autonomous driv-\ning datasets, achieving state-of-the-art results across two\nperception tasks.\n\u2022 Comprehensive ablation studies to highlight the impact of\nour proposed contributions.\n2. Related Work\nIn this section, we review the existing works related on point\ncloud SSL and Automotive LiDAR SSL.\n3D Self-Supervised Learning: The success of generative\nself-supervised learning in natural language processing and\ncomputer vision has inspired several works [24, 27, 34, 46]\nto explore masked auto-encoders for 3D point clouds. These\napproaches are typically tailored towards small-scale sin-\ngle object recognition tasks, where a standard ViT [13]\narchitecture suffices to encode the point cloud.\nFor ex-\nample, PointMAE [27] explicitly reconstructs point cloud\npatches using the Chamfer distance. MaskPoint [24] discrim-\ninates between the reconstructed points and noise points. In\nOcCo [34], point cloud completion is performed on occluded\nregions. Most prominently, Point2Vec [46] reconstructs the\nencoded features of a teacher model for the masked patches.\nAlternatively, contrastive methods can be employed with\npoint clouds to distinguish multiple partial views [39] or\npoint-level correspondences [19]. These pioneering works\nachieve promising results on object-scale and room-scale 3D\npoint clouds but are not usable for large-scale automotive\nLiDAR point clouds due to their inefficient scaling with the\nsize of the point cloud. In contrast, our work focuses on large-\nscale automotive LiDAR point clouds and employs efficient\nhierarchical architectures. Additionally, our work proposes\nself-supervision for multiple feature levels, contrary to the\nsingle-scale supervision employed in these works.\nAutomotive LiDAR Self-Supervised Learning: The focus\nof existing works on automotive large-scale point clouds is\ncomputational efficiency. One common technique is to re-\nduce 3D point clouds to a 2D bird\u2019s-eye-view (BEV) grid, us-\ning pillar architectures [3, 18, 33, 40, 43]. [18] reconstructs\n3D points inside masked 2D pillars and [40] additionally\npredicts the order of the 2D BEV pillars. GeoMAE [33]\npredicts centroids and 3D sub-occupancy in the pretraining.\nGD-MAE [43] utilizes a generative decoder to reconstruct\nthe whole scene to alleviate the leakage of positional infor-\nmation to the decoder. ALSO [3] reconstructs the surface\noccupancy of the point cloud. UniPAD [44] is a pioneer-\ning work that generates coarse 3D features of the whole\nscene and uses a neural rendering approach for supervision.\nOccupancy-MAE [26] proposes to utilize the occupancy as a\ncompressed representation of the point cloud and reconstruct\nthe occupancy in the 3D space using a masked point cloud.\nDespite the significant performance achieved by them, recon-\nstructing occupancy or features over the entire 3D volume is\nan expensive task, which allows supervision only on a coarse-\nscale. Furthermore, the large computational cost prohibits\nemploying modern 3D scene understanding architectures\nwith high voxel resolutions. As a result, the state of the art\nfor self-supervised learning lags behind the performance of\na fully supervised training of transformer architectures from\nscratch. In contrast, this paper addresses fully sparse SSL as\na remedy, scaling well with higher 3D voxel resolutions.\n3. Technical Approach\nFig. 2 presents an overview of our proposed framework\nfor self-supervised representation learning. The network\nconsists of an encoder (to be trained), a token upsampling\nmodule, and multiple decoders for the hierarchical masked\nvoxel reconstruction. We employ PTv3 [37] as the encoder.\nIn contrast to earlier works, we maintain a sparse feature\nspace and generate fine-grained features only for the visible\nvoxels using an upsampling module. We employ a sparse\ndecoder for masked voxel reconstruction. This decoder is de-\nsigned to be simple and lightweight, as described in Sec. 3.2,\nallowing us to deploy a separate decoder instance at each\nfeature scale in the multi-scale pretext (MSP), which is fur-\nther detailed in Sec. 3.3. The input point clouds are first\n22235\n\n=== Page 3 ===\nVoxelization\nHierarchal Mask Generator\n(HMG)\nEncoder\u00a0\nUpsampler\u00a0 \nDecoder\nDecoder\nDecoder\n.\n.\n.\n.\n.\n.\n.\n.\n.\nLidar point cloud\u00a0\nMulti-Scale Pretext\n(MSP)\n.\n.\n.\nFigure 2. Overview of the proposed NOMAE approach. The input point cloud is first voxelized and masked by the hierarchal mask\ngenerator. The encoder E processes the visible voxels Vv to yield a hierarchical representation. The upsampler Mu then fuses the multi-scale\nrepresentations to capture high-level features at each scale. For each feature scale, a separate neighboring decoder predicts occupancy in Vn,\ncorresponding to the immediate neighborhood of the visible voxels. The combination of independent learning tasks across multiple feature\nscales and the localized predictions by the neighboring decoders enables learning representations that are well-suited for 3D point clouds.\nvoxelized and then masked before being fed into the encoder.\nOur masking strategy ensures that there is adequate masking\ncoverage while also maintaining a sufficient number of occu-\npied voxels in the reconstructed neighborhoods at multiple\nhierarchical scales, as explained in Sec. 3.4.\n3.1. Encoder and Token Upsampling\nThis input point cloud P is first voxelized to obtain the set of\nall occupied voxels V, which is then split into a set of visible\nvoxels Vv and masked voxels Vm, as detailed in Sec. 3.4. A\nsparse transformer encoder E based on PTv3 [37] is used to\nencode the features V at the positions of Vv to generate the\nset of tokens\nF(s) = E(V )(s).\n(1)\nPTv3 employs partition-based pooling on the tokens to gen-\nerate more abstract representations for coarser resolutions,\nsimilar to pooling in CNNs. F(s) is the features (tokens) at\nthe s-th scale level of PTv3 and s \u2208{0, ..., S \u22121}.\nNOMAE uses an upsampling module Mu to propagate the\nabstract encoding of coarser resolution tokens to the tokens\nof finer resolution while keeping the representation sparse.\nThis is similar to a feature pyramid network for CNNs. Mu\nconsists of a single PTv3 transformer block at every scale,\nwhich is very lightweight.\n3.2. Neighboring Decoder\nPrior work on self-supervised learning for large-scale point\nclouds reconstruct exact locations of points [18], geometri-\ncal properties [33] or voxel ordering [40], for each masked\nvoxel Vm. This requires passing Vm to the decoder, caus-\ning information leakage. [26, 43] avoid this well-known\ninformation leakage by reconstructing the scene as a whole,\nwhich is computationally expensive. In contrast, NOMAE\nreconstructs the occupancy O(vn, s) of all voxels vn \u2208Vn\nwithin a certain neighborhood n of visible voxels Vv at scale\n(a) Hierarchical mask generation renders P to the coarsest scale s = S \u22121\nand then applies a random mask to divide occupied voxels into visible\nand masked\nvoxels. For the subsequent scales, only the visible voxels of\nthe previous scale undergo random masking. Masked voxels of a coarser\nscale always correspond to masked\nor empty\nvoxels at a finer scale.\n(b) Multiscale pretext reconstructs the cells around visible voxels\nat mul-\ntiple scales. The neighborhood Vn contains voxels that are reconstructed\nbut empty\nas well as masked and recovered voxels\n. Some masked\nvoxels which are too distant from visible voxels are not recovered\nand\ndo not contribute to the loss. The predicted occupancy \u02dcO at coarser scales\ncovers a wider region, while finer scales reconstruct more detail.\nFigure 3. Illustration of multiscale pretext (MSP) and hierarchical\nmask generation (HMG).\ns. To achieve this, we employ a decoder Ds consisting of\nsparse convolution layers.\n\u02dcO(vn, s) = Ds(Mu(F)(s)),\n(2)\nwhere \u02dcO is the networks prediction of O. Visible voxels of\nVv are excluded from Vn. An example is depicted in Fig. 3.\nWe note that Vn will not cover masked voxels in Vm that\nare not nearby visible voxels. This is an approximation\nthat we make in our approach and we observed that LiDAR\npoints in outdoor point clouds typically lie in the proximity\nof other points on the surfaces of objects. Hence, sufficient\nnumber of masked voxels in Vm are included in Vn. Far-\n22236\n\n=== Page 4 ===\naway isolated points do not contribute to the loss, which\nimproves performance, as evaluated in the ablation study\npresented in Sec. 4.6. Our interpretation is that such isolated\npoints belong to strongly occluded or masked objects and\nare infeasible to reconstruct, hence affecting the pretraining.\nThis approximation allows us to avoid reconstructing large\nvolumes of unoccupied space without leaking information\nabout Vm to the decoder Ds at the same time.\n3.3. Multi-Scale Pretext\nGeoMAE [33] is the only prior work that exploits multiple hi-\nerarchical scales in the reconstruction during self-supervised\ntraining. Most other works [3, 18, 26, 33, 43] use a single\nscale for their pretraining tasks. This is unexpected since it\nis common practice in automated driving perception models\nto attach task heads to feature representations at different\nscales. The intuition is that finer resolutions are more suit-\nable for small objects, such as pedestrians, while coarser res-\nolutions are more suitable for larger objects, such as trucks.\nHowever, the multi-scale reconstruction in GeoMAE [33] is\nderived from a single feature map. In our approach, we use\ns instances Ds of the decoder architecture D with separate\nweights. Coupled with the neighborhood size being scale-\ndependent, this implicitly encourages the coarse-grained\nfeatures to contain information from a larger area, while the\nfine-grained features contain more localized details.\n3.4. Hierarchal Mask Generator\nGenerally, the scale of the masking can be different from the\nscale of reconstruction. For example, a random mask can\nbe generated on a coarse scale and then upsampled to match\nthe resolution of the reconstruction. However, experiments\nin [18, 33, 43] showed that random masking on the same\nscale as the reconstruction scale performs best, which is intu-\nitive. Consequently, we generate random masks for multiple\nscales in the MSP. These masks should be consistent to avoid\ninformation leakage, i.e., a voxel that is masked on a coarser\nscale should not be visible on a finer scale [47]. A straight-\nforward manner to generate consistent masks would be to\ncreate a random mask for the finest scale and then derive\nthe masks of coarser scales by defining a coarse-scale voxel\nas masked if all its corresponding fine-scale sub-voxels are\nmasked. However, this would lead to rapidly decreasing\nmasking ratios when moving to coarser scales because a\nsingle visible sub-voxel is sufficient to mask a coarser-scale\nvoxel visible. Another alternative is masking at the coarsest\nscale and upsampling the mask [47] to finer scales which\nleads to a more consistent masking ratio across scales. How-\never, we found that this approach rapidly decreased the size\nof reconstructed neighborhoods n moving to finer scales.\nHence, we propose the masking scheme depicted in\nFig. 3a. We mask the coarsest scale first, using a random\nsampling of all occupied voxels V, and the probability that\na voxel is masked equals masking ratio r. Next, we take all\nvoxels at scale s \u22121 within visible voxels of the previous,\ncoarser, scale s and repeat the sampling of additional masked\nvoxels at scale s \u22121 with probability r.Consequently, the\ntotal masking ratio at scale rt(s) is approximately\nrt(s) \u22481 \u2212(1 \u2212r)S\u2212s+1.\n(3)\nBy doing so, we ensure that coarser scales have a sufficient\nnumber of masked voxels without reducing the size of re-\nconstructed neighborhoods at finer scales. Only the visible\nvoxels Vv of the finest scale are fed to the encoder backbone\nE. An ablation study presented in Sec. 4.6 quantifies the\npositive effect of HMG on the pretraining and downstream\ntask performance.\n3.5. Pretrainging Loss\nWe use the Binary Cross Entropy loss (BCE) as the occu-\npancy loss per scale. The final loss L is the average of all\nsingle scale losses L(s):\nL = 1\nS\nS\u22121\nX\ns=0\n1\n|Vn(s)|\nX\nv\u2208Vn(s)\nBCE( \u02dcO(v, s), O(v, s)),\n(4)\nwith the ground truth occupancy O(v, s) \u2208{0, 1}.\n4. Experiments\nIn this section, we discuss the datasets, metrics, and the\nevaluation protocol that we use for benchmarking. We com-\npare our proposed approach with state-of-the-art methods\nin the benchmarks and present extensive ablation studies to\ndemonstrate the novelty of our contributions.\n4.1. Datasets and Evaluation Metrics\nThe nuScenes dataset [4] is a challenging dataset due to the\nsparsity of the LiDAR point cloud. It consists of 700 driving\nsequences for training, 150 for validation, and 150 for testing,\nwith annotations for a variety of tasks. We evaluate both\nsemantic segmentation and object detection on the nuScenes\ndataset. We use the mean intersection over union (mIoU)\nas the main evaluation metric for semantic segmentation\nand the nuScenes detection score (NDS) and mean average\nprecision (mAP) for 3D object detection.\nThe Waymo Open Dataset [30] is a large-scale au-\ntonomous driving dataset. It consists of 798 driving se-\nquences for training, 202 validation sequences, and 150 test\nsequences. We use the mean intersection over union (mIoU)\nand mean accuracy (mAcc) as the main metrics for evaluat-\ning the semantic segmentation performance.\n4.2. Task Heads\nThis section discusses the methods to evaluate the effec-\ntiveness of the pretraining and the quality of the learned\nrepresentation.\n22237\n\n=== Page 5 ===\nTable 1. Comparison of LiDAR semantic segmentation performance on the nuScenes and Waymo Open datasets. For the first time, an SSL\npretraining method outperforms strong supervised learning models. Methods marked with \u2217are our implementation.\nMethod\nSSL\npretraining\nnuScenes\nWaymo\nval mIoU\nval mAcc\ntest mIoU\ntest fwIoU\nval mIoU\nval mAcc\ntest mIoU\nMinkUNet [9]\n-\n73.3\n-\n-\n-\n65.9\n76.6\n69.8\nSPVNAS [31]\n-\n-\n-\n77.4\n89.7\n-\n-\n68.0\nCylinder3D [49]\n-\n76.1\n-\n77.2\n89.9\n-\n-\n-\nAF2S3Net [8]\n-\n62.2\n-\n78.0\n88.5\n-\n-\n-\n2DPASS [41]\n-\n-\n-\n80.8\n-\n-\n-\n-\nSphereFormer [20]\n-\n78.4\n-\n81.9\n-\n69.9\n-\n-\nPTv2 [36]\n-\n80.2\n-\n82.6\n-\n70.6\n80.2\n-\nPTv3 [37]\n-\n80.4\n87.3\n82.7\n91.1\n71.3\n80.5\n-\nUniPAD [44]\n\u2713\n79.4\n-\n81.1\n-\n-\n-\n-\nGEO-MAE [33]\n\u2713\n78.6\n-\n-\n-\n-\n-\n-\nGEO-MAE [33] + PTv3 [37]\u2217\n\u2713\n78.9\n84.7\n-\n-\n-\n-\n-\nOccupancy-MAE [26]\n\u2713\n72.9\n-\n-\n-\n-\n-\n-\nOccupancy-MAE [26] + PTv3 [37]\u2217\n\u2713\n80.0\n86.1\n-\n-\n-\n-\n-\nNOMAE + MinkUnet (ours)\n\u2713\n80.1\n86.2\n-\n-\n-\n-\n-\nNOMAE + PTv3 (ours)\n\u2713\n81.8\n87.7\n82.6\n91.5\n72.3\n82.5\n70.3\nFine-tuning: In our comparisons with state-of-the-art meth-\nods, fine-tuning follows the self-supervised pre-training of\nthe encoder. For this purpose, a task-specific head is added\nwith randomly initialized weights, and both the encoder E\nand the task-specific head are trained using the annotated\ndataset. We use the same head as PTv3 [37] for the seman-\ntic segmentation tasks and the same head as our baseline\nUVTR [23] for the object detection task. We use layer-wise\nlearning rate decay (LLRD) [17] to avoid forgetting the SSL\nrepresentations in the encoder.\nNon-linear Probing: The purpose of the ablation study is to\nevaluate the learned representation from our SSL approach\non the downstream semantic segmentation task. Therefore,\nthe encoder is kept frozen after pre-training, i.e., no fine-\ntuning.\nFollowing Probe3D [1] and the insights of ear-\nlier works [6, 17], we use a multi-scale non-linear probe\n(NonLP) instead of the commonly used linear probing pro-\ntocol. NonLP aggregates the feature tokens of all scales\nafter up-sampling tokens of coarser scales before passing\nthem to a voxel-wise small MLP. NonLP avoids that the\nrepresentation learning happens in the head. Still, it is prob-\ning the stronger but non-linear features, correlating better\nwith transfer performance [6, 17]. Similar to the commonly\nused linear probe, NonLP is trained for a few epochs using\nannotated data.\n4.3. Implementation Details\nWe perform the experiments for semantic segmentation in the\nPointcept [11] framework and in the MMDetection3D [10]\nframework for the object detection task. We use PTv3 [37]\nas the encoder E, unless stated otherwise. We use a single\nNVIDIA A100 GPU for the pretraining. We use S = 4\nfor the reconstruction and masking scales, corresponding\nto target voxel sizes of {0.05, 0.10, 0.20, 0.40} meters. The\nTable 2.\nComparison of object detection performance on the\nnuScenes dataset with state-of-the-art point-based pre-training\nmethods. Following the evaluation protocol of [43, 44], the meth-\nods are finetuned using 20% labeled frames, without CBGS and\ncopy-paste augmentation.\nMethods\nNDS\nmAP\nUVTR-L (Baseline)\n46.7\n39.0\n+ALSO [3]\n48.2\n41.2\n+GD-MAE [43]\n48.8\n42.6\n+Learning from 2D [25]\n49.2\n48.8\n+UniPAD [44]\n55.8\n48.1\n+noMAE (ours)\n60.9\n54.4\ninput voxel dimension is 0.05 meters. The masking ratio rs\nof the finest scale is 70% for nuScenes and 85% for Waymo.\nIn the self-supervised pre-training, the decoder D consists\nof sparse convolution layers [12] of kernel size 5, followed\nby a single sparse submanifold convolution layer to generate\n\u02dcO. We use common augmentation techniques such as rota-\ntion, scaling, and jittering from the semantic segmentation\nliterature [9, 31, 37] during pretraining.\n4.4. Benchmarking Results\nIn this section, we present the benchmarking results for both\n3D semantic segmentation and 3D object detection.\n3D Semantic Segmentation: Tab. 1 summarizes the best-\nperforming methods on the nuScenes and Waymo Open\nDataset leaderboards for the LiDAR semantic segmentation\ntask. The results include the most performant self-supervised\npretraining methods. We fine-tuned our model as described\nin Sec. 4.2. We observe that our proposed self-supervised\npretraining achieves a mIoU score of 81.8 on the nuScenes\nvalidation set, adding 1.4 mIoU points over the baseline and\nsetting the state-of-the-art. NOMAE is an SSL method for\nany architecture, MinkUnet [9] pretrained with NOMAE out-\n22238\n\n=== Page 6 ===\nTable 3. Comparison of different SSL methods using the same\nbackbone with NonLP. The iteration time (iter.time) is computed\nusing a batch size of 1. Sup.res is the finest resolution of pretraining\nsupervision.\nModel\nmIoU mACC iter.time sup.res\nGEO-MAE [33] + PTv3 [37]\n53.8\n67.7\n40.1ms\n0.20m\nOccupancy-MAE [26] + PTv3 [37]\n59.0\n73.4\n74.0ms\n0.10m\nNOMAE (Ours)\n74.8\n85.0\n39.8ms\n0.05m\nof-the box achieves mIoU of 80.1 which is on par with SOTA\narchitectures. For more results using other architectures see\nSupplementary Sec. 8 and Sec. 11.\nThe results on the nuScenes semantic segmentation test\nset are on par with the strong PTv3 baseline for the mIoU\nscore and outperforms it in the frequency-weighted IoU\n(fwIoU) score by 0.4%. The improvement on the test set\nis lesser than the validation dataset because NOMAE has\nrelatively poor performance for the minority class bicycle,\nand we did not make special adaptations for the test set\nsubmission.\nOn the Waymo Open dataset val set, our method achieves\na mIoU score of 72.3 and mAcc of 75.2. NOMAE improves\nby 1.0 and 2.0 points in the mIoU and mAcc, respectively,\nover the baseline PTv3 [37]. The current version of the\nsemantic segmentation challenge is relatively new for the\nWaymo Open dataset, with only a few submissions. With\n70.3% mIoU score on the test set, NOMAE sets a new state-\nof-the-art on the Waymo Open Dataset single frame semantic\nsegmentation challenge.\n3D Object Detection: We present results for object detec-\ntion on the nuScenes validation set using the fine-tuning\napproach described in Sec. 4.2. We follow the experiment\nsetup of GD-MAE [43] and UniPAD [44], utilizing only\n20% of the annotated frames during fine-tuning, without the\nuse of CBGS [48] or Copy-and-Paste (object sample)[42]\naugmentation. We adopt the same training settings as the\nbaseline UVTR [23] and do not use test-time augmentation\nor model ensembling.\nTab. 2 shows that our approach achieves 60.9 and 54.4\nNDS and mAP scores respectively, improving by 14.2 in\nNDS points and 15.4 in mAP points over the UVTR-L [37]\nbaseline, and by 5.1 NDS points and 5.6 mAP over the clos-\nest contrastive SSL method Learning-from-2D [44]. The\nsignificant improvement demonstrates the effectiveness of\nour proposed localized multi-scale SSL for 3D object detec-\ntion with limited annotated data.\n4.5. Comparison with SSL Methods\nIn this experiment, we compare the performance of our pro-\nposed method with the self-supervised pretraining methods\nof Occupancy-MAE [26] and GeoMAE [33], with the same\nencoder architecture of PTv3 [37]. Tab. 1 shows that our re-\nimplementation of Occupancy-MAE [26] and GeoMAE [33]\nTable 4. Ablation study on the various components in NOMAE for\nsemantic segmentation on the nuScenes validation set. Lines with *\nare with fine-tuning, and all the other results are with NonLP. For\nmore details refer to Sec. 4.6\nModel\nmIoU\nmACC\nACC\nOccupancy-MAE + PTv3\n59.0\n73.4\n91.0\n+ reconstruct only Vn\n66.7\n78.2\n92.4\n+ MSP\nnaive masking\n70.2\n82.3\n93.5\nPoint-M2AE [47] masking\n70.5\n82.3\n93.6\n+ HMG\n72.6\n83.9\n93.7\n+ n = 9\n73.3\n84.3\n94.1\n+ batch size 8\n74.8\n85.0\n94.0\n+ fine-tuning = noMAE\u2217\n81.8\n87.7\n94.9\nwith the state-of-the-art PTv3 [37] backbones (marked with\n\u2217) outperforms the results reported in the original papers by\n0.3 and 7.1 mIoU points respectively for semantic segmen-\ntation with fine-tuning on the nuScenes dataset.\nTab. 3 shows the results of non-linear probing (NonLP).\nWe observe that the NonLP performance in Tab. 3 corre-\nlates with the fine-tuning results in Tab. 1. Furthermore,\nthe relative performance improvement of NOMAE over the\nbaselines is higher for NonLP in comparison to fine-tuning,\nwhich indicates richer representation learning. Additionally,\nthe time of a single training step (iteration time) is lowest\nfor NOMAE, despite the multi-scale pretraining and a much\nfiner resolution of the pretraining supervision. We note that\nthe iteration time of NOMAE is 10ms for the pretraining\nwith a single scale.\n4.6. Ablation Study\nIn this section, we present ablation studies on the nuScenes\nsemantic segmentation validation set to investigate the de-\nsign choices of the proposed method. We performed the ex-\nperiments using the pre-trained frozen encoder using NonLP.\nPlease refer to Sec. 4.2 for further details.\nDetailed Study of NOMAE: This experiment evaluates\nthe improvement due to our proposed contributions, and the\nresults are presented in Tab. 4. We start from our implemen-\ntation of Occupancy-MAE [26] as in Tab. 3. Reconstructing\nonly the local neighborhood Vn of visible voxels Vv increases\nthe NonLP mIoU from 59.0% to 66.7%. This requires re-\nplacing the Occupancy-MAE decoder with our proposed\nupsampling module and neighborhood decoder. Adding the\nmulti-scale reconstruction (MSP) from Sec. 3.3 further im-\nproves the mIoU to 70.1 for naive mask construction and\nto 70.5 for masking strategy from Point-M2AE [47], as op-\nposed to single-scale reconstruction in Occupancy-MAE.\nOur proposed hierarchical mask generation from Sec. 3.4\nyields an improvement of 72.46 mIoU, as further investi-\ngated in Sec. 4.6. Moreover, increasing the reconstruction\nneighborhood size from 5 to 9 improves the mIoU to 74.8,\nas further investigated in Sec. 4.6. Reducing the batch size\n22239\n\n=== Page 7 ===\nTable 5. Reconstruction and masking on different single scales, with\nmulti-scale pretext (MSP) and hierarchical mask generation (HMG).\nResults are reported on the nuScenes val set with the encoder frozen\nafter SSL pretraining, with nonlinear probing. IoU(p) and IoU(t)\nare the IoU for the classes pedestrian and truck, respectively.\nModel\nmIoU\nmACC\nACC\nIoU(p)\nIoU(t)\nSingle scale 2s = 1\n63.8\n74.4\n91.5\n72.6\n68.8\nSingle scale 2s = 2\n66.7\n78.2\n92.4\n75.6\n74.3\nSingle scale 2s = 4\n68.0\n80.0\n92.8\n76.5\n75.5\nSingle scale 2s = 8\n68.3\n80.5\n93.0\n75.8\n76.2\nSingle scale 2s = 16\n67.3\n80.3\n92.7\n73.4\n76.6\nMSP 2s \u2208{1, 2, 4, 8}\nnaive masking\n70.2\n82.3\n93.5\n83.2\n79.6\nPoint-M2AE [47]\n70.5\n82.3\n93.6\n82.5\n81.2\nHMG (ours)\n72.6\n83.9\n93.7\n85.2\n83.6\nto 8 (further investigated in Sec. 6.1 of the supplementary\nmaterial) yields the final NonLP performance of 74.8% and\nfine-tuning performance of 81.8% mIoU score.\nMask Block Size, MSP and HMG: This experiment in-\nvestigates different reconstruction and mask scales s for our\nSSL task. Tab. 5 compares the performance of reconstructing\ndifferent single scales with the proposed multi-scale pretext\n(MSP) from Sec. 3.3. The experiment of MSP without hierar-\nchical mask generation (HMG) uses either a random mask at\nthe finest scale, which is pooled to generate masks of coarser\nscales, as described in the naive solution in Sec. 3.4, or the\nmethod of Point-M2AE [47].\nIt can be observed that no single-scale occupancy task\nis suitable for all object types. For example, trucks benefit\nfrom a coarser occupancy reconstruction, while pedestrians\nprefer a finer resolution in the pretraining task, except for the\nvery fine scales of 2s \u2208{1, 2}. MSP combines coarse and\nfine tasks, thereby maximizing the overall mIoU. We observe\nthat HMG achieves an additional improvement of 2.4 and\n2.1% mIoU over random mask at the finest scale and the\nmask generation proposed in Point-M2AE [47] respectively.\nThis underlines the importance of proper training examples\nat all scales.\nNeighborhood Size:\nFig. 4 investigates the effect of the\nneighborhood size (number of voxels) to generate Vn from\nVv. For example, n = 5 indicates that a total of 5 vox-\nels are covered in all 3 (x,y,z) dimensions for every scale.\nExperiments use MSP and HMG. Since every scale has a\ndifferent voxel size, the reconstructed volume depends on the\nscale. We observe a maximum of the NonLP performance\nfor n = 9. Our interpretation is that a smaller neighbor-\nhood does not cover sufficient LiDAR measurements for\nreconstruction, while a larger neighborhood hinders local\nrepresentations in the pretraining. Further, we observe that\nlimiting the reconstruction size performs consistently better\nthan reconstructing the whole space, which would corre-\nspond to the method of Occupancy-MAE [26].\nMasking Ratio: Fig. 5 investigates the effect of different\n3\n5\n7\n9\n11\n13\n70\n71\n72\n73\n74\n69.7\n72.5\n72.4\n73.3\n73.2\n73.1\nSize in voxels\nNuscene SemSeg mIoU\nmIoU\nFigure 4. Size (number of voxels) of the reconstructed neighbor-\nhood n around visible voxels Vv, to create Vn in the proposed\npretext task. We observe that the downstream NonLP semantic\nsegmentation peaks at n = 9. Note that n \u2192\u221ecorresponds to\nthe method of [26].\n60\n65\n70\n75\n80\n85\n60\n61\n62\n63\n64\n65\n63\n63.7\n64.4\n62.7\n60.6\nWaymo SemSeg mIoU\n60\n65\n70\n75\n80\n85\n90\n95\n72\n73\n72.6\n72.9\n73.3\n72.8\n72\nTotal masking ratio rt(0)\nNuscenes SemSeg mIoU\nFigure 5. NonLP performance over the masking ratio rt on the\nnuScenes and Waymo datasets. We observe that the optimal rt(0)\nis 70% for the nuScenes and 85% for the Waymo Open Dataset.\nOur interpretation is that Waymo requires a higher masking ratio\ndue to the higher density of the LiDAR point cloud.\nmasking ratios on the quality of the representations for se-\nmantic segmentation on the Waymo and nuScenes datasets.\nThis experiment uses a constant r(s) = r to achieve the\ntotal masking ratio rt according to Eq. 3. We observe that\nthe optimal total masking ratio is 70% for the nuScenes and\n85% for the Waymo Open dataset. This is intuitive since the\npoint cloud density in Waymo (ca. 180k points per frame)\nis higher than the relatively sparse nuScenes (ca. 34k points\nper frame) LiDAR point clouds.\nData Efficient nuScenes: Tab. 6 analyzes the performance\nunder limited annotated data for semantic segmentation on\nthe nuScenes dataset. The sub-sampling of the data is per-\nformed sequence-wise, meaning that all frames of a sequence\nare either included or excluded. For example, 0.1% indicates\nthat only one of the 1000 scenes of the nuScenes dataset\nis used, namely scene-0392. In every experiment, all se-\nquences of the training set are used for the self-supervised\npertaining. We observe that NOMAE (fine-tune) consistently\noutperforms training from scratch, which demonstrates the\nmethod\u2019s ability to benefit from unannotated data. Further-\n22240\n\n=== Page 8 ===\nBaseline Output\nNOMAE Output\nImprovement/Error Map\n(a)\n(b)\nBaseline Output\nNOMAE Output\nGround Truth\n(c)\nFigure 6. Qualitative comparison of semantic segmentation performance with Ptv3 [37] and object detection performance with UVTR [23].\nTable 6. Results with varying amounts of annotated data, evaluated\nfor semantic segmentation mIoU on the nuScenes val set.\nMethod\nAnnotated Scenes\n0.1%\n1%\n10%\n50%\n100%\nPTv3 [37]\n28.2\n41.6\n68.7\n78.7\n80.4\nNOMAE (NonLP)\n35.7\n47.5\n67.7\n73.8\n74.2\nNOMAE (fine-tune)\n35.8\n48.1\n69.9\n80.1\n81.8\nmore, we observe that in experiments with very little data,\nNonLP performs similarly to fine-tuning. This suggests that\nNOMAE can benefit from the unannotated data to learn a\nsufficiently strong representation in the encoder, such that\nthe fine-tuning of the encoder has little benefit.\n4.7. Qualitative Evaluations\nWe present qualitative comparisons in Fig. 6. In (a) and\n(b) for semantic segmentation, we observe that NOMAE\nimproves the accuracy by reducing class mix-up and by im-\nproving the boundaries between objects. In (a), we observe\nthat the baseline mis-segments the truck while NOMAE\naccurately recognizes it. In (b), we see that NOMAE recog-\nnizes the smaller object missed by the baseline (the bottom\nleft pole). We can also see that it fails to recognize some\ndrivable areas in (a). In Example (c), we visualize the detec-\ntions from NOMAE. We see that compared to the baseline,\nNOMAE is able to more accurately estimate the orientation\nof the objects and has higher true positive detections. We\nalso see both models hallucinating in further away regions\n(on the left), and NOMAE fails to detect the pedestrian on\nthe left. More qualitative results are presented in Sec. 12 of\nthe supplementary material.\n5. Conclusion\nIn this work, we proposed NOMAE, a novel multi-scale self-\nsupervised learning framework for large-scale point clouds.\nObserving the large-scale nature of LiDAR point clouds,\nNOMAE reconstructs only local neighborhoods, keeping the\ncomputation tractable at higher voxel resolutions, avoiding\ninformation leakage, and learning a localized representation\nsuitable for diverse downstream perception tasks. Enabled\nby its efficiency, NOMAE utilizes multiple scales in the pre-\ntraining, enabling the model to learn both coarse and fine rep-\nresentations. A novel hierarchical mask generation scheme\nbalances the pre-training of coarse and fine features, which\nis important for objects of different sizes, such as pedestrians\nand trucks We presented experimental results that underline\nthe benefit of our proposed contributions, achieving state-of-\nthe-art performance on multiple benchmarks.\nLimitations: NOMAE is sensitive to the density of the point\ncloud and future work will investigate the proposed method\nfor sparse 3D sensors such as radar. Additionally, NOMAE\ndoes not utilize the temporal nature of LiDAR data which\ncan open the door for further performance improvement.\nFurthermore, the application of high-resolution sparse 3D\nrepresentations in the encoders should be further investigated\nfor the object detection task, where 2D bird\u2019s eye view and\nlow-resolution 3D approaches are still dominant.\n22241\n\n=== Page 9 ===\nReferences\n[1] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Ab-\nhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun,\nLeonidas J. Guibas, Justin Johnson, and Varun Jampani. Prob-\ning the 3d awareness of visual foundation models. Proc. of\nthe IEEE Conf. on Computer Vision and Pattern Recognition,\npages 21795\u201321806, 2024. 5\n[2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\nBERT pre-training of image transformers. In Int. Conf. on\nLearning Representations, 2022. 1\n[3] Alexandre Boulch, Corentin Sautier, Bj\u00a8orn Michele, Gilles\nPuy, and Renaud Marlet.\nALSO: automotive lidar self-\nsupervision by occupancy estimation. In Proc. of the IEEE\nConf. on Computer Vision and Pattern Recognition, 2023. 2,\n4, 5\n[4] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal\ndataset for autonomous driving. In Proc. of the IEEE Conf. on\nComputer Vision and Pattern Recognition, 2020. 2, 4\n[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof-\nfrey E. Hinton. A simple framework for contrastive learning\nof visual representations. In Int. Conf. on Machine Learning,\n2020. 1\n[6] Xinlei Chen and Kaiming He. Exploring simple siamese\nrepresentation learning. In Proc. of the IEEE Conf. on Com-\nputer Vision and Pattern Recognition, 2021. 5\n[7] Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming\nHe. Improved baselines with momentum contrastive learning.\narXiv preprint arXiv:2003.04297, 2020. 1\n[8] Ran Cheng, Ryan Razani, Ehsan Moeen Taghavi, Enxu Li,\nand Bingbing Liu. (af)2-s3net: Attentive feature fusion with\nadaptive feature selection for sparse semantic segmentation\nnetwork. Proc. of the IEEE Conf. on Computer Vision and\nPattern Recognition, pages 12542\u201312551, 2021. 5\n[9] Christopher B. Choy, JunYoung Gwak, and Silvio Savarese.\n4d spatio-temporal convnets: Minkowski convolutional neural\nnetworks. In Proc. of the IEEE Conf. on Computer Vision and\nPattern Recognition, 2019. 5, 13\n[10] MMDetection3D Contributors.\nMMDetection3D: Open-\nMMLab next-generation platform for general 3D object\ndetection.\nhttps://github.com/open- mmlab/\nmmdetection3d, 2020. 5, 11\n[11] Pointcept Contributors.\nPointcept: A codebase for point\ncloud perception research.\nhttps://github.com/\nPointcept/Pointcept, 2023. 5, 11\n[12] Spconv Contributors.\nSpconv: Spatially sparse convolu-\ntion library. https://github.com/traveller59/\nspconv, 2022. 5\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. Int. Conf. on Learning Representations, 2021. 2\n[14] Peng Gao, Teli Ma, Hongsheng Li, Ziyi Lin, Jifeng Dai, and\nYu Jiao Qiao. Mcmae: Masked convolution meets masked\nautoencoders. In Proc. of the Conf. on Neural Information\nProcessing Systems, 2022. 1\n[15] Nikhil Gosala, K\u00a8ursat Petek, Paulo LJ Drews-Jr, Wolfram\nBurgard, and Abhinav Valada. Skyeye: Self-supervised bird\u2019s-\neye-view semantic mapping using monocular frontal view\nimages. In Proc. of the IEEE Conf. on Computer Vision and\nPattern Recognition, pages 14901\u201314910, 2023.\n[16] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B.\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proc. of the IEEE Conf. on Com-\nputer Vision and Pattern Recognition, 2020.\n[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proc. of the IEEE Conf. on Computer Vi-\nsion and Pattern Recognition, 2022. 1, 5, 11\n[18] Georg Hess, Johan Jaxing, Elias Svensson, David Hagerman,\nChristoffer Petersson, and Lennart Svensson. Masked autoen-\ncoder for self-supervised pre-training on lidar point clouds. In\nProc. of the IEEE winter Conf. on applications of computer\nvision, 2023. 1, 2, 3, 4\n[19] Ji Hou, Benjamin Graham, Matthias Nie\u00dfner, and Saining\nXie. Exploring data-efficient 3d scene understanding with\ncontrastive scene contexts. Proc. of the IEEE Conf. on Com-\nputer Vision and Pattern Recognition, pages 15582\u201315592,\n2020. 2\n[20] Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, and Jiaya Jia.\nSpherical transformer for lidar-based 3d recognition. Proc. of\nthe IEEE Conf. on Computer Vision and Pattern Recognition,\npages 17545\u201317555, 2023. 5\n[21] Christopher Lang, Alexander Braun, Lars Schillingmann, and\nAbhinav Valada. Self-supervised multi-object tracking for au-\ntonomous driving from consistency across timescales. IEEE\nRobotics and Automation Letters, 8(11):7711\u20137718, 2023. 1\n[22] Christopher Lang, Alexander Braun, Lars Schillingmann,\nKarsten Haug, and Abhinav Valada. Self-supervised rep-\nresentation learning from temporal ordering of automated\ndriving sequences. IEEE Robotics and Automation Letters, 9\n(3):2582\u20132589, 2024. 1\n[23] Yanwei Li, Yilun Chen, Xiaojuan Qi, Zeming Li, Jian Sun,\nand Jiaya Jia. Unifying voxel-based representation with trans-\nformer for 3d object detection. In Proc. of the Conf. on Neural\nInformation Processing Systems, 2022. 5, 6, 8, 11, 15\n[24] Haotian Liu, Mu Cai, and Yong Jae Lee. Masked discrimina-\ntion for self-supervised learning on point clouds. In Proc. of\nthe Europ. Conf. on Computer Vision, 2022. 1, 2\n[25] Yueh-Cheng Liu, Yu-Kai Huang, HungYueh Chiang, Hung-\nTing Su, Zhe Yu Liu, Chin-Tang Chen, Ching-Yu Tseng, and\nWinston H. Hsu. Learning from 2d: Pixel-to-point knowledge\ntransfer for 3d pretraining. arXiv preprint arXiv:2104.04687,\n2021. 5\n[26] Chen Min, Xinli Xu, Dawei Zhao, Liang Xiao, Yiming Nie,\nand Bin Dai. Occupancy-mae: Self-supervised pre-training\nlarge-scale lidar point clouds with masked occupancy au-\ntoencoders. IEEE Transactions on Intelligent Vehicles, 9:\n5150\u20135162, 2022. 1, 2, 3, 4, 5, 6, 7\n[27] Yatian Pang, Wenxiao Wang, Francis E. H. Tay, Wei Liu,\nYonghong Tian, and Li Yuan. Masked autoencoders for point\n22242\n\n=== Page 10 ===\ncloud self-supervised learning. In Proc. of the Europ. Conf. on\nComputer Vision, 2022. 1, 2\n[28] Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, Heng-\nshuang Zhao, Zhuotao Tian, and Jiaya Jia. Oa-cnns: Omni-\nadaptive sparse cnns for 3d semantic segmentation. Proc. of\nthe IEEE Conf. on Computer Vision and Pattern Recognition,\npages 21305\u201321315, 2024. 13\n[29] Jonas Schramm, Niclas V\u00a8odisch, K\u00a8ursat Petek, B Ravi Kiran,\nSenthil Yogamani, Wolfram Burgard, and Abhinav Valada.\nBevcar: Camera-radar fusion for bev map and object segmen-\ntation. In Int. Conf. on Intelligent Robots and Systems, pages\n1435\u20131442, 2024. 1\n[30] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\nYuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han,\nJiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Et-\ntinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang,\nJonathon Shlens, Zhifeng Chen, and Dragomir Anguelov.\nScalability in perception for autonomous driving: Waymo\nopen dataset. In Proc. of the IEEE Conf. on Computer Vision\nand Pattern Recognition, 2020. 2, 4\n[31] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji\nLin, Hanrui Wang, and Song Han. Searching efficient 3d\narchitectures with sparse point-voxel convolution. In Proc. of\nthe Europ. Conf. on Computer Vision, 2020. 5\n[32] Keyu Tian, Yi Jiang, Qishuai Diao, Chen Lin, Liwei Wang,\nand Zehuan Yuan. Designing BERT for convolutional net-\nworks: Sparse and hierarchical masked modeling. In Int. Conf.\non Learning Representations, 2023. 1\n[33] Xiaoyu Tian, Haoxi Ran, Yue Wang, and Hang Zhao. Geo-\nmae: Masked geometric target prediction for self-supervised\npoint cloud pre-training. In Proc. of the IEEE Conf. on Com-\nputer Vision and Pattern Recognition, 2023. 1, 2, 3, 4, 5, 6,\n13\n[34] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and\nMatt J. Kusner. Unsupervised point cloud pre-training via\nocclusion completion. Int. Conf. on Computer Vision, pages\n9762\u20139772, 2020. 2\n[35] Peng-Shuai Wang. Octformer: Octree-based transformers for\n3d point clouds. ACM Transactions on Graphics, 42:1 \u2013 11,\n2023. 13\n[36] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Heng-\nshuang Zhao. Point transformer V2: grouped vector attention\nand partition-based pooling. In Advances in Neural Informa-\ntion Processing Systems, 2022. 5\n[37] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui\nLiu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang\nZhao. Point transformer v3: Simpler, faster, stronger. Proc. of\nthe IEEE Conf. on Computer Vision and Pattern Recognition,\npages 4840\u20134851, 2023. 2, 3, 5, 6, 8, 11, 12, 13, 14\n[38] Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xi-\nhui Liu, Kaicheng Yu, and Hengshuang Zhao.\nTowards\nlarge-scale 3d representation learning with multi-dataset point\nprompt training. In Proc. of the IEEE Conf. on Computer Vi-\nsion and Pattern Recognition, 2024. 11\n[39] Saining Xie, Jiatao Gu, Demi Guo, Charles R. Qi, Leonidas J.\nGuibas, and Or Litany. Pointcontrast: Unsupervised pre-\ntraining for 3d point cloud understanding. In Proc. of the\nEurop. Conf. on Computer Vision, 2020. 2\n[40] Runsen Xu, Tai Wang, Wenwei Zhang, Runjian Chen, Jinkun\nCao, Jiangmiao Pang, and Dahua Lin. MV-JAR: masked voxel\njigsaw and reconstruction for lidar-based self-supervised pre-\ntraining. In Proc. of the IEEE Conf. on Computer Vision and\nPattern Recognition, 2023. 1, 2, 3\n[41] Xu Yan, Jiantao Gao, Chaoda Zheng, Chaoda Zheng, Ruimao\nZhang, Shenghui Cui, and Zhen Li. 2dpass: 2d priors assisted\nsemantic segmentation on lidar point clouds. In Proc. of the\nEurop. Conf. on Computer Vision, 2022. 5\n[42] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely em-\nbedded convolutional detection. Sensors, 18(10), 2018. 6,\n16\n[43] Honghui Yang, Tong He, Jiaheng Liu, Hua Chen, Boxi Wu,\nBinbin Lin, Xiaofei He, and Wanli Ouyang. GD-MAE: gen-\nerative decoder for MAE pre-training on lidar point clouds.\nIn Proc. of the IEEE Conf. on Computer Vision and Pattern\nRecognition, 2023. 1, 2, 3, 4, 5, 6\n[44] Honghui Yang, Sha Zhang, Di Huang, Xiaoyang Wu, Haoyi\nZhu, Tong He, Shixiang Tang, Hengshuang Zhao, Qibo Qiu,\nBinbin Lin, Xiaofei He, and Wanli Ouyang. Unipad: A uni-\nversal pre-training paradigm for autonomous driving. Proc. of\nthe IEEE Conf. on Computer Vision and Pattern Recognition,\npages 15238\u201315250, 2023. 2, 5, 6\n[45] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie\nZhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud\ntransformers with masked point modeling. In Proc. of the\nIEEE Conf. on Computer Vision and Pattern Recognition,\n2022. 1\n[46] Karim Abou Zeid, Jonas Schult, Alexander Hermans, and\nBastian Leibe. Point2vec for self-supervised representation\nlearning on point clouds. In DAGM German Conference on\nPattern Recognition, pages 131\u2013146. Springer, 2023. 2\n[47] Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin\nZhao, Dong Wang, Yu Qiao, and Hongsheng Li. Point-m2ae:\nMulti-scale masked autoencoders for hierarchical point cloud\npre-training. In Proc. of the Conf. on Neural Information\nProcessing Systems, 2022. 1, 4, 6, 7\n[48] Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and\nGang Yu. Class-balanced grouping and sampling for point\ncloud 3d object detection. arXiv preprint arXiv:1908.09492,\n2019. 6, 16\n[49] Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin\nMa, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical and\nasymmetrical 3d convolution networks for lidar segmentation.\nIn Proc. of the IEEE Conf. on Computer Vision and Pattern\nRecognition, 2021. 5\n22243\n",
      "searchable_content": "page 1 multi-scale neighborhood occupancy masked autoencoder for self-supervised learning in lidar point clouds mohamed abdelsamad1 2 michael ulrich1 claudius gl aser1 abhinav valada2 1bosch center for ai 2university of freiburg abstract masked autoencoders mae have shown tremendous potential for self-supervised learning ssl in vision and beyond. however point clouds from lidars used in auto- mated driving are particularly challenging for maes since large areas of the 3d volume are empty. consequently ex- isting work suffers from leaking occupancy information into the decoder and has significant computational complexity thereby limiting the ssl pre-training to only 2d bird s eye view encoders in practice. in this work we propose the novel neighborhood occupancy mae nomae that over- comes the aforementioned challenges by employing masked occupancy reconstruction only in the neighborhood of non- masked voxels. we incorporate voxel masking and occu- pancy reconstruction at multiple scales with our proposed hierarchical mask generation technique to capture features of objects of different sizes in the point cloud. nomaes are extremely flexible and can be directly employed for ssl in existing 3d architectures. we perform extensive evaluations on the nuscenes and waymo open datasets for the down- stream perception tasks of semantic segmentation and 3d object detection comparing with both discriminative and generative ssl methods. the results demonstrate that no- mae sets the new state-of-the-art on multiple benchmarks for multiple point cloud perception tasks. 1. introduction sensors that generate point clouds such as lidars or radars have become a cornerstone in automated driving as they pro- vide high-resolution three-dimensional representations of the environment 29 . the rich spatial information repre- sented in point clouds enables vehicles to accurately detect and classify objects navigate complex environments and en- hance safety through real-time situational awareness. how- ever annotated point cloud datasets are significantly smaller than their image-based counterparts which makes learn- ing large-scale perception models extremely challenging. self-supervised learning ssl 2 5 7 14 17 21 22 32 through contrastive learning or masked modeling provides data-efficient nuscenes objdet nuscenes semseg 82 80 78 76 nomae ours ptv3 unipad 81.8 80.4 79.4 miou 73 71 69 67 miou nomae ours ptv3 72.3 71.3 waymo semseg 60 50 40 30 map nomae ours 54.4 48.1 uvtr-l 39.0 sphereformer 69.9 unipad figure 1. nomae enables masking and reconstructing occupancy as a self-supervised pretext task for large-scale point clouds. it lim- its the reconstruction of masked voxels to the neighborhood of visible voxels and reconstructs the masked occupancy at mul- tiple scales. nomae achieves state-of-the-art performance on nuscenes semantic segmentation waymo semantic segmentation and nuscenes object detection tasks outperforming existing self- supervised methods as well as transformer methods . an effective solution to this problem by learning meaningful representations from vast amounts of unlabeled data. ssl also reduces the reliance on arduous annotation processes while improving performance and generalization. pioneer- ing works 24 27 45 47 have successfully employed ssl to small-scale indoor point clouds with more recent efforts extending it to large-scale outdoor point clouds 18 33 40 . outdoor point clouds however pose a unique challenge for masked modeling as most of the measured 3d volume is empty space. current approaches resort to reconstructing precise point locations within occupied voxels 18 33 40 but this often leaks information to the decoder signaling that the queried voxel is occupied. recent methods 26 43 at- tempt to overcome this problem by reconstructing the entire scene but the computational complexity and class imbal- ance caused by the large number of empty voxels limit these approaches to either 2d bird s-eye-view representations or coarse-grained 3d reconstructions. this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 22234 page 2 in this work we propose neighborhood occupancy mae nomae the first multi-scale sparse self-supervised learn- ing framework for lidar point clouds that directly ad- dresses the problem of 3d point cloud sparsity in masked modeling. the novelty in nomae lies in the concept that the occupancy of fine-grained 3d voxels is only evaluated loss in the neighborhood of visible not masked occupied voxels. this is motivated by the fact that lidar points are typically clustered in the proximity of other lidar points in outdoor driving scenarios. thereby we avoid leaking in- formation about masked voxels to the decoder and eliminate the need for dense feature spaces or reconstructing large unoccupied areas. this makes our approach lightweight and usable with more modern sensors that have finer resolu- tions as well as state-of-the-art 3d transformer architectures. our framework employs self-supervision at multiple scales made feasible by the lightweight nature of our reconstruc- tion task. to facilitate this we introduce a hierarchical mask generation module that is suitable for multi-scale ssl. we perform extensive experiments with nomae on the com- petitive nuscenes 4 and waymo open 30 datasets that demonstrate state-of-art pretraining performance for mul- tiple downstream tasks illustrated in fig. 1 . our main contributions are as follows the novel localized reconstruction self-supervised learn- ing framework for point clouds neighborhood occupancy mae. a multi-scale ssl strategy where different feature levels are supervised at different scales. a novel mask generating scheme suitable for multi-scale ssl. extensive benchmarking on two standard autonomous driv- ing datasets achieving state-of-the-art results across two perception tasks. comprehensive ablation studies to highlight the impact of our proposed contributions. 2. related work in this section we review the existing works related on point cloud ssl and automotive lidar ssl. 3d self-supervised learning the success of generative self-supervised learning in natural language processing and computer vision has inspired several works 24 27 34 46 to explore masked auto-encoders for 3d point clouds. these approaches are typically tailored towards small-scale sin- gle object recognition tasks where a standard vit 13 architecture suffices to encode the point cloud. for ex- ample pointmae 27 explicitly reconstructs point cloud patches using the chamfer distance. maskpoint 24 discrim- inates between the reconstructed points and noise points. in occo 34 point cloud completion is performed on occluded regions. most prominently point2vec 46 reconstructs the encoded features of a teacher model for the masked patches. alternatively contrastive methods can be employed with point clouds to distinguish multiple partial views 39 or point-level correspondences 19 . these pioneering works achieve promising results on object-scale and room-scale 3d point clouds but are not usable for large-scale automotive lidar point clouds due to their inefficient scaling with the size of the point cloud. in contrast our work focuses on large- scale automotive lidar point clouds and employs efficient hierarchical architectures. additionally our work proposes self-supervision for multiple feature levels contrary to the single-scale supervision employed in these works. automotive lidar self-supervised learning the focus of existing works on automotive large-scale point clouds is computational efficiency. one common technique is to re- duce 3d point clouds to a 2d bird s-eye-view bev grid us- ing pillar architectures 3 18 33 40 43 . 18 reconstructs 3d points inside masked 2d pillars and 40 additionally predicts the order of the 2d bev pillars. geomae 33 predicts centroids and 3d sub-occupancy in the pretraining. gd-mae 43 utilizes a generative decoder to reconstruct the whole scene to alleviate the leakage of positional infor- mation to the decoder. also 3 reconstructs the surface occupancy of the point cloud. unipad 44 is a pioneer- ing work that generates coarse 3d features of the whole scene and uses a neural rendering approach for supervision. occupancy-mae 26 proposes to utilize the occupancy as a compressed representation of the point cloud and reconstruct the occupancy in the 3d space using a masked point cloud. despite the significant performance achieved by them recon- structing occupancy or features over the entire 3d volume is an expensive task which allows supervision only on a coarse- scale. furthermore the large computational cost prohibits employing modern 3d scene understanding architectures with high voxel resolutions. as a result the state of the art for self-supervised learning lags behind the performance of a fully supervised training of transformer architectures from scratch. in contrast this paper addresses fully sparse ssl as a remedy scaling well with higher 3d voxel resolutions. 3. technical approach fig. 2 presents an overview of our proposed framework for self-supervised representation learning. the network consists of an encoder to be trained a token upsampling module and multiple decoders for the hierarchical masked voxel reconstruction. we employ ptv3 37 as the encoder. in contrast to earlier works we maintain a sparse feature space and generate fine-grained features only for the visible voxels using an upsampling module. we employ a sparse decoder for masked voxel reconstruction. this decoder is de- signed to be simple and lightweight as described in sec. 3.2 allowing us to deploy a separate decoder instance at each feature scale in the multi-scale pretext msp which is fur- ther detailed in sec. 3.3. the input point clouds are first 22235 page 3 voxelization hierarchal mask generator hmg encoder upsampler decoder decoder decoder . . . . . . . . . lidar point cloud multi-scale pretext msp . . . figure 2. overview of the proposed nomae approach. the input point cloud is first voxelized and masked by the hierarchal mask generator. the encoder e processes the visible voxels vv to yield a hierarchical representation. the upsampler mu then fuses the multi-scale representations to capture high-level features at each scale. for each feature scale a separate neighboring decoder predicts occupancy in vn corresponding to the immediate neighborhood of the visible voxels. the combination of independent learning tasks across multiple feature scales and the localized predictions by the neighboring decoders enables learning representations that are well-suited for 3d point clouds. voxelized and then masked before being fed into the encoder. our masking strategy ensures that there is adequate masking coverage while also maintaining a sufficient number of occu- pied voxels in the reconstructed neighborhoods at multiple hierarchical scales as explained in sec. 3.4. 3.1. encoder and token upsampling this input point cloud p is first voxelized to obtain the set of all occupied voxels v which is then split into a set of visible voxels vv and masked voxels vm as detailed in sec. 3.4. a sparse transformer encoder e based on ptv3 37 is used to encode the features v at the positions of vv to generate the set of tokens f s e v s . 1 ptv3 employs partition-based pooling on the tokens to gen- erate more abstract representations for coarser resolutions similar to pooling in cnns. f s is the features tokens at the s-th scale level of ptv3 and s 0 ... s 1 . nomae uses an upsampling module mu to propagate the abstract encoding of coarser resolution tokens to the tokens of finer resolution while keeping the representation sparse. this is similar to a feature pyramid network for cnns. mu consists of a single ptv3 transformer block at every scale which is very lightweight. 3.2. neighboring decoder prior work on self-supervised learning for large-scale point clouds reconstruct exact locations of points 18 geometri- cal properties 33 or voxel ordering 40 for each masked voxel vm. this requires passing vm to the decoder caus- ing information leakage. 26 43 avoid this well-known information leakage by reconstructing the scene as a whole which is computationally expensive. in contrast nomae reconstructs the occupancy o vn s of all voxels vn vn within a certain neighborhood n of visible voxels vv at scale a hierarchical mask generation renders p to the coarsest scale s s 1 and then applies a random mask to divide occupied voxels into visible and masked voxels. for the subsequent scales only the visible voxels of the previous scale undergo random masking. masked voxels of a coarser scale always correspond to masked or empty voxels at a finer scale. b multiscale pretext reconstructs the cells around visible voxels at mul- tiple scales. the neighborhood vn contains voxels that are reconstructed but empty as well as masked and recovered voxels . some masked voxels which are too distant from visible voxels are not recovered and do not contribute to the loss. the predicted occupancy o at coarser scales covers a wider region while finer scales reconstruct more detail. figure 3. illustration of multiscale pretext msp and hierarchical mask generation hmg . s. to achieve this we employ a decoder ds consisting of sparse convolution layers. o vn s ds mu f s 2 where o is the networks prediction of o. visible voxels of vv are excluded from vn. an example is depicted in fig. 3. we note that vn will not cover masked voxels in vm that are not nearby visible voxels. this is an approximation that we make in our approach and we observed that lidar points in outdoor point clouds typically lie in the proximity of other points on the surfaces of objects. hence sufficient number of masked voxels in vm are included in vn. far- 22236 page 4 away isolated points do not contribute to the loss which improves performance as evaluated in the ablation study presented in sec. 4.6. our interpretation is that such isolated points belong to strongly occluded or masked objects and are infeasible to reconstruct hence affecting the pretraining. this approximation allows us to avoid reconstructing large volumes of unoccupied space without leaking information about vm to the decoder ds at the same time. 3.3. multi-scale pretext geomae 33 is the only prior work that exploits multiple hi- erarchical scales in the reconstruction during self-supervised training. most other works 3 18 26 33 43 use a single scale for their pretraining tasks. this is unexpected since it is common practice in automated driving perception models to attach task heads to feature representations at different scales. the intuition is that finer resolutions are more suit- able for small objects such as pedestrians while coarser res- olutions are more suitable for larger objects such as trucks. however the multi-scale reconstruction in geomae 33 is derived from a single feature map. in our approach we use s instances ds of the decoder architecture d with separate weights. coupled with the neighborhood size being scale- dependent this implicitly encourages the coarse-grained features to contain information from a larger area while the fine-grained features contain more localized details. 3.4. hierarchal mask generator generally the scale of the masking can be different from the scale of reconstruction. for example a random mask can be generated on a coarse scale and then upsampled to match the resolution of the reconstruction. however experiments in 18 33 43 showed that random masking on the same scale as the reconstruction scale performs best which is intu- itive. consequently we generate random masks for multiple scales in the msp. these masks should be consistent to avoid information leakage i.e. a voxel that is masked on a coarser scale should not be visible on a finer scale 47 . a straight- forward manner to generate consistent masks would be to create a random mask for the finest scale and then derive the masks of coarser scales by defining a coarse-scale voxel as masked if all its corresponding fine-scale sub-voxels are masked. however this would lead to rapidly decreasing masking ratios when moving to coarser scales because a single visible sub-voxel is sufficient to mask a coarser-scale voxel visible. another alternative is masking at the coarsest scale and upsampling the mask 47 to finer scales which leads to a more consistent masking ratio across scales. how- ever we found that this approach rapidly decreased the size of reconstructed neighborhoods n moving to finer scales. hence we propose the masking scheme depicted in fig. 3a. we mask the coarsest scale first using a random sampling of all occupied voxels v and the probability that a voxel is masked equals masking ratio r. next we take all voxels at scale s 1 within visible voxels of the previous coarser scale s and repeat the sampling of additional masked voxels at scale s 1 with probability r.consequently the total masking ratio at scale rt s is approximately rt s 1 1 r s s 1. 3 by doing so we ensure that coarser scales have a sufficient number of masked voxels without reducing the size of re- constructed neighborhoods at finer scales. only the visible voxels vv of the finest scale are fed to the encoder backbone e. an ablation study presented in sec. 4.6 quantifies the positive effect of hmg on the pretraining and downstream task performance. 3.5. pretrainging loss we use the binary cross entropy loss bce as the occu- pancy loss per scale. the final loss l is the average of all single scale losses l s l 1 s s 1 x s 0 1 vn s x v vn s bce o v s o v s 4 with the ground truth occupancy o v s 0 1 . 4. experiments in this section we discuss the datasets metrics and the evaluation protocol that we use for benchmarking. we com- pare our proposed approach with state-of-the-art methods in the benchmarks and present extensive ablation studies to demonstrate the novelty of our contributions. 4.1. datasets and evaluation metrics the nuscenes dataset 4 is a challenging dataset due to the sparsity of the lidar point cloud. it consists of 700 driving sequences for training 150 for validation and 150 for testing with annotations for a variety of tasks. we evaluate both semantic segmentation and object detection on the nuscenes dataset. we use the mean intersection over union miou as the main evaluation metric for semantic segmentation and the nuscenes detection score nds and mean average precision map for 3d object detection. the waymo open dataset 30 is a large-scale au- tonomous driving dataset. it consists of 798 driving se- quences for training 202 validation sequences and 150 test sequences. we use the mean intersection over union miou and mean accuracy macc as the main metrics for evaluat- ing the semantic segmentation performance. 4.2. task heads this section discusses the methods to evaluate the effec- tiveness of the pretraining and the quality of the learned representation. 22237 page 5 table 1. comparison of lidar semantic segmentation performance on the nuscenes and waymo open datasets. for the first time an ssl pretraining method outperforms strong supervised learning models. methods marked with are our implementation. method ssl pretraining nuscenes waymo val miou val macc test miou test fwiou val miou val macc test miou minkunet 9 - 73.3 - - - 65.9 76.6 69.8 spvnas 31 - - - 77.4 89.7 - - 68.0 cylinder3d 49 - 76.1 - 77.2 89.9 - - - af2s3net 8 - 62.2 - 78.0 88.5 - - - 2dpass 41 - - - 80.8 - - - - sphereformer 20 - 78.4 - 81.9 - 69.9 - - ptv2 36 - 80.2 - 82.6 - 70.6 80.2 - ptv3 37 - 80.4 87.3 82.7 91.1 71.3 80.5 - unipad 44 79.4 - 81.1 - - - - geo-mae 33 78.6 - - - - - - geo-mae 33 ptv3 37 78.9 84.7 - - - - - occupancy-mae 26 72.9 - - - - - - occupancy-mae 26 ptv3 37 80.0 86.1 - - - - - nomae minkunet ours 80.1 86.2 - - - - - nomae ptv3 ours 81.8 87.7 82.6 91.5 72.3 82.5 70.3 fine-tuning in our comparisons with state-of-the-art meth- ods fine-tuning follows the self-supervised pre-training of the encoder. for this purpose a task-specific head is added with randomly initialized weights and both the encoder e and the task-specific head are trained using the annotated dataset. we use the same head as ptv3 37 for the seman- tic segmentation tasks and the same head as our baseline uvtr 23 for the object detection task. we use layer-wise learning rate decay llrd 17 to avoid forgetting the ssl representations in the encoder. non-linear probing the purpose of the ablation study is to evaluate the learned representation from our ssl approach on the downstream semantic segmentation task. therefore the encoder is kept frozen after pre-training i.e. no fine- tuning. following probe3d 1 and the insights of ear- lier works 6 17 we use a multi-scale non-linear probe nonlp instead of the commonly used linear probing pro- tocol. nonlp aggregates the feature tokens of all scales after up-sampling tokens of coarser scales before passing them to a voxel-wise small mlp. nonlp avoids that the representation learning happens in the head. still it is prob- ing the stronger but non-linear features correlating better with transfer performance 6 17 . similar to the commonly used linear probe nonlp is trained for a few epochs using annotated data. 4.3. implementation details we perform the experiments for semantic segmentation in the pointcept 11 framework and in the mmdetection3d 10 framework for the object detection task. we use ptv3 37 as the encoder e unless stated otherwise. we use a single nvidia a100 gpu for the pretraining. we use s 4 for the reconstruction and masking scales corresponding to target voxel sizes of 0.05 0.10 0.20 0.40 meters. the table 2. comparison of object detection performance on the nuscenes dataset with state-of-the-art point-based pre-training methods. following the evaluation protocol of 43 44 the meth- ods are finetuned using 20 labeled frames without cbgs and copy-paste augmentation. methods nds map uvtr-l baseline 46.7 39.0 also 3 48.2 41.2 gd-mae 43 48.8 42.6 learning from 2d 25 49.2 48.8 unipad 44 55.8 48.1 nomae ours 60.9 54.4 input voxel dimension is 0.05 meters. the masking ratio rs of the finest scale is 70 for nuscenes and 85 for waymo. in the self-supervised pre-training the decoder d consists of sparse convolution layers 12 of kernel size 5 followed by a single sparse submanifold convolution layer to generate o. we use common augmentation techniques such as rota- tion scaling and jittering from the semantic segmentation literature 9 31 37 during pretraining. 4.4. benchmarking results in this section we present the benchmarking results for both 3d semantic segmentation and 3d object detection. 3d semantic segmentation tab. 1 summarizes the best- performing methods on the nuscenes and waymo open dataset leaderboards for the lidar semantic segmentation task. the results include the most performant self-supervised pretraining methods. we fine-tuned our model as described in sec. 4.2. we observe that our proposed self-supervised pretraining achieves a miou score of 81.8 on the nuscenes validation set adding 1.4 miou points over the baseline and setting the state-of-the-art. nomae is an ssl method for any architecture minkunet 9 pretrained with nomae out- 22238 page 6 table 3. comparison of different ssl methods using the same backbone with nonlp. the iteration time iter.time is computed using a batch size of 1. sup.res is the finest resolution of pretraining supervision. model miou macc iter.time sup.res geo-mae 33 ptv3 37 53.8 67.7 40.1ms 0.20m occupancy-mae 26 ptv3 37 59.0 73.4 74.0ms 0.10m nomae ours 74.8 85.0 39.8ms 0.05m of-the box achieves miou of 80.1 which is on par with sota architectures. for more results using other architectures see supplementary sec. 8 and sec. 11. the results on the nuscenes semantic segmentation test set are on par with the strong ptv3 baseline for the miou score and outperforms it in the frequency-weighted iou fwiou score by 0.4 . the improvement on the test set is lesser than the validation dataset because nomae has relatively poor performance for the minority class bicycle and we did not make special adaptations for the test set submission. on the waymo open dataset val set our method achieves a miou score of 72.3 and macc of 75.2. nomae improves by 1.0 and 2.0 points in the miou and macc respectively over the baseline ptv3 37 . the current version of the semantic segmentation challenge is relatively new for the waymo open dataset with only a few submissions. with 70.3 miou score on the test set nomae sets a new state- of-the-art on the waymo open dataset single frame semantic segmentation challenge. 3d object detection we present results for object detec- tion on the nuscenes validation set using the fine-tuning approach described in sec. 4.2. we follow the experiment setup of gd-mae 43 and unipad 44 utilizing only 20 of the annotated frames during fine-tuning without the use of cbgs 48 or copy-and-paste object sample 42 augmentation. we adopt the same training settings as the baseline uvtr 23 and do not use test-time augmentation or model ensembling. tab. 2 shows that our approach achieves 60.9 and 54.4 nds and map scores respectively improving by 14.2 in nds points and 15.4 in map points over the uvtr-l 37 baseline and by 5.1 nds points and 5.6 map over the clos- est contrastive ssl method learning-from-2d 44 . the significant improvement demonstrates the effectiveness of our proposed localized multi-scale ssl for 3d object detec- tion with limited annotated data. 4.5. comparison with ssl methods in this experiment we compare the performance of our pro- posed method with the self-supervised pretraining methods of occupancy-mae 26 and geomae 33 with the same encoder architecture of ptv3 37 . tab. 1 shows that our re- implementation of occupancy-mae 26 and geomae 33 table 4. ablation study on the various components in nomae for semantic segmentation on the nuscenes validation set. lines with are with fine-tuning and all the other results are with nonlp. for more details refer to sec. 4.6 model miou macc acc occupancy-mae ptv3 59.0 73.4 91.0 reconstruct only vn 66.7 78.2 92.4 msp naive masking 70.2 82.3 93.5 point-m2ae 47 masking 70.5 82.3 93.6 hmg 72.6 83.9 93.7 n 9 73.3 84.3 94.1 batch size 8 74.8 85.0 94.0 fine-tuning nomae 81.8 87.7 94.9 with the state-of-the-art ptv3 37 backbones marked with outperforms the results reported in the original papers by 0.3 and 7.1 miou points respectively for semantic segmen- tation with fine-tuning on the nuscenes dataset. tab. 3 shows the results of non-linear probing nonlp . we observe that the nonlp performance in tab. 3 corre- lates with the fine-tuning results in tab. 1. furthermore the relative performance improvement of nomae over the baselines is higher for nonlp in comparison to fine-tuning which indicates richer representation learning. additionally the time of a single training step iteration time is lowest for nomae despite the multi-scale pretraining and a much finer resolution of the pretraining supervision. we note that the iteration time of nomae is 10ms for the pretraining with a single scale. 4.6. ablation study in this section we present ablation studies on the nuscenes semantic segmentation validation set to investigate the de- sign choices of the proposed method. we performed the ex- periments using the pre-trained frozen encoder using nonlp. please refer to sec. 4.2 for further details. detailed study of nomae this experiment evaluates the improvement due to our proposed contributions and the results are presented in tab. 4. we start from our implemen- tation of occupancy-mae 26 as in tab. 3. reconstructing only the local neighborhood vn of visible voxels vv increases the nonlp miou from 59.0 to 66.7 . this requires re- placing the occupancy-mae decoder with our proposed upsampling module and neighborhood decoder. adding the multi-scale reconstruction msp from sec. 3.3 further im- proves the miou to 70.1 for naive mask construction and to 70.5 for masking strategy from point-m2ae 47 as op- posed to single-scale reconstruction in occupancy-mae. our proposed hierarchical mask generation from sec. 3.4 yields an improvement of 72.46 miou as further investi- gated in sec. 4.6. moreover increasing the reconstruction neighborhood size from 5 to 9 improves the miou to 74.8 as further investigated in sec. 4.6. reducing the batch size 22239 page 7 table 5. reconstruction and masking on different single scales with multi-scale pretext msp and hierarchical mask generation hmg . results are reported on the nuscenes val set with the encoder frozen after ssl pretraining with nonlinear probing. iou p and iou t are the iou for the classes pedestrian and truck respectively. model miou macc acc iou p iou t single scale 2s 1 63.8 74.4 91.5 72.6 68.8 single scale 2s 2 66.7 78.2 92.4 75.6 74.3 single scale 2s 4 68.0 80.0 92.8 76.5 75.5 single scale 2s 8 68.3 80.5 93.0 75.8 76.2 single scale 2s 16 67.3 80.3 92.7 73.4 76.6 msp 2s 1 2 4 8 naive masking 70.2 82.3 93.5 83.2 79.6 point-m2ae 47 70.5 82.3 93.6 82.5 81.2 hmg ours 72.6 83.9 93.7 85.2 83.6 to 8 further investigated in sec. 6.1 of the supplementary material yields the final nonlp performance of 74.8 and fine-tuning performance of 81.8 miou score. mask block size msp and hmg this experiment in- vestigates different reconstruction and mask scales s for our ssl task. tab. 5 compares the performance of reconstructing different single scales with the proposed multi-scale pretext msp from sec. 3.3. the experiment of msp without hierar- chical mask generation hmg uses either a random mask at the finest scale which is pooled to generate masks of coarser scales as described in the naive solution in sec. 3.4 or the method of point-m2ae 47 . it can be observed that no single-scale occupancy task is suitable for all object types. for example trucks benefit from a coarser occupancy reconstruction while pedestrians prefer a finer resolution in the pretraining task except for the very fine scales of 2s 1 2 . msp combines coarse and fine tasks thereby maximizing the overall miou. we observe that hmg achieves an additional improvement of 2.4 and 2.1 miou over random mask at the finest scale and the mask generation proposed in point-m2ae 47 respectively. this underlines the importance of proper training examples at all scales. neighborhood size fig. 4 investigates the effect of the neighborhood size number of voxels to generate vn from vv. for example n 5 indicates that a total of 5 vox- els are covered in all 3 x y z dimensions for every scale. experiments use msp and hmg. since every scale has a different voxel size the reconstructed volume depends on the scale. we observe a maximum of the nonlp performance for n 9. our interpretation is that a smaller neighbor- hood does not cover sufficient lidar measurements for reconstruction while a larger neighborhood hinders local representations in the pretraining. further we observe that limiting the reconstruction size performs consistently better than reconstructing the whole space which would corre- spond to the method of occupancy-mae 26 . masking ratio fig. 5 investigates the effect of different 3 5 7 9 11 13 70 71 72 73 74 69.7 72.5 72.4 73.3 73.2 73.1 size in voxels nuscene semseg miou miou figure 4. size number of voxels of the reconstructed neighbor- hood n around visible voxels vv to create vn in the proposed pretext task. we observe that the downstream nonlp semantic segmentation peaks at n 9. note that n corresponds to the method of 26 . 60 65 70 75 80 85 60 61 62 63 64 65 63 63.7 64.4 62.7 60.6 waymo semseg miou 60 65 70 75 80 85 90 95 72 73 72.6 72.9 73.3 72.8 72 total masking ratio rt 0 nuscenes semseg miou figure 5. nonlp performance over the masking ratio rt on the nuscenes and waymo datasets. we observe that the optimal rt 0 is 70 for the nuscenes and 85 for the waymo open dataset. our interpretation is that waymo requires a higher masking ratio due to the higher density of the lidar point cloud. masking ratios on the quality of the representations for se- mantic segmentation on the waymo and nuscenes datasets. this experiment uses a constant r s r to achieve the total masking ratio rt according to eq. 3. we observe that the optimal total masking ratio is 70 for the nuscenes and 85 for the waymo open dataset. this is intuitive since the point cloud density in waymo ca. 180k points per frame is higher than the relatively sparse nuscenes ca. 34k points per frame lidar point clouds. data efficient nuscenes tab. 6 analyzes the performance under limited annotated data for semantic segmentation on the nuscenes dataset. the sub-sampling of the data is per- formed sequence-wise meaning that all frames of a sequence are either included or excluded. for example 0.1 indicates that only one of the 1000 scenes of the nuscenes dataset is used namely scene-0392. in every experiment all se- quences of the training set are used for the self-supervised pertaining. we observe that nomae fine-tune consistently outperforms training from scratch which demonstrates the method s ability to benefit from unannotated data. further- 22240 page 8 baseline output nomae output improvement error map a b baseline output nomae output ground truth c figure 6. qualitative comparison of semantic segmentation performance with ptv3 37 and object detection performance with uvtr 23 . table 6. results with varying amounts of annotated data evaluated for semantic segmentation miou on the nuscenes val set. method annotated scenes 0.1 1 10 50 100 ptv3 37 28.2 41.6 68.7 78.7 80.4 nomae nonlp 35.7 47.5 67.7 73.8 74.2 nomae fine-tune 35.8 48.1 69.9 80.1 81.8 more we observe that in experiments with very little data nonlp performs similarly to fine-tuning. this suggests that nomae can benefit from the unannotated data to learn a sufficiently strong representation in the encoder such that the fine-tuning of the encoder has little benefit. 4.7. qualitative evaluations we present qualitative comparisons in fig. 6. in a and b for semantic segmentation we observe that nomae improves the accuracy by reducing class mix-up and by im- proving the boundaries between objects. in a we observe that the baseline mis-segments the truck while nomae accurately recognizes it. in b we see that nomae recog- nizes the smaller object missed by the baseline the bottom left pole . we can also see that it fails to recognize some drivable areas in a . in example c we visualize the detec- tions from nomae. we see that compared to the baseline nomae is able to more accurately estimate the orientation of the objects and has higher true positive detections. we also see both models hallucinating in further away regions on the left and nomae fails to detect the pedestrian on the left. more qualitative results are presented in sec. 12 of the supplementary material. 5. conclusion in this work we proposed nomae a novel multi-scale self- supervised learning framework for large-scale point clouds. observing the large-scale nature of lidar point clouds nomae reconstructs only local neighborhoods keeping the computation tractable at higher voxel resolutions avoiding information leakage and learning a localized representation suitable for diverse downstream perception tasks. enabled by its efficiency nomae utilizes multiple scales in the pre- training enabling the model to learn both coarse and fine rep- resentations. a novel hierarchical mask generation scheme balances the pre-training of coarse and fine features which is important for objects of different sizes such as pedestrians and trucks we presented experimental results that underline the benefit of our proposed contributions achieving state-of- the-art performance on multiple benchmarks. limitations nomae is sensitive to the density of the point cloud and future work will investigate the proposed method for sparse 3d sensors such as radar. additionally nomae does not utilize the temporal nature of lidar data which can open the door for further performance improvement. furthermore the application of high-resolution sparse 3d representations in the encoders should be further investigated for the object detection task where 2d bird s eye view and low-resolution 3d approaches are still dominant. 22241 page 9 references 1 mohamed el banani amit raj kevis-kokitsi maninis ab- hishek kar yuanzhen li michael rubinstein deqing sun leonidas j. guibas justin johnson and varun jampani. prob- ing the 3d awareness of visual foundation models. proc. of the ieee conf. on computer vision and pattern recognition pages 21795 21806 2024. 5 2 hangbo bao li dong songhao piao and furu wei. beit bert pre-training of image transformers. in int. conf. on learning representations 2022. 1 3 alexandre boulch corentin sautier bj orn michele gilles puy and renaud marlet. also automotive lidar self- supervision by occupancy estimation. in proc. of the ieee conf. on computer vision and pattern recognition 2023. 2 4 5 4 holger caesar varun bankiti alex h. lang sourabh vora venice erin liong qiang xu anush krishnan yu pan gi- ancarlo baldan and oscar beijbom. nuscenes a multimodal dataset for autonomous driving. in proc. of the ieee conf. on computer vision and pattern recognition 2020. 2 4 5 ting chen simon kornblith mohammad norouzi and geof- frey e. hinton. a simple framework for contrastive learning of visual representations. in int. conf. on machine learning 2020. 1 6 xinlei chen and kaiming he. exploring simple siamese representation learning. in proc. of the ieee conf. on com- puter vision and pattern recognition 2021. 5 7 xinlei chen haoqi fan ross b. girshick and kaiming he. improved baselines with momentum contrastive learning. arxiv preprint arxiv 2003.04297 2020. 1 8 ran cheng ryan razani ehsan moeen taghavi enxu li and bingbing liu. af 2-s3net attentive feature fusion with adaptive feature selection for sparse semantic segmentation network. proc. of the ieee conf. on computer vision and pattern recognition pages 12542 12551 2021. 5 9 christopher b. choy junyoung gwak and silvio savarese. 4d spatio-temporal convnets minkowski convolutional neural networks. in proc. of the ieee conf. on computer vision and pattern recognition 2019. 5 13 10 mmdetection3d contributors. mmdetection3d open- mmlab next-generation platform for general 3d object detection. https github.com open- mmlab mmdetection3d 2020. 5 11 11 pointcept contributors. pointcept a codebase for point cloud perception research. https github.com pointcept pointcept 2023. 5 11 12 spconv contributors. spconv spatially sparse convolu- tion library. https github.com traveller59 spconv 2022. 5 13 alexey dosovitskiy lucas beyer alexander kolesnikov dirk weissenborn xiaohua zhai thomas unterthiner mostafa dehghani matthias minderer georg heigold syl- vain gelly jakob uszkoreit and neil houlsby. an image is worth 16x16 words transformers for image recognition at scale. int. conf. on learning representations 2021. 2 14 peng gao teli ma hongsheng li ziyi lin jifeng dai and yu jiao qiao. mcmae masked convolution meets masked autoencoders. in proc. of the conf. on neural information processing systems 2022. 1 15 nikhil gosala k ursat petek paulo lj drews-jr wolfram burgard and abhinav valada. skyeye self-supervised bird s- eye-view semantic mapping using monocular frontal view images. in proc. of the ieee conf. on computer vision and pattern recognition pages 14901 14910 2023. 16 kaiming he haoqi fan yuxin wu saining xie and ross b. girshick. momentum contrast for unsupervised visual rep- resentation learning. in proc. of the ieee conf. on com- puter vision and pattern recognition 2020. 17 kaiming he xinlei chen saining xie yanghao li piotr doll ar and ross girshick. masked autoencoders are scalable vision learners. in proc. of the ieee conf. on computer vi- sion and pattern recognition 2022. 1 5 11 18 georg hess johan jaxing elias svensson david hagerman christoffer petersson and lennart svensson. masked autoen- coder for self-supervised pre-training on lidar point clouds. in proc. of the ieee winter conf. on applications of computer vision 2023. 1 2 3 4 19 ji hou benjamin graham matthias nie\u00dfner and saining xie. exploring data-efficient 3d scene understanding with contrastive scene contexts. proc. of the ieee conf. on com- puter vision and pattern recognition pages 15582 15592 2020. 2 20 xin lai yukang chen fanbin lu jianhui liu and jiaya jia. spherical transformer for lidar-based 3d recognition. proc. of the ieee conf. on computer vision and pattern recognition pages 17545 17555 2023. 5 21 christopher lang alexander braun lars schillingmann and abhinav valada. self-supervised multi-object tracking for au- tonomous driving from consistency across timescales. ieee robotics and automation letters 8 11 7711 7718 2023. 1 22 christopher lang alexander braun lars schillingmann karsten haug and abhinav valada. self-supervised rep- resentation learning from temporal ordering of automated driving sequences. ieee robotics and automation letters 9 3 2582 2589 2024. 1 23 yanwei li yilun chen xiaojuan qi zeming li jian sun and jiaya jia. unifying voxel-based representation with trans- former for 3d object detection. in proc. of the conf. on neural information processing systems 2022. 5 6 8 11 15 24 haotian liu mu cai and yong jae lee. masked discrimina- tion for self-supervised learning on point clouds. in proc. of the europ. conf. on computer vision 2022. 1 2 25 yueh-cheng liu yu-kai huang hungyueh chiang hung- ting su zhe yu liu chin-tang chen ching-yu tseng and winston h. hsu. learning from 2d pixel-to-point knowledge transfer for 3d pretraining. arxiv preprint arxiv 2104.04687 2021. 5 26 chen min xinli xu dawei zhao liang xiao yiming nie and bin dai. occupancy-mae self-supervised pre-training large-scale lidar point clouds with masked occupancy au- toencoders. ieee transactions on intelligent vehicles 9 5150 5162 2022. 1 2 3 4 5 6 7 27 yatian pang wenxiao wang francis e. h. tay wei liu yonghong tian and li yuan. masked autoencoders for point 22242 page 10 cloud self-supervised learning. in proc. of the europ. conf. on computer vision 2022. 1 2 28 bohao peng xiaoyang wu li jiang yukang chen heng- shuang zhao zhuotao tian and jiaya jia. oa-cnns omni- adaptive sparse cnns for 3d semantic segmentation. proc. of the ieee conf. on computer vision and pattern recognition pages 21305 21315 2024. 13 29 jonas schramm niclas v odisch k ursat petek b ravi kiran senthil yogamani wolfram burgard and abhinav valada. bevcar camera-radar fusion for bev map and object segmen- tation. in int. conf. on intelligent robots and systems pages 1435 1442 2024. 1 30 pei sun henrik kretzschmar xerxes dotiwalla aurelien chouard vijaysai patnaik paul tsui james guo yin zhou yuning chai benjamin caine vijay vasudevan wei han jiquan ngiam hang zhao aleksei timofeev scott et- tinger maxim krivokon amy gao aditya joshi yu zhang jonathon shlens zhifeng chen and dragomir anguelov. scalability in perception for autonomous driving waymo open dataset. in proc. of the ieee conf. on computer vision and pattern recognition 2020. 2 4 31 haotian tang zhijian liu shengyu zhao yujun lin ji lin hanrui wang and song han. searching efficient 3d architectures with sparse point-voxel convolution. in proc. of the europ. conf. on computer vision 2020. 5 32 keyu tian yi jiang qishuai diao chen lin liwei wang and zehuan yuan. designing bert for convolutional net- works sparse and hierarchical masked modeling. in int. conf. on learning representations 2023. 1 33 xiaoyu tian haoxi ran yue wang and hang zhao. geo- mae masked geometric target prediction for self-supervised point cloud pre-training. in proc. of the ieee conf. on com- puter vision and pattern recognition 2023. 1 2 3 4 5 6 13 34 hanchen wang qi liu xiangyu yue joan lasenby and matt j. kusner. unsupervised point cloud pre-training via occlusion completion. int. conf. on computer vision pages 9762 9772 2020. 2 35 peng-shuai wang. octformer octree-based transformers for 3d point clouds. acm transactions on graphics 42 1 11 2023. 13 36 xiaoyang wu yixing lao li jiang xihui liu and heng- shuang zhao. point transformer v2 grouped vector attention and partition-based pooling. in advances in neural informa- tion processing systems 2022. 5 37 xiaoyang wu li jiang peng-shuai wang zhijian liu xihui liu yu qiao wanli ouyang tong he and hengshuang zhao. point transformer v3 simpler faster stronger. proc. of the ieee conf. on computer vision and pattern recognition pages 4840 4851 2023. 2 3 5 6 8 11 12 13 14 38 xiaoyang wu zhuotao tian xin wen bohao peng xi- hui liu kaicheng yu and hengshuang zhao. towards large-scale 3d representation learning with multi-dataset point prompt training. in proc. of the ieee conf. on computer vi- sion and pattern recognition 2024. 11 39 saining xie jiatao gu demi guo charles r. qi leonidas j. guibas and or litany. pointcontrast unsupervised pre- training for 3d point cloud understanding. in proc. of the europ. conf. on computer vision 2020. 2 40 runsen xu tai wang wenwei zhang runjian chen jinkun cao jiangmiao pang and dahua lin. mv-jar masked voxel jigsaw and reconstruction for lidar-based self-supervised pre- training. in proc. of the ieee conf. on computer vision and pattern recognition 2023. 1 2 3 41 xu yan jiantao gao chaoda zheng chaoda zheng ruimao zhang shenghui cui and zhen li. 2dpass 2d priors assisted semantic segmentation on lidar point clouds. in proc. of the europ. conf. on computer vision 2022. 5 42 yan yan yuxing mao and bo li. second sparsely em- bedded convolutional detection. sensors 18 10 2018. 6 16 43 honghui yang tong he jiaheng liu hua chen boxi wu binbin lin xiaofei he and wanli ouyang. gd-mae gen- erative decoder for mae pre-training on lidar point clouds. in proc. of the ieee conf. on computer vision and pattern recognition 2023. 1 2 3 4 5 6 44 honghui yang sha zhang di huang xiaoyang wu haoyi zhu tong he shixiang tang hengshuang zhao qibo qiu binbin lin xiaofei he and wanli ouyang. unipad a uni- versal pre-training paradigm for autonomous driving. proc. of the ieee conf. on computer vision and pattern recognition pages 15238 15250 2023. 2 5 6 45 xumin yu lulu tang yongming rao tiejun huang jie zhou and jiwen lu. point-bert pre-training 3d point cloud transformers with masked point modeling. in proc. of the ieee conf. on computer vision and pattern recognition 2022. 1 46 karim abou zeid jonas schult alexander hermans and bastian leibe. point2vec for self-supervised representation learning on point clouds. in dagm german conference on pattern recognition pages 131 146. springer 2023. 2 47 renrui zhang ziyu guo peng gao rongyao fang bin zhao dong wang yu qiao and hongsheng li. point-m2ae multi-scale masked autoencoders for hierarchical point cloud pre-training. in proc. of the conf. on neural information processing systems 2022. 1 4 6 7 48 benjin zhu zhengkai jiang xiangxin zhou zeming li and gang yu. class-balanced grouping and sampling for point cloud 3d object detection. arxiv preprint arxiv 1908.09492 2019. 6 16 49 xinge zhu hui zhou tai wang fangzhou hong yuexin ma wei li hongsheng li and dahua lin. cylindrical and asymmetrical 3d convolution networks for lidar segmentation. in proc. of the ieee conf. on computer vision and pattern recognition 2021. 5 22243",
      "keywords": [
        "page",
        "multi-scale",
        "neighborhood",
        "occupancy",
        "masked",
        "autoencoder",
        "self-supervised",
        "learning",
        "lidar",
        "point",
        "clouds",
        "mohamed",
        "abdelsamad1",
        "michael",
        "ulrich1",
        "claudius",
        "aser1",
        "abhinav",
        "valada2",
        "1bosch",
        "center",
        "2university",
        "freiburg",
        "abstract",
        "autoencoders",
        "mae",
        "shown",
        "tremendous",
        "potential",
        "ssl",
        "vision",
        "beyond.",
        "however",
        "from",
        "lidars",
        "used",
        "auto-",
        "mated",
        "driving",
        "particularly",
        "challenging",
        "maes",
        "since",
        "large",
        "areas",
        "volume",
        "empty.",
        "consequently",
        "ex-",
        "isting",
        "work",
        "suffers",
        "leaking",
        "information",
        "into",
        "decoder",
        "significant",
        "computational",
        "complexity",
        "thereby",
        "limiting",
        "pre-training",
        "only",
        "bird",
        "eye",
        "view",
        "encoders",
        "practice.",
        "propose",
        "novel",
        "nomae",
        "over-",
        "comes",
        "aforementioned",
        "challenges",
        "employing",
        "reconstruction",
        "non-",
        "voxels.",
        "incorporate",
        "voxel",
        "masking",
        "occu-",
        "pancy",
        "multiple",
        "scales",
        "proposed",
        "hierarchical",
        "mask",
        "generation",
        "technique",
        "capture",
        "features",
        "objects",
        "different",
        "sizes",
        "cloud.",
        "nomaes",
        "extremely",
        "flexible"
      ],
      "metadata": {
        "filename": "Abdelsamad_Multi-Scale_Neighborhood_Occupancy_Masked_Autoencoder_for_Self-Supervised_Learning_in_LiDAR_CVPR_2025_paper.pdf",
        "original_filename": "Abdelsamad_Multi-Scale_Neighborhood_Occupancy_Masked_Autoencoder_for_Self-Supervised_Learning_in_LiDAR_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 4046168,
        "upload_file_id": "upload_1752161187528_2mpddpr44",
        "upload_timestamp": "2025-07-10T15:26:27.596683",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:26:28.074431+00:00",
      "content_length": 48832,
      "content_type": ".pdf",
      "filename": "Abdelsamad_Multi-Scale_Neighborhood_Occupancy_Masked_Autoencoder_for_Self-Supervised_Learning_in_LiDAR_CVPR_2025_paper.pdf"
    },
    "16": {
      "file_id": "16",
      "content": "\n=== Page 1 ===\nATP: Adaptive Threshold Pruning for Efficient Data Encoding in Quantum\nNeural Networks\nMohamed Afane1, *\nGabrielle Ebbrecht1\nYing Wang2\nJuntao Chen1, *\nJunaid Farooq3\n1Fordham University\n2Stevens Institute of Technology\n3University of Michigan-Dearborn\nAbstract\nQuantum Neural Networks (QNNs) offer promising capa-\nbilities for complex data tasks, but are often constrained\nby limited qubit resources and high entanglement, which\ncan hinder scalability and efficiency. In this paper, we in-\ntroduce Adaptive Threshold Pruning (ATP), an encoding\nmethod that reduces entanglement and optimizes data com-\nplexity for efficient computations in QNNs. ATP dynam-\nically prunes non-essential features in the data based on\nadaptive thresholds, effectively reducing quantum circuit\nrequirements while preserving high performance. Extensive\nexperiments across multiple datasets demonstrate that ATP\nreduces entanglement entropy and improves adversarial ro-\nbustness when combined with adversarial training meth-\nods like FGSM. Our results highlight ATP\u2019s ability to bal-\nance computational efficiency and model resilience, achiev-\ning significant performance improvements with fewer re-\nsources, which will help make QNNs more feasible in prac-\ntical, resource-constrained settings.\n1. Introduction\nQuantum machine learning (QML) has gained attention for\nits ability to solve problems that are difficult for classical\nmodels by using the unique properties of quantum systems\n[1]. It has shown practical benefits in fields such as chem-\nistry [32], optimization [13], and others involving high-\ndimensional or structured data [7, 37].However, quantum\nalgorithms are still constrained by the availability of qubits,\nnoise, and decoherence in hardware [19, 29]. Because of\nquantum hardware\u2019s sensitivity to the environment, qubits\nare prone to premature collapse that degrades the fidelity of\ncomputation. Qubits may also be disturbed by signals in-\ntended to alter the state of another qubit in close proximity\n[4] which reduces stability in hardware with many qubits\nand complicates the training of quantum models. As a re-\nsult, shallower Quantum Neural Networks (QNNs) are of-\nten more practical. [9, 38].\n*Corresponding authors: {mafane,jchen504}@fordham.edu\nFigure 1. Demonstration of the adaptive threshold pruning frame-\nwork.\nThe original image (top) is split into a 3x3 grid, with\neach section assessed for information density. The pruned image\n(bottom) shows filtered regions that fall below a defined intensity\nthreshold in blue indicating areas that do not contribute signifi-\ncantly to the classification task, effectively freeing qubits and opti-\nmizing resources by focusing only on key areas of high relevance.\nEfficient data encoding into quantum states remains a\ncrucial bottleneck for achieving scalability and accuracy in\nQML applications [30, 41]. For instance, limiting qubit in-\nteractions can reduce crosstalk and enhance stability, which\nis essential for noise resilience [2].\nWhile using larger\nand deeper QNNs can enhance a model\u2019s ability to cap-\nture complex data relationships and potentially improve per-\nformance, increasing encoding depth introduces new chal-\nlenges. As the depth of the encoding circuit grows, encoded\nquantum states tend to converge towards a maximally mixed\nstate at an exponential rate, becoming increasingly indistin-\nguishable from random noise [21]. This concentration ef-\nfect leads to a lack of state diversity and the emergence of\nbarren plateaus, where gradients vanish, significantly hin-\ndering the model\u2019s learning process [26]. These effects are\nespecially problematic when encoding is not adapted to the\nstructure or scale of the data. Thus, QNNs must balance ex-\npressibility with computational demands and sensitivity to\nbarren plateaus [25].\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n20427\n\n=== Page 2 ===\nFigure 2.\nAverage accuracy across four datasets, comparing\nthe performance of various encoding methods: Adaptive Thresh-\nold Pruning (ATP), Principal Component Analysis (PCA), Single\nQubit Encoding (SQE), Angle, and Amplitude encoding. Hori-\nzontal error bars represent entanglement entropy (EE), with longer\nbars indicating higher entanglement. ATP generally achieves the\nhighest accuracy with lower EE.\nWhile efficient data encoding can improve performance\nin classical models, such as by reducing training time or\nresource use, it is far more critical in QML. In quantum\nmodels, encoding directly impacts whether meaningful pat-\nterns can be learned in the first place, since poor encoding\ncan waste limited qubit resources or amplify noise to the\npoint that the model fails entirely [22]. For instance, some\nmethods only encode states with significant contributions to\nthe model, allowing qubit resources to focus on meaningful\ndata and enabling more effective scaling [34]. Amplitude\nencoding is one such approach, representing data compactly\nto reduce the number of required qubits, though it limits the\ntypes of data and operations that can be used [33]. Other\nstrategies, such as Qubit Lattice, a direct encoding method\nwith low circuit depth, and FRQI, a compact scheme that\nuses fewer qubits, demonstrate different tradeoffs between\nqubit efficiency and processing flexibility [22]. In general,\nencoding strategies that reduce resource overhead can op-\ntimize computation and limit potential errors by requiring\nfewer qubits to operate.\nIn this context, entanglement entropy (EE) serves as a\nkey metric for assessing a model\u2019s encoding efficiency. EE\nmeasures the degree of correlation between different qubits\nin a quantum system by quantifying the information shared\nbetween subsystems, typically calculated as the von Neu-\nmann entropy of a reduced density matrix of one part of a\nbipartite system. Higher EE indicates a greater level of in-\nterdependence between qubits, which can be advantageous\nfor capturing complex data structures, but excessive entan-\nglement may increase computational complexity and lead\nto overfitting. Anagiannis and Cheng, for instance, found\nthat as EE rises in q-convolutional neural networks, the\nmodel\u2019s cost function decreases, suggesting a link between\nstructured entanglement and efficient learning [3]. Simi-\nlarly, Martyn et al. demonstrated that while certain levels of\nentanglement improve training accuracy in quantum mod-\nels, over-entangling the qubits can lead to diminished gen-\neralization and higher resource demands [24]. Thus, EE\nnot only provides insight into the information distribution\nacross qubits but also helps balance model complexity and\ncomputational efficiency in quantum learning.\nTo address these challenges, we introduce Adaptive\nThreshold Pruning (ATP), a technique that optimizes the\nencoding process by adaptively pruning non-essential fea-\ntures, thereby reducing both qubit usage and entanglement\nin the circuit, as visualized by Figure 1. ATP sets dynamic\nthresholds for feature pruning, selectively removing low-\nvariance data to maintain model performance while lower-\ning EE. Figure 2 summarizes ATP\u2019s performance compared\nto other encoding methods, demonstrating improvements in\nboth accuracy and entanglement efficiency.\n2. Related Works\nMasking: In classical machine learning, masking strategies\nare commonly used in self-supervised learning by train-\ning models to reconstruct missing parts of the input from\npartial observations. Masked autoencoding, as used in the\nMasked Autoencoders (MAE) framework, achieves this by\nrandomly hiding image patches and requiring the model\nto reconstruct them, encouraging the learning of meaning-\nful representations without labels [14]. Although the pri-\nmary objective is representation learning, MAE improves\nefficiency by processing only visible patches through a\nlightweight encoder-decoder setup. This reduction in input\ncomplexity helps scale models to large datasets with fewer\ncomputational demands. Similar to how MAE filters input\nthrough random masking, our ATP approach uses threshold-\nbased pruning to discard low-variance regions before en-\ncoding, allowing quantum models to focus on the most rel-\nevant parts of the data while reducing resource usage [40].\nPruning: In contrast, pruning in QML has primarily been\napplied at the circuit level to reduce model complexity,\naddressing challenges such as noise and limited quantum\nresources.\nThese techniques streamline QNNs by selec-\ntively removing parameters with minimal impact on accu-\nracy, thereby enhancing computational efficiency and scal-\nability. Hu et al. [16] propose an architecture compres-\nsion method for quantum circuits, achieving reduced cir-\ncuit depth with minimal accuracy loss, while Wang et al.\n[39] demonstrate how pruning supports scalability in quan-\ntum neural networks. Circuit-level pruning methods, such\nas those by Sim et al. [36] and Kulshrestha et al. [20], adap-\ntively eliminate non-essential parameters, optimizing quan-\n20428\n\n=== Page 3 ===\ntum resources without compromising model performance.\nWhile architecture pruning addresses circuit complexity, ef-\nficient data encoding remains critical in quantum contexts,\ndirectly impacting qubit usage and entanglement manage-\nment.\nData encoding: Several encoding methods have been de-\nveloped to optimize qubit usage while preserving essential\ndata features in quantum neural networks. For example,\nSingle-Qubit Encoding (SQE) [10] minimizes resource re-\nquirements by encoding data into a single qubit rather than\nmultiple qubits. This technique uses a series of Z-Y-Z axis\nrotations on the Bloch sphere, parameterized by input data,\nto capture spatial relationships with minimal qubit and pa-\nrameter usage. Classical methods like Principal Component\nAnalysis (PCA) [18] and autoencoding [11] also improve\nquantum model efficiency by reducing data dimensionality\nwhile retaining key features [17]. PCA identifies principal\ncomponents with the highest variance, focusing on infor-\nmative aspects of the data, while autoencoders learn com-\npact representations through non-linear dimensionality re-\nduction.\nThe choice of encoding strategy directly impacts the ef-\nfectiveness of quantum methods by influencing qubit effi-\nciency and circuit complexity. Angle encoding, commonly\nused for classification tasks, maps classical data to quantum\ngate rotations due to its simplicity [31]. Amplitude encod-\ning, which embeds data as probability amplitudes, allows\nquantum representation of exponentially large datasets but\npresents scalability challenges. Basis encoding represents\nbinary data as computational basis states, suitable for bi-\nnary data but less practical for complex datasets. Hybrid\nmethods, such as amplitude-angle encoding, aim to com-\nbine strengths of individual techniques but often increase\ncircuit depth, posing issues on noisy hardware with limited\ncoherence times [6]. These limitations emphasize the need\nfor adaptable encoding methods like ATP that optimize en-\ncoding based on data relevance and variability.\nOur approach: ATP distinguishes itself by dynamically\npruning with adaptive thresholds. Unlike PCA, which ap-\nplies linear transformations to capture data variance, ATP\nzeroes out less informative data points based on their rele-\nvance, adapting to each dataset\u2019s unique structure. Unlike\nSQE, which minimizes resources by encoding data into a\nsingle qubit, ATP selectively retains expressive data fea-\ntures with minimal resource overhead. By concentrating\nquantum resources on essential data, ATP balances compu-\ntational efficiency and entanglement requirements, offering\na scalable solution tailored to the specific constraints and\nadvantages of quantum systems.\nTo our knowledge, no prior work has addressed adaptive\nfeature pruning within quantum data encoding. ATP thus\nfills this gap by offering a systematic approach to reduce\nquantum overhead while preserving model accuracy.\n3. Preliminaries\n3.1. Quantum Computing Fundamentals\nQuantum computing exploits principles of quantum me-\nchanics, providing a framework that differs profoundly\nfrom classical systems. In classical computing, data is rep-\nresented by bits in distinct states of 0 or 1. Quantum com-\nputing, however, relies on qubits, which can exist in linear\ncombinations of 0 and 1 states, represented as:\n  | \\ psi \\ rangle = \\alpha |0\\rangle + \\beta |1\\rangle , \n(1)\nwhere  \\alpha and  \\beta are complex numbers satisfying  |\\a l\npha | ^2 + |\\beta |^2 = 1 . This property, known as superposition, allows\nqubits to perform concurrent calculations, making quantum\nalgorithms inherently parallel [15, 35].\n3.1.1\nEntanglement and Measurement\nEntanglement is a quantum phenomenon in which qubits\nbecome correlated in such a way that the state of one qubit\ndirectly influences the state of another, regardless of physi-\ncal distance [28]. For two entangled qubits, the state can be\nexpressed as:\n  | \\\np\ns\ni  \\ran g le = \\frac {1}{\\sqrt {2}} \\left ( |00\\rangle + |11\\rangle \\right ), \n(2)\nforming a maximally entangled Bell state. Entanglement\nis a crucial resource in QNNs, as it enables intricate data\nencoding and interaction patterns that cannot be replicated\nclassically. However, the control and preservation of en-\ntanglement are challenging due to decoherence and noise,\nmaking efficient encoding schemes essential for managing\nentanglement and optimizing qubit usage [12].\n3.2. Quantum Neural Networks\nQNNs use quantum circuits with parameterized gates,\nknown as PQCs, to process data. A QNN layer can be de-\nfined by a series of parameterized unitary transformations\n[43]:\n  U( \\\nt\nh\neta ) = \\prod _{j} U_j(\\theta _j), \n(3)\nwhere  \\ theta = \\{\\theta _j\\} are tunable parameters. Each  U_j represents\na quantum gate that rotates or entangles qubits based on in-\nput data. These rotations enable the QNN to encode input\nfeatures, transforming them into high-dimensional Hilbert\nspace representations. During training, gradients of these\nparameters are optimized to minimize a loss function, anal-\nogous to classical neural networks, but constrained by quan-\ntum hardware limitations and noise sensitivity [1].\n20429\n\n=== Page 4 ===\n3.3. Data Encoding in Quantum Neural Networks\nEfficient data encoding is critical for the practicality of\nQNNs. In this work, we apply ATP before encoding data\ninto quantum states.\nPrimarily, we use angle encoding,\nwhere each pixel value  x_{i,j} of an image is mapped onto a\nrotation angle  \\theta _{i,j} for the RX gate:\n  \\t h e t a _{i,j} = \\pi \\cdot x_{i,j}, \n(4)\nresulting in the transformation  RX(\\theta _{i,j}) applied to each\nqubit [27].\nThis method straightforwardly represents\npixel intensities but can introduce redundancy for higher-\ndimensional data, as unnecessary values in qubit states in-\ncrease entanglement and computational complexity.\nWe also conduct experiments using amplitude encoding,\nwhich encodes normalized data values as amplitudes of a\nquantum state. For an input vector  \\mathbf {x} with dimension 2^n\n(for  n qubits), amplitude encoding transforms it to:\n  | \\\npsi \n\\\nran\ngle = \\sum _{i=0}^{2^n-1} x_i |i\\rangle , \n(5)\nallowing all features to be encoded simultaneously in a sin-\ngle quantum state. While amplitude encoding can be effi-\ncient for certain data dimensions, it faces scalability chal-\nlenges for large datasets.\nAdditionally, alternative encoding strategies exist, such\nas dense encoding techniques that represent multiple data\nfeatures using fewer qubits by encoding them into differ-\nent parameters of multi-qubit gates. These methods aim to\nbalance representational power with resource constraints,\nbut each encoding approach carries unique trade-offs in\ncomplexity and scalability. Across these various encoding\nschemes, ATP helps reduce redundancy by pruning non-\nessential features before encoding, leading to more efficient\nand robust QNN performance.\n3.4. Entanglement Entropy\nEE is a metric from quantum information theory that quan-\ntifies the correlation between different parts of a quantum\nsystem, typically measured using the von Neumann en-\ntropy of a reduced density matrix [3].\nIn QML, EE re-\nflects a model\u2019s capacity to capture intricate data relation-\nships. Higher EE often indicates stronger interdependence\nbetween qubits, which can increase the model\u2019s expressive\npower but also adds computational complexity and risks\noverfitting [5]. Conversely, lower EE may represent effi-\ncient resource usage and reduced complexity, which is ad-\nvantageous for simpler datasets [23]. Balancing EE is thus\ncrucial, as it influences both learning potential and robust-\nness in noisy environments. Therefore, achieving high per-\nformance with lower EE, or reduced complexity, is an im-\nportant aim of this study.\n4. Methods\nTo select effective threshold values for data pruning, we\nformalize threshold selection as a constrained optimization\nproblem tailored to adapt to each dataset\u2019s unique char-\nacteristics.\nThis approach employs the Limited-memory\nBroyden-Fletcher-Goldfarb-Shanno algorithm with box\nconstraints (L-BFGS-B), a quasi-Newton method that effi-\nciently handles high-dimensional optimization by leverag-\ning gradient-based updates to refine threshold levels. By\nusing this optimization framework, we systematically bal-\nance accuracy with computational efficiency, ensuring that\nthresholds are adjusted to retain essential information while\nminimizing redundancy. The following sections present the\nmathematical formulation and algorithmic steps that drive\nthis threshold-tuning process for enhanced QNN perfor-\nmance.\n4.1. Pruning Function Definition\nTo define the threshold \u03c4 for filtering, we calculate the av-\nerage pixel intensity matrices \u00afx0 and \u00afx1 across all train-\ning samples in each binary class. Specifically, \u00afx0(i, j) and\n\u00afx1(i, j) represent the average intensity values at position\n(i, j) for classes 0 and 1, respectively. Using these aver-\nages, we prune data by setting values to zero at positions\n(i, j) where both class averages fall below the threshold \u03c4,\nas follows:\n  x_{ \\t a\nu\n }\n(i , j) =  \\ b e gin  {case s}  0,\n & \\ tex\nt {if } \\bar {x}_0(i,j) < \\tau \\text { and } \\bar {x}_1(i,j) < \\tau , \\\\ x(i,j), & \\text {otherwise,} \\end {cases} \n(6)\nwhere x(i, j) represents the original pixel intensity at po-\nsition (i, j). This operation generates a pruned dataset X\u03c4\nthat retains only grid positions with sufficient intensity to\ncontribute effectively to classification.\nTo determine the optimal threshold \u03c4 \u2217, we maximize the\ntest accuracy Acctest(X\u03c4):\n  \\ tau\n ^*\n = \\arg \\m ax _{\\tau \\in [0, \\tau _{\\max }]} \\text {Acc}_{\\text {test}}(\\mathcal {X}_{\\tau }), \n(7)\nwhere \u03c4max is the upper bound on \u03c4.\nThis optimiza-\ntion is equivalent to minimizing the negative accuracy\n\u2212Acctest(X\u03c4) over the interval [0, \u03c4max].\n4.2. Gradient-Based Optimization via L-BFGS-B\nTo solve the threshold optimization problem, we use the L-\nBFGS-B algorithm [44], which efficiently manages high-\ndimensional optimization with limited memory. The algo-\nrithm iteratively adjusts \u03c4 by minimizing the negative accu-\nracy:\n  f( \\ tau ) = -\\text {Acc}_{\\text {test}}(\\mathcal {X}_{\\tau }), \n(8)\nupdating \u03c4 at each step according to:\n  \\t a u _ {k +1} = \\tau _k - \\alpha _k \\, H_k \\nabla f(\\tau _k), \n(9)\n20430\n\n=== Page 5 ===\nwhere \u03b1k is the step size, Hk is an approximation of the\ninverse Hessian matrix, and \u2207f(\u03c4k) is the gradient of f(\u03c4k)\nwith respect to \u03c4. The gradient \u2207f(\u03c4k) is computed as:\n  \\nab l\na\n f(\\t\nau\n _k) =  \\ s um _{( i,\nj)\n} \\frac {\\partial f}{\\partial x_{\\tau }(i,j)} \\cdot \\frac {\\partial x_{\\tau }(i,j)}{\\partial \\tau }. \n(10)\nThis approach, as detailed in Algorithm 1, leverages\ngradient information to efficiently converge on the optimal\nthreshold \u03c4 \u2217, ensuring a refined selection of relevant data\nregions.\nAlgorithm 1 Bi-Level Threshold Optimization for QNN\nRequire: Binary class pair (c0, c1), dataset (x, y), encod-\ning function fenc, grid size s, epochs, threshold range\nEnsure: Optimized threshold \u03c4 \u2217and classification accu-\nracy\n1: Filter and resize data for selected class pair (c0, c1),\nconverting labels to binary\n2: Calculate average values for each class and set pixels to\nzero where averages are below \u03c4\n3: Convert processed images to quantum circuits using\nfenc\n4: Initialize QNN model with angle encoding\n5: Define bi-level optimization:\n6:\nInner level: apply threshold, filter data, train QNN\non (xtrain, ytrain)\n7:\nOuter level: maximize test accuracy on (xtest, ytest)\n8: Optimize over \u03c4 within range using L-BFGS-B\n9: Calculate entanglement entropy of final model on test\nset\n10: return optimal threshold \u03c4 \u2217, test accuracy, and entan-\nglement entropy\n4.3. Constraint Handling and Convergence Criteria\nThe box constraints 0 \u2264\u03c4 \u2264\u03c4max are enforced within\nL-BFGS-B by projecting any out-of-bounds updates back\nto the feasible region, ensuring that the threshold remains\nwithin the desired range throughout the optimization. Con-\nvergence is achieved when the norm of the projected gra-\ndient \u2225\u2207f(\u03c4k)\u2225\u221efalls below a predefined tolerance, \u03f5, or\nwhen the maximum number of iterations is reached:\n  \\|\\nabl a  f(\\tau _k)\\|_{\\infty } < \\epsilon . \n(11)\nAdditionally, the accuracy is evaluated on a validation\nset Xval after each iteration to ensure that the optimization\nprocess generalizes effectively to unseen data, mitigating\noverfitting to the training set.\n4.4. Approximation of the Inverse Hessian\nTo achieve computational efficiency, L-BFGS-B approxi-\nmates the Hessian matrix rather than computing it directly\n[44].\nUsing a limited history of m gradient and posi-\ntion vectors {si, yi}k\u22121\ni=k\u2212m, where si = \u03c4i+1 \u2212\u03c4i and\nyi = \u2207f(\u03c4i+1) \u2212\u2207f(\u03c4i), the inverse Hessian approxima-\ntion Hk is updated using the recursive formula:\n  H_ { k +\n1 } = V _k^T H\n_ k V_k + \\rho _k s_k s_k^T, \n(12)\nwhere \u03c1k = (yT\nk sk)\u22121 and Vk = I \u2212\u03c1kyksT\nk . This re-\ncursive update captures the essential curvature information\nwhile minimizing memory overhead, making the optimiza-\ntion feasible for high-dimensional datasets.\nAfter identifying the optimal threshold \u03c4 \u2217, we generate\nthe filtered datasets X\u03c4 \u2217for training and testing, where ir-\nrelevant positions have been pruned based on \u03c4 \u2217. The QNN\nis then trained on X\u03c4 \u2217, leveraging the optimized threshold\nto focus on critical data regions while minimizing computa-\ntional load and enhancing classification performance. Test\naccuracy Acctest(X\u03c4 \u2217) is subsequently evaluated to confirm\nthe effectiveness of threshold optimization.\n5. Experiments\n5.1. Setup\nTo evaluate our approach, we conducted binary classifica-\ntion experiments on multiple benchmark datasets: MNIST,\nFashionMNIST, CIFAR, and PneumoniaMNIST. These\ndatasets were chosen to assess the model\u2019s adaptability\nacross varied image types and complexities. The experi-\nments were implemented using a compact three-layer QNN\nmodel, which serves as the base architecture for all encod-\ning techniques tested. The QNN model applies a parame-\nterized quantum circuit built with data qubits and a readout\nqubit, initialized with an X gate followed by a Hadamard\ngate to prepare it in superposition.\nThe circuit incorpo-\nrates both entangling XX and ZZ gates between the data\nqubits and the readout qubit, with learnable parameters to\ncapture complex dependencies within the input data. A fi-\nnal Hadamard gate is applied to the readout qubit before\nmeasurement, completing the encoding and entanglement\nprocess for each layer.\n5.1.1\nEncoding and Preprocessing Techniques\nFor encoding, Angle and Amplitude methods were applied\ndirectly to the data. In contrast, ATP, SQE, and PCA were\nused beforehand, each applying a different approach to pre-\npare the data before encoding. Specifically, ATP and PCA\nrefine data structure before encoding, while SQE focuses\non single-qubit encoding. The results presented here are\nfrom experiments where these preparatory methods were\nfollowed by Angle encoding. Additional experiments with\ndirect Amplitude encoding, as well as further analyses, are\nincluded in the supplementary materials and ablation study\nsection.\n20431\n\n=== Page 6 ===\n5.1.2\nRealistic Noise Conditions\nTo evaluate robustness in realistic settings, depolarizing\nnoise was introduced at intensities of 3%, 5%, and 10%, al-\nlowing for a comparison of baseline performance on clean\ndata and the model\u2019s resilience under these quantum noise\nlevels. This setup enabled a thorough assessment of each\nencoding method\u2019s effectiveness, revealing how the QNN\nmodel performs across both ideal and noisy environments.\n5.2. Manual Threshold Pruning\nIn this part, we examine the effect of pruning with differ-\nent thresholds on QNN performance. Figure 3 illustrates\nthe data distribution for each position within a 3x3 division\nof the image, where the values represent average intensities\nacross samples from a specific class. As shown, positions\nwith lower information content tend to have closely overlap-\nping values between classes, which can make training more\nchallenging by introducing less distinctive information into\nthe QNN.\nFigure 3.\nData distribution across positions in FashionMNIST\nfor two classes (T-shirt/top and Trouser), with dashed lines mark-\ning threshold levels. Positions with lower variance are pruned to\nstreamline training and focus on more informative regions.\nTo further investigate the impact of these thresholds on\nperformance, we apply a range of manual thresholds across\ndifferent class pairs in both MNIST and FashionMNIST\ndatasets.\nFigures 4 and 5 show that moderate pruning\ngenerally leads to improved accuracy by eliminating non-\nessential features while preserving critical distinctions. In\nFigure 4, results for MNIST indicate that moderate thresh-\nolds enhance accuracy, while higher thresholds reduce it by\nremoving valuable details. Similarly, for FashionMNIST\n(Figure 5), the optimal thresholds cluster between 0.1 and\n0.3, but specific values vary with data distribution charac-\nteristics.\nFigure 4.\nTest accuracy for MNIST class pairs with varying\npruning thresholds. Moderate thresholds improve accuracy, while\nhigher thresholds may exclude key information.\nFigure 5. Test accuracy for FashionMNIST class pairs with vary-\ning thresholds. Similar to MNIST, a threshold around 0.3 gener-\nally provides optimal performance.\nThese results demonstrate that the optimal pruning level\nis influenced by the data distribution, motivating the need\nfor an adaptive thresholding approach.\nThe following\nsection details the results from the bi-level optimization\nmethod used in our framework to automate threshold se-\nlection based on each dataset\u2019s characteristics.\n5.3. Performance Results\nTables 1 and 2 present the classification accuracy and EE,\nrespectively, for different encoding techniques across the\nfour datasets used in the QNN binary classification tasks.\nFor MNIST and FashionMNIST, multiple class pairs were\nchosen to evaluate the model\u2019s capability in distinguishing\nvarious subsets, providing a broader assessment of encod-\ning effectiveness. Across the majority of datasets and class\npairs, ATP achieved the highest accuracy. ATP also consis-\ntently minimized EE compared to other methods, indicating\na more efficient use of quantum resources by lowering com-\n20432\n\n=== Page 7 ===\nTable 1. Performance of encoding techniques on various classes\nfor binary classifications (Accuracy).\nClasses\nAngle\nAmplitude\nATP\nPCA\nSQE\nMNIST\n(0,1)\n96.0\n95.5\n99.0\n99.0\n88.0\n(0,3)\n89.0\n88.5\n91.0\n88.0\n86.0\n(2,4)\n85.0\n84.0\n86.0\n84.5\n82.0\n(5,6)\n86.0\n85.5\n87.0\n85.0\n83.5\n(2,8)\n81.0\n79.5\n83.0\n86.0\n78.5\nFashion MNIST\n(0,1)\n88.5\n88.0\n91.5\n88.5\n86.0\n(2,8)\n86.0\n84.5\n86.0\n86.0\n83.0\n(3,9)\n94.0\n87.0\n94.0\n93.0\n91.0\n(7,9)\n82.0\n78.0\n83.0\n79.0\n77.0\nCIFAR\n(0,1)\n70.0\n68.5\n74.2\n68.0\n66.0\nPneumoniaMNIST\n(0,1)\n81.0\n68.5\n87.0\n80.0\n75.5\nTable 2. Entanglement entropy of encoding techniques on various\nclasses for binary classifications.\nClasses\nAngle\nAmplitude\nATP\nPCA\nSQE\nMNIST\n(0,1)\n0.67\n0.52\n0.39\n0.60\n0.45\n(0,3)\n0.59\n0.53\n0.34\n0.55\n0.44\n(2,4)\n0.77\n0.64\n0.45\n0.56\n0.41\n(5,6)\n0.82\n0.53\n0.41\n0.61\n0.39\n(2,8)\n0.63\n0.56\n0.34\n0.36\n0.34\nFashion MNIST\n(0,1)\n0.56\n0.47\n0.35\n0.58\n0.39\n(2,8)\n0.52\n0.50\n0.35\n0.41\n0.38\n(3,9)\n0.54\n0.62\n0.38\n0.55\n0.42\n(7,9)\n0.59\n0.58\n0.41\n0.64\n0.43\nCIFAR\n(0,1)\n0.63\n0.51\n0.46\n0.65\n0.43\nPneumoniaMNIST\n(0,1)\n0.88\n0.79\n0.37\n0.59\n0.42\nplexity without compromising accuracy.\nTo assess the encoding methods\u2019 resilience to noise, we\nexamined model performance under various levels of de-\npolarizing noise (3-10%). Table 3 highlights that encoding\ntechniques with lower entanglement entropy, such as ATP\nand SQE, showed stronger robustness with accuracy reduc-\ntions typically limited to 3\u20138 points. In contrast, Angle,\nAmplitude, and PCA experienced more substantial drops\nin accuracy of 4\u201317 points. These results emphasize ATP\nand SQE\u2019s effectiveness in preserving model accuracy un-\nder challenging noise conditions, establishing a baseline for\nQNN stability in practical, noisy environments.\nTable 3. Performance of encoding techniques on various classes\nfor binary classifications under depolarizing noise (Accuracy).\nClasses\nAngle\nAmplitude\nATP\nPCA\nSQE\nMNIST\n(0,1)\n87.0\n86.4\n89.0\n89.0\n88.2\n(0,3)\n82.4\n80.5\n83.0\n78.9\n81.9\n(2,4)\n80.2\n68.8\n71.0\n79.7\n79.8\n(5,6)\n70.8\n69.4\n73.0\n68.5\n74.0\n(2,8)\n68.8\n57.4\n76.5\n74.9\n75.2\nFashion MNIST\n(0,1)\n80.5\n79.0\n84.9\n84.9\n82.1\n(2,8)\n76.1\n75.4\n79.3\n78.6\n79.5\n(3,9)\n84.3\n78.2\n87.9\n86.5\n86.7\n(7,9)\n70.0\n66.2\n68.4\n73.2\n74.5\nCIFAR\n(0,1)\n60.4\n59.0\n61.0\n57.8\n58.5\nPneumoniaMNIST\n(0,1)\n70.5\n63.4\n76.0\n67.3\n68.7\n5.4. Adversarial Robustness\nTo evaluate the resilience of each encoding method against\nadversarial attacks, we applied Fast Gradient Sign Method\n(FGSM) with an attack strength of \u03f5 = 0.3 across the\ndatasets. Table 4 provides accuracy results for binary clas-\nsification tasks under these adversarial conditions. In most\ncases, direct encoding methods like Angle and Amplitude\nencoding were generally more susceptible to the attacks,\ndisplaying substantial accuracy reductions. ATP, PCA, and\nSQE exhibited a moderate level of robustness, although per-\nformance varied across datasets. Notably, ATP and PCA\nmaintained comparable accuracy levels in several cases, but\nin certain tasks (PneumoniaMNIST and CIFAR), Angle en-\ncoding unexpectedly achieved the highest accuracy, sug-\ngesting that additional adversarial training methods may be\nrequired to further improve model resilience.\nThe encoding techniques used in this study, includ-\ning ATP, lack inherent robustness against adversarial at-\ntacks, as shown in prior evaluations.\nTo address this,\nadversarial training was applied as an additional defense.\nFigure 6 presents post-training accuracy for each encod-\ning method under FGSM attacks, highlighting ATP and\nPCA\u2019s enhanced performance due to their filtering of non-\ninformative features, which decreases sensitivity to irrele-\nvant perturbations. Additional results for other attack mod-\nels are provided in the supplementary material.\nAs shown in Figure 6, adversarial training enhances\nresilience across most encoding methods, particularly at\nlower \u03f5 values.\nATP consistently achieves higher accu-\nracy across attack strengths, maintaining a moderate lead\nover PCA and SQE despite performance declines as \u03f5 in-\ncreases. Amplitude and Angle encoding remain more sus-\n20433\n\n=== Page 8 ===\nTable 4. Performance of encoding techniques on various classes\nfor binary classifications under FGSM attack with \u03f5 = 0.3 (Accu-\nracy).\nClasses\nAngle\nAmplitude\nATP\nPCA\nSQE\nMNIST\n(0,1)\n62.0\n58.4\n67.2\n66.5\n65.0\n(0,3)\n63.2\n61.0\n67.3\n69.0\n63.7\n(2,4)\n55.2\n54.7\n60.0\n61.5\n59.0\n(5,6)\n64.3\n61.5\n66.7\n65.0\n62.5\n(2,8)\n48.6\n50.5\n56.0\n57.8\n54.0\nFashion MNIST\n(0,1)\n68.4\n66.0\n72.5\n73.0\n70.2\n(2,8)\n66.2\n65.5\n70.3\n71.0\n69.0\n(3,9)\n77.5\n74.2\n76.0\n75.8\n73.5\n(7,9)\n60.0\n59.2\n64.8\n63.5\n62.3\nCIFAR\n(0,1)\n55.0\n51.4\n59.5\n61.2\n58.0\nPneumoniaMNIST\n(0,1)\n66.5\n62.0\n64.0\n65.2\n63.4\nFigure 6.\nEffectiveness of different encoding methods against\nadversarial attacks after adversarial training, evaluated at vary-\ning attack strengths (measured by \u03f5) for (0,1) classification tasks\non MNIST. ATP shows higher robustness across most \u03f5 values,\nthough all models experience performance declines as \u03f5 increases.\nceptible to attacks, especially at higher \u03f5 values, indicating\nlower robustness. These results underscore ATP\u2019s advan-\ntage in accuracy preservation, though further measures may\nbe needed for robustness under stronger adversarial condi-\ntions.\n5.5. Experiments on IBM Quantum Hardware\nTo evaluate ATP\u2019s robustness on real quantum hardware, we\nextended our experiments to the IBM Sherbrooke backend\nusing Qiskit\u2019s Runtime Service. We implemented the pro-\nposed framework with Qiskit\u2019s native EstimatorQNN in-\nterface, using default settings for error mitigation and back-\nend resilience. The model was trained with a COBYLA op-\ntimizer for 30 iterations. Across five MNIST class subsets,\nATP demonstrated an average performance improvement of\n7% compared to the direct encoding approach, with notable\naccuracy gains on the (2,4) and (2,8) classification tasks, the\nlatter showing the highest improvement.\n6. Discussion and Limitations\nWhile ATP achieved notable performance gains across\ndatasets, some limitations remain. First, we focus on binary\nclassification for two main reasons: comparability with ex-\nisting literature and the efficiency of quantum measurement\nprocesses. Comparability is important, as most prior works\npredominantly use binary classification to evaluate state-of-\nthe-art QNN implementations. Moreover, binary classifi-\ncation simplifies quantum measurement, as the class label\ncan be inferred from a single-qubit von Neumann measure-\nment\u2014such as the sign of the expectation value of the  \\sigma _z \nobservable. This approach, including measuring only the\nlast qubit to determine the output label with minimal cir-\ncuit complexity, has been well-documented in prior studies\n[17, 42]. In contrast, most multi-class classification meth-\nods require a hybrid between classical and quantum models\n[8], making binary classification the preferred choice both\nin the literature and in our study. Additionally, ATP\u2019s opti-\nmization process incurs moderate computational overhead,\nwith threshold selection and pruning taking approximately\n15% longer than direct angle encoding (compared to 12%\nfor PCA and 4% for SQE). While this may affect efficiency\nin larger applications, further optimizations in the thresh-\nolding algorithm could help reduce these demands.\nFuture work could also explore ATP\u2019s adaptability to\ndatasets with varied underlying distributions, as current\nevaluations focus on standard QNN benchmarks. Investi-\ngating performance on more diverse datasets may offer in-\nsights for improving ATP\u2019s robustness in broader real-world\nscenarios.\n7. Conclusion\nWe presented Adaptive Threshold Pruning (ATP) to address\nthe resource constraints of quantum systems by removing\nnon-essential input features before encoding, enabling more\nefficient use of qubits. Rather than reducing circuit com-\nplexity directly, ATP lowers input redundancy, which leads\nto reduced entanglement and improved computational effi-\nciency. Across multiple datasets, ATP consistently outper-\nforms other encoding methods by achieving higher accu-\nracy with lower entanglement entropy. By adapting pruning\nthresholds to the variance structure of the data, ATP offers a\nflexible and scalable encoding strategy that enhances QNN\nperformance in resource-limited settings.\n20434\n\n=== Page 9 ===\nReferences\n[1] Amira Abbas, David Sutter, Christa Zoufal, Aur\u00b4elien Lucchi,\nAlessio Figalli, and Stefan Woerner. The power of quantum\nneural networks. Nature Computational Science, 1(6):403\u2013\n409, 2021.\n[2] Mahabubul Alam and Swaroop Ghosh. Qnet: A scalable and\nnoise-resilient quantum neural network architecture for noisy\nintermediate-scale quantum computers. Frontiers in physics,\n9:755139, 2022.\n[3] Vassilis Anagiannis and Miranda CN Cheng. Entangled q-\nconvolutional neural nets. Machine Learning: Science and\nTechnology, 2(4):045026, 2021.\n[4] Abdullah Ash-Saki, Mahabubul Alam, and Swaroop Ghosh.\nExperimental characterization, modeling, and analysis of\ncrosstalk in a quantum computer.\nIEEE Transactions on\nQuantum Engineering, 1:1\u20136, 2020.\n[5] Sheng-Chen Bai, Yi-Cheng Tang, and Shi-Ju Ran. Unsuper-\nvised recognition of informative features via tensor network\nmachine learning and quantum entanglement variations. Chi-\nnese Physics Letters, 39:100701, 2022.\n[6] Bhattaraprot Bhabhatsatam and Sucha Smanchat.\nHybrid\nquantum encoding: Combining amplitude and basis encod-\ning for enhanced data storage and processing in quantum\ncomputing. In 2023 20th International Joint Conference on\nComputer Science and Software Engineering (JCSSE), pages\n512\u2013516. IEEE, 2023.\n[7] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick\nRebentrost, Nathan Wiebe, and Seth Lloyd. Quantum ma-\nchine learning. Nature, 549(7671):195\u2013202, 2017.\n[8] Denis Bokhan, Alena S Mastiukova, Aleksey S Boev,\nDmitrii N Trubnikov, and Aleksey K Fedorov. Multiclass\nclassification using quantum convolutional neural networks\nwith hybrid quantum-classical learning. Frontiers in Physics,\n10:1069985, 2022.\n[9] Marco Cerezo,\nGuillaume Verdon,\nHsin-Yuan Huang,\nLukasz Cincio, and Patrick J Coles. Challenges and opportu-\nnities in quantum machine learning. Nature Computational\nScience, 2(9):567\u2013576, 2022.\n[10] Philip Easom-McCaldin, Ahmed Bouridane, Ammar Bela-\ntreche, Richard Jiang, and Somaya Al-Maadeed. Efficient\nquantum image classification using single qubit encoding.\nIEEE Transactions on Neural Networks and Learning Sys-\ntems, 35(2):1472\u20131486, 2022.\n[11] Ian Goodfellow. Deep learning, 2016.\n[12] Mayank Gupta and Manisha J Nene. Quantum computing:\nAn entanglement measurement. In 2020 IEEE International\nConference on Advent Trends in Multidisciplinary Research\nand Innovation (ICATMRI), pages 1\u20136. IEEE, 2020.\n[13] Stuart Hadfield, Zhihui Wang, Bryan O\u2019gorman, Eleanor G\nRieffel, Davide Venturelli, and Rupak Biswas.\nFrom the\nquantum approximate optimization algorithm to a quantum\nalternating operator ansatz. Algorithms, 12(2):34, 2019.\n[14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 16000\u2013\n16009, 2022.\n[15] Mika Hirvensalo. Quantum computing. Springer Science &\nBusiness Media, 2013.\n[16] Zhirui Hu, Peiyan Dong, Zhepeng Wang, Youzuo Lin,\nYanzhi Wang, and Weiwen Jiang. Quantum neural network\ncompression. In Proceedings of the 41st IEEE/ACM Inter-\nnational Conference on Computer-Aided Design, pages 1\u20139,\n2022.\n[17] Tak Hur, Leeseok Kim, and Daniel K Park. Quantum con-\nvolutional neural network for classical data classification.\nQuantum Machine Intelligence, 4(1):3, 2022.\n[18] Ian T Joliffe and BJT Morgan. Principal component anal-\nysis and exploratory factor analysis. Statistical methods in\nmedical research, 1(1):69\u201395, 1992.\n[19] Emanuel Knill. Quantum computing with realistically noisy\ndevices. Nature, 434(7029):39\u201344, 2005.\n[20] Ankit\nKulshrestha,\nXiaoyuan\nLiu,\nHayato\nUshijima-\nMwesigwa, Bao Bach, and Ilya Safro. Qadaprune: Adaptive\nparameter pruning for training variational quantum circuits.\narXiv preprint arXiv:2408.13352, 2024.\n[21] Guangxi Li, Ruilin Ye, Xuanqiang Zhao, and Xin Wang.\nConcentration of data encoding in parameterized quantum\ncircuits. Advances in Neural Information Processing Sys-\ntems, 35:19456\u201319469, 2022.\n[22] Marina Lisnichenko and Stanislav Protasov. Quantum image\nrepresentation: A review. Quantum Machine Intelligence, 5\n(1):2, 2023.\n[23] Yuhan Liu, Wen-Jun Li, Xiao Zhang, Maciej Lewenstein,\nGang Su, and Shi-Ju Ran. Entanglement-based feature ex-\ntraction by tensor network machine learning. Frontiers in\nApplied Mathematics and Statistics, 7:716044, 2021.\n[24] John Martyn, Guifre Vidal, Chase Roberts, and Stefan Le-\nichenauer.\nEntanglement and tensor networks for super-\nvised image classification. arXiv preprint arXiv:2007.06082,\n2020.\n[25] Tuyen Nguyen,\nIncheon Paik,\nYutaka Watanobe,\nand\nTruong Cong Thang.\nAn evaluation of hardware-efficient\nquantum neural networks for image data classification. Elec-\ntronics, 11(3):437, 2022.\n[26] Carlos Ortiz Marrero, M\u00b4aria Kieferov\u00b4a, and Nathan Wiebe.\nEntanglement-induced barren plateaus. PRX Quantum, 2(4):\n040316, 2021.\n[27] Emmanuel Ovalle-Magallanes, Dora E Alvarado-Carrillo,\nJuan Gabriel Avina-Cervantes, Ivan Cruz-Aceves, and Jose\nRuiz-Pinales. Quantum angle encoding with learnable rota-\ntion applied to quantum\u2013classical convolutional neural net-\nworks. Applied Soft Computing, 141:110307, 2023.\n[28] John Preskill.\nQuantum computing and the entanglement\nfrontier. arXiv preprint arXiv:1203.5813, 2012.\n[29] John Preskill. Quantum computing in the nisq era and be-\nyond. Quantum, 2:79, 2018.\n[30] Deepak Ranga, Aryan Rana, Sunil Prajapat, Pankaj Kumar,\nKranti Kumar, and Athanasios V Vasilakos. Quantum ma-\nchine learning: Exploring the role of data encoding tech-\nniques, challenges, and future directions. Mathematics, 12\n(21):3318, 2024.\n[31] Minati Rath et al.\nQuantum data encoding: A compar-\native analysis of classical-to-quantum mapping techniques\n20435\n\n=== Page 10 ===\nand their impact on machine learning accuracy. EPJ Quan-\ntum Technology, 11(1):1\u201322, 2024.\n[32] Manas Sajjan, Junxu Li, Raja Selvarajan, Shree Hari Suresh-\nbabu, Sumit Suresh Kale, Rishabh Gupta, Vinit Singh, and\nSabre Kais. Quantum machine learning for chemistry and\nphysics.\nChemical Society Reviews, 51(15):6475\u20136573,\n2022.\n[33] Maria Schuld and Francesco Petruccione. Supervised learn-\ning with quantum computers. Springer, 2018.\n[34] Yu Shee, Pei-Kai Tsai, Cheng-Lin Hong, Hao-Chung Cheng,\nand Hsi-Sheng Goan. Qubit-efficient encoding scheme for\nquantum simulations of electronic structure. Physical Review\nResearch, 4(2):023154, 2022.\n[35] Peter W Shor. Quantum computing. Documenta Mathemat-\nica, 1(1000):467\u2013486, 1998.\n[36] Sukin Sim, Jonathan Romero, J\u00b4er\u02c6ome F Gonthier, and\nAlexander A Kunitsa. Adaptive pruning-based optimization\nof parameterized quantum circuits. Quantum Science and\nTechnology, 6(2):025019, 2021.\n[37] Jinkai Tian, Xiaoyu Sun, Yuxuan Du, Shanshan Zhao, Qing\nLiu, Kaining Zhang, Wei Yi, Wanrong Huang, Chaoyue\nWang, Xingyao Wu, et al. Recent advances for quantum neu-\nral networks in generative learning. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 45(10):12321\u2013\n12340, 2023.\n[38] Aijuan Wang, Jianglong Hu, Shiyue Zhang, and Lusi Li.\nShallow hybrid quantum-classical convolutional neural net-\nwork model for image classification. Quantum Information\nProcessing, 23(1):17, 2024.\n[39] Xinbiao Wang, Junyu Liu, Tongliang Liu, Yong Luo, Yuxuan\nDu, and Dacheng Tao. Symmetric pruning in quantum neural\nnetworks. arXiv preprint arXiv:2208.14057, 2022.\n[40] Zeyu Wang, Xianhang Li, Hongru Zhu, and Cihang Xie.\nRevisiting adversarial training at scale. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 24675\u201324685, 2024.\n[41] Manuela Weigold, Johanna Barzen, Frank Leymann, and\nMarie Salm. Data encoding patterns for quantum computing.\nIn Proceedings of the 27th Conference on Pattern Languages\nof Programs, pages 1\u201311, 2020.\n[42] Wenjie Wu, Ge Yan, Xudong Lu, Kaisen Pan, and Junchi\nYan.\nQuantumdarts: differentiable quantum architecture\nsearch for variational quantum algorithms.\nIn Interna-\ntional Conference on Machine Learning, pages 37745\u2013\n37764. PMLR, 2023.\n[43] Renxin Zhao and Shi Wang.\nA review of quantum neu-\nral networks: methods, models, dilemma.\narXiv preprint\narXiv:2109.01840, 2021.\n[44] Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge No-\ncedal.\nAlgorithm 778: L-bfgs-b: Fortran subroutines for\nlarge-scale bound-constrained optimization.\nACM Trans-\nactions on mathematical software (TOMS), 23(4):550\u2013560,\n1997.\n20436\n",
      "searchable_content": "page 1 atp adaptive threshold pruning for efficient data encoding in quantum neural networks mohamed afane1 gabrielle ebbrecht1 ying wang2 juntao chen1 junaid farooq3 1fordham university 2stevens institute of technology 3university of michigan-dearborn abstract quantum neural networks qnns offer promising capa- bilities for complex data tasks but are often constrained by limited qubit resources and high entanglement which can hinder scalability and efficiency. in this paper we in- troduce adaptive threshold pruning atp an encoding method that reduces entanglement and optimizes data com- plexity for efficient computations in qnns. atp dynam- ically prunes non-essential features in the data based on adaptive thresholds effectively reducing quantum circuit requirements while preserving high performance. extensive experiments across multiple datasets demonstrate that atp reduces entanglement entropy and improves adversarial ro- bustness when combined with adversarial training meth- ods like fgsm. our results highlight atp s ability to bal- ance computational efficiency and model resilience achiev- ing significant performance improvements with fewer re- sources which will help make qnns more feasible in prac- tical resource-constrained settings. 1. introduction quantum machine learning qml has gained attention for its ability to solve problems that are difficult for classical models by using the unique properties of quantum systems 1 . it has shown practical benefits in fields such as chem- istry 32 optimization 13 and others involving high- dimensional or structured data 7 37 .however quantum algorithms are still constrained by the availability of qubits noise and decoherence in hardware 19 29 . because of quantum hardware s sensitivity to the environment qubits are prone to premature collapse that degrades the fidelity of computation. qubits may also be disturbed by signals in- tended to alter the state of another qubit in close proximity 4 which reduces stability in hardware with many qubits and complicates the training of quantum models. as a re- sult shallower quantum neural networks qnns are of- ten more practical. 9 38 . corresponding authors mafane jchen504 fordham.edu figure 1. demonstration of the adaptive threshold pruning frame- work. the original image top is split into a 3x3 grid with each section assessed for information density. the pruned image bottom shows filtered regions that fall below a defined intensity threshold in blue indicating areas that do not contribute signifi- cantly to the classification task effectively freeing qubits and opti- mizing resources by focusing only on key areas of high relevance. efficient data encoding into quantum states remains a crucial bottleneck for achieving scalability and accuracy in qml applications 30 41 . for instance limiting qubit in- teractions can reduce crosstalk and enhance stability which is essential for noise resilience 2 . while using larger and deeper qnns can enhance a model s ability to cap- ture complex data relationships and potentially improve per- formance increasing encoding depth introduces new chal- lenges. as the depth of the encoding circuit grows encoded quantum states tend to converge towards a maximally mixed state at an exponential rate becoming increasingly indistin- guishable from random noise 21 . this concentration ef- fect leads to a lack of state diversity and the emergence of barren plateaus where gradients vanish significantly hin- dering the model s learning process 26 . these effects are especially problematic when encoding is not adapted to the structure or scale of the data. thus qnns must balance ex- pressibility with computational demands and sensitivity to barren plateaus 25 . this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 20427 page 2 figure 2. average accuracy across four datasets comparing the performance of various encoding methods adaptive thresh- old pruning atp principal component analysis pca single qubit encoding sqe angle and amplitude encoding. hori- zontal error bars represent entanglement entropy ee with longer bars indicating higher entanglement. atp generally achieves the highest accuracy with lower ee. while efficient data encoding can improve performance in classical models such as by reducing training time or resource use it is far more critical in qml. in quantum models encoding directly impacts whether meaningful pat- terns can be learned in the first place since poor encoding can waste limited qubit resources or amplify noise to the point that the model fails entirely 22 . for instance some methods only encode states with significant contributions to the model allowing qubit resources to focus on meaningful data and enabling more effective scaling 34 . amplitude encoding is one such approach representing data compactly to reduce the number of required qubits though it limits the types of data and operations that can be used 33 . other strategies such as qubit lattice a direct encoding method with low circuit depth and frqi a compact scheme that uses fewer qubits demonstrate different tradeoffs between qubit efficiency and processing flexibility 22 . in general encoding strategies that reduce resource overhead can op- timize computation and limit potential errors by requiring fewer qubits to operate. in this context entanglement entropy ee serves as a key metric for assessing a model s encoding efficiency. ee measures the degree of correlation between different qubits in a quantum system by quantifying the information shared between subsystems typically calculated as the von neu- mann entropy of a reduced density matrix of one part of a bipartite system. higher ee indicates a greater level of in- terdependence between qubits which can be advantageous for capturing complex data structures but excessive entan- glement may increase computational complexity and lead to overfitting. anagiannis and cheng for instance found that as ee rises in q-convolutional neural networks the model s cost function decreases suggesting a link between structured entanglement and efficient learning 3 . simi- larly martyn et al. demonstrated that while certain levels of entanglement improve training accuracy in quantum mod- els over-entangling the qubits can lead to diminished gen- eralization and higher resource demands 24 . thus ee not only provides insight into the information distribution across qubits but also helps balance model complexity and computational efficiency in quantum learning. to address these challenges we introduce adaptive threshold pruning atp a technique that optimizes the encoding process by adaptively pruning non-essential fea- tures thereby reducing both qubit usage and entanglement in the circuit as visualized by figure 1. atp sets dynamic thresholds for feature pruning selectively removing low- variance data to maintain model performance while lower- ing ee. figure 2 summarizes atp s performance compared to other encoding methods demonstrating improvements in both accuracy and entanglement efficiency. 2. related works masking in classical machine learning masking strategies are commonly used in self-supervised learning by train- ing models to reconstruct missing parts of the input from partial observations. masked autoencoding as used in the masked autoencoders mae framework achieves this by randomly hiding image patches and requiring the model to reconstruct them encouraging the learning of meaning- ful representations without labels 14 . although the pri- mary objective is representation learning mae improves efficiency by processing only visible patches through a lightweight encoder-decoder setup. this reduction in input complexity helps scale models to large datasets with fewer computational demands. similar to how mae filters input through random masking our atp approach uses threshold- based pruning to discard low-variance regions before en- coding allowing quantum models to focus on the most rel- evant parts of the data while reducing resource usage 40 . pruning in contrast pruning in qml has primarily been applied at the circuit level to reduce model complexity addressing challenges such as noise and limited quantum resources. these techniques streamline qnns by selec- tively removing parameters with minimal impact on accu- racy thereby enhancing computational efficiency and scal- ability. hu et al. 16 propose an architecture compres- sion method for quantum circuits achieving reduced cir- cuit depth with minimal accuracy loss while wang et al. 39 demonstrate how pruning supports scalability in quan- tum neural networks. circuit-level pruning methods such as those by sim et al. 36 and kulshrestha et al. 20 adap- tively eliminate non-essential parameters optimizing quan- 20428 page 3 tum resources without compromising model performance. while architecture pruning addresses circuit complexity ef- ficient data encoding remains critical in quantum contexts directly impacting qubit usage and entanglement manage- ment. data encoding several encoding methods have been de- veloped to optimize qubit usage while preserving essential data features in quantum neural networks. for example single-qubit encoding sqe 10 minimizes resource re- quirements by encoding data into a single qubit rather than multiple qubits. this technique uses a series of z-y-z axis rotations on the bloch sphere parameterized by input data to capture spatial relationships with minimal qubit and pa- rameter usage. classical methods like principal component analysis pca 18 and autoencoding 11 also improve quantum model efficiency by reducing data dimensionality while retaining key features 17 . pca identifies principal components with the highest variance focusing on infor- mative aspects of the data while autoencoders learn com- pact representations through non-linear dimensionality re- duction. the choice of encoding strategy directly impacts the ef- fectiveness of quantum methods by influencing qubit effi- ciency and circuit complexity. angle encoding commonly used for classification tasks maps classical data to quantum gate rotations due to its simplicity 31 . amplitude encod- ing which embeds data as probability amplitudes allows quantum representation of exponentially large datasets but presents scalability challenges. basis encoding represents binary data as computational basis states suitable for bi- nary data but less practical for complex datasets. hybrid methods such as amplitude-angle encoding aim to com- bine strengths of individual techniques but often increase circuit depth posing issues on noisy hardware with limited coherence times 6 . these limitations emphasize the need for adaptable encoding methods like atp that optimize en- coding based on data relevance and variability. our approach atp distinguishes itself by dynamically pruning with adaptive thresholds. unlike pca which ap- plies linear transformations to capture data variance atp zeroes out less informative data points based on their rele- vance adapting to each dataset s unique structure. unlike sqe which minimizes resources by encoding data into a single qubit atp selectively retains expressive data fea- tures with minimal resource overhead. by concentrating quantum resources on essential data atp balances compu- tational efficiency and entanglement requirements offering a scalable solution tailored to the specific constraints and advantages of quantum systems. to our knowledge no prior work has addressed adaptive feature pruning within quantum data encoding. atp thus fills this gap by offering a systematic approach to reduce quantum overhead while preserving model accuracy. 3. preliminaries 3.1. quantum computing fundamentals quantum computing exploits principles of quantum me- chanics providing a framework that differs profoundly from classical systems. in classical computing data is rep- resented by bits in distinct states of 0 or 1. quantum com- puting however relies on qubits which can exist in linear combinations of 0 and 1 states represented as psi rangle alpha 0 rangle beta 1 rangle 1 where alpha and beta are complex numbers satisfying a l pha 2 beta 2 1 . this property known as superposition allows qubits to perform concurrent calculations making quantum algorithms inherently parallel 15 35 . 3.1.1 entanglement and measurement entanglement is a quantum phenomenon in which qubits become correlated in such a way that the state of one qubit directly influences the state of another regardless of physi- cal distance 28 . for two entangled qubits the state can be expressed as p s i ran g le frac 1 sqrt 2 left 00 rangle 11 rangle right 2 forming a maximally entangled bell state. entanglement is a crucial resource in qnns as it enables intricate data encoding and interaction patterns that cannot be replicated classically. however the control and preservation of en- tanglement are challenging due to decoherence and noise making efficient encoding schemes essential for managing entanglement and optimizing qubit usage 12 . 3.2. quantum neural networks qnns use quantum circuits with parameterized gates known as pqcs to process data. a qnn layer can be de- fined by a series of parameterized unitary transformations 43 u t h eta prod _ j u_j theta _j 3 where theta theta _j are tunable parameters. each u_j represents a quantum gate that rotates or entangles qubits based on in- put data. these rotations enable the qnn to encode input features transforming them into high-dimensional hilbert space representations. during training gradients of these parameters are optimized to minimize a loss function anal- ogous to classical neural networks but constrained by quan- tum hardware limitations and noise sensitivity 1 . 20429 page 4 3.3. data encoding in quantum neural networks efficient data encoding is critical for the practicality of qnns. in this work we apply atp before encoding data into quantum states. primarily we use angle encoding where each pixel value x_ i j of an image is mapped onto a rotation angle theta _ i j for the rx gate t h e t a _ i j pi cdot x_ i j 4 resulting in the transformation rx theta _ i j applied to each qubit 27 . this method straightforwardly represents pixel intensities but can introduce redundancy for higher- dimensional data as unnecessary values in qubit states in- crease entanglement and computational complexity. we also conduct experiments using amplitude encoding which encodes normalized data values as amplitudes of a quantum state. for an input vector mathbf x with dimension 2 n for n qubits amplitude encoding transforms it to psi ran gle sum _ i 0 2 n-1 x_i i rangle 5 allowing all features to be encoded simultaneously in a sin- gle quantum state. while amplitude encoding can be effi- cient for certain data dimensions it faces scalability chal- lenges for large datasets. additionally alternative encoding strategies exist such as dense encoding techniques that represent multiple data features using fewer qubits by encoding them into differ- ent parameters of multi-qubit gates. these methods aim to balance representational power with resource constraints but each encoding approach carries unique trade-offs in complexity and scalability. across these various encoding schemes atp helps reduce redundancy by pruning non- essential features before encoding leading to more efficient and robust qnn performance. 3.4. entanglement entropy ee is a metric from quantum information theory that quan- tifies the correlation between different parts of a quantum system typically measured using the von neumann en- tropy of a reduced density matrix 3 . in qml ee re- flects a model s capacity to capture intricate data relation- ships. higher ee often indicates stronger interdependence between qubits which can increase the model s expressive power but also adds computational complexity and risks overfitting 5 . conversely lower ee may represent effi- cient resource usage and reduced complexity which is ad- vantageous for simpler datasets 23 . balancing ee is thus crucial as it influences both learning potential and robust- ness in noisy environments. therefore achieving high per- formance with lower ee or reduced complexity is an im- portant aim of this study. 4. methods to select effective threshold values for data pruning we formalize threshold selection as a constrained optimization problem tailored to adapt to each dataset s unique char- acteristics. this approach employs the limited-memory broyden-fletcher-goldfarb-shanno algorithm with box constraints l-bfgs-b a quasi-newton method that effi- ciently handles high-dimensional optimization by leverag- ing gradient-based updates to refine threshold levels. by using this optimization framework we systematically bal- ance accuracy with computational efficiency ensuring that thresholds are adjusted to retain essential information while minimizing redundancy. the following sections present the mathematical formulation and algorithmic steps that drive this threshold-tuning process for enhanced qnn perfor- mance. 4.1. pruning function definition to define the threshold \u03c4 for filtering we calculate the av- erage pixel intensity matrices x0 and x1 across all train- ing samples in each binary class. specifically x0 i j and x1 i j represent the average intensity values at position i j for classes 0 and 1 respectively. using these aver- ages we prune data by setting values to zero at positions i j where both class averages fall below the threshold \u03c4 as follows x_ t a u i j b e gin case s 0 tex t if bar x _0 i j tau text and bar x _1 i j tau x i j text otherwise end cases 6 where x i j represents the original pixel intensity at po- sition i j . this operation generates a pruned dataset x\u03c4 that retains only grid positions with sufficient intensity to contribute effectively to classification. to determine the optimal threshold \u03c4 we maximize the test accuracy acctest x\u03c4 tau arg m ax _ tau in 0 tau _ max text acc _ text test mathcal x _ tau 7 where \u03c4max is the upper bound on \u03c4. this optimiza- tion is equivalent to minimizing the negative accuracy acctest x\u03c4 over the interval 0 \u03c4max . 4.2. gradient-based optimization via l-bfgs-b to solve the threshold optimization problem we use the l- bfgs-b algorithm 44 which efficiently manages high- dimensional optimization with limited memory. the algo- rithm iteratively adjusts \u03c4 by minimizing the negative accu- racy f tau - text acc _ text test mathcal x _ tau 8 updating \u03c4 at each step according to t a u _ k 1 tau _k - alpha _k h_k nabla f tau _k 9 20430 page 5 where \u03b1k is the step size hk is an approximation of the inverse hessian matrix and f \u03c4k is the gradient of f \u03c4k with respect to \u03c4. the gradient f \u03c4k is computed as nab l a f t au _k s um _ i j frac partial f partial x_ tau i j cdot frac partial x_ tau i j partial tau . 10 this approach as detailed in algorithm 1 leverages gradient information to efficiently converge on the optimal threshold \u03c4 ensuring a refined selection of relevant data regions. algorithm 1 bi-level threshold optimization for qnn require binary class pair c0 c1 dataset x y encod- ing function fenc grid size s epochs threshold range ensure optimized threshold \u03c4 and classification accu- racy 1 filter and resize data for selected class pair c0 c1 converting labels to binary 2 calculate average values for each class and set pixels to zero where averages are below \u03c4 3 convert processed images to quantum circuits using fenc 4 initialize qnn model with angle encoding 5 define bi-level optimization 6 inner level apply threshold filter data train qnn on xtrain ytrain 7 outer level maximize test accuracy on xtest ytest 8 optimize over \u03c4 within range using l-bfgs-b 9 calculate entanglement entropy of final model on test set 10 return optimal threshold \u03c4 test accuracy and entan- glement entropy 4.3. constraint handling and convergence criteria the box constraints 0 \u03c4 \u03c4max are enforced within l-bfgs-b by projecting any out-of-bounds updates back to the feasible region ensuring that the threshold remains within the desired range throughout the optimization. con- vergence is achieved when the norm of the projected gra- dient f \u03c4k falls below a predefined tolerance \u03f5 or when the maximum number of iterations is reached nabl a f tau _k _ infty epsilon . 11 additionally the accuracy is evaluated on a validation set xval after each iteration to ensure that the optimization process generalizes effectively to unseen data mitigating overfitting to the training set. 4.4. approximation of the inverse hessian to achieve computational efficiency l-bfgs-b approxi- mates the hessian matrix rather than computing it directly 44 . using a limited history of m gradient and posi- tion vectors si yi k 1 i k m where si \u03c4i 1 \u03c4i and yi f \u03c4i 1 f \u03c4i the inverse hessian approxima- tion hk is updated using the recursive formula h_ k 1 v _k t h _ k v_k rho _k s_k s_k t 12 where \u03c1k yt k sk 1 and vk i \u03c1kykst k . this re- cursive update captures the essential curvature information while minimizing memory overhead making the optimiza- tion feasible for high-dimensional datasets. after identifying the optimal threshold \u03c4 we generate the filtered datasets x\u03c4 for training and testing where ir- relevant positions have been pruned based on \u03c4 . the qnn is then trained on x\u03c4 leveraging the optimized threshold to focus on critical data regions while minimizing computa- tional load and enhancing classification performance. test accuracy acctest x\u03c4 is subsequently evaluated to confirm the effectiveness of threshold optimization. 5. experiments 5.1. setup to evaluate our approach we conducted binary classifica- tion experiments on multiple benchmark datasets mnist fashionmnist cifar and pneumoniamnist. these datasets were chosen to assess the model s adaptability across varied image types and complexities. the experi- ments were implemented using a compact three-layer qnn model which serves as the base architecture for all encod- ing techniques tested. the qnn model applies a parame- terized quantum circuit built with data qubits and a readout qubit initialized with an x gate followed by a hadamard gate to prepare it in superposition. the circuit incorpo- rates both entangling xx and zz gates between the data qubits and the readout qubit with learnable parameters to capture complex dependencies within the input data. a fi- nal hadamard gate is applied to the readout qubit before measurement completing the encoding and entanglement process for each layer. 5.1.1 encoding and preprocessing techniques for encoding angle and amplitude methods were applied directly to the data. in contrast atp sqe and pca were used beforehand each applying a different approach to pre- pare the data before encoding. specifically atp and pca refine data structure before encoding while sqe focuses on single-qubit encoding. the results presented here are from experiments where these preparatory methods were followed by angle encoding. additional experiments with direct amplitude encoding as well as further analyses are included in the supplementary materials and ablation study section. 20431 page 6 5.1.2 realistic noise conditions to evaluate robustness in realistic settings depolarizing noise was introduced at intensities of 3 5 and 10 al- lowing for a comparison of baseline performance on clean data and the model s resilience under these quantum noise levels. this setup enabled a thorough assessment of each encoding method s effectiveness revealing how the qnn model performs across both ideal and noisy environments. 5.2. manual threshold pruning in this part we examine the effect of pruning with differ- ent thresholds on qnn performance. figure 3 illustrates the data distribution for each position within a 3x3 division of the image where the values represent average intensities across samples from a specific class. as shown positions with lower information content tend to have closely overlap- ping values between classes which can make training more challenging by introducing less distinctive information into the qnn. figure 3. data distribution across positions in fashionmnist for two classes t-shirt top and trouser with dashed lines mark- ing threshold levels. positions with lower variance are pruned to streamline training and focus on more informative regions. to further investigate the impact of these thresholds on performance we apply a range of manual thresholds across different class pairs in both mnist and fashionmnist datasets. figures 4 and 5 show that moderate pruning generally leads to improved accuracy by eliminating non- essential features while preserving critical distinctions. in figure 4 results for mnist indicate that moderate thresh- olds enhance accuracy while higher thresholds reduce it by removing valuable details. similarly for fashionmnist figure 5 the optimal thresholds cluster between 0.1 and 0.3 but specific values vary with data distribution charac- teristics. figure 4. test accuracy for mnist class pairs with varying pruning thresholds. moderate thresholds improve accuracy while higher thresholds may exclude key information. figure 5. test accuracy for fashionmnist class pairs with vary- ing thresholds. similar to mnist a threshold around 0.3 gener- ally provides optimal performance. these results demonstrate that the optimal pruning level is influenced by the data distribution motivating the need for an adaptive thresholding approach. the following section details the results from the bi-level optimization method used in our framework to automate threshold se- lection based on each dataset s characteristics. 5.3. performance results tables 1 and 2 present the classification accuracy and ee respectively for different encoding techniques across the four datasets used in the qnn binary classification tasks. for mnist and fashionmnist multiple class pairs were chosen to evaluate the model s capability in distinguishing various subsets providing a broader assessment of encod- ing effectiveness. across the majority of datasets and class pairs atp achieved the highest accuracy. atp also consis- tently minimized ee compared to other methods indicating a more efficient use of quantum resources by lowering com- 20432 page 7 table 1. performance of encoding techniques on various classes for binary classifications accuracy . classes angle amplitude atp pca sqe mnist 0 1 96.0 95.5 99.0 99.0 88.0 0 3 89.0 88.5 91.0 88.0 86.0 2 4 85.0 84.0 86.0 84.5 82.0 5 6 86.0 85.5 87.0 85.0 83.5 2 8 81.0 79.5 83.0 86.0 78.5 fashion mnist 0 1 88.5 88.0 91.5 88.5 86.0 2 8 86.0 84.5 86.0 86.0 83.0 3 9 94.0 87.0 94.0 93.0 91.0 7 9 82.0 78.0 83.0 79.0 77.0 cifar 0 1 70.0 68.5 74.2 68.0 66.0 pneumoniamnist 0 1 81.0 68.5 87.0 80.0 75.5 table 2. entanglement entropy of encoding techniques on various classes for binary classifications. classes angle amplitude atp pca sqe mnist 0 1 0.67 0.52 0.39 0.60 0.45 0 3 0.59 0.53 0.34 0.55 0.44 2 4 0.77 0.64 0.45 0.56 0.41 5 6 0.82 0.53 0.41 0.61 0.39 2 8 0.63 0.56 0.34 0.36 0.34 fashion mnist 0 1 0.56 0.47 0.35 0.58 0.39 2 8 0.52 0.50 0.35 0.41 0.38 3 9 0.54 0.62 0.38 0.55 0.42 7 9 0.59 0.58 0.41 0.64 0.43 cifar 0 1 0.63 0.51 0.46 0.65 0.43 pneumoniamnist 0 1 0.88 0.79 0.37 0.59 0.42 plexity without compromising accuracy. to assess the encoding methods resilience to noise we examined model performance under various levels of de- polarizing noise 3-10 . table 3 highlights that encoding techniques with lower entanglement entropy such as atp and sqe showed stronger robustness with accuracy reduc- tions typically limited to 3 8 points. in contrast angle amplitude and pca experienced more substantial drops in accuracy of 4 17 points. these results emphasize atp and sqe s effectiveness in preserving model accuracy un- der challenging noise conditions establishing a baseline for qnn stability in practical noisy environments. table 3. performance of encoding techniques on various classes for binary classifications under depolarizing noise accuracy . classes angle amplitude atp pca sqe mnist 0 1 87.0 86.4 89.0 89.0 88.2 0 3 82.4 80.5 83.0 78.9 81.9 2 4 80.2 68.8 71.0 79.7 79.8 5 6 70.8 69.4 73.0 68.5 74.0 2 8 68.8 57.4 76.5 74.9 75.2 fashion mnist 0 1 80.5 79.0 84.9 84.9 82.1 2 8 76.1 75.4 79.3 78.6 79.5 3 9 84.3 78.2 87.9 86.5 86.7 7 9 70.0 66.2 68.4 73.2 74.5 cifar 0 1 60.4 59.0 61.0 57.8 58.5 pneumoniamnist 0 1 70.5 63.4 76.0 67.3 68.7 5.4. adversarial robustness to evaluate the resilience of each encoding method against adversarial attacks we applied fast gradient sign method fgsm with an attack strength of \u03f5 0.3 across the datasets. table 4 provides accuracy results for binary clas- sification tasks under these adversarial conditions. in most cases direct encoding methods like angle and amplitude encoding were generally more susceptible to the attacks displaying substantial accuracy reductions. atp pca and sqe exhibited a moderate level of robustness although per- formance varied across datasets. notably atp and pca maintained comparable accuracy levels in several cases but in certain tasks pneumoniamnist and cifar angle en- coding unexpectedly achieved the highest accuracy sug- gesting that additional adversarial training methods may be required to further improve model resilience. the encoding techniques used in this study includ- ing atp lack inherent robustness against adversarial at- tacks as shown in prior evaluations. to address this adversarial training was applied as an additional defense. figure 6 presents post-training accuracy for each encod- ing method under fgsm attacks highlighting atp and pca s enhanced performance due to their filtering of non- informative features which decreases sensitivity to irrele- vant perturbations. additional results for other attack mod- els are provided in the supplementary material. as shown in figure 6 adversarial training enhances resilience across most encoding methods particularly at lower \u03f5 values. atp consistently achieves higher accu- racy across attack strengths maintaining a moderate lead over pca and sqe despite performance declines as \u03f5 in- creases. amplitude and angle encoding remain more sus- 20433 page 8 table 4. performance of encoding techniques on various classes for binary classifications under fgsm attack with \u03f5 0.3 accu- racy . classes angle amplitude atp pca sqe mnist 0 1 62.0 58.4 67.2 66.5 65.0 0 3 63.2 61.0 67.3 69.0 63.7 2 4 55.2 54.7 60.0 61.5 59.0 5 6 64.3 61.5 66.7 65.0 62.5 2 8 48.6 50.5 56.0 57.8 54.0 fashion mnist 0 1 68.4 66.0 72.5 73.0 70.2 2 8 66.2 65.5 70.3 71.0 69.0 3 9 77.5 74.2 76.0 75.8 73.5 7 9 60.0 59.2 64.8 63.5 62.3 cifar 0 1 55.0 51.4 59.5 61.2 58.0 pneumoniamnist 0 1 66.5 62.0 64.0 65.2 63.4 figure 6. effectiveness of different encoding methods against adversarial attacks after adversarial training evaluated at vary- ing attack strengths measured by \u03f5 for 0 1 classification tasks on mnist. atp shows higher robustness across most \u03f5 values though all models experience performance declines as \u03f5 increases. ceptible to attacks especially at higher \u03f5 values indicating lower robustness. these results underscore atp s advan- tage in accuracy preservation though further measures may be needed for robustness under stronger adversarial condi- tions. 5.5. experiments on ibm quantum hardware to evaluate atp s robustness on real quantum hardware we extended our experiments to the ibm sherbrooke backend using qiskit s runtime service. we implemented the pro- posed framework with qiskit s native estimatorqnn in- terface using default settings for error mitigation and back- end resilience. the model was trained with a cobyla op- timizer for 30 iterations. across five mnist class subsets atp demonstrated an average performance improvement of 7 compared to the direct encoding approach with notable accuracy gains on the 2 4 and 2 8 classification tasks the latter showing the highest improvement. 6. discussion and limitations while atp achieved notable performance gains across datasets some limitations remain. first we focus on binary classification for two main reasons comparability with ex- isting literature and the efficiency of quantum measurement processes. comparability is important as most prior works predominantly use binary classification to evaluate state-of- the-art qnn implementations. moreover binary classifi- cation simplifies quantum measurement as the class label can be inferred from a single-qubit von neumann measure- ment such as the sign of the expectation value of the sigma _z observable. this approach including measuring only the last qubit to determine the output label with minimal cir- cuit complexity has been well-documented in prior studies 17 42 . in contrast most multi-class classification meth- ods require a hybrid between classical and quantum models 8 making binary classification the preferred choice both in the literature and in our study. additionally atp s opti- mization process incurs moderate computational overhead with threshold selection and pruning taking approximately 15 longer than direct angle encoding compared to 12 for pca and 4 for sqe . while this may affect efficiency in larger applications further optimizations in the thresh- olding algorithm could help reduce these demands. future work could also explore atp s adaptability to datasets with varied underlying distributions as current evaluations focus on standard qnn benchmarks. investi- gating performance on more diverse datasets may offer in- sights for improving atp s robustness in broader real-world scenarios. 7. conclusion we presented adaptive threshold pruning atp to address the resource constraints of quantum systems by removing non-essential input features before encoding enabling more efficient use of qubits. rather than reducing circuit com- plexity directly atp lowers input redundancy which leads to reduced entanglement and improved computational effi- ciency. across multiple datasets atp consistently outper- forms other encoding methods by achieving higher accu- racy with lower entanglement entropy. by adapting pruning thresholds to the variance structure of the data atp offers a flexible and scalable encoding strategy that enhances qnn performance in resource-limited settings. 20434 page 9 references 1 amira abbas david sutter christa zoufal aur elien lucchi alessio figalli and stefan woerner. the power of quantum neural networks. nature computational science 1 6 403 409 2021. 2 mahabubul alam and swaroop ghosh. qnet a scalable and noise-resilient quantum neural network architecture for noisy intermediate-scale quantum computers. frontiers in physics 9 755139 2022. 3 vassilis anagiannis and miranda cn cheng. entangled q- convolutional neural nets. machine learning science and technology 2 4 045026 2021. 4 abdullah ash-saki mahabubul alam and swaroop ghosh. experimental characterization modeling and analysis of crosstalk in a quantum computer. ieee transactions on quantum engineering 1 1 6 2020. 5 sheng-chen bai yi-cheng tang and shi-ju ran. unsuper- vised recognition of informative features via tensor network machine learning and quantum entanglement variations. chi- nese physics letters 39 100701 2022. 6 bhattaraprot bhabhatsatam and sucha smanchat. hybrid quantum encoding combining amplitude and basis encod- ing for enhanced data storage and processing in quantum computing. in 2023 20th international joint conference on computer science and software engineering jcsse pages 512 516. ieee 2023. 7 jacob biamonte peter wittek nicola pancotti patrick rebentrost nathan wiebe and seth lloyd. quantum ma- chine learning. nature 549 7671 195 202 2017. 8 denis bokhan alena s mastiukova aleksey s boev dmitrii n trubnikov and aleksey k fedorov. multiclass classification using quantum convolutional neural networks with hybrid quantum-classical learning. frontiers in physics 10 1069985 2022. 9 marco cerezo guillaume verdon hsin-yuan huang lukasz cincio and patrick j coles. challenges and opportu- nities in quantum machine learning. nature computational science 2 9 567 576 2022. 10 philip easom-mccaldin ahmed bouridane ammar bela- treche richard jiang and somaya al-maadeed. efficient quantum image classification using single qubit encoding. ieee transactions on neural networks and learning sys- tems 35 2 1472 1486 2022. 11 ian goodfellow. deep learning 2016. 12 mayank gupta and manisha j nene. quantum computing an entanglement measurement. in 2020 ieee international conference on advent trends in multidisciplinary research and innovation icatmri pages 1 6. ieee 2020. 13 stuart hadfield zhihui wang bryan o gorman eleanor g rieffel davide venturelli and rupak biswas. from the quantum approximate optimization algorithm to a quantum alternating operator ansatz. algorithms 12 2 34 2019. 14 kaiming he xinlei chen saining xie yanghao li piotr doll ar and ross girshick. masked autoencoders are scalable vision learners. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 16000 16009 2022. 15 mika hirvensalo. quantum computing. springer science business media 2013. 16 zhirui hu peiyan dong zhepeng wang youzuo lin yanzhi wang and weiwen jiang. quantum neural network compression. in proceedings of the 41st ieee acm inter- national conference on computer-aided design pages 1 9 2022. 17 tak hur leeseok kim and daniel k park. quantum con- volutional neural network for classical data classification. quantum machine intelligence 4 1 3 2022. 18 ian t joliffe and bjt morgan. principal component anal- ysis and exploratory factor analysis. statistical methods in medical research 1 1 69 95 1992. 19 emanuel knill. quantum computing with realistically noisy devices. nature 434 7029 39 44 2005. 20 ankit kulshrestha xiaoyuan liu hayato ushijima- mwesigwa bao bach and ilya safro. qadaprune adaptive parameter pruning for training variational quantum circuits. arxiv preprint arxiv 2408.13352 2024. 21 guangxi li ruilin ye xuanqiang zhao and xin wang. concentration of data encoding in parameterized quantum circuits. advances in neural information processing sys- tems 35 19456 19469 2022. 22 marina lisnichenko and stanislav protasov. quantum image representation a review. quantum machine intelligence 5 1 2 2023. 23 yuhan liu wen-jun li xiao zhang maciej lewenstein gang su and shi-ju ran. entanglement-based feature ex- traction by tensor network machine learning. frontiers in applied mathematics and statistics 7 716044 2021. 24 john martyn guifre vidal chase roberts and stefan le- ichenauer. entanglement and tensor networks for super- vised image classification. arxiv preprint arxiv 2007.06082 2020. 25 tuyen nguyen incheon paik yutaka watanobe and truong cong thang. an evaluation of hardware-efficient quantum neural networks for image data classification. elec- tronics 11 3 437 2022. 26 carlos ortiz marrero m aria kieferov a and nathan wiebe. entanglement-induced barren plateaus. prx quantum 2 4 040316 2021. 27 emmanuel ovalle-magallanes dora e alvarado-carrillo juan gabriel avina-cervantes ivan cruz-aceves and jose ruiz-pinales. quantum angle encoding with learnable rota- tion applied to quantum classical convolutional neural net- works. applied soft computing 141 110307 2023. 28 john preskill. quantum computing and the entanglement frontier. arxiv preprint arxiv 1203.5813 2012. 29 john preskill. quantum computing in the nisq era and be- yond. quantum 2 79 2018. 30 deepak ranga aryan rana sunil prajapat pankaj kumar kranti kumar and athanasios v vasilakos. quantum ma- chine learning exploring the role of data encoding tech- niques challenges and future directions. mathematics 12 21 3318 2024. 31 minati rath et al. quantum data encoding a compar- ative analysis of classical-to-quantum mapping techniques 20435 page 10 and their impact on machine learning accuracy. epj quan- tum technology 11 1 1 22 2024. 32 manas sajjan junxu li raja selvarajan shree hari suresh- babu sumit suresh kale rishabh gupta vinit singh and sabre kais. quantum machine learning for chemistry and physics. chemical society reviews 51 15 6475 6573 2022. 33 maria schuld and francesco petruccione. supervised learn- ing with quantum computers. springer 2018. 34 yu shee pei-kai tsai cheng-lin hong hao-chung cheng and hsi-sheng goan. qubit-efficient encoding scheme for quantum simulations of electronic structure. physical review research 4 2 023154 2022. 35 peter w shor. quantum computing. documenta mathemat- ica 1 1000 467 486 1998. 36 sukin sim jonathan romero j er\u02c6ome f gonthier and alexander a kunitsa. adaptive pruning-based optimization of parameterized quantum circuits. quantum science and technology 6 2 025019 2021. 37 jinkai tian xiaoyu sun yuxuan du shanshan zhao qing liu kaining zhang wei yi wanrong huang chaoyue wang xingyao wu et al. recent advances for quantum neu- ral networks in generative learning. ieee transactions on pattern analysis and machine intelligence 45 10 12321 12340 2023. 38 aijuan wang jianglong hu shiyue zhang and lusi li. shallow hybrid quantum-classical convolutional neural net- work model for image classification. quantum information processing 23 1 17 2024. 39 xinbiao wang junyu liu tongliang liu yong luo yuxuan du and dacheng tao. symmetric pruning in quantum neural networks. arxiv preprint arxiv 2208.14057 2022. 40 zeyu wang xianhang li hongru zhu and cihang xie. revisiting adversarial training at scale. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 24675 24685 2024. 41 manuela weigold johanna barzen frank leymann and marie salm. data encoding patterns for quantum computing. in proceedings of the 27th conference on pattern languages of programs pages 1 11 2020. 42 wenjie wu ge yan xudong lu kaisen pan and junchi yan. quantumdarts differentiable quantum architecture search for variational quantum algorithms. in interna- tional conference on machine learning pages 37745 37764. pmlr 2023. 43 renxin zhao and shi wang. a review of quantum neu- ral networks methods models dilemma. arxiv preprint arxiv 2109.01840 2021. 44 ciyou zhu richard h byrd peihuang lu and jorge no- cedal. algorithm 778 l-bfgs-b fortran subroutines for large-scale bound-constrained optimization. acm trans- actions on mathematical software toms 23 4 550 560 1997. 20436",
      "keywords": [
        "page",
        "atp",
        "adaptive",
        "threshold",
        "pruning",
        "efficient",
        "data",
        "encoding",
        "quantum",
        "neural",
        "networks",
        "mohamed",
        "afane1",
        "gabrielle",
        "ebbrecht1",
        "ying",
        "wang2",
        "juntao",
        "chen1",
        "junaid",
        "farooq3",
        "1fordham",
        "university",
        "2stevens",
        "institute",
        "technology",
        "3university",
        "michigan-dearborn",
        "abstract",
        "qnns",
        "offer",
        "promising",
        "capa-",
        "bilities",
        "complex",
        "tasks",
        "often",
        "constrained",
        "limited",
        "qubit",
        "resources",
        "high",
        "entanglement",
        "which",
        "hinder",
        "scalability",
        "efficiency.",
        "paper",
        "in-",
        "troduce",
        "method",
        "reduces",
        "optimizes",
        "com-",
        "plexity",
        "computations",
        "qnns.",
        "dynam-",
        "ically",
        "prunes",
        "non-essential",
        "features",
        "based",
        "thresholds",
        "effectively",
        "reducing",
        "circuit",
        "requirements",
        "while",
        "preserving",
        "performance.",
        "extensive",
        "experiments",
        "across",
        "multiple",
        "datasets",
        "demonstrate",
        "entropy",
        "improves",
        "adversarial",
        "ro-",
        "bustness",
        "when",
        "combined",
        "training",
        "meth-",
        "ods",
        "like",
        "fgsm.",
        "results",
        "highlight",
        "ability",
        "bal-",
        "ance",
        "computational",
        "efficiency",
        "model",
        "resilience",
        "achiev-",
        "ing"
      ],
      "metadata": {
        "filename": "Afane_ATP_Adaptive_Threshold_Pruning_for_Efficient_Data_Encoding_in_Quantum_CVPR_2025_paper.pdf",
        "original_filename": "Afane_ATP_Adaptive_Threshold_Pruning_for_Efficient_Data_Encoding_in_Quantum_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 909036,
        "upload_file_id": "upload_1752162082707_77hjg5pvl",
        "upload_timestamp": "2025-07-10T15:41:22.772851",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:41:23.259187+00:00",
      "content_length": 43900,
      "content_type": ".pdf",
      "filename": "Afane_ATP_Adaptive_Threshold_Pruning_for_Efficient_Data_Encoding_in_Quantum_CVPR_2025_paper.pdf"
    },
    "17": {
      "file_id": "17",
      "content": "\n=== Page 1 ===\nTIDE: Training Locally Interpretable Domain Generalization Models\nEnables Test-time Correction\nAishwarya Agarwal1,2*\nSrikrishna Karanam2\u2020\nVineet Gandhi1\u2021\n1CVIT, Kohli Centre for Intelligent Systems, IIIT Hyderabad, India\n2Adobe Research, Bengaluru, India\nAbstract\nWe consider the problem of single-source domain gener-\nalization. Existing methods typically rely on extensive aug-\nmentations to synthetically cover diverse domains during\ntraining. However, they struggle with semantic shifts (e.g.,\nbackground and viewpoint changes), as they often learn\nglobal features instead of local concepts that tend to be do-\nmain invariant. To address this gap, we propose an ap-\nproach that compels models to leverage such local concepts\nduring prediction. Given no suitable dataset with per-class\nconcepts and localization maps exists, we first develop a\nnovel pipeline to generate annotations by exploiting the rich\nfeatures of diffusion and large-language models. Our next\ninnovation is TIDE, a novel training scheme with a con-\ncept saliency alignment loss that ensures model focus on the\nright per-concept regions and a local concept contrastive\nloss that promotes learning domain-invariant concept rep-\nresentations. This not only gives a robust model but also can\nbe visually interpreted using the predicted concept saliency\nmaps. Given these maps at test time, our final contribution\nis a new correction algorithm that uses the corresponding\nlocal concept representations to iteratively refine the pre-\ndiction until it aligns with prototypical concept representa-\ntions that we store at the end of model training. We evaluate\nour approach extensively on four standard DG benchmark\ndatasets and substantially outperform the current state-of-\nthe-art (12% improvement on average) while also demon-\nstrating that our predictions can be visually interpreted.\n1. Introduction\nEnhancing deep neural networks to generalize to out-of-\ndistribution samples remains a core challenge in machine\nlearning and computer vision research, as real-world test\ndata often diverges significantly from the training distribu-\n*aishwarya.agarwal@research.iiit.ac.in, aishagar@adobe.com\n\u2020skaranam@adobe.com\n\u2021vgandhi@iiit.ac.in\nTraining Domain\nTest Domain\nTraining Domain\nTest Domain\nABA\nTIDE\nbird\nperson\nbeak\nfeathers\neyes\nlips\n(Caltech101)\n(VOC2007)\n(Photo)\n(Sketch)\nFigure 1. Samples from VLCS (left) and PACS dataset (right)\nacross domain shifts, corresponding to bird and person class. First\nrow displays GradCAM maps [43] for ABA\u2019s class predictions.\nWe observe that model attention of ABA [5] falters across domain\nshifts. Second and third row display the concept specific Grad-\nCAM maps from TIDE. We posit that accurate concept learning\nand localization facilitates DG.\ntion [31]. The challenges compound when obtaining la-\nbeled samples from the target domain is expensive or un-\nfeasible, hindering application of semi-supervised learning\nor domain adaptation [25, 32, 49]. The problem of Do-\nmain Generalization (DG) [30, 44, 54, 61, 63] represents\na promising avenue for developing techniques that capture\ndomain-invariant patterns and improve performance on out-\nof-distribution samples. In this paper, we focus on Single\nSource Domain Generalization (SSDG), where a model is\ntrained on data from a single domain and aims to generalize\nwell to unseen domains [40, 51]. It represents the most strict\nform of DG, as the model must extract domain-invariant\nfeatures from a single, often limited perspective, without\nexposure to the variation present across multiple domains.\nMost previous work on SSDG [5, 7, 8, 19, 20, 22, 59, 63]\nrelies extensively on data augmentations to support the\nlearning of domain-generalized features. The premise is\nthat constructing an extensive repertoire of augmentations\nsynthesizes instances encompassing a wide spectrum of\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n30210\n\n=== Page 2 ===\nhuman-recognizable domains. However, accounting for all\nconceivable real-world augmentations presents an immense\nchallenge. These models have shown reasonable success in\naddressing variations in style and texture [45, 64]; however,\ntheir performance remains modest when faced with more\nsemantic domain shifts, such as changes in background and\nviewpoint e.g., in the VLCS dataset [41, 48].\nConsider the results of the state-of-the-art ABA [5]\nmodel on the two examples in Figure 1. In the first ex-\nample (left), domain shifts manifest as variations in back-\nground and viewpoint. In the second example (right), the\ntraining data comprises photos of human faces, whereas the\ntest data consists of skeletal sketches. In both cases, the do-\nmain shifts extend beyond style or texture variations, with\nABA failing to correctly classify the samples. Class-level\nsaliency maps [43] reveal that this misclassification stems\nfrom inadequate focus on critical local concepts under do-\nmain shifts, such as beak and feathers for birds, or\neyes and lips for persons. Such local concepts are inte-\ngral to class definition and remain invariant across domains,\nand a robust DG model must adeptly capture these stable\nfeatures. We posit that prior efforts learn global features per\nclass; and if the model fails to learn the correct feature set in\nthe training domain, its generalization performance is com-\npromised as noted from Figure 1 above. The inconsistency\nin concept localization further exacerbates the interpretabil-\nity and explainability of these models.\nWe adopt an alternative approach, wherein, rather than\nattempting to encompass all potential augmentations, we\ncompel the model to leverage essential class-specific con-\ncepts for classification. Our key idea is to force the model\nto attend to these concepts during training. The primary\nhurdle in this path is lack of annotated data specifying rele-\nvant class-specific concepts along with their corresponding\nlocalizations. To this end, our first contribution is a novel\npipeline that harnesses large language models (LLMs) and\ndiffusion models (DMs) to identify key concepts and gen-\nerate concept-level saliency maps in a scalable, automated\nfashion. We demonstrate that DMs, via cross-attention lay-\ners, can generate concept-level saliency maps for generated\nimages with notable granularity. Given these maps for a sin-\ngle synthesized image within a class, we leverage the rich\nfeature space of DMs to transfer them to images across di-\nverse domains in DG benchmarks [47].\nWe subsequently introduce TIDE, our second contribu-\ntion which employs cross-entropy losses over class and con-\ncept labels along with a novel pair of loss functions: a con-\ncept saliency alignment loss that ensures the model attends\nto the correct regions for each local concept, and a local\nconcept contrastive loss that promotes learning domain in-\nvariant features for these regions. As shown in Figure 1,\nTIDE consistently attends to local concepts such as beak\nand eyes in the training domain as well as across substan-\ntial domain shifts. This precise localization significantly\nbolsters SSDG performance while enabling the generation\nof visual explanations for model decisions.\nMarking a major leap in model interpretability and per-\nformance gains, our third contribution is to demonstrate\nhow our model\u2019s concept-level localization can be effec-\ntively leveraged for performance verification and test-time\ncorrection. To this end, we introduce the notion of local\nconcept signatures: prototypical features derived by pool-\ning concept-level features across training samples, guided\nby corresponding saliency maps. If the concept features\nassociated with the predicted class label do not align with\ntheir signatures, it signals the use of incorrect features for\nprediction. Consequently, we employ iterative refinement\nthrough concept saliency masking until concept predictions\nalign with their corresponding signatures. More formally,\nour paper makes following contributions:\n\u2022 We propose a novel synthetic annotation pipeline to au-\ntomatically identify key per-class local concepts and their\nlocalization maps, and transfer them to real images found\nin DG benchmarks.\n\u2022 We propose a novel approach, TIDE, to train locally in-\nterpretable models with a novel concept saliency align-\nment loss that ensures accurate concept localization and a\nnovel local concept contrastive loss that ensures domain-\ninvariant features.\n\u2022 We show TIDE enables correcting model prediction at\ntest time using the predicted concept saliency maps as part\nof a novel iterative attention refinement strategy.\n\u2022 We report extensive results on four standard DG bench-\nmarks, outperforming the current state-of-the-art by a sig-\nnificant 12% on average.\n2. Related Works\nMulti Source domain generalization (MSDG) methods as-\nsume access to multiple source domains and domain-\nspecific knowledge during training and have proposed ways\nto learn multi-source domain-invariant features [10, 15, 34,\n35, 53], utilize domain labels to capture domain shifts [11,\n35, 56], and design target-specific augmentation strate-\ngies [14, 16, 23]. However, these assumptions are not prac-\ntical for real-world applications [44, 57] and we instead fo-\ncus on the more challenging SSDG setting, where only one\nsource domain and no prior target knowledge is available.\nMost SSDG approaches have used augmentations to im-\nprove DG [5, 7, 8, 19, 20, 22, 59, 63]. While [59] also uses\ndiffusion models (DMs) [42] for data augmentation, our ap-\nproach differs by leveraging the rich feature space of DMs\nprimarily for offline saliency map annotation instead of aug-\nmentation. More recent work in SSDG [4, 28, 29] has also\nturned to approaches based on domain-specific prompting\nand causal inference instead of data augmentations. How-\never, these methods learn global features, limiting invari-\n30211\n\n=== Page 3 ===\nLocalized Contrastive Training\nTest-time correction\nTraining Phase\nAnchor\nPositive\nNegative\nFC Layers\nperson\nClass Prediction\nActivations\nGradients\nFC Layers\neyes\nlips\n\ud835\udc58!\n\ud835\udc58\"\n\ud835\udc66 \"\n\ud835\udc66 #\n\ud835\udc50\nConcept Prediction and Alignment \nActivations\nGradients\n\ud835\udc53$#\nGlobal Avg Pool\nguitar\n\ud835\udc50\nstrings\nknobs\n\ud835\udc58!\n\ud835\udc58\"\nSuppress irrelevant features\nDetect  mis-classification\n\ud835\udc39$\nCNN\n\ud835\udc53$!\n#\n\ud835\udc53$\"\n#\n\ud835\udc46$\n##\n\ud835\udc3a$\n##\n(\ud835\udc65 %) \n(\ud835\udc65 & )\n(\ud835\udc65)  \nCNN\n\ud835\udc39$\"\n\ud835\udc39$!\nCNN\nFeature \nMaps\n\ud835\udc66 \", (\n\ud835\udc66 #, (\n\ud835\udc46$\n#$, (\n\ud835\udc46$\n#%, ( \n\ud835\udc46$\n\", (\n\ud835\udc36\n\ud835\udc4a\n\ud835\udc3b\n\ud835\udc40 \", (\nTest Image (\ud835\udc65)  \n\ud835\udc36\n1\n\u2112 \"\n\u2112 #\n\u2112 $%&\n\u2112 '$$\nFigure 2. The TIDE pipeline: Left\u2014Training on a single domain with cross-entropy losses for class (Lc) and concept labels (Lk), alongside\nConcept Saliency Alignment (LCSA) and Local Concept Contrastive losses (LLCC). Right\u2014Test-time correction strategy applied in TIDE.\nance across unseen domains. Finally, these methods also do\nnot provide any signals to interpret model predictions.\nOn the other hand, while methods like Concept Bottle-\nneck Models (CBMs) [24, 33] also learn local concepts,\nthey are restricted to a predefined set of concepts and more\ncrucially, cannot ground them to image regions that these\nconcepts represent. Recent methods [37, 58] have scaled\nCBMs to a large number of classes and also proposed to\nways to learn multi-scale representations [52], but they are\nalso unable to connect these concepts to image pixels. Our\nSSDG method addresses these challenges by producing lo-\ncal concept saliency maps, enhancing both DG performance\nand model prediction interpretability.\n3. Methodology\nThe proposed framework is depicted in Figure 2. Training\ncomprises three components: class prediction, concept pre-\ndiction with saliency alignment, and localized contrastive\ntraining. During inference, class and concept predictions\nare integrated with a test-time correction mechanism.\nIn the concept alignment phase, we employ concept-\nlevel saliency maps as ground truth to enforce focus on in-\nterpretable features, using a saliency alignment loss. Do-\nmain invariance is further promoted by localized contrastive\ntraining, where we train the model to cluster similar con-\ncepts (e.g., eyes across augmentations) while separating\nunrelated concepts (e.g., strings). Finally, a test-time\ncorrection mechanism iteratively refines attention by sup-\npressing irrelevant regions, leveraging concept-signatures to\nredirect focus and improve classification accuracy. We pro-\nceed with a concise review of the key notations, followed\nby an in-depth exposition of our approach and the pipeline\nfor generating ground truth concept-level annotations.\nWe assume the source data is observed from a single\ndomain, denoted as D = {xi, yc\ni , yk\ni }N\ni=1, where xi repre-\nsents the i-th image, yc\ni the class label, yk\ni the concept label,\nand N the total sample count in the source domain. The\nshared CNN backbone is used to obtain the backbone fea-\ntures Fx \u2208RW \u00d7H\u00d7C. The automatically generated ground\ntruth concept-level saliency maps are denoted by Gk\nx. The\nGradCAM [43] saliency maps corresponding to class and\nconcepts labels are denoted by Sc\nx and Sk\nx respectively.\n3.1. Generating Concept-level Annotations\nDuring training, we aim our model to identify and spatially\nattend to stable, discriminative regions. However, existing\nDG datasets lack fine-grained, concept-level annotations for\nsuch regions. To address this, we propose a novel pipeline\nthat uses LLMs and DMs to automate scalable concept-level\nsaliency map generation.\nOur primary insight is that DMs can be harnessed to gen-\nerate high-quality, concept-level saliency maps for synthe-\nsized images. Extracting cross-attention maps [1, 3] from\na DM, given the prompt with concepts and correspond-\ning synthesized image, yields highly granular concept-level\nsaliency maps. Figure 3 demonstrates how DMs emphasize\nspecific regions for distinct concepts, capturing fine-grained\nattention to features such as a cat\u2019s whiskers or a snow-\nman\u2019s hands. With this technique serving as an efficient\ntool for yielding concept maps for synthesized images, the\nensuing questions are: (i) how to automate the identification\nof pertinent concepts for each class across datasets, and (ii)\nhow to transfer these concept maps to real-world images to\ngenerate ground-truth annotations.\nFirst, to identify the key concepts associated with each\nclass, we use GPT-3.5 [2], which we prompt to generate a\n30212\n\n=== Page 4 ===\nGenerated Image\nExtracted Cross Attention Maps\nPrompt: A photo of a cat with its whiskers, ears, eyes\nPrompt: A photo of a snowman with its nose, eyes, hands\nFigure 3. The first column displays the image generated from the\ngiven prompt, while the subsequent three columns show the cross-\nattention maps corresponding to each concept in the prompt.\nlist of distinctive, stable features (prompt in the supplement)\nfor each class. For instance, GPT-3.5 outputs concepts such\nas whiskers, ears, and eyes for a cat. We generate\na prompt leveraging these concepts, which is then used to\nsynthesize an exemplar image for each class. We further de-\nrive the corresponding concept-level attention maps as out-\nlined in the preceding paragraph.\nGiven a single synthesized exemplar image for each class\nand the concept-level saliency maps, we turn to the task\nof transferring these saliency annotations to real-world im-\nages.\nDM\u2019s feature space is particularly well-suited for\nthis, as it captures detailed, semantically-rich representa-\ntions that allow us to match synthetic concept regions to real\nimages across domains. Leveraging this, we use the Diffu-\nsion Feature Transfer (DIFT) method [47], which computes\npixel-level correspondences by comparing cosine similar-\nities between DM features.\nThrough this approach, we\nestablish a region-to-region correspondence between syn-\nthetic saliency maps (e.g., mouth of a dog) and similar\nregions in real-world images. This enables us to generate\ncomprehensive, consistent concept-level annotations across\ndomains as shown in Figure 4.\nWe obtain these concept-level annotations for widely-\nused benchmarks, including VLCS [13], PACS [27], Do-\nmainNet [38], and Office-Home [50]. For an image x, the\nbinarized concept-level maps (Gk\nx) are henceforth referred\nto as ground-truth saliency maps (GT-maps) in this paper.\nHaving established a process for generating saliency maps,\nthe next challenge is identifying the subset of concepts that\nthe model actually relies on for making predictions.\n3.1.1\nConcepts that matter\nWe restrict our method to essential concepts, filtering out\nthose that do not contribute meaningfully to classification.\nSynthesized \nExemplar\nPACS \u2013 photo\nPACS \u2013 sketch\nPACS \u2013 painting\nmouth\nPACS \u2013 cartoon\near\nPrompt: A photo of a dog with its ear, mouth\nFigure 4. The first row presents the prompt, corresponding synthe-\nsized image and attention maps for the concepts ear and mouth.\nBelow, we demonstrate that using diffusion features correspon-\ndences these concept saliency maps from a single exemplar can be\nautomatically transferred on dog images across domains.\nTo do this, we train a ResNet-18 [18] classifier on the source\ndomain [26], compute GradCAM [43] saliency maps for\neach class label, and use them to identify regions the model\nfocuses on when making predictions.\nWe compute the overlap between the GradCAM maps\nand GT-maps for each known concept in the dataset. Given\nimage x, its saliency map Sc\nx for class c and the GT-map Gk\nx\nfor concept k, we define the overlap Ok\nc (x) as:\n  \nO _c^ k\n(\nx) = \\fra\nc {\\ sum  _\n{i,j } \\\nm\nin (S^c_x (i\n,j), G^k_x(i,j))}{\\sum _{i,j} G^k_x(i,j)}. \n(1)\nwhere (i, j) are matrix indices. This measures how much\nof the model\u2019s attention for a given class aligns with the\nregions corresponding to concept k. We compute this over-\nlap for all concepts and images in the training set, and for\neach class c, we define the set of important concepts Kc as\nthose that consistently exhibit high overlap with the saliency\nmaps:\n  \\\nm\na\nthcal\n \n{K\n}\n_c =\n \\\nl eft  \\\n{\n k \\ \\middle |\\ \\frac {1}{N_c} \\sum _{x \\in \\mathcal {D}_c} O_c^k(x) > \\tau \\right \\}, \n(2)\nwhere Nc is the number of training examples in class c,\nDc is the set of images in class c, and \u03c4 is a threshold that\ndetermines the importance of the concept. This procedure\nfunctions as a concept discovery module, identifying which\n30213\n\n=== Page 5 ===\nlocal concepts are critical for predicting each class.\n3.2. TIDE\nAs shown in Figure 2, TIDE utilizes cross-entropy losses\nLc and Lk for class and concept labels, respectively, inte-\ngrated with novel concept saliency alignment and local con-\ncept contrastive losses. We detail these loss terms below.\n3.2.1\nConcept saliency alignment loss\nFor each image x, we predict important concepts Kc and\nenforce alignment between the saliency maps Sk\nx for pre-\ndicted concepts and the GT-maps Gk\nx. This is encouraged\nby our proposed concept saliency alignment (CSA) loss:\n  \\m a\nt\nhcal\n \n{L}_\n{\\t\ne x t \n{CS\nA}} = \\frac {1}{|\\mathcal {K}_c|} \\sum _{k \\in \\mathcal {K}_c} \\| S_x^k - G_x^k \\|_2^2. \n(3)\nAligning the model\u2019s attention with GT-maps enables\nclass-specific reasoning, thereby elucidating the rationale\nbehind predictions by linking them to relevant local fea-\ntures.\n3.2.2\nLocal concept contrastive loss\nWhile the CSA loss facilitates explicit localization of con-\ncepts, it is equally essential for these concept-level features\nto exhibit invariance across domain shifts. To achieve do-\nmain invariance, we propose a local concept contrastive\n(LCC) loss, employing a triplet strategy to cluster similar\nconcepts (e.g., eyes) across domains while distinguishing\nunrelated ones (e.g., feathers and ears).\nLet x be an anchor image containing concept k, a posi-\ntive image x+ (an augmentation of x that retains concept k),\nand a negative image x\u2212(containing a different concept k\u2032).\nFor each image, we compute a concept-specific feature vec-\ntor f k\nx \u2208RC, emphasizing the concept\u2019s relevant regions\nusing the GT-map Gk\nx. Each element f k\nx(l) is computed as:\n  \nf_x^ k\n(\nl) \n= \n\\ s um _{ i, j} G_x^k \\cdot F_x(i,j,l). \\label {eqn:fx_compute} \n(4)\nwhere \u00b7 is an element-wise multiplication operation, Gk\nx\nis a matrix representing the GT-map, Fx represents convo-\nlutional feature maps from the last convolution layer, and\n(i, j, l) denotes the lth channel\u2019s (i, j) element. These vec-\ntors focus on the concepts of interest (concept k for x and\nx+, k\u2032 for x\u2212), in contrast to the global features used in\nprior works. The LCC loss is then defined as:\n  \\m a thcal {L} _\n{\\ t e\nxt { LCC }\n} =  \\\nmax  (0, d(f_x^k, f_{x^+}^k) - d(f_x^k, f_{x^-}^{k'}) + \\alpha ), \n(5)\nwhere d(.) is the euclidean distance and \u03b1 is the margin.\nAlgorithm 1 Iterative Test-time Correction\nInput: Test image x, initial class prediction c(0), signatures pk for each concept k\nParameter: Threshold \u03b4, max iterations T\nOutput: Corrected class prediction, cfinal\n1: Sc(0),0\nx\n\u2190GradCAM(x, c(0)) {Initialize saliency map for iteration t = 0}\n2: x0\nmasked \u2190x\n3: for each k \u2208Kc(0) do\n4:\nf k,0\nx\n\u2190P\ni,j Sk\nx \u00b7 Fx(i, j, :)\n5:\nd(f k,0\nx\n, pk) \u21901 \u2212\nfk,0\nx\n\u00b7pk\n\u2225fk,0\nx\n\u2225\u2225pk\u2225\n6:\nif d(f k,0\nx\n, pk) > \u03b4 then\n7:\nCorrection Phase:\n8:\nfor t \u21901 to T do\n9:\nM c(t\u22121),t \u2190Binarize(Sc(t\u22121),t\u22121\nx\n)\n10:\nxt\nmasked \u2190xt\u22121\nmasked \u00b7 M c(t\u22121),t\n11:\nc(t) \u2190PredictClass(xt\nmasked)\n12:\nSc(t),t\nxmasked \u2190GradCAM(xt\nmasked, c(t))\n13:\nfor each k \u2208Kc(t) do\n14:\nf k,t\nxmasked \u2190P\ni,j Sk,t\nxmasked \u00b7 Fx(i, j, :)\n15:\nd(f k,t\nxmasked, pk) \u21901 \u2212\nfk,t\nxmasked \u00b7pk\n\u2225fk,t\nxmasked \u2225\u2225pk\u2225\n16:\nif d(f k,t\nxmasked, pk) \u2264\u03b4 then\n17:\nReturn cfinal \u2190c(t)\n18:\nend if\n19:\nend for\n20:\nend for\n21:\nend if\n22:\nReturn cfinal \u2190c(0)\n23: end for\n3.3. Test-time Correction\nWe establish that our localized, interpretable approach facil-\nitates correction of misclassifications through concept-level\nfeature verification. In this section, we first introduce con-\ncept signatures and detail the proposed correction strategy.\n3.3.1\nLocal concept signatures\nFor each concept k, we define a concept-signature pk \u2208\nRC, as its representative vector. We derive pk by averaging\nthe concept-specific feature vectors f k\nx across all training\nsamples x \u2208D containing concept k (denoted as Dk).\n  p\n^\nk = \n\\\nfrac\n {\n1}{|\\mathcal {D}^k|} \\sum _{x \\in \\mathcal {D}^k} f^k_x. \n(6)\nThese vectors act as reference points, helping the model\nrecognize when its attention is aligned with the right con-\ncepts during prediction, even when encountering new, un-\nseen domains.\n3.3.2\nDetecting and Correcting misclassifications\nConsider the example of test-time correction in Figure 2,\nwhere the model misclassifies a person as a guitar. The pre-\ndicted class guitar, involves concepts like strings and\n30214\n\n=== Page 6 ===\nknobs, but these are absent in the image. As a result, the\nmodel erroneously focuses on irrelevant features (e.g., the\nperson\u2019s legs or the background), as reflected in the Grad-\nCAM saliency maps.\nTo address misclassifications, we employ a two-step ap-\nproach: possible mistake detection followed by correction.\nThe process is outlined in Algorithm 1. First, the model ex-\ntracts concept-level saliency maps (Sk\nx), for all the concepts\ncorresponding to the predicted class, which are then used to\ncompute concept-level features f k\nx (step 4). In Step 5 and 6,\nwe compare the features to the stored concept signatures; a\ndeviation exceeding \u03b4 signals a misclassification.\nOnce a misalignment is detected, the model enters an it-\nerative refinement phase. The features prominent for the\ncurrent class level predictions are masked through the cor-\nresponding saliency map Sc\nx (step 9 and 10). The masking\nprocess is cumulative, in effect utilizing all masks from the\nfirst to the tth iteration. The masked features are then used\nfor subsequent predictions (step 11). This process contin-\nues until the concepts corresponding to the predicted class\naligns with concept-signatures or the maximum iteration\ncount is reached (steps 8\u201316 in the algorithm). If alignment\nis achieved, the predicted class in that iteration is confirmed\nas the final output.\n4. Experiments\nDatasets: We conduct experiments on four widely used\ndatasets - PACS [27], VLCS [13], OfficeHome [50], and\nDomainNet [38], within the DomainBed [17] evaluation\nbenchmarks.\nPACS contains 9,991 images across 7\ncategories in four domains:\n\u2018sketch\u2019, \u2018photo\u2019, \u2018clipart\u2019,\nand \u2018painting\u2019. VLCS consists of 10,729 images over 5\ncategories and 4 domains. The domains are from VOC2007\n(V), LabelMe (L), Caltech101 (C), and SUN09 (S), with\ndomain shifts primarily driven by background and view-\npoint variations. OfficeHome, with 15,500 images across\n65 categories, emphasizes indoor classes across \u2018product\u2019,\n\u2018real\u2019, \u2018clipart\u2019, and \u2018art\u2019 domains.\nFor DomainNet, we\nfollow prior work [39, 46] and use a subset of the 40\nmost common classes across \u2018sketch\u2019, \u2018real\u2019, \u2018clipart\u2019, and\n\u2018painting\u2019.\nExperimental Setup: We adhere to the SSDG paradigm,\ntraining on one source domain and testing across three\ntarget domains, i.e., the model is trained independently\nfour times, with the averaged test accuracies across target\ndomains reported in each case.\nTraining utilizes solely\nsource-domain GT-maps,\nexcluding any target-domain\nprior knowledge, data or annotations. Minimal augmen-\ntations (quantization, blurring, and canny edge), were\nused to introduce slight perturbations to create the triplets.\nWe use SDv2.1 [42] for generating exemplar images and\ncomputing cross-attention maps [1, 3].\nTo ensure a fair\ncomparison, we adopt a ResNet-18 backbone throughout\nall the experiments. We use the Adam optimizer with an\ninitial learning rate of 1 \u00d7 10\u22124 and a warm-up schedule\nover the first 1000 steps, after which the rate remains\nconstant. Margin value \u03b1 is set to 1.0. The batch size is\nset to 32. We empirically set \u03b4 = 0.1 and cap test-time\ncorrection at T = 10 iterations.\nCompared Approaches: We compare our method with\nERM [17] baseline and existing approaches that utilize\naugmentation-based techniques (NJPP [59], AugMix [19],\nMixStyle [63], CutMix [60], RandAugment [7]), self-\nsupervised and domain adaptation methods (RSC [22],\npAdaIn [36], L2D [55], RSC+ASR [12]), uncertainty mod-\neling (DSU [29], DSU-MAD [41]), attention and meta-\nlearning methods (ACVC [8], P-RC [6], Meta-Casual [4]),\nand prompt-based learning (PromptD [28]). The number\nof methods evaluated differs by dataset. The discrepancy\nstems from PACS being the dominant benchmark in prior\nSSDG works, resulting in a larger number of methods eval-\nuated on it. For VLCS, OfficeHome, and DomainNet, we\nrely on reported results from respective papers, or compute\nthem ourselves (where code was available) to ensure a fair\ncomparison.\n4.1. Main Results\nTable 1 provide a comparison of average classification ac-\ncuracy for our approach against existing methods across the\nPACS, VLCS, OfficeHome, and DomainNet datasets. Each\ncolumn in these tables represents a source domain used for\ntraining, with the numerical values indicating the average\naccuracy on the three target domains.\nThe last column\npresents the average of the four columns.\nTIDE decisively outperforms existing approaches across\nall datasets. It achieves average accuracy gains of 8.33%,\n13.37%, 16.16%, and 8.84% over the second-best ap-\nproach on PACS, VLCS, OfficeHome, and DomainNet, re-\nspectively.\nTable 1b details the performance on VLCS,\nwhere domain shifts primarily stem from background and\nviewpoint variations, with scenes spanning urban to ru-\nral and favoring non-standard viewpoints. Data augmenta-\ntion based methods, which focus on style variation, yield\nlimited gains on VLCS relative to PACS. Nonetheless,\nTIDE secures substantial improvements, notably achiev-\ning a 25.34% gain over DSU-MAD when Caltech101 (C)\nserves as the source domain. Another noteworthy obser-\nvation is that TIDE maintains strong performance across\nvarying class counts, from VLCS\u2019s 5 to OfficeHome\u2019s 65,\nunderscoring the scalability of the proposed approach.\n4.2. Ablations\nComponents of TIDE: Ablation experiments on the PACS\ndataset are conducted to assess the contribution of each loss\n30215\n\n=== Page 7 ===\nMethod Venue\nArt\nCartoon\nSketch\nPhoto\nAverage\nERM\n65.38\n64.20\n34.15\n33.65\n49.35\nAugmix [19] ICLR\u201921\n66.54\n70.16\n52.48\n38.30\n57.12\nRSC [22] ECCV\u201920\n73.40\n75.90\n56.20\n41.60\n61.03\nMixstyle [63] ICLR\u201921\n67.60\n70.38\n34.57\n37.44\n52.00\npAdaIn [36] CVPR\u201921\n64.96\n65.24\n32.04\n33.66\n49.72\nRSC+ASR [12] CVPR\u201921\n76.70\n79.30\n61.60\n54.60\n68.30\nL2D [55] ICCV\u201921\n76.91\n77.88\n53.66\n52.29\n65.93\nDSU [29] ICLR\u201922\n71.54\n74.51\n47.75\n42.10\n58.73\nACVC [8] CVPR\u201922\n73.68\n77.39\n55.30\n48.05\n63.10\nDSU-MAD [41] CVPR\u201923\n72.41\n74.47\n49.60\n44.15\n60.66\nP-RC [6] CVPR\u201923\n76.98\n78.54\n62.89\n57.11\n68.88\nMeta-Casual [4] CVPR\u201923\n77.13\n80.14\n62.55\n59.60\n69.86\nABA [5] ICCV\u201923\n75.69\n77.36\n54.12\n59.04\n66.30\nPromptD [28] CVPR\u201924\n78.77\n82.69\n62.94\n60.09\n71.87\nTIDE\n86.24\n86.37\n73.11\n74.36\n80.02\n(a) PACS\nMethod\nV\nL\nC\nS\nAverage\nERM\n76.72\n58.86\n44.95\n57.71\n59.06\nAugmix [19] ICLR\u201919\n75.25\n59.52\n45.90\n57.43\n59.03\npAdaIn [36] CVPR\u201921\n76.03\n65.21\n43.17\n57.94\n60.34\nMixstyle [63] ICLR\u201921\n75.73\n61.29\n44.66\n56.57\n59.06\nACVC [8] CVPR\u201922\n76.15\n61.23\n47.43\n60.18\n61.75\nDSU [29] ICLR\u201922\n76.93\n69.20\n46.54\n58.36\n62.11\nDSU-MAD [41] CVPR\u201923\n76.99\n70.85\n44.78\n62.23\n63.71\nTIDE\n82.62\n86.13\n70.12\n72.44\n77.08\n(b) VLCS\nMethod\nArt\nClipart\nProduct\nReal\nAverage\nERM\n57.43\n50.83\n48.9\n58.68\n53.96\nMixUp [21] ICLR\u201918\n50.41\n43.19\n41.24\n51.89\n46.93\nCutMix [60] ICCV\u201919\n49.17\n46.15\n41.2\n53.64\n47.04\nAugmix [19] ICLR\u201919\n56.86\n54.12\n52.02\n60.12\n56.03\nRandAugment [7] CVPRW\u201920\n58.07\n55.32\n52.02\n60.82\n56.56\nCutOut [9] arXiv:1708\n54.36\n50.79\n47.68\n58.24\n52.77\nRSC [22] ECCV\u201920\n53.51\n48.98\n47.16\n58.3\n52.73\nMEADA [62] NIPS\u201920\n57.0\n53.2\n48.81\n59.21\n54.80\nPixMix [20] CVPR\u201922\n53.77\n52.68\n48.91\n58.68\n53.51\nL2D [55] ICCV\u201921\n52.79\n48.97\n47.75\n58.31\n51.71\nACVC [8] CVPR\u201922\n54.3\n51.32\n47.69\n56.25\n52.89\nNJPP [59] ICML\u201924\n60.72\n54.95\n52.47\n61.26\n57.85\nTIDE\n72.32\n75.13\n75.22\n73.37\n74.01\n(c) OfficeHome\nMethod\nClipart\nPainting\nReal\nSketch\nAverage\nERM\n68.73\n66.12\n68.51\n69.44\n68.2\nMixUp [21] ICLR\u201918\n70.31\n64.34\n69.21\n68.82\n68.17\nCutMix [60] ICCV\u201919\n71.52\n63.84\n67.13\n69.41\n67.98\nAugmix [19] ICLR\u201919\n72.37\n62.91\n69.84\n71.22\n69.09\nRandAugment [7] CVPRW\u201920\n69.71\n65.51\n68.36\n66.93\n67.63\nCutOut [9] arXiv:1708\n70.86\n64.48\n69.92\n71.55\n69.20\nRSC [22] ECCV\u201920\n68.25\n67.91\n70.76\n66.18\n68.28\nPixMix [20] CVPR\u201922\n72.12\n63.51\n71.34\n65.46\n68.12\nNJPP [59] ICML\u201924\n76.14\n69.24\n76.61\n71.21\n73.3\nTIDE\n82.42\n80.37\n84.15\n81.61\n82.14\n(d) DomainNet\nTable 1. SSDG classification accuracies (%) on PACS, VLCS, Of-\nficeHome and DomainNet datasets, with ResNet-18 as backbone.\nEach column title indicates the source domain, and the numerical\nvalues represent the average performance in the target domains.\nterm on the overall performance. In Table 2, we present\nclassification accuracy by incrementally adding compo-\nnents to the training pipeline, demonstrating the contribu-\nMethod\nArt\nCartoon\nSketch\nPhoto\nAverage\nLc\n65.38\n64.20\n34.15\n33.65\n49.35\nLc + Lk\n65.47\n64.12\n35.44\n33.81\n49.71\nLc + Lk + LCSA\n66.11\n64.23\n35.14\n34.27\n49.93\nLc + Lk + LCSA + LLCC\n80.28\n82.91\n66.12\n65.37\n73.67\n+ test-time correction\n86.24\n86.37\n73.11\n74.36\n80.02\nTable 2. Ablation study (%) on PACS.\nphoto\nsketch\nphoto\ncartoon\nphoto\npainting\neyes\nhorns\nmouth\nwindows\n\ud835\udc73\ud835\udc73\ud835\udc6a\ud835\udc6a\nwithout\n\ud835\udc73\ud835\udc73\ud835\udc6a\ud835\udc6a\nwith\nFigure 5. t-SNE visualizations to demonstrate impact of LLCC.\nEach column represents a test domain (Sketch, Cartoon, Painting),\nwith the top row showing t-SNE plots without LLCC applied and\nthe bottom one with it. Please zoom in for optimal viewing.\ntion of each element to the model\u2019s performance. We ob-\nserve that the introduction of the concept classification loss\nLk and concept saliency alignment loss LCSA does not sig-\nnificantly affect the classifier\u2019s accuracy on the test domain.\nHowever, these components are integral to our approach,\nenabling test-time correction and, in turn, enhancing both\nperformance and model interpretability. We observe that\nthe introduction of the LLCC loss leads to a substantial in-\ncrease in test domain accuracy, clearly demonstrating its\neffectiveness in fostering domain invariance. Finally, we\ncompare results before and after test-time correction, high-\nlighting that even prior to correction, our method achieves\nstate-of-the-art performance across all source domains on\nPACS, underscoring the strength of our approach.\nError analysis of test-time correction: For this analy-\nsis, we train TIDE on the photo domain of PACS and test\nit on the sketch domain. The model gives an initial accu-\nracy of 74.79%, which improves to 82.29% post test-time\ncorrection. In 72.2% of test samples, TIDE does not invoke\ntest-time correction, with class predictions correct in 93.8%\nof these cases, demonstrating substantial reliability. The\nsupplement provides examples of cases where the model\nmisclassifications are not picked-up in the signature match-\ning step of test-time correction. In the remaining 27.8% of\ncases where correction is initiated, 52.5% successfully con-\nverge to the correct classification, significantly contributing\n30216\n\n=== Page 8 ===\nLabelMe (bird)\nSUN09 (chair)\nSketch (dog)\nCartoon (guitar)\nSketch (cat)\nPainting (mushroom)\nClipart (kettle)\nPACS\nVLCS\nOfficeHome\nDomainNet\nProduct (candles)\nstrings\nknobs\nsnout\nears\nbeak\nfeathers\nlegs\nseat\nopening\nhandle\nwick\nwick\nstem\ncap\nwhiskers\neyes\nFigure 6. Illustrative examples of concept level GradCAM maps corresponding to TIDE\u2019s predictions, across the four studied datasets. The\nconcept names are displayed beneath the maps, with the target domain and predicted class indicated on the left. More results in supplement.\nInitial Prediction: dog\nPost Correction: person\nears\nsnout\neyes\nlips\nInitial Prediction: elephant\nPost Correction: house\ntusks\ntrunk\nroof\nwindows\nFigure 7. The top row shows initial class predictions and Grad-\nCAM maps for the concepts, while the bottom row presents the\nresults after test-time correction.\nto the final post-correction accuracy of 82.29%.\nt-SNE plots: To demonstrate the impact of LLCC, we\npresent t-SNE visualizations in Figure 5, showing the dis-\ntribution of concept-specific vectors (as computed in Equa-\ntion 4). We individually plot the source domain (photo)\nalong with sketch, cartoon, and painting domains as tar-\ngets. Each case includes two plots: one without LLCC (top\nrow) and one with it (bottom row). We observe that with\nLLCC, concept samples (e.g., mouth) align closely across\ndomains, while distinct separation occurs between different\nconcepts (e.g., mouth and horns). Without LLCC, cluster\nseparation and alignment across domains are weaker, high-\nlighting its role in improving intra-concept compactness and\ninter-concept separability.\n4.3. Qualitative Results\nConcept Localization:\nWe train the SSDG model on\nphoto domain for PACS, OfficeHome, and DomainNet and\non Caltech101 for VLCS. The predicted class, concepts\nand corresponding saliency maps are shown in Figure 6.\nFor each target class (e.g., mushroom, kettle, guitar), the\nmodel reliably highlights key concept-specific regions (e.g.,\nstem, handle, strings) essential for classification.\nThe model effectively isolates key features across diverse\ncontexts, such as the legs and seat of a chair in com-\nplex zoomed-out scenes, the beak and feathers of cam-\nouflaged birds, the eyes and whiskers in deformed\nsketches of cats, and varying feature sizes, with the wick\noccupying a small area and the mushroom cap spanning\na larger one.\nTest-time Correction: The Figure 7 visually illustrates the\nefficacy of TIDE\u2019s test time correction abilities, by compar-\ning the initial and corrected results. The model initially mis-\nclassifies the images as dog and elephant, resulting in poorly\naligned concept-level saliency maps for corresponding con-\ncepts i.e. ears, snout, tusks and trunk. TIDE detects\nand rectifies such errors during inference, leading to accu-\nrate classification accompanied by precise concept localiza-\ntion. This qualitative evaluation reinforces the robustness of\nour approach in refining predictions and generating reliable,\nclass-specific concept maps.\n5. Conclusion\nIn this work, we considered the problem of single-source\ndomain generalization and observed that the current state-\nof-the-art methods fail in cases of semantic domain shifts\nprimarily due to the global nature of their learned features.\nTo alleviate this issue, we proposed TIDE, a new approach\nthat not only learns local concept representations but also\nproduces localization maps for these concepts. With these\nmaps, we showed we could visually interpret model deci-\nsions while also enabling correction of these decisions at\ntest time using our iterative attention refinement strategy.\nExtensive experimentation on standard benchmarks demon-\nstrated substantial and consistent performance gains over\nthe current state-of-the-art. Future work will rigorously ex-\nplore methods to generate confidence scores grounded in\nTIDE\u2019s concept verification strategy and would explore the\napplication of TIDE on other (e.g. fine-grained) classifica-\ntion datasets.\n30217\n\n=== Page 9 ===\nReferences\n[1] Aishwarya Agarwal,\nSrikrishna Karanam,\nKJ Joseph,\nApoorv Saxena, Koustava Goswami, and Balaji Vasan Srini-\nvasan. A-star: Test-time attention segregation and retention\nfor text-to-image synthesis. In International Conference on\nComputer Vision (ICCV), 2023. 3, 6\n[2] Tom B Brown. Language models are few-shot learners. Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2020. 3\n[3] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models. ACM\nTransactions on Graphics (TOG), 42(4):1\u201310, 2023. 3, 6\n[4] Jin Chen, Zhi Gao, Xinxiao Wu, and Jiebo Luo. Meta-causal\nlearning for single domain generalization. In Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 2,\n6, 7\n[5] Sheng Cheng, Tejas Gokhale, and Yezhou Yang. Adversar-\nial bayesian augmentation for single-source domain gener-\nalization. In International Conference on Computer Vision\n(ICCV), 2023. 1, 2, 7\n[6] Seokeon Choi, Debasmit Das, Sungha Choi, Seunghan Yang,\nHyunsin Park, and Sungrack Yun. Progressive random con-\nvolutions for single domain generalization. In Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 6,\n7\n[7] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In Conference on Computer\nVision and Pattern Recognition Workshops (CVPRW), 2020.\n1, 2, 6, 7\n[8] Ilke Cugu, Massimiliano Mancini, Yanbei Chen, and Zeynep\nAkata.\nAttention consistency on visual corruptions for\nsingle-source domain generalization. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2022. 1, 2, 6,\n7\n[9] Terrance Devries and Graham W. Taylor. Improved regular-\nization of convolutional neural networks with cutout. ArXiv,\nabs/1708.04552, 2017. 7\n[10] Yingjun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong\nZhen, Cees GM Snoek, and Ling Shao. Learning to learn\nwith variational information bottleneck for domain gener-\nalization.\nIn European Conference on Computer Vision\n(ECCV), 2020. 2\n[11] Antonio D\u2019Innocente and Barbara Caputo. Domain gener-\nalization with domain-specific aggregation modules. In The\nGerman Conference on Pattern Recognition, 2019. 2\n[12] Xinjie Fan, Qifei Wang, Junjie Ke, Feng Yang, Boqing Gong,\nand Mingyuan Zhou. Adversarially adaptive normalization\nfor single domain generalization. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2021. 6, 7\n[13] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased met-\nric learning: On the utilization of multiple datasets and web\nimages for softening bias. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2013. 4, 6\n[14] Robert Geirhos,\nPatricia Rubisch,\nClaudio Michaelis,\nMatthias Bethge, Felix A Wichmann, and Wieland Brendel.\nImagenet-trained cnns are biased towards texture; increas-\ning shape bias improves accuracy and robustness. In Inter-\nnational Conference on Learning Representations (ICLR),\n2019. 2\n[15] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang,\nand David Balduzzi. Domain generalization for object recog-\nnition with multi-task autoencoders. In International Confer-\nence on Computer Vision (ICCV), 2015. 2\n[16] Tejas Gokhale, Rushil Anirudh, Bhavya Kailkhura, Ja-\nyaraman J Thiagarajan, Chitta Baral, and Yezhou Yang.\nAttribute-guided adversarial training for robustness to nat-\nural perturbations. In Association for the Advancement of\nArtificial Intelligence (AAAI), 2021. 2\n[17] Ishaan Gulrajani and David Lopez-Paz. In search of lost do-\nmain generalization. International Conference on Learning\nRepresentations (ICLR), 2020. 6\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Conference\non Computer Vision and Pattern Recognition (CVPR), 2016.\n4\n[19] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph,\nJustin Gilmer, and Balaji Lakshminarayanan. Augmix: A\nsimple data processing method to improve robustness and\nuncertainty. International Conference on Learning Repre-\nsentations (ICLR), 2020. 1, 2, 6, 7\n[20] Dan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang,\nBo Li, Dawn Song, and Jacob Steinhardt. Pixmix: Dream-\nlike pictures comprehensively improve safety measures. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2022. 1, 2, 7\n[21] Yann\nN.\nDauphin\nDavid\nLopez-Paz\nHongyi\nZhang,\nMoustapha Cisse. mixup: Beyond empirical risk minimiza-\ntion. International Conference on Learning Representations\n(ICLR), 2018. 7\n[22] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang.\nSelf-challenging improves cross-domain generalization. In\nEuropean Conference on Computer Vision (ECCV), 2020. 1,\n2, 6, 7\n[23] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial transformer networks. Advances in Neural Informa-\ntion Processing Systems (NeurIPS), 2015. 2\n[24] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen\nMussmann, Emma Pierson, Been Kim, and Percy Liang.\nConcept bottleneck models. In International Conference on\nMachine Learning (ICML), 2020. 3\n[25] Wouter M Kouw and Marco Loog. A review of domain adap-\ntation without target labels. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (PAMI), 43(3):766\u2013785,\n2019. 1\n[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In Advances in Neural Information Processing Sys-\ntems (NeurIPS), 2012. 4\n[27] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M\nHospedales.\nDeeper, broader and artier domain general-\nization.\nIn International Conference on Computer Vision\n(ICCV), 2017. 4, 6\n30218\n\n=== Page 10 ===\n[28] Deng Li, Aming Wu, Yaowei Wang, and Yahong Han.\nPrompt-driven dynamic object-centric learning for single do-\nmain generalization. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2024. 2, 6, 7\n[29] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying\nShan, and Ling-Yu Duan.\nUncertainty modeling for out-\nof-distribution generalization. International Conference on\nLearning Representations (ICLR), 2022. 2, 6, 7\n[30] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang\nLiu, Kun Zhang, and Dacheng Tao.\nDeep domain gener-\nalization via conditional invariant adversarial networks. In\nEuropean Conference on Computer Vision (ECCV), 2018. 1\n[31] Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Ren-\nzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution\ngeneralization: A survey. ArXiv, abs/2108.13624, 2021. 1\n[32] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor-\ndan. Learning transferable features with deep adaptation net-\nworks. In International Conference on Machine Learning\n(ICML), 2015. 1\n[33] Andrei Margeloiu, Matthew Ashman, Umang Bhatt, Yanzhi\nChen, Mateja Jamnik, and Adrian Weller. Do concept bot-\ntleneck models learn as intended? International Conference\non Learning Representations (ICLR), 2021. 3\n[34] Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gi-\nanfranco Doretto.\nUnified deep supervised domain adap-\ntation and generalization.\nIn International Conference on\nComputer Vision (ICCV), 2017. 2\n[35] Krikamol\nMuandet,\nDavid\nBalduzzi,\nand\nBernhard\nSch\u00a8olkopf.\nDomain generalization via invariant fea-\nture representation. In International Conference on Machine\nLearning (ICML), 2013. 2\n[36] Oren Nuriel, Sagie Benaim, and Lior Wolf. Permuted adain:\nReducing the bias towards global statistics in image clas-\nsification. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021. 6, 7\n[37] Tuomas Oikarinen, Subhro Das, Lam M Nguyen, and Tsui-\nWei Weng. Label-free concept bottleneck models. Inter-\nnational Conference on Learning Representations (ICLR),\n2023. 3\n[38] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\nSaenko, and Bo Wang. Moment matching for multi-source\ndomain adaptation.\nIn International Conference on Com-\nputer Vision (ICCV), 2019. 4, 6\n[39] Daiqing Qi, Handong Zhao, Aidong Zhang, and Sheng Li.\nGeneralizing to unseen domains via text-guided augmenta-\ntion. In European Conference on Computer Vision (ECCV),\n2024. 6\n[40] Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn\nsingle domain generalization. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2020. 1\n[41] Sanqing Qu, Yingwei Pan, Guang Chen, Ting Yao, Changjun\nJiang, and Tao Mei. Modality-agnostic debiasing for single\ndomain generalization. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 2, 6, 7\n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2022. 2, 6\n[43] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam:\nVisual explanations from deep networks via\ngradient-based localization. In International Conference on\nComputer Vision (ICCV), 2017. 1, 2, 3, 4\n[44] Sarath Sivaprasad, Akshay Goindani, Mario Fritz, and Vi-\nneet Gandhi.\nClass-wise domain generalization: A novel\nframework for evaluating distributional shift.\nIn NeurIPS\nWorkshop on Distribution Shifts: Connecting Methods and\nApplications, 2022. 1, 2\n[45] Nathan Somavarapu, Chih-Yao Ma, and Zsolt Kira. Frus-\ntratingly simple domain generalization via image stylization.\nArXiv, abs/2006.11207, 2020. 2\n[46] Shuhan Tan, Xingchao Peng, and Kate Saenko.\nClass-\nimbalanced domain adaptation: An empirical odyssey. In\nEuropean Conference on Computer Vision Workshops (EC-\nCVW), 2020. 6\n[47] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan.\nEmergent correspondence\nfrom image diffusion. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2023. 2, 4\n[48] Antonio Torralba and Alexei A Efros.\nUnbiased look at\ndataset bias. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2011. 2\n[49] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.\nAdversarial discriminative domain adaptation.\nIn Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2017. 1\n[50] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty,\nand Sethuraman Panchanathan. Deep hashing network for\nunsupervised domain adaptation.\nIn Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2017. 4, 6\n[51] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C\nDuchi, Vittorio Murino, and Silvio Savarese. Generalizing\nto unseen domains via adversarial data augmentation. Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2018. 1\n[52] Bor-Shiun Wang, Chien-Yi Wang, and Wei-Chen Chiu.\nMcpnet: An interpretable classifier via multi-level concept\nprototypes. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2024. 3\n[53] Haohan Wang, Zexue He, Zachary C Lipton, and Eric P\nXing. Learning robust representations by projecting super-\nficial statistics out. International Conference on Learning\nRepresentations (ICLR), 2019. 2\n[54] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang,\nTao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and S Yu\nPhilip. Generalizing to unseen domains: A survey on do-\nmain generalization. IEEE transactions on knowledge and\ndata engineering, 35(8):8052\u20138072, 2022. 1\n[55] Zijian Wang, Yadan Luo, Ruihong Qiu, Zi Huang, and\nMahsa Baktashmotlagh. Learning to diversify for single do-\nmain generalization. In International Conference on Com-\nputer Vision (ICCV), 2021. 6, 7\n[56] Zehao Xiao, Jiayi Shen, Xiantong Zhen, Ling Shao, and\nCees Snoek. A bit more bayesian: Domain-invariant learning\nwith uncertainty. In International Conference on Machine\nLearning (ICML), 2021. 2\n30219\n\n=== Page 11 ===\n[57] Qinwei Xu, Ruipeng Zhang, Yi-Yan Wu, Ya Zhang, Ning\nLiu, and Yanfeng Wang.\nSimde: A simple domain ex-\npansion approach for single-source domain generalization.\nIn Conference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 2\n[58] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel\nJin, Chris Callison-Burch, and Mark Yatskar. Language in\na bottle: Language model guided concept bottlenecks for in-\nterpretable image classification. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2023. 3\n[59] Jianhao Yuan, Francesco Pinto, Adam Davies, and Philip\nTorr. Not just pretty pictures: Toward interventional data\naugmentation using text-to-image generators. International\nConference on Machine Learning (ICML), 2024. 1, 2, 6, 7\n[60] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localizable\nfeatures. In International Conference on Computer Vision\n(ICCV), 2019. 6, 7\n[61] Hanlin Zhang, Yi-Fan Zhang, Weiyang Liu, Adrian Weller,\nBernhard Sch\u00a8olkopf, and Eric P Xing. Towards principled\ndisentanglement for domain generalization. In Conference\non Computer Vision and Pattern Recognition (CVPR), 2022.\n1\n[62] Long Zhao, Ting Liu, Xi Peng, and Dimitris Metaxas.\nMaximum-entropy adversarial data augmentation for im-\nproved generalization and robustness. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2020. 7\n[63] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do-\nmain generalization with mixstyle. International Conference\non Learning Representations (ICLR), 2021. 1, 2, 6, 7\n[64] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xi-\nang.\nMixstyle neural networks for domain generalization\nand adaptation. International Journal of Computer Vision\n(IJCV), 132(3):822\u2013836, 2024. 2\n30220\n",
      "searchable_content": "page 1 tide training locally interpretable domain generalization models enables test-time correction aishwarya agarwal1 2 srikrishna karanam2 vineet gandhi1 1cvit kohli centre for intelligent systems iiit hyderabad india 2adobe research bengaluru india abstract we consider the problem of single-source domain gener- alization. existing methods typically rely on extensive aug- mentations to synthetically cover diverse domains during training. however they struggle with semantic shifts e.g. background and viewpoint changes as they often learn global features instead of local concepts that tend to be do- main invariant. to address this gap we propose an ap- proach that compels models to leverage such local concepts during prediction. given no suitable dataset with per-class concepts and localization maps exists we first develop a novel pipeline to generate annotations by exploiting the rich features of diffusion and large-language models. our next innovation is tide a novel training scheme with a con- cept saliency alignment loss that ensures model focus on the right per-concept regions and a local concept contrastive loss that promotes learning domain-invariant concept rep- resentations. this not only gives a robust model but also can be visually interpreted using the predicted concept saliency maps. given these maps at test time our final contribution is a new correction algorithm that uses the corresponding local concept representations to iteratively refine the pre- diction until it aligns with prototypical concept representa- tions that we store at the end of model training. we evaluate our approach extensively on four standard dg benchmark datasets and substantially outperform the current state-of- the-art 12 improvement on average while also demon- strating that our predictions can be visually interpreted. 1. introduction enhancing deep neural networks to generalize to out-of- distribution samples remains a core challenge in machine learning and computer vision research as real-world test data often diverges significantly from the training distribu- aishwarya.agarwal research.iiit.ac.in aishagar adobe.com skaranam adobe.com vgandhi iiit.ac.in training domain test domain training domain test domain aba tide bird person beak feathers eyes lips caltech101 voc2007 photo sketch figure 1. samples from vlcs left and pacs dataset right across domain shifts corresponding to bird and person class. first row displays gradcam maps 43 for aba s class predictions. we observe that model attention of aba 5 falters across domain shifts. second and third row display the concept specific grad- cam maps from tide. we posit that accurate concept learning and localization facilitates dg. tion 31 . the challenges compound when obtaining la- beled samples from the target domain is expensive or un- feasible hindering application of semi-supervised learning or domain adaptation 25 32 49 . the problem of do- main generalization dg 30 44 54 61 63 represents a promising avenue for developing techniques that capture domain-invariant patterns and improve performance on out- of-distribution samples. in this paper we focus on single source domain generalization ssdg where a model is trained on data from a single domain and aims to generalize well to unseen domains 40 51 . it represents the most strict form of dg as the model must extract domain-invariant features from a single often limited perspective without exposure to the variation present across multiple domains. most previous work on ssdg 5 7 8 19 20 22 59 63 relies extensively on data augmentations to support the learning of domain-generalized features. the premise is that constructing an extensive repertoire of augmentations synthesizes instances encompassing a wide spectrum of this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 30210 page 2 human-recognizable domains. however accounting for all conceivable real-world augmentations presents an immense challenge. these models have shown reasonable success in addressing variations in style and texture 45 64 however their performance remains modest when faced with more semantic domain shifts such as changes in background and viewpoint e.g. in the vlcs dataset 41 48 . consider the results of the state-of-the-art aba 5 model on the two examples in figure 1. in the first ex- ample left domain shifts manifest as variations in back- ground and viewpoint. in the second example right the training data comprises photos of human faces whereas the test data consists of skeletal sketches. in both cases the do- main shifts extend beyond style or texture variations with aba failing to correctly classify the samples. class-level saliency maps 43 reveal that this misclassification stems from inadequate focus on critical local concepts under do- main shifts such as beak and feathers for birds or eyes and lips for persons. such local concepts are inte- gral to class definition and remain invariant across domains and a robust dg model must adeptly capture these stable features. we posit that prior efforts learn global features per class and if the model fails to learn the correct feature set in the training domain its generalization performance is com- promised as noted from figure 1 above. the inconsistency in concept localization further exacerbates the interpretabil- ity and explainability of these models. we adopt an alternative approach wherein rather than attempting to encompass all potential augmentations we compel the model to leverage essential class-specific con- cepts for classification. our key idea is to force the model to attend to these concepts during training. the primary hurdle in this path is lack of annotated data specifying rele- vant class-specific concepts along with their corresponding localizations. to this end our first contribution is a novel pipeline that harnesses large language models llms and diffusion models dms to identify key concepts and gen- erate concept-level saliency maps in a scalable automated fashion. we demonstrate that dms via cross-attention lay- ers can generate concept-level saliency maps for generated images with notable granularity. given these maps for a sin- gle synthesized image within a class we leverage the rich feature space of dms to transfer them to images across di- verse domains in dg benchmarks 47 . we subsequently introduce tide our second contribu- tion which employs cross-entropy losses over class and con- cept labels along with a novel pair of loss functions a con- cept saliency alignment loss that ensures the model attends to the correct regions for each local concept and a local concept contrastive loss that promotes learning domain in- variant features for these regions. as shown in figure 1 tide consistently attends to local concepts such as beak and eyes in the training domain as well as across substan- tial domain shifts. this precise localization significantly bolsters ssdg performance while enabling the generation of visual explanations for model decisions. marking a major leap in model interpretability and per- formance gains our third contribution is to demonstrate how our model s concept-level localization can be effec- tively leveraged for performance verification and test-time correction. to this end we introduce the notion of local concept signatures prototypical features derived by pool- ing concept-level features across training samples guided by corresponding saliency maps. if the concept features associated with the predicted class label do not align with their signatures it signals the use of incorrect features for prediction. consequently we employ iterative refinement through concept saliency masking until concept predictions align with their corresponding signatures. more formally our paper makes following contributions we propose a novel synthetic annotation pipeline to au- tomatically identify key per-class local concepts and their localization maps and transfer them to real images found in dg benchmarks. we propose a novel approach tide to train locally in- terpretable models with a novel concept saliency align- ment loss that ensures accurate concept localization and a novel local concept contrastive loss that ensures domain- invariant features. we show tide enables correcting model prediction at test time using the predicted concept saliency maps as part of a novel iterative attention refinement strategy. we report extensive results on four standard dg bench- marks outperforming the current state-of-the-art by a sig- nificant 12 on average. 2. related works multi source domain generalization msdg methods as- sume access to multiple source domains and domain- specific knowledge during training and have proposed ways to learn multi-source domain-invariant features 10 15 34 35 53 utilize domain labels to capture domain shifts 11 35 56 and design target-specific augmentation strate- gies 14 16 23 . however these assumptions are not prac- tical for real-world applications 44 57 and we instead fo- cus on the more challenging ssdg setting where only one source domain and no prior target knowledge is available. most ssdg approaches have used augmentations to im- prove dg 5 7 8 19 20 22 59 63 . while 59 also uses diffusion models dms 42 for data augmentation our ap- proach differs by leveraging the rich feature space of dms primarily for offline saliency map annotation instead of aug- mentation. more recent work in ssdg 4 28 29 has also turned to approaches based on domain-specific prompting and causal inference instead of data augmentations. how- ever these methods learn global features limiting invari- 30211 page 3 localized contrastive training test-time correction training phase anchor positive negative fc layers person class prediction activations gradients fc layers eyes lips \ud835\udc58 \ud835\udc58 \ud835\udc66 \ud835\udc66 \ud835\udc50 concept prediction and alignment activations gradients \ud835\udc53 global avg pool guitar \ud835\udc50 strings knobs \ud835\udc58 \ud835\udc58 suppress irrelevant features detect mis-classification \ud835\udc39 cnn \ud835\udc53 \ud835\udc53 \ud835\udc46 \ud835\udc3a \ud835\udc65 \ud835\udc65 \ud835\udc65 cnn \ud835\udc39 \ud835\udc39 cnn feature maps \ud835\udc66 \ud835\udc66 \ud835\udc46 \ud835\udc46 \ud835\udc46 \ud835\udc36 \ud835\udc4a \ud835\udc3b \ud835\udc40 test image \ud835\udc65 \ud835\udc36 1 \u2112 \u2112 \u2112 \u2112 figure 2. the tide pipeline left training on a single domain with cross-entropy losses for class lc and concept labels lk alongside concept saliency alignment lcsa and local concept contrastive losses llcc . right test-time correction strategy applied in tide. ance across unseen domains. finally these methods also do not provide any signals to interpret model predictions. on the other hand while methods like concept bottle- neck models cbms 24 33 also learn local concepts they are restricted to a predefined set of concepts and more crucially cannot ground them to image regions that these concepts represent. recent methods 37 58 have scaled cbms to a large number of classes and also proposed to ways to learn multi-scale representations 52 but they are also unable to connect these concepts to image pixels. our ssdg method addresses these challenges by producing lo- cal concept saliency maps enhancing both dg performance and model prediction interpretability. 3. methodology the proposed framework is depicted in figure 2. training comprises three components class prediction concept pre- diction with saliency alignment and localized contrastive training. during inference class and concept predictions are integrated with a test-time correction mechanism. in the concept alignment phase we employ concept- level saliency maps as ground truth to enforce focus on in- terpretable features using a saliency alignment loss. do- main invariance is further promoted by localized contrastive training where we train the model to cluster similar con- cepts e.g. eyes across augmentations while separating unrelated concepts e.g. strings . finally a test-time correction mechanism iteratively refines attention by sup- pressing irrelevant regions leveraging concept-signatures to redirect focus and improve classification accuracy. we pro- ceed with a concise review of the key notations followed by an in-depth exposition of our approach and the pipeline for generating ground truth concept-level annotations. we assume the source data is observed from a single domain denoted as d xi yc i yk i n i 1 where xi repre- sents the i-th image yc i the class label yk i the concept label and n the total sample count in the source domain. the shared cnn backbone is used to obtain the backbone fea- tures fx rw h c. the automatically generated ground truth concept-level saliency maps are denoted by gk x. the gradcam 43 saliency maps corresponding to class and concepts labels are denoted by sc x and sk x respectively. 3.1. generating concept-level annotations during training we aim our model to identify and spatially attend to stable discriminative regions. however existing dg datasets lack fine-grained concept-level annotations for such regions. to address this we propose a novel pipeline that uses llms and dms to automate scalable concept-level saliency map generation. our primary insight is that dms can be harnessed to gen- erate high-quality concept-level saliency maps for synthe- sized images. extracting cross-attention maps 1 3 from a dm given the prompt with concepts and correspond- ing synthesized image yields highly granular concept-level saliency maps. figure 3 demonstrates how dms emphasize specific regions for distinct concepts capturing fine-grained attention to features such as a cat s whiskers or a snow- man s hands. with this technique serving as an efficient tool for yielding concept maps for synthesized images the ensuing questions are i how to automate the identification of pertinent concepts for each class across datasets and ii how to transfer these concept maps to real-world images to generate ground-truth annotations. first to identify the key concepts associated with each class we use gpt-3.5 2 which we prompt to generate a 30212 page 4 generated image extracted cross attention maps prompt a photo of a cat with its whiskers ears eyes prompt a photo of a snowman with its nose eyes hands figure 3. the first column displays the image generated from the given prompt while the subsequent three columns show the cross- attention maps corresponding to each concept in the prompt. list of distinctive stable features prompt in the supplement for each class. for instance gpt-3.5 outputs concepts such as whiskers ears and eyes for a cat. we generate a prompt leveraging these concepts which is then used to synthesize an exemplar image for each class. we further de- rive the corresponding concept-level attention maps as out- lined in the preceding paragraph. given a single synthesized exemplar image for each class and the concept-level saliency maps we turn to the task of transferring these saliency annotations to real-world im- ages. dm s feature space is particularly well-suited for this as it captures detailed semantically-rich representa- tions that allow us to match synthetic concept regions to real images across domains. leveraging this we use the diffu- sion feature transfer dift method 47 which computes pixel-level correspondences by comparing cosine similar- ities between dm features. through this approach we establish a region-to-region correspondence between syn- thetic saliency maps e.g. mouth of a dog and similar regions in real-world images. this enables us to generate comprehensive consistent concept-level annotations across domains as shown in figure 4. we obtain these concept-level annotations for widely- used benchmarks including vlcs 13 pacs 27 do- mainnet 38 and office-home 50 . for an image x the binarized concept-level maps gk x are henceforth referred to as ground-truth saliency maps gt-maps in this paper. having established a process for generating saliency maps the next challenge is identifying the subset of concepts that the model actually relies on for making predictions. 3.1.1 concepts that matter we restrict our method to essential concepts filtering out those that do not contribute meaningfully to classification. synthesized exemplar pacs photo pacs sketch pacs painting mouth pacs cartoon ear prompt a photo of a dog with its ear mouth figure 4. the first row presents the prompt corresponding synthe- sized image and attention maps for the concepts ear and mouth. below we demonstrate that using diffusion features correspon- dences these concept saliency maps from a single exemplar can be automatically transferred on dog images across domains. to do this we train a resnet-18 18 classifier on the source domain 26 compute gradcam 43 saliency maps for each class label and use them to identify regions the model focuses on when making predictions. we compute the overlap between the gradcam maps and gt-maps for each known concept in the dataset. given image x its saliency map sc x for class c and the gt-map gk x for concept k we define the overlap ok c x as o _c k x fra c sum _ i j m in s c_x i j g k_x i j sum _ i j g k_x i j . 1 where i j are matrix indices. this measures how much of the model s attention for a given class aligns with the regions corresponding to concept k. we compute this over- lap for all concepts and images in the training set and for each class c we define the set of important concepts kc as those that consistently exhibit high overlap with the saliency maps m a thcal k _c l eft k middle frac 1 n_c sum _ x in mathcal d _c o_c k x tau right 2 where nc is the number of training examples in class c dc is the set of images in class c and \u03c4 is a threshold that determines the importance of the concept. this procedure functions as a concept discovery module identifying which 30213 page 5 local concepts are critical for predicting each class. 3.2. tide as shown in figure 2 tide utilizes cross-entropy losses lc and lk for class and concept labels respectively inte- grated with novel concept saliency alignment and local con- cept contrastive losses. we detail these loss terms below. 3.2.1 concept saliency alignment loss for each image x we predict important concepts kc and enforce alignment between the saliency maps sk x for pre- dicted concepts and the gt-maps gk x. this is encouraged by our proposed concept saliency alignment csa loss m a t hcal l _ t e x t cs a frac 1 mathcal k _c sum _ k in mathcal k _c s_x k - g_x k _2 2. 3 aligning the model s attention with gt-maps enables class-specific reasoning thereby elucidating the rationale behind predictions by linking them to relevant local fea- tures. 3.2.2 local concept contrastive loss while the csa loss facilitates explicit localization of con- cepts it is equally essential for these concept-level features to exhibit invariance across domain shifts. to achieve do- main invariance we propose a local concept contrastive lcc loss employing a triplet strategy to cluster similar concepts e.g. eyes across domains while distinguishing unrelated ones e.g. feathers and ears . let x be an anchor image containing concept k a posi- tive image x an augmentation of x that retains concept k and a negative image x containing a different concept k . for each image we compute a concept-specific feature vec- tor f k x rc emphasizing the concept s relevant regions using the gt-map gk x. each element f k x l is computed as f_x k l s um _ i j g_x k cdot f_x i j l . label eqn fx_compute 4 where is an element-wise multiplication operation gk x is a matrix representing the gt-map fx represents convo- lutional feature maps from the last convolution layer and i j l denotes the lth channel s i j element. these vec- tors focus on the concepts of interest concept k for x and x k for x in contrast to the global features used in prior works. the lcc loss is then defined as m a thcal l _ t e xt lcc max 0 d f_x k f_ x k - d f_x k f_ x - k alpha 5 where d . is the euclidean distance and \u03b1 is the margin. algorithm 1 iterative test-time correction input test image x initial class prediction c 0 signatures pk for each concept k parameter threshold \u03b4 max iterations t output corrected class prediction cfinal 1 sc 0 0 x gradcam x c 0 initialize saliency map for iteration t 0 2 x0 masked x 3 for each k kc 0 do 4 f k 0 x p i j sk x fx i j 5 d f k 0 x pk 1 fk 0 x pk fk 0 x pk 6 if d f k 0 x pk \u03b4 then 7 correction phase 8 for t 1 to t do 9 m c t 1 t binarize sc t 1 t 1 x 10 xt masked xt 1 masked m c t 1 t 11 c t predictclass xt masked 12 sc t t xmasked gradcam xt masked c t 13 for each k kc t do 14 f k t xmasked p i j sk t xmasked fx i j 15 d f k t xmasked pk 1 fk t xmasked pk fk t xmasked pk 16 if d f k t xmasked pk \u03b4 then 17 return cfinal c t 18 end if 19 end for 20 end for 21 end if 22 return cfinal c 0 23 end for 3.3. test-time correction we establish that our localized interpretable approach facil- itates correction of misclassifications through concept-level feature verification. in this section we first introduce con- cept signatures and detail the proposed correction strategy. 3.3.1 local concept signatures for each concept k we define a concept-signature pk rc as its representative vector. we derive pk by averaging the concept-specific feature vectors f k x across all training samples x d containing concept k denoted as dk . p k frac 1 mathcal d k sum _ x in mathcal d k f k_x. 6 these vectors act as reference points helping the model recognize when its attention is aligned with the right con- cepts during prediction even when encountering new un- seen domains. 3.3.2 detecting and correcting misclassifications consider the example of test-time correction in figure 2 where the model misclassifies a person as a guitar. the pre- dicted class guitar involves concepts like strings and 30214 page 6 knobs but these are absent in the image. as a result the model erroneously focuses on irrelevant features e.g. the person s legs or the background as reflected in the grad- cam saliency maps. to address misclassifications we employ a two-step ap- proach possible mistake detection followed by correction. the process is outlined in algorithm 1. first the model ex- tracts concept-level saliency maps sk x for all the concepts corresponding to the predicted class which are then used to compute concept-level features f k x step 4 . in step 5 and 6 we compare the features to the stored concept signatures a deviation exceeding \u03b4 signals a misclassification. once a misalignment is detected the model enters an it- erative refinement phase. the features prominent for the current class level predictions are masked through the cor- responding saliency map sc x step 9 and 10 . the masking process is cumulative in effect utilizing all masks from the first to the tth iteration. the masked features are then used for subsequent predictions step 11 . this process contin- ues until the concepts corresponding to the predicted class aligns with concept-signatures or the maximum iteration count is reached steps 8 16 in the algorithm . if alignment is achieved the predicted class in that iteration is confirmed as the final output. 4. experiments datasets we conduct experiments on four widely used datasets - pacs 27 vlcs 13 officehome 50 and domainnet 38 within the domainbed 17 evaluation benchmarks. pacs contains 9 991 images across 7 categories in four domains sketch photo clipart and painting . vlcs consists of 10 729 images over 5 categories and 4 domains. the domains are from voc2007 v labelme l caltech101 c and sun09 s with domain shifts primarily driven by background and view- point variations. officehome with 15 500 images across 65 categories emphasizes indoor classes across product real clipart and art domains. for domainnet we follow prior work 39 46 and use a subset of the 40 most common classes across sketch real clipart and painting . experimental setup we adhere to the ssdg paradigm training on one source domain and testing across three target domains i.e. the model is trained independently four times with the averaged test accuracies across target domains reported in each case. training utilizes solely source-domain gt-maps excluding any target-domain prior knowledge data or annotations. minimal augmen- tations quantization blurring and canny edge were used to introduce slight perturbations to create the triplets. we use sdv2.1 42 for generating exemplar images and computing cross-attention maps 1 3 . to ensure a fair comparison we adopt a resnet-18 backbone throughout all the experiments. we use the adam optimizer with an initial learning rate of 1 10 4 and a warm-up schedule over the first 1000 steps after which the rate remains constant. margin value \u03b1 is set to 1.0. the batch size is set to 32. we empirically set \u03b4 0.1 and cap test-time correction at t 10 iterations. compared approaches we compare our method with erm 17 baseline and existing approaches that utilize augmentation-based techniques njpp 59 augmix 19 mixstyle 63 cutmix 60 randaugment 7 self- supervised and domain adaptation methods rsc 22 padain 36 l2d 55 rsc asr 12 uncertainty mod- eling dsu 29 dsu-mad 41 attention and meta- learning methods acvc 8 p-rc 6 meta-casual 4 and prompt-based learning promptd 28 . the number of methods evaluated differs by dataset. the discrepancy stems from pacs being the dominant benchmark in prior ssdg works resulting in a larger number of methods eval- uated on it. for vlcs officehome and domainnet we rely on reported results from respective papers or compute them ourselves where code was available to ensure a fair comparison. 4.1. main results table 1 provide a comparison of average classification ac- curacy for our approach against existing methods across the pacs vlcs officehome and domainnet datasets. each column in these tables represents a source domain used for training with the numerical values indicating the average accuracy on the three target domains. the last column presents the average of the four columns. tide decisively outperforms existing approaches across all datasets. it achieves average accuracy gains of 8.33 13.37 16.16 and 8.84 over the second-best ap- proach on pacs vlcs officehome and domainnet re- spectively. table 1b details the performance on vlcs where domain shifts primarily stem from background and viewpoint variations with scenes spanning urban to ru- ral and favoring non-standard viewpoints. data augmenta- tion based methods which focus on style variation yield limited gains on vlcs relative to pacs. nonetheless tide secures substantial improvements notably achiev- ing a 25.34 gain over dsu-mad when caltech101 c serves as the source domain. another noteworthy obser- vation is that tide maintains strong performance across varying class counts from vlcs s 5 to officehome s 65 underscoring the scalability of the proposed approach. 4.2. ablations components of tide ablation experiments on the pacs dataset are conducted to assess the contribution of each loss 30215 page 7 method venue art cartoon sketch photo average erm 65.38 64.20 34.15 33.65 49.35 augmix 19 iclr 21 66.54 70.16 52.48 38.30 57.12 rsc 22 eccv 20 73.40 75.90 56.20 41.60 61.03 mixstyle 63 iclr 21 67.60 70.38 34.57 37.44 52.00 padain 36 cvpr 21 64.96 65.24 32.04 33.66 49.72 rsc asr 12 cvpr 21 76.70 79.30 61.60 54.60 68.30 l2d 55 iccv 21 76.91 77.88 53.66 52.29 65.93 dsu 29 iclr 22 71.54 74.51 47.75 42.10 58.73 acvc 8 cvpr 22 73.68 77.39 55.30 48.05 63.10 dsu-mad 41 cvpr 23 72.41 74.47 49.60 44.15 60.66 p-rc 6 cvpr 23 76.98 78.54 62.89 57.11 68.88 meta-casual 4 cvpr 23 77.13 80.14 62.55 59.60 69.86 aba 5 iccv 23 75.69 77.36 54.12 59.04 66.30 promptd 28 cvpr 24 78.77 82.69 62.94 60.09 71.87 tide 86.24 86.37 73.11 74.36 80.02 a pacs method v l c s average erm 76.72 58.86 44.95 57.71 59.06 augmix 19 iclr 19 75.25 59.52 45.90 57.43 59.03 padain 36 cvpr 21 76.03 65.21 43.17 57.94 60.34 mixstyle 63 iclr 21 75.73 61.29 44.66 56.57 59.06 acvc 8 cvpr 22 76.15 61.23 47.43 60.18 61.75 dsu 29 iclr 22 76.93 69.20 46.54 58.36 62.11 dsu-mad 41 cvpr 23 76.99 70.85 44.78 62.23 63.71 tide 82.62 86.13 70.12 72.44 77.08 b vlcs method art clipart product real average erm 57.43 50.83 48.9 58.68 53.96 mixup 21 iclr 18 50.41 43.19 41.24 51.89 46.93 cutmix 60 iccv 19 49.17 46.15 41.2 53.64 47.04 augmix 19 iclr 19 56.86 54.12 52.02 60.12 56.03 randaugment 7 cvprw 20 58.07 55.32 52.02 60.82 56.56 cutout 9 arxiv 1708 54.36 50.79 47.68 58.24 52.77 rsc 22 eccv 20 53.51 48.98 47.16 58.3 52.73 meada 62 nips 20 57.0 53.2 48.81 59.21 54.80 pixmix 20 cvpr 22 53.77 52.68 48.91 58.68 53.51 l2d 55 iccv 21 52.79 48.97 47.75 58.31 51.71 acvc 8 cvpr 22 54.3 51.32 47.69 56.25 52.89 njpp 59 icml 24 60.72 54.95 52.47 61.26 57.85 tide 72.32 75.13 75.22 73.37 74.01 c officehome method clipart painting real sketch average erm 68.73 66.12 68.51 69.44 68.2 mixup 21 iclr 18 70.31 64.34 69.21 68.82 68.17 cutmix 60 iccv 19 71.52 63.84 67.13 69.41 67.98 augmix 19 iclr 19 72.37 62.91 69.84 71.22 69.09 randaugment 7 cvprw 20 69.71 65.51 68.36 66.93 67.63 cutout 9 arxiv 1708 70.86 64.48 69.92 71.55 69.20 rsc 22 eccv 20 68.25 67.91 70.76 66.18 68.28 pixmix 20 cvpr 22 72.12 63.51 71.34 65.46 68.12 njpp 59 icml 24 76.14 69.24 76.61 71.21 73.3 tide 82.42 80.37 84.15 81.61 82.14 d domainnet table 1. ssdg classification accuracies on pacs vlcs of- ficehome and domainnet datasets with resnet-18 as backbone. each column title indicates the source domain and the numerical values represent the average performance in the target domains. term on the overall performance. in table 2 we present classification accuracy by incrementally adding compo- nents to the training pipeline demonstrating the contribu- method art cartoon sketch photo average lc 65.38 64.20 34.15 33.65 49.35 lc lk 65.47 64.12 35.44 33.81 49.71 lc lk lcsa 66.11 64.23 35.14 34.27 49.93 lc lk lcsa llcc 80.28 82.91 66.12 65.37 73.67 test-time correction 86.24 86.37 73.11 74.36 80.02 table 2. ablation study on pacs. photo sketch photo cartoon photo painting eyes horns mouth windows \ud835\udc73\ud835\udc73\ud835\udc6a\ud835\udc6a without \ud835\udc73\ud835\udc73\ud835\udc6a\ud835\udc6a with figure 5. t-sne visualizations to demonstrate impact of llcc. each column represents a test domain sketch cartoon painting with the top row showing t-sne plots without llcc applied and the bottom one with it. please zoom in for optimal viewing. tion of each element to the model s performance. we ob- serve that the introduction of the concept classification loss lk and concept saliency alignment loss lcsa does not sig- nificantly affect the classifier s accuracy on the test domain. however these components are integral to our approach enabling test-time correction and in turn enhancing both performance and model interpretability. we observe that the introduction of the llcc loss leads to a substantial in- crease in test domain accuracy clearly demonstrating its effectiveness in fostering domain invariance. finally we compare results before and after test-time correction high- lighting that even prior to correction our method achieves state-of-the-art performance across all source domains on pacs underscoring the strength of our approach. error analysis of test-time correction for this analy- sis we train tide on the photo domain of pacs and test it on the sketch domain. the model gives an initial accu- racy of 74.79 which improves to 82.29 post test-time correction. in 72.2 of test samples tide does not invoke test-time correction with class predictions correct in 93.8 of these cases demonstrating substantial reliability. the supplement provides examples of cases where the model misclassifications are not picked-up in the signature match- ing step of test-time correction. in the remaining 27.8 of cases where correction is initiated 52.5 successfully con- verge to the correct classification significantly contributing 30216 page 8 labelme bird sun09 chair sketch dog cartoon guitar sketch cat painting mushroom clipart kettle pacs vlcs officehome domainnet product candles strings knobs snout ears beak feathers legs seat opening handle wick wick stem cap whiskers eyes figure 6. illustrative examples of concept level gradcam maps corresponding to tide s predictions across the four studied datasets. the concept names are displayed beneath the maps with the target domain and predicted class indicated on the left. more results in supplement. initial prediction dog post correction person ears snout eyes lips initial prediction elephant post correction house tusks trunk roof windows figure 7. the top row shows initial class predictions and grad- cam maps for the concepts while the bottom row presents the results after test-time correction. to the final post-correction accuracy of 82.29 . t-sne plots to demonstrate the impact of llcc we present t-sne visualizations in figure 5 showing the dis- tribution of concept-specific vectors as computed in equa- tion 4 . we individually plot the source domain photo along with sketch cartoon and painting domains as tar- gets. each case includes two plots one without llcc top row and one with it bottom row . we observe that with llcc concept samples e.g. mouth align closely across domains while distinct separation occurs between different concepts e.g. mouth and horns . without llcc cluster separation and alignment across domains are weaker high- lighting its role in improving intra-concept compactness and inter-concept separability. 4.3. qualitative results concept localization we train the ssdg model on photo domain for pacs officehome and domainnet and on caltech101 for vlcs. the predicted class concepts and corresponding saliency maps are shown in figure 6. for each target class e.g. mushroom kettle guitar the model reliably highlights key concept-specific regions e.g. stem handle strings essential for classification. the model effectively isolates key features across diverse contexts such as the legs and seat of a chair in com- plex zoomed-out scenes the beak and feathers of cam- ouflaged birds the eyes and whiskers in deformed sketches of cats and varying feature sizes with the wick occupying a small area and the mushroom cap spanning a larger one. test-time correction the figure 7 visually illustrates the efficacy of tide s test time correction abilities by compar- ing the initial and corrected results. the model initially mis- classifies the images as dog and elephant resulting in poorly aligned concept-level saliency maps for corresponding con- cepts i.e. ears snout tusks and trunk. tide detects and rectifies such errors during inference leading to accu- rate classification accompanied by precise concept localiza- tion. this qualitative evaluation reinforces the robustness of our approach in refining predictions and generating reliable class-specific concept maps. 5. conclusion in this work we considered the problem of single-source domain generalization and observed that the current state- of-the-art methods fail in cases of semantic domain shifts primarily due to the global nature of their learned features. to alleviate this issue we proposed tide a new approach that not only learns local concept representations but also produces localization maps for these concepts. with these maps we showed we could visually interpret model deci- sions while also enabling correction of these decisions at test time using our iterative attention refinement strategy. extensive experimentation on standard benchmarks demon- strated substantial and consistent performance gains over the current state-of-the-art. future work will rigorously ex- plore methods to generate confidence scores grounded in tide s concept verification strategy and would explore the application of tide on other e.g. fine-grained classifica- tion datasets. 30217 page 9 references 1 aishwarya agarwal srikrishna karanam kj joseph apoorv saxena koustava goswami and balaji vasan srini- vasan. a-star test-time attention segregation and retention for text-to-image synthesis. in international conference on computer vision iccv 2023. 3 6 2 tom b brown. language models are few-shot learners. ad- vances in neural information processing systems neurips 2020. 3 3 hila chefer yuval alaluf yael vinker lior wolf and daniel cohen-or. attend-and-excite attention-based se- mantic guidance for text-to-image diffusion models. acm transactions on graphics tog 42 4 1 10 2023. 3 6 4 jin chen zhi gao xinxiao wu and jiebo luo. meta-causal learning for single domain generalization. in conference on computer vision and pattern recognition cvpr 2023. 2 6 7 5 sheng cheng tejas gokhale and yezhou yang. adversar- ial bayesian augmentation for single-source domain gener- alization. in international conference on computer vision iccv 2023. 1 2 7 6 seokeon choi debasmit das sungha choi seunghan yang hyunsin park and sungrack yun. progressive random con- volutions for single domain generalization. in conference on computer vision and pattern recognition cvpr 2023. 6 7 7 ekin d cubuk barret zoph jonathon shlens and quoc v le. randaugment practical automated data augmentation with a reduced search space. in conference on computer vision and pattern recognition workshops cvprw 2020. 1 2 6 7 8 ilke cugu massimiliano mancini yanbei chen and zeynep akata. attention consistency on visual corruptions for single-source domain generalization. in conference on com- puter vision and pattern recognition cvpr 2022. 1 2 6 7 9 terrance devries and graham w. taylor. improved regular- ization of convolutional neural networks with cutout. arxiv abs 1708.04552 2017. 7 10 yingjun du jun xu huan xiong qiang qiu xiantong zhen cees gm snoek and ling shao. learning to learn with variational information bottleneck for domain gener- alization. in european conference on computer vision eccv 2020. 2 11 antonio d innocente and barbara caputo. domain gener- alization with domain-specific aggregation modules. in the german conference on pattern recognition 2019. 2 12 xinjie fan qifei wang junjie ke feng yang boqing gong and mingyuan zhou. adversarially adaptive normalization for single domain generalization. in conference on com- puter vision and pattern recognition cvpr 2021. 6 7 13 chen fang ye xu and daniel n rockmore. unbiased met- ric learning on the utilization of multiple datasets and web images for softening bias. in conference on computer vision and pattern recognition cvpr 2013. 4 6 14 robert geirhos patricia rubisch claudio michaelis matthias bethge felix a wichmann and wieland brendel. imagenet-trained cnns are biased towards texture increas- ing shape bias improves accuracy and robustness. in inter- national conference on learning representations iclr 2019. 2 15 muhammad ghifary w bastiaan kleijn mengjie zhang and david balduzzi. domain generalization for object recog- nition with multi-task autoencoders. in international confer- ence on computer vision iccv 2015. 2 16 tejas gokhale rushil anirudh bhavya kailkhura ja- yaraman j thiagarajan chitta baral and yezhou yang. attribute-guided adversarial training for robustness to nat- ural perturbations. in association for the advancement of artificial intelligence aaai 2021. 2 17 ishaan gulrajani and david lopez-paz. in search of lost do- main generalization. international conference on learning representations iclr 2020. 6 18 kaiming he xiangyu zhang shaoqing ren and jian sun. deep residual learning for image recognition. in conference on computer vision and pattern recognition cvpr 2016. 4 19 dan hendrycks norman mu ekin d cubuk barret zoph justin gilmer and balaji lakshminarayanan. augmix a simple data processing method to improve robustness and uncertainty. international conference on learning repre- sentations iclr 2020. 1 2 6 7 20 dan hendrycks andy zou mantas mazeika leonard tang bo li dawn song and jacob steinhardt. pixmix dream- like pictures comprehensively improve safety measures. in conference on computer vision and pattern recognition cvpr 2022. 1 2 7 21 yann n. dauphin david lopez-paz hongyi zhang moustapha cisse. mixup beyond empirical risk minimiza- tion. international conference on learning representations iclr 2018. 7 22 zeyi huang haohan wang eric p xing and dong huang. self-challenging improves cross-domain generalization. in european conference on computer vision eccv 2020. 1 2 6 7 23 max jaderberg karen simonyan andrew zisserman et al. spatial transformer networks. advances in neural informa- tion processing systems neurips 2015. 2 24 pang wei koh thao nguyen yew siang tang stephen mussmann emma pierson been kim and percy liang. concept bottleneck models. in international conference on machine learning icml 2020. 3 25 wouter m kouw and marco loog. a review of domain adap- tation without target labels. ieee transactions on pattern analysis and machine intelligence pami 43 3 766 785 2019. 1 26 alex krizhevsky ilya sutskever and geoffrey e hinton. imagenet classification with deep convolutional neural net- works. in advances in neural information processing sys- tems neurips 2012. 4 27 da li yongxin yang yi-zhe song and timothy m hospedales. deeper broader and artier domain general- ization. in international conference on computer vision iccv 2017. 4 6 30218 page 10 28 deng li aming wu yaowei wang and yahong han. prompt-driven dynamic object-centric learning for single do- main generalization. in conference on computer vision and pattern recognition cvpr 2024. 2 6 7 29 xiaotong li yongxing dai yixiao ge jun liu ying shan and ling-yu duan. uncertainty modeling for out- of-distribution generalization. international conference on learning representations iclr 2022. 2 6 7 30 ya li xinmei tian mingming gong yajing liu tongliang liu kun zhang and dacheng tao. deep domain gener- alization via conditional invariant adversarial networks. in european conference on computer vision eccv 2018. 1 31 jiashuo liu zheyan shen yue he xingxuan zhang ren- zhe xu han yu and peng cui. towards out-of-distribution generalization a survey. arxiv abs 2108.13624 2021. 1 32 mingsheng long yue cao jianmin wang and michael jor- dan. learning transferable features with deep adaptation net- works. in international conference on machine learning icml 2015. 1 33 andrei margeloiu matthew ashman umang bhatt yanzhi chen mateja jamnik and adrian weller. do concept bot- tleneck models learn as intended international conference on learning representations iclr 2021. 3 34 saeid motiian marco piccirilli donald a adjeroh and gi- anfranco doretto. unified deep supervised domain adap- tation and generalization. in international conference on computer vision iccv 2017. 2 35 krikamol muandet david balduzzi and bernhard sch olkopf. domain generalization via invariant fea- ture representation. in international conference on machine learning icml 2013. 2 36 oren nuriel sagie benaim and lior wolf. permuted adain reducing the bias towards global statistics in image clas- sification. in conference on computer vision and pattern recognition cvpr 2021. 6 7 37 tuomas oikarinen subhro das lam m nguyen and tsui- wei weng. label-free concept bottleneck models. inter- national conference on learning representations iclr 2023. 3 38 xingchao peng qinxun bai xide xia zijun huang kate saenko and bo wang. moment matching for multi-source domain adaptation. in international conference on com- puter vision iccv 2019. 4 6 39 daiqing qi handong zhao aidong zhang and sheng li. generalizing to unseen domains via text-guided augmenta- tion. in european conference on computer vision eccv 2024. 6 40 fengchun qiao long zhao and xi peng. learning to learn single domain generalization. in conference on computer vision and pattern recognition cvpr 2020. 1 41 sanqing qu yingwei pan guang chen ting yao changjun jiang and tao mei. modality-agnostic debiasing for single domain generalization. in conference on computer vision and pattern recognition cvpr 2023. 2 6 7 42 robin rombach andreas blattmann dominik lorenz patrick esser and bj orn ommer. high-resolution image syn- thesis with latent diffusion models. in conference on com- puter vision and pattern recognition cvpr 2022. 2 6 43 ramprasaath r selvaraju michael cogswell abhishek das ramakrishna vedantam devi parikh and dhruv batra. grad-cam visual explanations from deep networks via gradient-based localization. in international conference on computer vision iccv 2017. 1 2 3 4 44 sarath sivaprasad akshay goindani mario fritz and vi- neet gandhi. class-wise domain generalization a novel framework for evaluating distributional shift. in neurips workshop on distribution shifts connecting methods and applications 2022. 1 2 45 nathan somavarapu chih-yao ma and zsolt kira. frus- tratingly simple domain generalization via image stylization. arxiv abs 2006.11207 2020. 2 46 shuhan tan xingchao peng and kate saenko. class- imbalanced domain adaptation an empirical odyssey. in european conference on computer vision workshops ec- cvw 2020. 6 47 luming tang menglin jia qianqian wang cheng perng phoo and bharath hariharan. emergent correspondence from image diffusion. in advances in neural information processing systems neurips 2023. 2 4 48 antonio torralba and alexei a efros. unbiased look at dataset bias. in conference on computer vision and pattern recognition cvpr 2011. 2 49 eric tzeng judy hoffman kate saenko and trevor darrell. adversarial discriminative domain adaptation. in confer- ence on computer vision and pattern recognition cvpr 2017. 1 50 hemanth venkateswara jose eusebio shayok chakraborty and sethuraman panchanathan. deep hashing network for unsupervised domain adaptation. in conference on com- puter vision and pattern recognition cvpr 2017. 4 6 51 riccardo volpi hongseok namkoong ozan sener john c duchi vittorio murino and silvio savarese. generalizing to unseen domains via adversarial data augmentation. ad- vances in neural information processing systems neurips 2018. 1 52 bor-shiun wang chien-yi wang and wei-chen chiu. mcpnet an interpretable classifier via multi-level concept prototypes. in conference on computer vision and pattern recognition cvpr 2024. 3 53 haohan wang zexue he zachary c lipton and eric p xing. learning robust representations by projecting super- ficial statistics out. international conference on learning representations iclr 2019. 2 54 jindong wang cuiling lan chang liu yidong ouyang tao qin wang lu yiqiang chen wenjun zeng and s yu philip. generalizing to unseen domains a survey on do- main generalization. ieee transactions on knowledge and data engineering 35 8 8052 8072 2022. 1 55 zijian wang yadan luo ruihong qiu zi huang and mahsa baktashmotlagh. learning to diversify for single do- main generalization. in international conference on com- puter vision iccv 2021. 6 7 56 zehao xiao jiayi shen xiantong zhen ling shao and cees snoek. a bit more bayesian domain-invariant learning with uncertainty. in international conference on machine learning icml 2021. 2 30219 page 11 57 qinwei xu ruipeng zhang yi-yan wu ya zhang ning liu and yanfeng wang. simde a simple domain ex- pansion approach for single-source domain generalization. in conference on computer vision and pattern recognition cvpr 2023. 2 58 yue yang artemis panagopoulou shenghao zhou daniel jin chris callison-burch and mark yatskar. language in a bottle language model guided concept bottlenecks for in- terpretable image classification. in conference on computer vision and pattern recognition cvpr 2023. 3 59 jianhao yuan francesco pinto adam davies and philip torr. not just pretty pictures toward interventional data augmentation using text-to-image generators. international conference on machine learning icml 2024. 1 2 6 7 60 sangdoo yun dongyoon han seong joon oh sanghyuk chun junsuk choe and youngjoon yoo. cutmix regu- larization strategy to train strong classifiers with localizable features. in international conference on computer vision iccv 2019. 6 7 61 hanlin zhang yi-fan zhang weiyang liu adrian weller bernhard sch olkopf and eric p xing. towards principled disentanglement for domain generalization. in conference on computer vision and pattern recognition cvpr 2022. 1 62 long zhao ting liu xi peng and dimitris metaxas. maximum-entropy adversarial data augmentation for im- proved generalization and robustness. in advances in neural information processing systems neurips 2020. 7 63 kaiyang zhou yongxin yang yu qiao and tao xiang. do- main generalization with mixstyle. international conference on learning representations iclr 2021. 1 2 6 7 64 kaiyang zhou yongxin yang yu qiao and tao xi- ang. mixstyle neural networks for domain generalization and adaptation. international journal of computer vision ijcv 132 3 822 836 2024. 2 30220",
      "keywords": [
        "page",
        "tide",
        "training",
        "locally",
        "interpretable",
        "domain",
        "generalization",
        "models",
        "enables",
        "test-time",
        "correction",
        "aishwarya",
        "agarwal1",
        "srikrishna",
        "karanam2",
        "vineet",
        "gandhi1",
        "1cvit",
        "kohli",
        "centre",
        "intelligent",
        "systems",
        "iiit",
        "hyderabad",
        "india",
        "2adobe",
        "research",
        "bengaluru",
        "abstract",
        "consider",
        "problem",
        "single-source",
        "gener-",
        "alization.",
        "existing",
        "methods",
        "typically",
        "rely",
        "extensive",
        "aug-",
        "mentations",
        "synthetically",
        "cover",
        "diverse",
        "domains",
        "during",
        "training.",
        "however",
        "struggle",
        "semantic",
        "shifts",
        "e.g.",
        "background",
        "viewpoint",
        "changes",
        "often",
        "learn",
        "global",
        "features",
        "instead",
        "local",
        "concepts",
        "tend",
        "do-",
        "main",
        "invariant.",
        "address",
        "gap",
        "propose",
        "ap-",
        "proach",
        "compels",
        "leverage",
        "such",
        "prediction.",
        "given",
        "suitable",
        "dataset",
        "per-class",
        "localization",
        "maps",
        "exists",
        "first",
        "develop",
        "novel",
        "pipeline",
        "generate",
        "annotations",
        "exploiting",
        "rich",
        "diffusion",
        "large-language",
        "models.",
        "next",
        "innovation",
        "scheme",
        "con-",
        "cept",
        "saliency",
        "alignment"
      ],
      "metadata": {
        "filename": "Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction_CVPR_2025_paper.pdf",
        "original_filename": "Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 5969923,
        "upload_file_id": "upload_1752162217958_a7y6gx599",
        "upload_timestamp": "2025-07-10T15:43:38.025999",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T15:43:38.406120+00:00",
      "content_length": 50504,
      "content_type": ".pdf",
      "filename": "Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction_CVPR_2025_paper.pdf"
    },
    "18": {
      "file_id": "18",
      "content": "\n=== Page 1 ===\nEvaluating Vision-Language Models as Evaluators in Path Planning\nMohamed Aghzal1, Xiang Yue2, Erion Plaku3*, Ziyu Yao1\n1George Mason University, 2Carnegie Mellon University, 3National Science Foundation\n{maghzal, ziyuyao}@gmu.edu, xyue2@andrew.cmu.edu, eplaku@nsf.gov\nAbstract\nDespite their promise to perform complex reasoning,\nlarge language models (LLMs) have been shown to have\nlimited effectiveness in end-to-end planning. This has in-\nspired an intriguing question: if these models cannot plan\nwell, can they still contribute to the planning framework as\na helpful plan evaluator? In this work, we generalize this\nquestion to consider LLMs augmented with visual under-\nstanding, i.e., Vision-Language Models (VLMs). We intro-\nduce PATHEVAL, a novel benchmark evaluating VLMs as\nplan evaluators in complex path-planning scenarios. Suc-\nceeding in the benchmark requires a VLM to be able to ab-\nstract traits of optimal paths from the scenario description,\ndemonstrate precise low-level perception on each path, and\nintegrate this information to decide the better path. Our\nanalysis of state-of-the-art VLMs reveals that these mod-\nels face signi\ufb01cant challenges on the benchmark. We ob-\nserve that the VLMs can precisely abstract given scenar-\nios to identify the desired traits and exhibit mixed perfor-\nmance in integrating the provided information. Yet, their vi-\nsion component presents a critical bottleneck, with models\nstruggling to perceive low-level details about a path. Our\nexperimental results show that this issue cannot be trivially\naddressed via end-to-end \ufb01ne-tuning; rather, task-speci\ufb01c\ndiscriminative adaptation of these vision encoders is needed\nfor these VLMs to become effective path evaluators.12\n1. Introduction\nThe impressive capabilities of Large Language Models\n(LLMs) [6, 8, 71] and Vision-Language Models (VLMs)\n[82] have led to an increasing interest in applying them to\nautomated motion planning and navigation tasks [50, 51].\nHowever, the inherent limitations of these models in long-\n*The work by E. Plaku is supported by (while serving at) the National\nScience Foundation. Any opinion, \ufb01ndings, and conclusions or recom-\nmendations expressed in this material are those of the authors and do not\nnecessarily re\ufb02ect the views of the National Science Foundation.\n1Source code: https://github.com/MohamedAghzal/PathEval\n2Dataset: https://huggingface.co/datasets/maghzal/PathEval\nhorizon planning have rendered them ineffective as end-\nto-end motion planners [2, 3, 11, 75]. This has made re-\nsearchers wonder: if these models cannot be good motion\nplanners themselves, can they still support a motion plan-\nning framework? Intuitively, using these models still holds\nthe promise of signi\ufb01cantly enhancing the motion plan-\nning framework, as they have learned extensive factual and\ncommonsense knowledge that could bene\ufb01t planning dur-\ning their pre-training. As a result, there has been an emerg-\ning paradigm exploring how these models can be leveraged\nin combination with traditional methods [27].\nOne particularly interesting approach within this line of\nwork involves using these models as plan evaluators. Mo-\ntivated by the intuition that \u201cevaluation is easier than gen-\neration\u201d [31], several efforts have explored leveraging these\nmodels as \u201ccritics\u201d to assess the quality of generated plans\n[5, 21, 73, 83]. However, most of these efforts have focused\non scenarios that require only limited, high-level visual per-\nception, without necessitating \ufb01ne-grained or precise per-\nceptual abilities. On the other hand, while there have been\nprior works similarly investigating VLMs\u2019 low-level per-\nception [26, 64], studies speci\ufb01cally about the use of these\nmodels in planning remain limited. Tasks such as motion\nplanning often require \ufb01ne-grained visual understanding in\nhighly speci\ufb01c contexts, while also drawing on broad com-\nmonsense knowledge acquired during pre-training. Hence,\nthere is a pressing need to investigate the potential of VLMs\nto understand both low-level visual details and leverage\nthese visual signals for high-level reasoning.\nIn this work, we explore whether we can utilize VLMs\nas evaluators in highly intricate continuous path planning\nproblems. We introduce PATHEVAL (Figure 1), a control-\nlable benchmark designed around path planning in complex\nenvironments under diverse commonsense decision-making\nscenarios (e.g., A \ufb01re\ufb01ghting robot has to cover as much of\nthe area as possible to extinguish \ufb01res and scout for sur-\nvivors, thus needs to prioritize paths with more coverage\nand higher clearance). Traditionally, encoding constraints\nin a planning algorithm requires signi\ufb01cant human effort in\nhand-crafting scenario-speci\ufb01c criteria for path evaluation,\nsuch that the constraints can be injected into the planner\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n6886\n\n=== Page 2 ===\nExample of an instance of PATHEVAL\n2D\n3D\nPath Descriptors:\nPath 1: Min. clearance: 1.07, Max. clearance: 8.91, Avg. clearance: 2.62, Path length: 243.46, Smoothness: 843.07, Number of sharp turns: 4,\nMaximum angle: 124.35.\nPath 2: Min. clearance: 1.57, Max. clearance: 8.91, Avg. clearance: 3.18, Path length: 243.75, Smoothness: 809.65, Number of sharp turns: 1,\nMaximum angle: 93.12.\nScenario #1: The agent navigating this maze is a large truck, so sharp turns (90 degrees or larger) are harder to make. It should also stay on a\nstraight line (unless it is making a turn) as it is driving through heavy traf\ufb01c.\nRequired Descriptors: Smoothness (\u2192), Number of Sharp turns (\u2192), Maximum angle (\u2192)\nGround Truth: Path 2\nScenario #2: An autonomous \ufb01re\ufb01ghting robot is designed to navigate and operate within burning buildings to extinguish \ufb01res and rescue trapped\nindividuals. It should explore as much of the area as possible, while maintaining a safe distance from the walls to avoid damage.\nRequired Descriptors: Avg. Clearance (\u2191), Min. Clearance (\u2191), Path length (\u2191)\nGround Truth: Path 1\nScenario #3: As the vehicle is traversing a warzone, it must stay concealed from enemy operatives, making use of covers like walls and avoiding\nopen spaces as much as possible. It should also reach its target as quickly as possible.\nRequired Descriptors: Avg. Clearance (\u2192), Max. Clearance (\u2192), Path length (\u2192)\nGround Truth: Path 2\nPrompt\nThe agent navigating this maze is a large truck, so sharp turns (90 degrees or larger) are harder to make. It should also stay on a straight line\n(unless it is making a turn) as it is driving through heavy traf\ufb01c. Which path better achieves the task? Path 1 is on the left side and Path 2 is on\nthe right side. Your answer should follow the format below:\nAnswer: Path 1 or Path 2.\nExplanation: Why you chose the path (1 or 2).\nGPT-4o\nAnswer: Path 1.\nExplanation: Path 1 has fewer sharp turns and maintains longer straight segments compared to Path 2. This is more suitable for a large truck\nnavigating through heavy traf\ufb01c, as it minimizes the need for dif\ufb01cult maneuvers and allows for smoother driving. Path 2, on the other hand, has\nmore frequent and sharper turns, which would be more challenging for a large truck to navigate.\nLLaVA-NeXT-7b\nAnswer: Path 1.\nExplanation: Path 1 is better suited for a large truck because it has fewer sharp turns (90 degrees or larger) compared to Path 2. The truck\nwould have to make only one 90-degree turn, which is easier to execute than the two 90-degree turns in Path 2. Additionally, Path 1 has more\nstraight sections, which would be more ef\ufb01cient for a large truck to navigate through heavy traf\ufb01c. The fewer turns and the greater number of\nstraight sections make Path 1 the more suitable route for a large truck.\nFigure 1. Example of PATHEVAL. The benchmark consists of 14,550 instances of path pairs rendered in both 2D and 3D and mapped to 15\ndecision-making scenarios. Success on this task is tied to three distinct levels: 1) Attribute abstraction: recognizing what aspects make\na path ideal, 2) Low-level perception: extracting the required attributes for each path from the images, and 2) Information integration:\nsynthesizing the collected information to make a decision. We test a set of VLMs on the task and \ufb01nd that they struggle particularly with\nlow-level perception. Incorrect answers by different models are shown (explanations indicating misperception are highlighted).\nduring the search process [36, 52, 54]. However, this not\nonly is a tedious and costly process but also cannot scale up\nto handle the countless intricate scenarios in meeting real-\nlife planning needs. An effective VLM as a \u201creward model\u201d\nrelieving humans from customizing evaluation criteria for\nspeci\ufb01c scenarios could enable scalable, general-purpose\nplanners that can adapt to diverse commonsense scenarios\ndescribed in natural language.\n6887\n\n=== Page 3 ===\nIn PATHEVAL, a VLM is tasked with comparing two\npaths within a given decision-making context and select-\ning the one that better satis\ufb01es the constraints outlined by\nthe scenario. Success on this task requires effective per-\nformance across three distinct levels: 1) attribute abstrac-\ntion: recognizing the attributes that de\ufb01ne a favorable path\nin a particular decision-making context; 2) low-level per-\nception: demonstrating precise low-level perception to de-\ntermine which path performs better based on the given cri-\nteria; and 3) information integration: integrating and syn-\nthesizing the perceived information to produce an answer.\nUsing this benchmark, we analyze the performance of 9\nstate-of-the-art (SOTA) VLMs, including both the closed-\nsource GPT-4o and GPT-4o-mini [47] and 7 different open-\nsource VLMs of different sizes (i.e., LLaVA-NeXT 7b and\n13b [39], Qwen2-VL-7b [69], LLaVA-OneVision-7b [32],\nLLaMA-3.2-11b [17], and Intern-VL2 8b and 40b [12]).\nWe \ufb01nd that these models struggle with the path evalua-\ntion task (e.g., Qwen2-VL-7b achieving only 50.2% accu-\nracy). However, when providing these VLMs with verbal-\nized path speci\ufb01cations, their performance signi\ufb01cantly im-\nproves (e.g., 74.2% accuracy for Qwen2-VL-7b), which re-\nveals a potential vision bottleneck of these VLMs. Our fur-\nther analysis con\ufb01rms these models\u2019 weakness in low-level\nperception, especially when they are tasked to perceive the\nclearance of a path with respect to surrounding obstacles,\nand this weakness could be more prominent when the envi-\nronment and the path representation become more complex.\nWe discover the source of this weakness from the vision\nencoders used by these VLMs, yet simply \ufb01ne-tuning the\nVLMs end-to-end with the vision encoders does not address\nthe issue. Rather, our experiments suggest performing task-\nspeci\ufb01c discriminative adaptation of these vision encoders.\n2. Related Work\n2.1. Vision-Language Models\nThe outstanding success of decoder-only LLMs [6, 48, 65]\nhas driven the development of Vision-Language Models\n(VLMs), which extend LLMs with a vision component in an\nattempt to generalize their performance into a multi-modal\nsetting [12, 17, 32, 37, 38, 62, 69]. VLMs are designed for\ntasks that require unifying visual and linguistic representa-\ntions (e.g. visual question answering [4]). Typically, this is\nachieved by connecting a vision encoder to a language de-\ncoder and projecting the representations into a shared space,\nallowing visual elements and linguistic components to be\nlinked in a semantically meaningful manner. One of the\nearliest successful models at achieving this was the CLIP\nencoder [55], which was trained using contrastive learning\nto learn representations that map images and their corre-\nsponding textual descriptions.\nSeveral varieties of CLIP\nhave then been introduced [25, 56, 76, 80]. These mod-\nels, while showing tremendous promises have shown sev-\neral limitations when it comes to visual tasks [57]; as such\nseveral works [64, 72] have sought ways to improve such\nrepresentations by combining them with vision foundation\nmodels such as DINO [8, 49]. In this work, we contribute\nto the research of VLMs with a new benchmark, namely\nPATHEVAL, focusing on evaluating VLMs as evaluators for\npath planning. This benchmark tests VLMs seamlessly on\ntheir commonsense understanding (i.e., being able to ab-\nstract critical concepts expressed in the described planning\nscenarios), low-level perception (i.e., precisely perceiving\ndetails about paths in complex environments), and the abil-\nity to reason about the collected textual and visual informa-\ntion for decision-making.\n2.2. Automated Planning with VLMs\nVision-language planning promises more \ufb02exible planning\nframeworks and enhanced human-AI interaction. There-\nfore, designing systems that can effectively understand nat-\nural language instruction and leverage perceptual inputs to\nconduct planning tasks has been a topic of interest in re-\ncent years [20, 61, 70]. The rise of VLMs has led many\nto investigate the use of these models as vision-language\nplanning agents [15, 16, 50, 51]. However, existing litera-\nture highlights the limitations of LLMs in spatial reasoning\n[2, 26, 74] as well as long-horizon planning [3, 67, 68]. This\npushed researchers to explore alternative ways to incorpo-\nrate VLMs and LLMs into planning frameworks more reli-\nably [27, 58]. One potential direction is their use as plan\nevaluators, either through the generation of reward func-\ntions [23, 33, 73], or by using them directly as off-the-shelf\ncritics [5, 21, 83]. The success of such frameworks assumes\nperfect perception and that the models can accurately per-\nceive visual information and reason about it in order to pro-\nduce an answer; nevertheless, it has been shown that the\nrepresentations used by these models fail in highly intricate\nvisual settings [64]. Several works have explored the use\nof VLMs as well as LLMs for path and motion planning\n[2, 3, 7, 9, 11, 14, 44, 75], however, to the knowledge of the\nauthors there is no work that explores the use of VLMs as\npath critics in this context. Accordingly, we aim to evaluate\nthe ability of VLMs to serve as evaluators in cases requir-\ning navigation in complex environments while adhering to\ndecision-making constraints speci\ufb01ed in natural language.\n2.3. Vision-Language Model Benchmarks\nThe introduction of multimodal models has prompted the\ndevelopment of several benchmarks that are capable of as-\nsessing the performance of these models on visual rea-\nsoning tasks such as visual question-answering datasets\n[4, 18, 19, 24, 42, 43, 84]. However, the rise of founda-\ntion models has produced the need for a more holistic eval-\nuation of the perceptual and reasoning capabilities of large\n6888\n\n=== Page 4 ===\nVLMs, leading to benchmarks such as MM-Vet [77], MM-\nBench [40] , MMMU [45, 78, 79, 81] and OmniBench [35].\nSeveral benchmarks have also speci\ufb01cally been designed to\nassess the perception capabilities of these models and ex-\nplore the limitations associated with visual hallucinations\nand optical illusions [10, 22, 34, 60, 64]. Our proposed\nbenchmark provides a \ufb02exible yet challenging framework\nfor interleavedly assessing the low-level perception and rea-\nsoning capabilities of VLMs.\n3. The PATHEVAL Benchmark\nMotivated by the need to evaluate VLMs as path evalua-\ntors in real-world planning scenarios, we introduce PATHE-\nVAL, a controllable and extensible benchmark focused on\npath planning [28\u201330, 41] in complex environments under\na diverse set of decision-making constraints. We list the\ndecision-making scenarios as well as the descriptors they\nattempt to optimize in Appendix A. In total, PATHEVAL\nincludes 14,550 tasks over more than 1,150 distinct envi-\nronments and 15 distinct scenarios. Below, we introduce a\nformal description of the task and the dataset construction.\n3.1. Task Formulation\nGiven two paths, P1 and P2, and a scenario S, the objective\nis to determine which path better satis\ufb01es the scenario\u2019s op-\ntimization criteria. Each scenario S is a high-level descrip-\ntion that aims to optimize over a set of path descriptors (or\nmetrics) M = {m1, m2, . . . , mk}, where each descriptor\nmj : P \u2192R evaluates a speci\ufb01c property of a path (e.g.,\nlength, smoothness, or proximity to obstacles). A VLM V\nis presented with two images presenting P1 and P2 in the\nsame environment, respectively. The model must then de-\ncide which path better satis\ufb01es the scenario\u2019s criteria. To\nexplore the sensitivity of VLMs to the way how a path is\npresented, PATHEVAL includes both the 2D and 3D images\nof the path illustration. The model is also prompted to gen-\nerate an explanation to justify its choice.\n3.2. Environment and Path Generation\nEnvironment Generation:\nAn environment, as shown\nin Figure 1,\nis de\ufb01ned by a set of walls O\n=\n{O1, O2, . . . , On}, where each wall Oi represents an ob-\nstacle in the 2D space. Each wall is a closed geometric\nshape described by its vertices, and the set O forms the ob-\nstacles that the path must avoid. In this work, we leverage\nthe environments of Plaku et. al. [53], which consists of\nfour types of obstacle arrangements: (1) rings, where the\nenvironments are structured as mazes with circular walls,\n(2) waves, which consist of wavy horizontal obstacles, (3)\nmazes, which consist of both vertical and horizontal walls\nforming a complex maze structure, as well as (4) random,\nwhich consist of randomly placed obstacles.\nPath Synthesis via the Randomly-exploring Rapid\nTree (RRT) algorithm:\nTo generate path candidates in\nPATHEVAL, we leverage the RRT path planning algo-\nrithm [30]. Starting from the initial location in the envi-\nronment, the algorithm works by building a tree which ex-\npands itself by randomly selecting the next location in the\nenvironment while avoiding obstacles, until it reaches the\ngoal. In this work, we use the Open Motion Planning Li-\nbrary (OMPL) [59] and implement the RRT-Connect algo-\nrithm [29]. We note that while we use RRT in the current\nbenchmark, our codebase is adaptable and can incorporate\nmost path-planning algorithms provided by OMPL. We en-\ncourage future research building on our benchmark to ex-\nperiment with other algorithms as well.\n3.3. Path Descriptors\nWe collect the following descriptors M for each of a gen-\nerated path: Minimum Clearance measures the smallest\ndistance between any point on the path and the nearest ob-\nstacle; Maximum Clearance measures the largest distance\nbetween any point on the path and the nearest obstacle; Av-\nerage Clearance computes the average distance between all\npoints on the path and the nearest obstacle; Path Length is\ncalculated by summing up the Euclidean distances between\nconsecutive points on the path; Smoothness is de\ufb01ned as\nthe sum of the angles between consecutive segments of the\npath, measuring how smoothly the path changes direction;\nNumber of Sharp Turns counts the number of turns in\nthe path where the angle between consecutive segments ex-\nceeds 90 degrees; and Maximum Angle denotes the largest\nangle between any two consecutive segments of the path.\nThe three Clearance metrics and Path Length share the same\nmeasuring unit, i.e., one grid size; Smoothness and Maxi-\nmum Angle are measured by degree; and the Number of\nSharp Turns is an integer count. We include the formula of\neach descriptor in Appendix B.\n3.4. Natural Language Descriptions of Scenarios\nTo create a suf\ufb01ciently challenging path-planning evalua-\ntion benchmark, we design a total of 15 decision-making\nscenarios that aim to optimize different combinations of the\npath descriptors. For instance, Scenario #2 (as shown in\nFigure 1) requires searching through an area affected with\n\ufb01re in search for survivors, and the agent thus must cover as\nmuch ground as possible. In contrast, Scenario #3 indicates\nthat the path is to be executed within a warzone; as a result,\nthe vehicle has to remain hidden and take the shortest route.\nAs such, given the same set of paths, the one minimizing\nthe path length is favored by Scenario #3 while Scenario #2\nneeds to maximize this value. A complete summary of the\n15 scenarios, along with the descriptors each scenario aims\nto optimize, is presented in Tables 6-7 in Appendix A.\n6889\n\n=== Page 5 ===\n3.5. Task Pairs Selection\nFor each environment we synthesized, we ran the RRT plan-\nner 30 times to generate different paths. Upon eliminating\npaths that did not reach the goal, we selected path pairs that\nexhibited the greatest dissimilarity in terms of path descrip-\ntors. Speci\ufb01cally, we \ufb01rst represented each path using a (7-\ndimension) vector of its path descriptor values. Given that\neach path descriptor ranged in a dramatically different scale,\nwe normalized the vector by performing the min-max scal-\ning, i.e., scaling each value x in the vector to x\u2192=\nx\u2191xmin\nxmax\u2191xmin ,\nwhere x represents each value in the vector, and xmin and\nxmax are the minimum and maximum values of each de-\nscriptor across paths sampled from the same environment,\nrespectively. We then measured the distance between two\npaths by calculating the Euclidean distance between their\nnormalized path descriptor vectors, and selected 5 path pairs\nwith the largest distances. Each path is included in only one\npair to avoid redundancy. We repeated the same selection\nprocedure for a total of 1,150 environments.\nUpon performing this generation, we matched each pair\nwith all \ufb01fteen scenarios; however, we only kept pairs where\nthere was a signi\ufb01cant difference in at least one of the\ndescriptors required for the particular scenario.\nIn other\nwords, we ensure that the absolute difference is greater than\na prede\ufb01ned threshold (0.8 for clearance descriptors, 50 for\npath length, 90 for smoothness, 1 for the number of sharp\nturns, and 30 for maximum angle) for at least one of the\nrequired descriptors. This makes it more likely that the dif-\nference is more noticeable to the naked eye, and thus the\npaths can be compared by visual inspection.\nWe constructed our \ufb01nal evaluation benchmark by ran-\ndomly selecting 70 task pairs from each scenario, resulting\nin 1,050 pairs in total. The remaining task pairs (a total\nof 13,500) are used as the training set to facilitate the \ufb01ne-\ntuning experiments in Section 5.\n4. Can SOTA VLMs Evaluate Planned Paths?\nIn order for VLMs to perform successfully in our bench-\nmark, they need to perform successfully at three differ-\nent levels, i.e, recognizing the critical descriptors required\nby each scenario (Attribute Abstraction), exhibiting sharp\nlow-level perception to precisely assess each path\u2019s proper-\nties (Low-level Perception), and integrating the prior in-\nformation to make a rational decision on the better path\n(Information Integration). Among them, the \ufb01rst two lev-\nels re\ufb02ect parallel properties that serve as a foundation for\nthe third level. In this section, we evaluated a set of 9 VLMs\non PATHEVAL in a zero-shot manner and analyzed their ca-\npabilities at these three levels. These VLMs include (1) two\nclosed-source VLMs, i.e., GPT-4o and GPT-4o-mini [47],\nand (2) seven open-source VLMs with various sizes, in-\ncluding LLaVA-NeXT-7b and LLaVA-NeXT-13b [38, 39],\nLLaVA-OneVision-7b [32], Qwen2-VL-7b [69], LLaMA-\n3.2-11b [17], and Intern-VL2-8b and Intern-VL2-40b [12].\nWe include all prompt scripts used in this Section in Ap-\npendix C.1.\n4.1. Overview of VLMs on PATHEVAL\nThe performance of the 9 VLMs on PATHEVAL is show-\ncased in Table 1.\nWe notice that all of the models, ex-\ncept GPT-4o, fail to perform signi\ufb01cantly better than a sim-\nple random baseline, indicating signi\ufb01cant limitations. For\nGPT-4o, we also notice a 4% higher accuracy on 2D im-\nages compared to prompting with 3D inputs. This obser-\nvation indicates that the model is prone to visual illusions\nintroduced by the 3D images, when it has to rely on solely\nthe image for decision-making (although we observe an op-\nposite effect of 2D vs. 3D when verbalized descriptor val-\nues are provided). In the remaining section, we will further\nbreak down these models\u2019 capabilities to gain a deeper un-\nderstanding of their failure on this task.\nProviding verbalized path information yields better task\naccuracy, implying reasonable VLM performance in at-\ntribute abstraction. In Table 1, we further show the per-\nformance of each model when we explicitly list the value\nfor each descriptor as part of the language prompt (i.e.,\n\u201cPATHEVAL w Desc.\u201d). We notice a 11.1%-27.1% improve-\nment across most models, indicating that when given low-\nlevel details, the models can better \ufb01lter out the information\nand make better comparisons. This points out that the bot-\ntleneck for these VLMs\u2019s better performance lies in their\ninability to accurately perceive low-level information about\nthe paths (we discuss this in more detail in Section 4.2),\nwhereas these models generally have a reasonable capabil-\nity in abstracting the critical attributes for decision making\nin various scenarios. In Appendix D.1, we include an ex-\nperiment where we explicitly query each VLM to identify\nthe critical path metrics for each scenario; the result corrob-\norates our hypothesis. In particular, we \ufb01nd that for most\nmodels, the success rate for identifying a required descrip-\ntor is over 92%. Finally, a surprising observation happens\nto LLaVa-NeXT-7b. We notice that this model suffers par-\nticularly severely from hallucination; even when the textual\ndescriptor values are provided and when the model can cor-\nrectly identify critical path metrics based on our analysis in\nAppendix D.1, it fails to pick the better paths. We show\nan example of its explanation when textual descriptors are\nprovided in Figure 5 of Appendix E.\nVLMs exhibit mixed performance in integrating vi-\nsual and textual path information.\nWe take a further\nlook into the model performance when both the image\nand the textual descriptor values are provided, and con-\ntrast it with their performance when only the textual path\ndescriptions are provided (i.e., \u201cDesc Only\u201d).\nInterest-\n6890\n\n=== Page 6 ===\nModel\n2D\n3D\nDesc Only\nPATHEVAL\nPATHEVAL w Desc.\nPATHEVAL\nPATHEVAL w Desc.\nGPT-4o-mini\n0.520\n0.750\n0.508\n0.745\n0.680\nGPT-4o\n0.665\n0.860\n0.624\n0.895\n0.894\nLLaVa-NeXT-7b\n0.501\n0.524\n0.499\n0.517\n0.514\nQwen2-VL-7b\n0.502\n0.731\n0.511\n0.742\n0.737\nLLaVA-OneVision-7b\n0.505\n0.718\n0.509\n0.739\n0.721\nIntern-VL2-8b\n0.489\n0.654\n0.505\n0.691\n0.648\nLLaMa-3.2-11b\n0.480\n0.695\n0.460\n0.680\n0.686\nLLaVa-NeXT-13b\n0.509\n0.620\n0.494\n0.601\n0.630\nIntern-VL2-40b\n0.506\n0.688\n0.496\n0.717\n0.679\nRandom Baseline\n0.500\n0.500\n0.500\n0.500\n0.500\nTable 1. Accuracy of VLMs on PATHEVAL based on 2D and 3D environment images. To investigate the potential vision bottleneck of\nVLMs, we additionally present each model\u2019s accuracy when explicit descriptor values are provided in the language prompts (\u201cPATHEVAL\nw Desc.\u201d). The last column (\u201cDesc Only\u201d) shows the model performance when only the textual descriptor values are provided.\ningly, we observe mixed information integration perfor-\nmance from these models.\nFor GPT-4o, Qwen2-VL-7b,\nLLaVA-OneVision-7b, LLaMa-3.2-11b, and LLaVa-NeXT-\n13b, their performance based on only descriptor values has\nno obvious difference to their best performance when (2D\nor 3D) images are also provided. This observation implies\nthat these models do not bene\ufb01t from the additional image\ninformation when the textual path descriptions are provided.\nInstead, sometimes the images (e.g., 2D images for GPT-4o\nand 3D images for LLaVa-NeXT-13b) may confuse their\nunderstanding of the textual descriptors, resulting in a worse\naccuracy compared to Desc Only. For GPT-4o-mini, Intern-\nVL2-8b, and Intern-VL2-40b, however, providing both the\nvisual and textual path information offers \u21914%-7% perfor-\nmance gain over Desc Only, indicating better information\nintegration from these VLMs.\nIn Appendix D.2, we present a breakdown of GPT-\n4o\u2019s performance by scenarios. We show that performance\nvaries greatly from one scenario to the other.\nInterest-\ningly, we observe that GPT-4o may overuse their common-\nsense knowledge. For instance, in the case of Scenario #2\nshown in Figure 1, where the agent is required to maxi-\nmize the path length for better coverage, GPT-4o still fa-\nvors the shorter path. This scenario represents a counter-\nfactual situation as models are often trained or instructed\nto seek the shortest paths. Evaluating VLMs in such coun-\nterfactual scenarios allows us to effectively probe their task\nunderstanding and reasoning, which we consider to be an\nimportant direction for future research.\nThe reasoning of VLMs can be unreliable.\nLimited\nby the vision bottleneck, we noticed these VLMs fabricat-\ning seemingly plausible explanations to justify their path\nevaluation, despite the fact that they could not actually per-\nceive the necessary path details to perform the task. This\nfabrication echos \ufb01ndings from other recent work, where\nLLMs were shown to produce reasoning traces that do not\nModel\nDefault\nFlipped\nRandom IDs\nGPT-4o-mini\n34/1016\n22/1026\n94/956\nGPT-4o\n278/772\n258/792\n291/759\nLLaVa-NeXT-7b\n1028/22\n1041/9\n580/470\nQwen2-VL-7b\n21/1029\n130/920\n127/923\nLLaVA-OneVision-7b\n438/612\n440/610\n262/788\nIntern-VL2-8b\n433/610\n430/620\n418/632\nLLaMA-3.2-11b*\n601/251\n620/264\n731/319\nLLaVa-NeXT-13b\n795/255\n939/111\n305/745\nIntern-VL2-40b\n394/656\n410/639\n510/540\nGround truth labels\n530/520\nTable 2. Performance on the 2D case (#of times \ufb01rst path is chosen\n/ #of times second path is chosen) when we \ufb02ip the path order or\nreplace their default names with random IDs. *There are several\ncases where LLaMA-3.2-11b does not follow the required format and/or\ndoes not give an answer, we omit those cases from this table.\naccurately re\ufb02ect the process of how the models reach an\nanswer [1, 66].\nTo gain further insights, we performed\nan analysis comparing model performance on PATHEVAL\nwith cases that consist of: 1) \ufb02ipping the order of paths\nin the pair, and 2) assigning random IDs to the paths\n(e.g. instead of referring to them as \u201cPath 1\u201d and \u201cPath\n2\u201d, we use a random sequence such as \u201cPath Xu2q\u201d and\n\u201cPath fP48\u201d). The results presented in Table 2 showcase\nthat VLMs demonstrate bias for a particular label, when\nthey actually do not have the capability to resolve the task.\nFor example, when no matter the default or the \ufb02ipping la-\nbels are used, LLaVA-NeXT-7b consistently selects Path\n1 98% of the time and fabricates incorrect observations of\nthe two paths in its explanations (Figure 1) to support this\nchoice. As we discussed earlier, LLaVA-NeXT-7b is par-\nticularly prone to hallucination in explanations, leading to\nits random-guess performance with or without textual de-\nscriptors. Introducing random IDs as path names mitigates\nthis bias for LLaVa-NeXT-7b (although the model still ob-\ntains a close-to-random accuracy on PATHEVAL) but does\nnot seem to help other models dramatically.\n6891\n\n=== Page 7 ===\nDescriptor\nTest Set\n\u03c91\n\u03c92\n\u03c93\nMin. Clearance\n0.46/0.46\n0.50/0.46\n0.74/0.70\n0.86/0.74\nMax. Clearance\n0.44/0.46\n0.41/0.49\n0.46/0.55\n0.50/0.60\nAvg. Clearance\n0.53/0.55\n0.50/0.52\n0.70/0.57\n0.73/0.60\nPath Length\n0.58/0.70\n0.86/0.91\n0.92/0.86\n0.94/0.94\nSmoothness\n0.74/0.72\n0.86/0.82\n0.90/0.90\n0.90/0.89\n# of Sharp Turns\n0.76/0.73\n0.80/0.84\n0.77/0.70\n0.60/0.55\nMax. Angle\n0.71/0.70\n0.82/0.84\n0.86/0.88\n0.94/0.96\nTable 3.\nGPT-4o\u2019s \ufb01ne-grained perception accuracy (2D/3D)\non the test set of PATHEVAL and three additionally synthesized\ndatasets with increasing metric differences.\n4.2. What Can Vision-Language Models See?\nThe previous subsection clearly highlights the vision com-\nponent as the bottleneck for path evaluation on PATHEVAL.\nIn this subsection, we conduct an analysis of the visual per-\nception capabilities of VLMs. We focus our analysis on\nGPT-4o since it is the only model that performs substan-\ntially better than the random baseline in the case where no\ndescriptors are required.\nGPT-4o can perceive paths that are signi\ufb01cantly differ-\nent. In order to get a better understanding of the model\u2019s\nperception capability, we break down its performance in\nterms of perception by individual metric. Speci\ufb01cally, we\nprompt GPT-4o to select which path in the pair provides\na lower value on each individual metric and report its ac-\ncuracy. We perform this analysis on both the task pairs in\nthe test set of PATHEVAL and an additionally synthesized\ndataset consisting of task pairs with three levels (denoted\nas \u03c91, \u03c92, and \u03c93) of increasing differences in their descrip-\ntor values. We describe the data generation process for this\ndataset in more detail in Appendix C.2.\nThe results for both experiments are shown in Table 3.\nUpon evaluating the performance on the test set of PATHE-\nVAL by individual metrics, we notice that GPT-4o particu-\nlarly struggles with Clearance metrics. These metrics typ-\nically require a lower level of perception and is naturally\nmore challenging to discriminate than other metrics. On the\nother hand, Smoothness, Number of Sharpest Turns, and\nMax Angle appear to be easier for the model to capture.\nWe also note that GPT-4o perceives the path length much\nmore easily in a 3D environment presentation.\nFurther-\nmore, GPT-4o\u2019s performance increases as we increase the\ndescriptor difference between paths. This, however, is not\nthe case for the Number of Sharp Turns. We conjecture that\nwhen we increase the number of sharp turns, we also en-\nforce the challenge of \u201ccounting\u201d the number of satisfying\nturns, which VLMs have been shown with limitations [46].\nDoes segment complexity affect performance? We look\ninto whether the complexity of the path segment is the key\nreason for GPT-4o\u2019s limited perception of clearance. To\nthis end, we test GPT-4o on segments of varying complex-\nity (i.e., points, straight lines, and curves), in a simpli\ufb01ed\n(a) Point\n(b) Straight line\n(c) Curve\n(d) GPT-4o performance on the distance to obstacle under different seg-\nment complexities.\nFigure 2. Example segment complexity test cases in simpli\ufb01ed\nenvironments and performance across the various settings.\nenvironment with only one rectangular obstacle at the cen-\nter (Figure 2), and evaluate its accuracy in identifying seg-\nments that are closer to the obstacle. For individual points,\nthe clearance is de\ufb01ned by the perpendicular distance from\nthe point to the obstacle; for straight lines and curves, we\nconsider a path closer if one of its endpoints is closer to the\nobstacle. For each segment type, we synthesize 100 pairs\nby \ufb01rst randomly generating 200 segments and then pairing\neach with the segment with the greatest distance difference\nfrom the obstacle relative to it (i.e., maximizing the absolute\ndifference between the distances of the two segments from\nthe obstacle). This increases the likelihood that the distance\nis signi\ufb01cant enough to be perceivable. The average differ-\nence in the clearances of the pairs of segments are 14.76,\n14.42, and 14.28 for points, lines, and curves respectively.\nThe results in Figure 2d show that GPT-4o can perform\nbetter in very easy scenarios; however, it struggles more as\nthe segment complexity increases. For instance, the model\nwas able to identify the closer points in 77% of the cases;\nhowever, when considering straight and curved lines, its\nperformance drops to 64% and 59%, respectively. A sur-\nprising observation is that, in the case of curved lines, GPT-\n4o\u2019s performance is dramatically better in 3D images. As\nshown in Appendix C.2, the average Clearance difference of\npath pairs in PATHEVAL is merely 0.12 \u2013 1.31. As the paths\nin PATHEVAL are much more complicated than the curves\nin this experiment, it is expected that GPT-4o exhibits dif-\n\ufb01culty in judging paths\u2019 clearances. The complexity of the\nenvironments (compared to a single square obstacle) could\nadd challenges.\n5. Fine-tuning a Path Evaluator\nOne intuitive question is whether simply \ufb01ne-tuning the\nVLMs can relieve their vision bottleneck. To answer this\n6892\n\n=== Page 8 ===\nModel\nFrozen\nFine-tuned\nAccuracy\nAvg. Cosine Similarity\nAccuracy\nAvg. Cosine Similarity\nclip-vit-base-patch32\n0.510\n0.914\n0.783\n0.514\nclip-vit-large-patch14-336\n0.498\n0.907\n0.749\n0.548\nOpenCLIP-vit-B-32\n0.540\n0.883\n0.743\n0.475\nsiglip-base-patch16-224\n0.529\n0.895\n0.731\n0.612\ndino-vit-16\n0.495\n0.911\n0.763\n0.754\ndino-v2-base\n0.510\n0.761\n0.721\n0.681\nTable 4. Probing accuracy and average cosine similarity between distinct path images (2D) when the vision encoder is frozen or \ufb01ne-tuned.\nSetting\nFrozen\nTunable\nImage Only\n0.52\n0.51\nImage w Descriptors\n0.96\n-\nImage Only (Random IDs)\n0.48\n0.52\nTable 5. Fine-tuned LLaVA-NeXT-7b performance (2D) when we\nkeep the vision encoder frozen or tunable.\nquestion, we experiment with LLaVA-NeXT-7b and \ufb01ne-\ntune it on the training set (13,500 pairs) of PATHEVAL. We\nfocus on the 2D case for the set of experiments described in\nthis section. We consider three separate settings for train-\ning: (1) training with only images as input, (2) training with\nimages and textual descriptor values as input, and (3) the\nsame setting as (1) with using random IDs as target labels.\nDetails in experimental setup are included in Appendix C.3.\n5.1. Overall Performance\nFine-tuning does not help with vision-language map-\nping. Table 5 shows that the model fails to learn meaningful\npatterns in the data, even after training 50 epochs. How-\never, when the textual descriptors are provided as input, the\nmodel can easily learn the function achieving 96% accu-\nracy (a 45% improvement from the zero-shot setting). This\nshows that the model is unable to extract the same descrip-\ntor values from the image input. Unfreezing the encoder for\n\ufb01ne-tuning also does not provide any signi\ufb01cant improve-\nment. We include a further discussion in Appendix D.3.\nThe results point to a limitation in the vision model\u2019s ability\nto encode the images, which we will investigate next.\n5.2. Understanding the Visual Representations\nIn order to further understand the limitations of the vision\ncomponent, we conduct an analysis to better understand\nhow well different vision encoder models can differentiate\nbetween different paths in the dataset. To this end, we \ufb01rst\napply a linear probe to see how easily distinguishable differ-\nent images are. Speci\ufb01cally, given a pair of images, we \ufb01rst\nuse the vision encoder to extract high-dimensional feature\nrepresentations for both images. These features are then\nconcatenated and passed through a simple binary classi\ufb01ca-\ntion layer (i.e., the probe). The probe is trained to predict a\nlabel of 1 if the images are the same and 0 otherwise. We ex-\nperiment with various SOTA vision encoders, namely CLIP\n[55] base and large varieties, LAION-OpenCLIP [25, 56],\nSigLip [80], DINO [8], and DINO-v2 [49] and analyze how\nwell their learned representations capture visual similarities\nand differences. We use a set of 1,000 randomly sampled\npath pairs with a balanced label distribution to train the\nprobe, and look at whether the model can learn to distin-\nguish between these paths. We also perform an experiment\nwhere we \ufb01ne-tune the vision encoder along with the probe.\nIn this setting, our goal is to gauge if carefully \ufb01ne-tuning a\nvision encoder can potentially improve the model\u2019s perfor-\nmance in low-level perception. Finally, in both settings, we\npresent the average cosine similarity between distinct paths.\nVision encoders cannot distinguish between paths. From\nTable 4, it can be seen that vision encoder models are unable\nto provide representations that are signi\ufb01cantly different for\nthe probe to tell if they are the same. This is further sup-\nported by the high values for the average cosine similarity\nacross all models.\nFine-tuning the encoders on a discrimination task can\nhelp disentangle the visual representations.\nBy un-\nfreezing the encoder weights and training them to identify\nwhether two paths are identical, we enhance their adapt-\nability to the task. The results in Table 4 demonstrate that\nthis approach effectively disentangles the learned represen-\ntations, resulting in signi\ufb01cantly improved performance and\nincreased separability, as evidenced by the notably lower\ncosine similarity between non-identical paths. The results\nthus imply the need for carefully \ufb01ne-tuning task-speci\ufb01c\nvision encoders for path evaluation on PATHEVAL.\n6. Conclusion\nThis work explored the use of VLMs as evaluators in path-\nplanning scenarios. We evaluated a number of VLMs on\nour proposed benchmark, PATHEVAL, and found that these\nmodels struggle with low-level perception. Speci\ufb01cally, we\n\ufb01nd that visual encoders used by SOTA models are unable\nto discern the differences between different paths in intri-\ncate scenarios. We hope that PATHEVAL will inspire re-\nsearchers to further explore ways to improve the visual ca-\npabilities of VLMs and contribute to \ufb01nding better ways to\nincorporate foundation models for developing more \ufb02exi-\nble, robust, and scalable planning paradigms.\n6893\n\n=== Page 9 ===\nAcknowledgments\nThis project was supported by resources provided by the\nOf\ufb01ce of Research Computing at George Mason University\n(URL: https://orc.gmu.edu) and funded in part by grants\nfrom the National Science Foundation (Award Number\n2018631).\nReferences\n[1] Chirag Agarwal, Sree Harsha Tanneru, and Himabindu\nLakkaraju.\nFaithfulness\nvs.\nPlausibility:\nOn\nthe\n(Un)Reliability of Explanations from Large Language Mod-\nels, 2024. 6\n[2] Mohamed Aghzal, Erion Plaku, and Ziyu Yao. Can Large\nLanguage Models be Good Path Planners? A Benchmark\nand Investigation on Spatial-temporal Reasoning. In ICLR\n2024 Workshop on Large Language Model (LLM) Agents,\n2024. 1, 3\n[3] Mohamed Aghzal, Erion Plaku, and Ziyu Yao. Look Further\nAhead: Testing the Limits of GPT-4 in Path Planning. IEEE\nCASE, 2024. 1, 3\n[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh. VQA: Visual Question Answering. In International\nConference on Computer Vision (ICCV), 2015. 3\n[5] Kate Baumli, Satinder Baveja, Feryal Behbahani, Harris\nChan, Gheorghe Comanici, Sebastian Flennerhag, Maxime\nGazeau, Kristian Holsheimer, Dan Horgan, Michael Laskin,\nClare Lyle, Hussain Masoom, Kay McKinney, Volodymyr\nMnih, Alexander Neitz, Dmitry Nikulin, Fabio Pardo, Jack\nParker-Holder, John Quan, Tim Rockt\u00a8aschel, Himanshu\nSahni, Tom Schaul, Yannick Schroecker, Stephen Spencer,\nRichie Steigerwald, Luyu Wang, and Lei Zhang.\nVision-\nLanguage Models as a Source of Rewards, 2024. 1, 3\n[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners, 2020. 1, 3\n[7] Xu Cao, Tong Zhou, Yunsheng Ma, Wenqian Ye, Can\nCui, Kun Tang, Zhipeng Cao, Kaizhao Liang, Ziran Wang,\nJames M. Rehg, and Chao Zheng. MAPLM: A Real-World\nLarge-Scale Vision-Language Benchmark for Map and Traf-\n\ufb01c Scene Understanding. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 21819\u201321830, 2024. 3\n[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the International Conference on Computer Vi-\nsion (ICCV), 2021. 1, 3, 8\n[9] Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xi-\naodan Liang, and Kwan-Yee Wong.\nMapGPT: Map-\nguided prompting with adaptive path planning for vision-\nand-language navigation. In Proceedings of the 62nd An-\nnual Meeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 9796\u20139810, Bangkok,\nThailand, 2024. Association for Computational Linguistics.\n3\n[10] Xuweiyi Chen, Ziqiao Ma, Xuejun Zhang, Sihan Xu,\nShengyi Qian, Jianing Yang, David Fouhey, and Joyce Chai.\nMulti-object hallucination in vision language models.\nIn\nThe Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024. 4\n[11] Yongchao Chen, Jacob Arkin, Charles Dawson, Yang Zhang,\nNicholas Roy, and Chuchu Fan. Autotamp: Autoregressive\ntask and motion planning with llms as translators and check-\ners. In 2024 IEEE International Conference on Robotics and\nAutomation (ICRA), pages 6695\u20136702, 2024. 1, 3\n[12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,\nSen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\nLewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng\nDai. InternVL: Scaling up Vision Foundation Models and\nAligning for Generic Visual-Linguistic Tasks. arXiv preprint\narXiv:2312.14238, 2023. 3, 5\n[13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing.\nVicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality, 2023. 4\n[14] Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang.\nTask and motion planning with large language models for ob-\nject rearrangement. In 2023 IEEE/RSJ International Confer-\nence on Intelligent Robots and Systems (IROS), pages 2086\u2013\n2092. IEEE, 2023. 3\n[15] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\nMarc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,\nand Pete Florence. Palm-e: an embodied multimodal lan-\nguage model. In Proceedings of the 40th International Con-\nference on Machine Learning. JMLR.org, 2023. 3\n[16] Yilun Du, Sherry Yang, Pete Florence, Fei Xia, Ayzaan\nWahid, brian ichter, Pierre Sermanet, Tianhe Yu, Pieter\nAbbeel, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Andy\nZeng, and Jonathan Tompson. Video language planning. In\nThe Twelfth International Conference on Learning Represen-\ntations, 2024. 3\n[17] Abhimanyu Dubey et. al.\nThe Llama 3 Herd of Models,\n2024. 3, 5\n[18] Difei Gao, Ruiping Wang, Shiguang Shan, and Xilin Chen.\nCric: A vqa dataset for compositional reasoning on vision\nand commonsense. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 45(5):5561\u20135578, 2022. 3\n[19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\n6894\n\n=== Page 10 ===\nthe role of image understanding in visual question answer-\ning. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2017. 3\n[20] Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, and Xin\nWang. Vision-and-language navigation: A survey of tasks,\nmethods, and future directions. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers). Association for Computa-\ntional Linguistics, 2022. 3\n[21] Lin Guan, Yifan Zhou, Denis Liu, Yantian Zha, Heni Ben\nAmor, and Subbarao Kambhampati.\nTask success is not\nenough: Investigating the use of video-language models as\nbehavior critics for catching undesirable agent behaviors. In\nFirst Conference on Language Modeling, 2024. 1, 3\n[22] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia\nLi, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang,\nYaser Yacoob, Dinesh Manocha, and Tianyi Zhou.\nHal-\nlusionbench: An advanced diagnostic suite for entangled\nlanguage hallucination & visual illusion in large vision-\nlanguage models, 2023. 4\n[23] Xu Han, Qiannan Yang, Xianda Chen, Xiaowen Chu, and\nMeixin Zhu. Generating and evolving reward functions for\nhighway driving with large language models, 2024. 3\n[24] Drew A. Hudson and Christopher D. Manning. GQA: A New\nDataset for Real-World Visual Reasoning and Compositional\nQuestion Answering.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2019. 3\n[25] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, 2021. If you use this software, please cite it as below.\n3, 8\n[26] Amita Kamath, Jack Hessel, and Kai-Wei Chang. What\u2019s\n\u201dup\u201d with vision-language models? Investigating their strug-\ngle with spatial reasoning. In The 2023 Conference on Em-\npirical Methods in Natural Language Processing, 2023. 1,\n3\n[27] Subbarao Kambhampati, Karthik Valmeekam, Lin Guan,\nMudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Paul\nSaldyt, and Anil B Murthy. Position: LLMs Can\u2019t Plan, But\nCan Help Planning in LLM-Modulo Frameworks. In Forty-\n\ufb01rst International Conference on Machine Learning, 2024.\n1, 3\n[28] L.E. Kavraki, P. Svestka, J.-C. Latombe, and M.H. Over-\nmars.\nProbabilistic roadmaps for path planning in high-\ndimensional con\ufb01guration spaces.\nIEEE Transactions on\nRobotics and Automation, 12(4):566\u2013580, 1996. 4\n[29] J.J. Kuffner and S.M. LaValle. RRT-connect: An ef\ufb01cient ap-\nproach to single-query path planning. In Proceedings 2000\nICRA. Millennium Conference. IEEE International Confer-\nence on Robotics and Automation. Symposia Proceedings\n(Cat. No.00CH37065), pages 995\u20131001 vol.2, 2000. 4\n[30] Steven M. LaValle. Rapidly-exploring random trees : a new\ntool for path planning. The annual research report, 1998. 4\n[31] Jan\nLeike.\nWhy\ni\u2019m\noptimistic\nabout\nour\nalign-\nment approach:\nEvaluation is easier than generation.\nhttps://aligned.substack.com/i/88447351/\nevaluation - is - easier - than - generation,\n2022. Accessed: 2024-11. 1\n[32] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng\nLi, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and\nChunyuan Li. LLaVA-OneVision: Easy Visual Task Trans-\nfer. arXiv preprint arXiv:2408.03326, 2024. 3, 5\n[33] Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu\nQiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, and Jifeng\nDai. Auto mc-reward: Automated dense reward design with\nlarge language models for minecraft. In IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 2024. 3\n[34] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucination in\nlarge vision-language models. In The 2023 Conference on\nEmpirical Methods in Natural Language Processing, 2023.\n4\n[35] Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu,\nHangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Wang,\nJian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue\nZhang, Zhenzhu Yang, Xiangzhou Wang, Zhaoxiang Zhang,\nZachary Liu, Emmanouil Benetos, Wenhao Huang, and\nChenghua Lin. OmniBench: Towards The Future of Uni-\nversal Omni-Language Models, 2024. 4\n[36] Hui Sheng Lim, Shuangshuang Fan, Christopher K.H.\nChin, Shuhong Chai, Neil Bose, and Eonjoo Kim.\nCon-\nstrained path planning of autonomous underwater vehicle us-\ning selectively-hybridized particle swarm optimization algo-\nrithms. IFAC-PapersOnLine, 52(21):315\u2013322, 2019. 12th\nIFAC Conference on Control Applications in Marine Sys-\ntems, Robotics, and Vehicles CAMS 2019. 2\n[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In Advances in Neural Information\nProcessing Systems, pages 34892\u201334916. Curran Associates,\nInc., 2023. 3\n[38] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved Baselines with Visual Instruction Tuning, 2024. 3,\n5\n[39] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Im-\nproved reasoning, OCR, and world knowledge, 2024. 3, 5\n[40] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your\nmulti-modal model an all-around player? In Computer Vi-\nsion \u2013 ECCV 2024, pages 216\u2013233, Cham, 2025. Springer\nNature Switzerland. 4\n[41] Tom\u00b4as Lozano-P\u00b4erez and Michael A. Wesley. An algorithm\nfor planning collision-free paths among polyhedral obsta-\ncles. Communications of the ACM, 22(10):560\u2013570, 1979.\n4\n[42] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li,\nHannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel\nGalley, and Jianfeng Gao.\nMathvista: Evaluating mathe-\nmatical reasoning of foundation models in visual contexts.\nIn The Twelfth International Conference on Learning Repre-\nsentations, 2024. 3\n6895\n\n=== Page 11 ===\n[43] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019. 3\n[44] Silin Meng, Yiwei Wang, Cheng-Fu Yang, Nanyun Peng,\nand Kai-Wei Chang. LLM-a*: Large language model en-\nhanced incremental heuristic search on path planning.\nIn\nFindings of the Association for Computational Linguistics:\nEMNLP 2024, pages 1087\u20131102, Miami, Florida, USA,\n2024. Association for Computational Linguistics. 3\n[45] Shota Onohara, Atsuyuki Miyai, Yuki Imajuku, Kazuki\nEgashira, Jeonghun Baek, Xiang Yue, Graham Neubig, and\nKiyoharu Aizawa.\nJmmmu: A japanese massive multi-\ndiscipline multimodal understanding benchmark for culture-\naware evaluation, 2024. 4\n[46] OpenAI. GPT-4V(ision) System Card. 2023. 7\n[47] OpenAI. GPT-4o System Card, 2024. 3, 5\n[48] OpenAI. GPT-4 Technical Report, 2024. 3\n[49] Maxime Oquab, Timoth\u00b4ee Darcet, Theo Moutakanni, Huy V.\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-\nsell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-\nWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-\nlas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,\nJulien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-\njanowski. Dinov2: Learning robust visual features without\nsupervision, 2023. 3, 8\n[50] Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris,\nAude Oliva, Phillip Isola, and Yoon Kim. LangNav: Lan-\nguage as a perceptual representation for navigation.\nIn\nFindings of the Association for Computational Linguistics:\nNAACL 2024, pages 950\u2013974, Mexico City, Mexico, 2024.\nAssociation for Computational Linguistics. 1, 3\n[51] Chenbin Pan, Burhaneddin Yaman, Tommaso Nesti, Abhirup\nMallik, Alessandro G Allievi, Senem Velipasalar, and Liu\nRen.\nVLP: Vision Language Planning for Autonomous\nDriving . In 2024 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 14760\u201314769,\nLos Alamitos, CA, USA, 2024. IEEE Computer Society. 1,\n3\n[52] Clment Petres, Yan Pailhas, Pedro Patron, Yvan Petil-\nlot, Jonathan Evans, and David Lane.\nPath planning for\nautonomous underwater vehicles.\nIEEE Transactions on\nRobotics, 23(2):331\u2013341, 2007. 2\n[53] Evis Plaku, Erion Plaku, and Patricio Simari.\nClearance-\ndriven motion planning for mobile robots with differential\nconstraints. Robotica, 36(7):971\u2013993, 2018. 4\n[54] Patrick A. Plonski, Pratap Tokekar, and Volkan Isler. Energy-\nEf\ufb01cient Path Planning for Solar-Powered Mobile Robots,\npages 717\u2013731. Springer International Publishing, Heidel-\nberg, 2013. 2\n[55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pam Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. Proceedings of\nthe 38th International Conference on Machine Learning,\n2021. 3, 8\n[56] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade W Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev.\nLAION-5b: An open large-scale dataset for train-\ning next generation image-text models. In Thirty-sixth Con-\nference on Neural Information Processing Systems Datasets\nand Benchmarks Track, 2022. 3, 8\n[57] Jie-Jing Shao, Jiang-Xin Shi, Xiao-Wen Yang, Lan-Zhe Guo,\nand Yu-Feng Li. Investigating the Limitation of CLIP Mod-\nels: The Worst-Performing Categories, 2023. 3\n[58] Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B. Tenen-\nbaum, Leslie Pack Kaelbling, and Michael Katz. General-\nized planning in pddl domains with pretrained large language\nmodels, 2023. 3\n[59] Ioan A. Sucan, Mark Moll, and Lydia E. Kavraki.\nThe\nOpen Motion Planning Library. IEEE Robotics & Automa-\ntion Magazine, 19(4):72\u201382, 2012. 4\n[60] Yinan Sun, Zicheng Zhang, Haoning Wu, Xiaohong Liu,\nWeisi Lin, Guangtao Zhai, and Xiongkuo Min. Explore the\nhallucination on low-level perception for mllms, 2024. 4\n[61] Hao Tan, Licheng Yu, and Mohit Bansal. Learning to nav-\nigate unseen environments: Back translation with environ-\nmental dropout. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 2610\u20132621, Minneapolis,\nMinnesota, 2019. Association for Computational Linguis-\ntics. 3\n[62] Gemini Team. Gemini: A family of highly capable multi-\nmodal models, 2024. 3\n[63] Shengbang Tong, Erik Jones, and Jacob Steinhardt. Mass-\nproducing failures of multimodal systems with language\nmodels. In Proceedings of the 37th International Confer-\nence on Neural Information Processing Systems, Red Hook,\nNY, USA, 2024. Curran Associates Inc. 4\n[64] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann\nLeCun, and Saining Xie. Eyes Wide Shut? Exploring the\nVisual Shortcomings of Multimodal LLMs, 2024. 1, 3, 4\n[65] Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer,\nMoya Chen, Guillem Cucurull, David Esiobu, Jude Fer-\nnandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia\nGao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Vik-\ntor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-\nrenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning\nMao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiao-\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,\n6896\n\n=== Page 12 ===\nJian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov,\nand Thomas Scialom. Llama 2: Open foundation and \ufb01ne-\ntuned chat models, 2023. 3\n[66] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R.\nBowman.\nLanguage models don\u2019t always say what they\nthink: unfaithful explanations in chain-of-thought prompt-\ning.\nIn Proceedings of the 37th International Conference\non Neural Information Processing Systems, Red Hook, NY,\nUSA, 2024. Curran Associates Inc. 6\n[67] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and\nSubbarao Kambhampati. Large Language Models Still Can\u2019t\nPlan (A Benchmark for LLMs on Planning and Reasoning\nabout Change). In NeurIPS 2022 Foundation Models for De-\ncision Making Workshop, 2022. 3\n[68] Karthik Valmeekam, Kaya Stechly, and Subbarao Kamb-\nhampati. LLMs Still Can\u2019t Plan; Can LRMs? A Preliminary\nEvaluation of OpenAI\u2019s o1 on PlanBench, 2024. 3\n[69] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui\nMen, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Jun-\nyang Lin. Qwen2-VL: Enhancing Vision-Language Model\u2019s\nPerception of the World at Any Resolution. arXiv preprint\narXiv:2409.12191, 2024. 3, 5\n[70] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao,\nDinghan Shen, Yuan-Fang Wang, William Yang Wang, and\nLei Zhang.\nReinforced cross-modal matching and self-\nsupervised imitation learning for vision-language navigation.\nIn 2019 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 6622\u20136631, 2019. 3\n[71] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and\nDenny Zhou. Chain of Thought Prompting Elicits Reasoning\nin Large Language Models. In Advances in Neural Informa-\ntion Processing Systems, 2022. 1\n[72] Monika Wysocza\u00b4nska, Oriane Sim\u00b4eoni, Micha\u00a8el Ramamon-\njisoa, Andrei Bursuc, Tomasz Trzci\u00b4nski, and Patrick P\u00b4erez.\nClip-dinoiser: Teaching clip a few dino tricks for open-\nvocabulary semantic segmentation. ECCV, 2024. 3\n[73] Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu,\nQian Luo, Victor Zhong, Yanchao Yang, and Tao Yu.\nText2Reward: Reward Shaping with Language Models for\nReinforcement Learning. In The Twelfth International Con-\nference on Learning Representations, 2024. 1, 3\n[74] Yutaro Yamada, Yihan Bao, Andrew Kyle Lampinen, Jungo\nKasai, and Ilker Yildirim. Evaluating spatial understanding\nof large language models. Transactions on Machine Learn-\ning Research, 2024. 3\n[75] Zhutian Yang, Caelan Garrett, Dieter Fox, Tom\u00b4as Lozano-\nP\u00b4erez, and Leslie Pack Kaelbling. Guiding long-horizon task\nand motion planning with vision language models, 2024. 1,\n3\n[76] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models. Transactions\non Machine Learning Research, 2022. 3\n[77] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. In International conference on machine learn-\ning. PMLR, 2024. 4\n[78] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Ren-\nliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo\nLiu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMMMU: A Massive Multi-discipline Multimodal Under-\nstanding and Reasoning Benchmark for Expert AGI. In Pro-\nceedings of CVPR, 2024. 4\n[79] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang,\nKai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge\nZhang, Huan Sun, Yu Su, Wenhu Chen, and Graham\nNeubig.\nMMMU-Pro: A More Robust Multi-discipline\nMultimodal Understanding Benchmark.\narXiv preprint\narXiv:2409.02813, 2024. 4\n[80] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 11975\u201311986, 2023. 3, 8\n[81] Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu\nLuo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu\nXu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang,\nRuibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan\nTsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, and Jie\nFu. CMMMU: A Chinese Massive Multi-discipline Multi-\nmodal Understanding Benchmark, 2024. 4\n[82] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George\nKarypis, and Alex Smola. Multimodal chain-of-thought rea-\nsoning in language models, 2024. 1\n[83] Victor Zhong, Dipendra Misra, Xingdi Yuan, and Marc-\nAlexandre C\u02c6ot\u00b4e. Policy Improvement using Language Feed-\nback Models, 2024. 1, 3\n[84] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.\nVisual7W: Grounded Question Answering in Images.\nIn\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, 2016. 3\n6897\n",
      "searchable_content": "page 1 evaluating vision-language models as evaluators in path planning mohamed aghzal1 xiang yue2 erion plaku3 ziyu yao1 1george mason university 2carnegie mellon university 3national science foundation maghzal ziyuyao gmu.edu xyue2 andrew.cmu.edu eplaku nsf.gov abstract despite their promise to perform complex reasoning large language models llms have been shown to have limited effectiveness in end-to-end planning. this has in- spired an intriguing question if these models cannot plan well can they still contribute to the planning framework as a helpful plan evaluator in this work we generalize this question to consider llms augmented with visual under- standing i.e. vision-language models vlms . we intro- duce patheval a novel benchmark evaluating vlms as plan evaluators in complex path-planning scenarios. suc- ceeding in the benchmark requires a vlm to be able to ab- stract traits of optimal paths from the scenario description demonstrate precise low-level perception on each path and integrate this information to decide the better path. our analysis of state-of-the-art vlms reveals that these mod- els face signi\ufb01cant challenges on the benchmark. we ob- serve that the vlms can precisely abstract given scenar- ios to identify the desired traits and exhibit mixed perfor- mance in integrating the provided information. yet their vi- sion component presents a critical bottleneck with models struggling to perceive low-level details about a path. our experimental results show that this issue cannot be trivially addressed via end-to-end \ufb01ne-tuning rather task-speci\ufb01c discriminative adaptation of these vision encoders is needed for these vlms to become effective path evaluators.12 1. introduction the impressive capabilities of large language models llms 6 8 71 and vision-language models vlms 82 have led to an increasing interest in applying them to automated motion planning and navigation tasks 50 51 . however the inherent limitations of these models in long- the work by e. plaku is supported by while serving at the national science foundation. any opinion \ufb01ndings and conclusions or recom- mendations expressed in this material are those of the authors and do not necessarily re\ufb02ect the views of the national science foundation. 1source code https github.com mohamedaghzal patheval 2dataset https huggingface.co datasets maghzal patheval horizon planning have rendered them ineffective as end- to-end motion planners 2 3 11 75 . this has made re- searchers wonder if these models cannot be good motion planners themselves can they still support a motion plan- ning framework intuitively using these models still holds the promise of signi\ufb01cantly enhancing the motion plan- ning framework as they have learned extensive factual and commonsense knowledge that could bene\ufb01t planning dur- ing their pre-training. as a result there has been an emerg- ing paradigm exploring how these models can be leveraged in combination with traditional methods 27 . one particularly interesting approach within this line of work involves using these models as plan evaluators. mo- tivated by the intuition that evaluation is easier than gen- eration 31 several efforts have explored leveraging these models as critics to assess the quality of generated plans 5 21 73 83 . however most of these efforts have focused on scenarios that require only limited high-level visual per- ception without necessitating \ufb01ne-grained or precise per- ceptual abilities. on the other hand while there have been prior works similarly investigating vlms low-level per- ception 26 64 studies speci\ufb01cally about the use of these models in planning remain limited. tasks such as motion planning often require \ufb01ne-grained visual understanding in highly speci\ufb01c contexts while also drawing on broad com- monsense knowledge acquired during pre-training. hence there is a pressing need to investigate the potential of vlms to understand both low-level visual details and leverage these visual signals for high-level reasoning. in this work we explore whether we can utilize vlms as evaluators in highly intricate continuous path planning problems. we introduce patheval figure 1 a control- lable benchmark designed around path planning in complex environments under diverse commonsense decision-making scenarios e.g. a \ufb01re\ufb01ghting robot has to cover as much of the area as possible to extinguish \ufb01res and scout for sur- vivors thus needs to prioritize paths with more coverage and higher clearance . traditionally encoding constraints in a planning algorithm requires signi\ufb01cant human effort in hand-crafting scenario-speci\ufb01c criteria for path evaluation such that the constraints can be injected into the planner this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 6886 page 2 example of an instance of patheval 2d 3d path descriptors path 1 min. clearance 1.07 max. clearance 8.91 avg. clearance 2.62 path length 243.46 smoothness 843.07 number of sharp turns 4 maximum angle 124.35. path 2 min. clearance 1.57 max. clearance 8.91 avg. clearance 3.18 path length 243.75 smoothness 809.65 number of sharp turns 1 maximum angle 93.12. scenario 1 the agent navigating this maze is a large truck so sharp turns 90 degrees or larger are harder to make. it should also stay on a straight line unless it is making a turn as it is driving through heavy traf\ufb01c. required descriptors smoothness number of sharp turns maximum angle ground truth path 2 scenario 2 an autonomous \ufb01re\ufb01ghting robot is designed to navigate and operate within burning buildings to extinguish \ufb01res and rescue trapped individuals. it should explore as much of the area as possible while maintaining a safe distance from the walls to avoid damage. required descriptors avg. clearance min. clearance path length ground truth path 1 scenario 3 as the vehicle is traversing a warzone it must stay concealed from enemy operatives making use of covers like walls and avoiding open spaces as much as possible. it should also reach its target as quickly as possible. required descriptors avg. clearance max. clearance path length ground truth path 2 prompt the agent navigating this maze is a large truck so sharp turns 90 degrees or larger are harder to make. it should also stay on a straight line unless it is making a turn as it is driving through heavy traf\ufb01c. which path better achieves the task path 1 is on the left side and path 2 is on the right side. your answer should follow the format below answer path 1 or path 2. explanation why you chose the path 1 or 2 . gpt-4o answer path 1. explanation path 1 has fewer sharp turns and maintains longer straight segments compared to path 2. this is more suitable for a large truck navigating through heavy traf\ufb01c as it minimizes the need for dif\ufb01cult maneuvers and allows for smoother driving. path 2 on the other hand has more frequent and sharper turns which would be more challenging for a large truck to navigate. llava-next-7b answer path 1. explanation path 1 is better suited for a large truck because it has fewer sharp turns 90 degrees or larger compared to path 2. the truck would have to make only one 90-degree turn which is easier to execute than the two 90-degree turns in path 2. additionally path 1 has more straight sections which would be more ef\ufb01cient for a large truck to navigate through heavy traf\ufb01c. the fewer turns and the greater number of straight sections make path 1 the more suitable route for a large truck. figure 1. example of patheval. the benchmark consists of 14 550 instances of path pairs rendered in both 2d and 3d and mapped to 15 decision-making scenarios. success on this task is tied to three distinct levels 1 attribute abstraction recognizing what aspects make a path ideal 2 low-level perception extracting the required attributes for each path from the images and 2 information integration synthesizing the collected information to make a decision. we test a set of vlms on the task and \ufb01nd that they struggle particularly with low-level perception. incorrect answers by different models are shown explanations indicating misperception are highlighted . during the search process 36 52 54 . however this not only is a tedious and costly process but also cannot scale up to handle the countless intricate scenarios in meeting real- life planning needs. an effective vlm as a reward model relieving humans from customizing evaluation criteria for speci\ufb01c scenarios could enable scalable general-purpose planners that can adapt to diverse commonsense scenarios described in natural language. 6887 page 3 in patheval a vlm is tasked with comparing two paths within a given decision-making context and select- ing the one that better satis\ufb01es the constraints outlined by the scenario. success on this task requires effective per- formance across three distinct levels 1 attribute abstrac- tion recognizing the attributes that de\ufb01ne a favorable path in a particular decision-making context 2 low-level per- ception demonstrating precise low-level perception to de- termine which path performs better based on the given cri- teria and 3 information integration integrating and syn- thesizing the perceived information to produce an answer. using this benchmark we analyze the performance of 9 state-of-the-art sota vlms including both the closed- source gpt-4o and gpt-4o-mini 47 and 7 different open- source vlms of different sizes i.e. llava-next 7b and 13b 39 qwen2-vl-7b 69 llava-onevision-7b 32 llama-3.2-11b 17 and intern-vl2 8b and 40b 12 . we \ufb01nd that these models struggle with the path evalua- tion task e.g. qwen2-vl-7b achieving only 50.2 accu- racy . however when providing these vlms with verbal- ized path speci\ufb01cations their performance signi\ufb01cantly im- proves e.g. 74.2 accuracy for qwen2-vl-7b which re- veals a potential vision bottleneck of these vlms. our fur- ther analysis con\ufb01rms these models weakness in low-level perception especially when they are tasked to perceive the clearance of a path with respect to surrounding obstacles and this weakness could be more prominent when the envi- ronment and the path representation become more complex. we discover the source of this weakness from the vision encoders used by these vlms yet simply \ufb01ne-tuning the vlms end-to-end with the vision encoders does not address the issue. rather our experiments suggest performing task- speci\ufb01c discriminative adaptation of these vision encoders. 2. related work 2.1. vision-language models the outstanding success of decoder-only llms 6 48 65 has driven the development of vision-language models vlms which extend llms with a vision component in an attempt to generalize their performance into a multi-modal setting 12 17 32 37 38 62 69 . vlms are designed for tasks that require unifying visual and linguistic representa- tions e.g. visual question answering 4 . typically this is achieved by connecting a vision encoder to a language de- coder and projecting the representations into a shared space allowing visual elements and linguistic components to be linked in a semantically meaningful manner. one of the earliest successful models at achieving this was the clip encoder 55 which was trained using contrastive learning to learn representations that map images and their corre- sponding textual descriptions. several varieties of clip have then been introduced 25 56 76 80 . these mod- els while showing tremendous promises have shown sev- eral limitations when it comes to visual tasks 57 as such several works 64 72 have sought ways to improve such representations by combining them with vision foundation models such as dino 8 49 . in this work we contribute to the research of vlms with a new benchmark namely patheval focusing on evaluating vlms as evaluators for path planning. this benchmark tests vlms seamlessly on their commonsense understanding i.e. being able to ab- stract critical concepts expressed in the described planning scenarios low-level perception i.e. precisely perceiving details about paths in complex environments and the abil- ity to reason about the collected textual and visual informa- tion for decision-making. 2.2. automated planning with vlms vision-language planning promises more \ufb02exible planning frameworks and enhanced human-ai interaction. there- fore designing systems that can effectively understand nat- ural language instruction and leverage perceptual inputs to conduct planning tasks has been a topic of interest in re- cent years 20 61 70 . the rise of vlms has led many to investigate the use of these models as vision-language planning agents 15 16 50 51 . however existing litera- ture highlights the limitations of llms in spatial reasoning 2 26 74 as well as long-horizon planning 3 67 68 . this pushed researchers to explore alternative ways to incorpo- rate vlms and llms into planning frameworks more reli- ably 27 58 . one potential direction is their use as plan evaluators either through the generation of reward func- tions 23 33 73 or by using them directly as off-the-shelf critics 5 21 83 . the success of such frameworks assumes perfect perception and that the models can accurately per- ceive visual information and reason about it in order to pro- duce an answer nevertheless it has been shown that the representations used by these models fail in highly intricate visual settings 64 . several works have explored the use of vlms as well as llms for path and motion planning 2 3 7 9 11 14 44 75 however to the knowledge of the authors there is no work that explores the use of vlms as path critics in this context. accordingly we aim to evaluate the ability of vlms to serve as evaluators in cases requir- ing navigation in complex environments while adhering to decision-making constraints speci\ufb01ed in natural language. 2.3. vision-language model benchmarks the introduction of multimodal models has prompted the development of several benchmarks that are capable of as- sessing the performance of these models on visual rea- soning tasks such as visual question-answering datasets 4 18 19 24 42 43 84 . however the rise of founda- tion models has produced the need for a more holistic eval- uation of the perceptual and reasoning capabilities of large 6888 page 4 vlms leading to benchmarks such as mm-vet 77 mm- bench 40 mmmu 45 78 79 81 and omnibench 35 . several benchmarks have also speci\ufb01cally been designed to assess the perception capabilities of these models and ex- plore the limitations associated with visual hallucinations and optical illusions 10 22 34 60 64 . our proposed benchmark provides a \ufb02exible yet challenging framework for interleavedly assessing the low-level perception and rea- soning capabilities of vlms. 3. the patheval benchmark motivated by the need to evaluate vlms as path evalua- tors in real-world planning scenarios we introduce pathe- val a controllable and extensible benchmark focused on path planning 28 30 41 in complex environments under a diverse set of decision-making constraints. we list the decision-making scenarios as well as the descriptors they attempt to optimize in appendix a. in total patheval includes 14 550 tasks over more than 1 150 distinct envi- ronments and 15 distinct scenarios. below we introduce a formal description of the task and the dataset construction. 3.1. task formulation given two paths p1 and p2 and a scenario s the objective is to determine which path better satis\ufb01es the scenario s op- timization criteria. each scenario s is a high-level descrip- tion that aims to optimize over a set of path descriptors or metrics m m1 m2 . . . mk where each descriptor mj p r evaluates a speci\ufb01c property of a path e.g. length smoothness or proximity to obstacles . a vlm v is presented with two images presenting p1 and p2 in the same environment respectively. the model must then de- cide which path better satis\ufb01es the scenario s criteria. to explore the sensitivity of vlms to the way how a path is presented patheval includes both the 2d and 3d images of the path illustration. the model is also prompted to gen- erate an explanation to justify its choice. 3.2. environment and path generation environment generation an environment as shown in figure 1 is de\ufb01ned by a set of walls o o1 o2 . . . on where each wall oi represents an ob- stacle in the 2d space. each wall is a closed geometric shape described by its vertices and the set o forms the ob- stacles that the path must avoid. in this work we leverage the environments of plaku et. al. 53 which consists of four types of obstacle arrangements 1 rings where the environments are structured as mazes with circular walls 2 waves which consist of wavy horizontal obstacles 3 mazes which consist of both vertical and horizontal walls forming a complex maze structure as well as 4 random which consist of randomly placed obstacles. path synthesis via the randomly-exploring rapid tree rrt algorithm to generate path candidates in patheval we leverage the rrt path planning algo- rithm 30 . starting from the initial location in the envi- ronment the algorithm works by building a tree which ex- pands itself by randomly selecting the next location in the environment while avoiding obstacles until it reaches the goal. in this work we use the open motion planning li- brary ompl 59 and implement the rrt-connect algo- rithm 29 . we note that while we use rrt in the current benchmark our codebase is adaptable and can incorporate most path-planning algorithms provided by ompl. we en- courage future research building on our benchmark to ex- periment with other algorithms as well. 3.3. path descriptors we collect the following descriptors m for each of a gen- erated path minimum clearance measures the smallest distance between any point on the path and the nearest ob- stacle maximum clearance measures the largest distance between any point on the path and the nearest obstacle av- erage clearance computes the average distance between all points on the path and the nearest obstacle path length is calculated by summing up the euclidean distances between consecutive points on the path smoothness is de\ufb01ned as the sum of the angles between consecutive segments of the path measuring how smoothly the path changes direction number of sharp turns counts the number of turns in the path where the angle between consecutive segments ex- ceeds 90 degrees and maximum angle denotes the largest angle between any two consecutive segments of the path. the three clearance metrics and path length share the same measuring unit i.e. one grid size smoothness and maxi- mum angle are measured by degree and the number of sharp turns is an integer count. we include the formula of each descriptor in appendix b. 3.4. natural language descriptions of scenarios to create a suf\ufb01ciently challenging path-planning evalua- tion benchmark we design a total of 15 decision-making scenarios that aim to optimize different combinations of the path descriptors. for instance scenario 2 as shown in figure 1 requires searching through an area affected with \ufb01re in search for survivors and the agent thus must cover as much ground as possible. in contrast scenario 3 indicates that the path is to be executed within a warzone as a result the vehicle has to remain hidden and take the shortest route. as such given the same set of paths the one minimizing the path length is favored by scenario 3 while scenario 2 needs to maximize this value. a complete summary of the 15 scenarios along with the descriptors each scenario aims to optimize is presented in tables 6-7 in appendix a. 6889 page 5 3.5. task pairs selection for each environment we synthesized we ran the rrt plan- ner 30 times to generate different paths. upon eliminating paths that did not reach the goal we selected path pairs that exhibited the greatest dissimilarity in terms of path descrip- tors. speci\ufb01cally we \ufb01rst represented each path using a 7- dimension vector of its path descriptor values. given that each path descriptor ranged in a dramatically different scale we normalized the vector by performing the min-max scal- ing i.e. scaling each value x in the vector to x x xmin xmax xmin where x represents each value in the vector and xmin and xmax are the minimum and maximum values of each de- scriptor across paths sampled from the same environment respectively. we then measured the distance between two paths by calculating the euclidean distance between their normalized path descriptor vectors and selected 5 path pairs with the largest distances. each path is included in only one pair to avoid redundancy. we repeated the same selection procedure for a total of 1 150 environments. upon performing this generation we matched each pair with all \ufb01fteen scenarios however we only kept pairs where there was a signi\ufb01cant difference in at least one of the descriptors required for the particular scenario. in other words we ensure that the absolute difference is greater than a prede\ufb01ned threshold 0.8 for clearance descriptors 50 for path length 90 for smoothness 1 for the number of sharp turns and 30 for maximum angle for at least one of the required descriptors. this makes it more likely that the dif- ference is more noticeable to the naked eye and thus the paths can be compared by visual inspection. we constructed our \ufb01nal evaluation benchmark by ran- domly selecting 70 task pairs from each scenario resulting in 1 050 pairs in total. the remaining task pairs a total of 13 500 are used as the training set to facilitate the \ufb01ne- tuning experiments in section 5. 4. can sota vlms evaluate planned paths in order for vlms to perform successfully in our bench- mark they need to perform successfully at three differ- ent levels i.e recognizing the critical descriptors required by each scenario attribute abstraction exhibiting sharp low-level perception to precisely assess each path s proper- ties low-level perception and integrating the prior in- formation to make a rational decision on the better path information integration . among them the \ufb01rst two lev- els re\ufb02ect parallel properties that serve as a foundation for the third level. in this section we evaluated a set of 9 vlms on patheval in a zero-shot manner and analyzed their ca- pabilities at these three levels. these vlms include 1 two closed-source vlms i.e. gpt-4o and gpt-4o-mini 47 and 2 seven open-source vlms with various sizes in- cluding llava-next-7b and llava-next-13b 38 39 llava-onevision-7b 32 qwen2-vl-7b 69 llama- 3.2-11b 17 and intern-vl2-8b and intern-vl2-40b 12 . we include all prompt scripts used in this section in ap- pendix c.1. 4.1. overview of vlms on patheval the performance of the 9 vlms on patheval is show- cased in table 1. we notice that all of the models ex- cept gpt-4o fail to perform signi\ufb01cantly better than a sim- ple random baseline indicating signi\ufb01cant limitations. for gpt-4o we also notice a 4 higher accuracy on 2d im- ages compared to prompting with 3d inputs. this obser- vation indicates that the model is prone to visual illusions introduced by the 3d images when it has to rely on solely the image for decision-making although we observe an op- posite effect of 2d vs. 3d when verbalized descriptor val- ues are provided . in the remaining section we will further break down these models capabilities to gain a deeper un- derstanding of their failure on this task. providing verbalized path information yields better task accuracy implying reasonable vlm performance in at- tribute abstraction. in table 1 we further show the per- formance of each model when we explicitly list the value for each descriptor as part of the language prompt i.e. patheval w desc. . we notice a 11.1 -27.1 improve- ment across most models indicating that when given low- level details the models can better \ufb01lter out the information and make better comparisons. this points out that the bot- tleneck for these vlms s better performance lies in their inability to accurately perceive low-level information about the paths we discuss this in more detail in section 4.2 whereas these models generally have a reasonable capabil- ity in abstracting the critical attributes for decision making in various scenarios. in appendix d.1 we include an ex- periment where we explicitly query each vlm to identify the critical path metrics for each scenario the result corrob- orates our hypothesis. in particular we \ufb01nd that for most models the success rate for identifying a required descrip- tor is over 92 . finally a surprising observation happens to llava-next-7b. we notice that this model suffers par- ticularly severely from hallucination even when the textual descriptor values are provided and when the model can cor- rectly identify critical path metrics based on our analysis in appendix d.1 it fails to pick the better paths. we show an example of its explanation when textual descriptors are provided in figure 5 of appendix e. vlms exhibit mixed performance in integrating vi- sual and textual path information. we take a further look into the model performance when both the image and the textual descriptor values are provided and con- trast it with their performance when only the textual path descriptions are provided i.e. desc only . interest- 6890 page 6 model 2d 3d desc only patheval patheval w desc. patheval patheval w desc. gpt-4o-mini 0.520 0.750 0.508 0.745 0.680 gpt-4o 0.665 0.860 0.624 0.895 0.894 llava-next-7b 0.501 0.524 0.499 0.517 0.514 qwen2-vl-7b 0.502 0.731 0.511 0.742 0.737 llava-onevision-7b 0.505 0.718 0.509 0.739 0.721 intern-vl2-8b 0.489 0.654 0.505 0.691 0.648 llama-3.2-11b 0.480 0.695 0.460 0.680 0.686 llava-next-13b 0.509 0.620 0.494 0.601 0.630 intern-vl2-40b 0.506 0.688 0.496 0.717 0.679 random baseline 0.500 0.500 0.500 0.500 0.500 table 1. accuracy of vlms on patheval based on 2d and 3d environment images. to investigate the potential vision bottleneck of vlms we additionally present each model s accuracy when explicit descriptor values are provided in the language prompts patheval w desc. . the last column desc only shows the model performance when only the textual descriptor values are provided. ingly we observe mixed information integration perfor- mance from these models. for gpt-4o qwen2-vl-7b llava-onevision-7b llama-3.2-11b and llava-next- 13b their performance based on only descriptor values has no obvious difference to their best performance when 2d or 3d images are also provided. this observation implies that these models do not bene\ufb01t from the additional image information when the textual path descriptions are provided. instead sometimes the images e.g. 2d images for gpt-4o and 3d images for llava-next-13b may confuse their understanding of the textual descriptors resulting in a worse accuracy compared to desc only. for gpt-4o-mini intern- vl2-8b and intern-vl2-40b however providing both the visual and textual path information offers 4 -7 perfor- mance gain over desc only indicating better information integration from these vlms. in appendix d.2 we present a breakdown of gpt- 4o s performance by scenarios. we show that performance varies greatly from one scenario to the other. interest- ingly we observe that gpt-4o may overuse their common- sense knowledge. for instance in the case of scenario 2 shown in figure 1 where the agent is required to maxi- mize the path length for better coverage gpt-4o still fa- vors the shorter path. this scenario represents a counter- factual situation as models are often trained or instructed to seek the shortest paths. evaluating vlms in such coun- terfactual scenarios allows us to effectively probe their task understanding and reasoning which we consider to be an important direction for future research. the reasoning of vlms can be unreliable. limited by the vision bottleneck we noticed these vlms fabricat- ing seemingly plausible explanations to justify their path evaluation despite the fact that they could not actually per- ceive the necessary path details to perform the task. this fabrication echos \ufb01ndings from other recent work where llms were shown to produce reasoning traces that do not model default flipped random ids gpt-4o-mini 34 1016 22 1026 94 956 gpt-4o 278 772 258 792 291 759 llava-next-7b 1028 22 1041 9 580 470 qwen2-vl-7b 21 1029 130 920 127 923 llava-onevision-7b 438 612 440 610 262 788 intern-vl2-8b 433 610 430 620 418 632 llama-3.2-11b 601 251 620 264 731 319 llava-next-13b 795 255 939 111 305 745 intern-vl2-40b 394 656 410 639 510 540 ground truth labels 530 520 table 2. performance on the 2d case of times \ufb01rst path is chosen of times second path is chosen when we \ufb02ip the path order or replace their default names with random ids. there are several cases where llama-3.2-11b does not follow the required format and or does not give an answer we omit those cases from this table. accurately re\ufb02ect the process of how the models reach an answer 1 66 . to gain further insights we performed an analysis comparing model performance on patheval with cases that consist of 1 \ufb02ipping the order of paths in the pair and 2 assigning random ids to the paths e.g. instead of referring to them as path 1 and path 2 we use a random sequence such as path xu2q and path fp48 . the results presented in table 2 showcase that vlms demonstrate bias for a particular label when they actually do not have the capability to resolve the task. for example when no matter the default or the \ufb02ipping la- bels are used llava-next-7b consistently selects path 1 98 of the time and fabricates incorrect observations of the two paths in its explanations figure 1 to support this choice. as we discussed earlier llava-next-7b is par- ticularly prone to hallucination in explanations leading to its random-guess performance with or without textual de- scriptors. introducing random ids as path names mitigates this bias for llava-next-7b although the model still ob- tains a close-to-random accuracy on patheval but does not seem to help other models dramatically. 6891 page 7 descriptor test set \u03c91 \u03c92 \u03c93 min. clearance 0.46 0.46 0.50 0.46 0.74 0.70 0.86 0.74 max. clearance 0.44 0.46 0.41 0.49 0.46 0.55 0.50 0.60 avg. clearance 0.53 0.55 0.50 0.52 0.70 0.57 0.73 0.60 path length 0.58 0.70 0.86 0.91 0.92 0.86 0.94 0.94 smoothness 0.74 0.72 0.86 0.82 0.90 0.90 0.90 0.89 of sharp turns 0.76 0.73 0.80 0.84 0.77 0.70 0.60 0.55 max. angle 0.71 0.70 0.82 0.84 0.86 0.88 0.94 0.96 table 3. gpt-4o s \ufb01ne-grained perception accuracy 2d 3d on the test set of patheval and three additionally synthesized datasets with increasing metric differences. 4.2. what can vision-language models see the previous subsection clearly highlights the vision com- ponent as the bottleneck for path evaluation on patheval. in this subsection we conduct an analysis of the visual per- ception capabilities of vlms. we focus our analysis on gpt-4o since it is the only model that performs substan- tially better than the random baseline in the case where no descriptors are required. gpt-4o can perceive paths that are signi\ufb01cantly differ- ent. in order to get a better understanding of the model s perception capability we break down its performance in terms of perception by individual metric. speci\ufb01cally we prompt gpt-4o to select which path in the pair provides a lower value on each individual metric and report its ac- curacy. we perform this analysis on both the task pairs in the test set of patheval and an additionally synthesized dataset consisting of task pairs with three levels denoted as \u03c91 \u03c92 and \u03c93 of increasing differences in their descrip- tor values. we describe the data generation process for this dataset in more detail in appendix c.2. the results for both experiments are shown in table 3. upon evaluating the performance on the test set of pathe- val by individual metrics we notice that gpt-4o particu- larly struggles with clearance metrics. these metrics typ- ically require a lower level of perception and is naturally more challenging to discriminate than other metrics. on the other hand smoothness number of sharpest turns and max angle appear to be easier for the model to capture. we also note that gpt-4o perceives the path length much more easily in a 3d environment presentation. further- more gpt-4o s performance increases as we increase the descriptor difference between paths. this however is not the case for the number of sharp turns. we conjecture that when we increase the number of sharp turns we also en- force the challenge of counting the number of satisfying turns which vlms have been shown with limitations 46 . does segment complexity affect performance we look into whether the complexity of the path segment is the key reason for gpt-4o s limited perception of clearance. to this end we test gpt-4o on segments of varying complex- ity i.e. points straight lines and curves in a simpli\ufb01ed a point b straight line c curve d gpt-4o performance on the distance to obstacle under different seg- ment complexities. figure 2. example segment complexity test cases in simpli\ufb01ed environments and performance across the various settings. environment with only one rectangular obstacle at the cen- ter figure 2 and evaluate its accuracy in identifying seg- ments that are closer to the obstacle. for individual points the clearance is de\ufb01ned by the perpendicular distance from the point to the obstacle for straight lines and curves we consider a path closer if one of its endpoints is closer to the obstacle. for each segment type we synthesize 100 pairs by \ufb01rst randomly generating 200 segments and then pairing each with the segment with the greatest distance difference from the obstacle relative to it i.e. maximizing the absolute difference between the distances of the two segments from the obstacle . this increases the likelihood that the distance is signi\ufb01cant enough to be perceivable. the average differ- ence in the clearances of the pairs of segments are 14.76 14.42 and 14.28 for points lines and curves respectively. the results in figure 2d show that gpt-4o can perform better in very easy scenarios however it struggles more as the segment complexity increases. for instance the model was able to identify the closer points in 77 of the cases however when considering straight and curved lines its performance drops to 64 and 59 respectively. a sur- prising observation is that in the case of curved lines gpt- 4o s performance is dramatically better in 3d images. as shown in appendix c.2 the average clearance difference of path pairs in patheval is merely 0.12 1.31. as the paths in patheval are much more complicated than the curves in this experiment it is expected that gpt-4o exhibits dif- \ufb01culty in judging paths clearances. the complexity of the environments compared to a single square obstacle could add challenges. 5. fine-tuning a path evaluator one intuitive question is whether simply \ufb01ne-tuning the vlms can relieve their vision bottleneck. to answer this 6892 page 8 model frozen fine-tuned accuracy avg. cosine similarity accuracy avg. cosine similarity clip-vit-base-patch32 0.510 0.914 0.783 0.514 clip-vit-large-patch14-336 0.498 0.907 0.749 0.548 openclip-vit-b-32 0.540 0.883 0.743 0.475 siglip-base-patch16-224 0.529 0.895 0.731 0.612 dino-vit-16 0.495 0.911 0.763 0.754 dino-v2-base 0.510 0.761 0.721 0.681 table 4. probing accuracy and average cosine similarity between distinct path images 2d when the vision encoder is frozen or \ufb01ne-tuned. setting frozen tunable image only 0.52 0.51 image w descriptors 0.96 - image only random ids 0.48 0.52 table 5. fine-tuned llava-next-7b performance 2d when we keep the vision encoder frozen or tunable. question we experiment with llava-next-7b and \ufb01ne- tune it on the training set 13 500 pairs of patheval. we focus on the 2d case for the set of experiments described in this section. we consider three separate settings for train- ing 1 training with only images as input 2 training with images and textual descriptor values as input and 3 the same setting as 1 with using random ids as target labels. details in experimental setup are included in appendix c.3. 5.1. overall performance fine-tuning does not help with vision-language map- ping. table 5 shows that the model fails to learn meaningful patterns in the data even after training 50 epochs. how- ever when the textual descriptors are provided as input the model can easily learn the function achieving 96 accu- racy a 45 improvement from the zero-shot setting . this shows that the model is unable to extract the same descrip- tor values from the image input. unfreezing the encoder for \ufb01ne-tuning also does not provide any signi\ufb01cant improve- ment. we include a further discussion in appendix d.3. the results point to a limitation in the vision model s ability to encode the images which we will investigate next. 5.2. understanding the visual representations in order to further understand the limitations of the vision component we conduct an analysis to better understand how well different vision encoder models can differentiate between different paths in the dataset. to this end we \ufb01rst apply a linear probe to see how easily distinguishable differ- ent images are. speci\ufb01cally given a pair of images we \ufb01rst use the vision encoder to extract high-dimensional feature representations for both images. these features are then concatenated and passed through a simple binary classi\ufb01ca- tion layer i.e. the probe . the probe is trained to predict a label of 1 if the images are the same and 0 otherwise. we ex- periment with various sota vision encoders namely clip 55 base and large varieties laion-openclip 25 56 siglip 80 dino 8 and dino-v2 49 and analyze how well their learned representations capture visual similarities and differences. we use a set of 1 000 randomly sampled path pairs with a balanced label distribution to train the probe and look at whether the model can learn to distin- guish between these paths. we also perform an experiment where we \ufb01ne-tune the vision encoder along with the probe. in this setting our goal is to gauge if carefully \ufb01ne-tuning a vision encoder can potentially improve the model s perfor- mance in low-level perception. finally in both settings we present the average cosine similarity between distinct paths. vision encoders cannot distinguish between paths. from table 4 it can be seen that vision encoder models are unable to provide representations that are signi\ufb01cantly different for the probe to tell if they are the same. this is further sup- ported by the high values for the average cosine similarity across all models. fine-tuning the encoders on a discrimination task can help disentangle the visual representations. by un- freezing the encoder weights and training them to identify whether two paths are identical we enhance their adapt- ability to the task. the results in table 4 demonstrate that this approach effectively disentangles the learned represen- tations resulting in signi\ufb01cantly improved performance and increased separability as evidenced by the notably lower cosine similarity between non-identical paths. the results thus imply the need for carefully \ufb01ne-tuning task-speci\ufb01c vision encoders for path evaluation on patheval. 6. conclusion this work explored the use of vlms as evaluators in path- planning scenarios. we evaluated a number of vlms on our proposed benchmark patheval and found that these models struggle with low-level perception. speci\ufb01cally we \ufb01nd that visual encoders used by sota models are unable to discern the differences between different paths in intri- cate scenarios. we hope that patheval will inspire re- searchers to further explore ways to improve the visual ca- pabilities of vlms and contribute to \ufb01nding better ways to incorporate foundation models for developing more \ufb02exi- ble robust and scalable planning paradigms. 6893 page 9 acknowledgments this project was supported by resources provided by the of\ufb01ce of research computing at george mason university url https orc.gmu.edu and funded in part by grants from the national science foundation award number 2018631 . references 1 chirag agarwal sree harsha tanneru and himabindu lakkaraju. faithfulness vs. plausibility on the un reliability of explanations from large language mod- els 2024. 6 2 mohamed aghzal erion plaku and ziyu yao. can large language models be good path planners a benchmark and investigation on spatial-temporal reasoning. in iclr 2024 workshop on large language model llm agents 2024. 1 3 3 mohamed aghzal erion plaku and ziyu yao. look further ahead testing the limits of gpt-4 in path planning. ieee case 2024. 1 3 4 stanislaw antol aishwarya agrawal jiasen lu margaret mitchell dhruv batra c. lawrence zitnick and devi parikh. vqa visual question answering. in international conference on computer vision iccv 2015. 3 5 kate baumli satinder baveja feryal behbahani harris chan gheorghe comanici sebastian flennerhag maxime gazeau kristian holsheimer dan horgan michael laskin clare lyle hussain masoom kay mckinney volodymyr mnih alexander neitz dmitry nikulin fabio pardo jack parker-holder john quan tim rockt aschel himanshu sahni tom schaul yannick schroecker stephen spencer richie steigerwald luyu wang and lei zhang. vision- language models as a source of rewards 2024. 1 3 6 tom b. brown benjamin mann nick ryder melanie sub- biah jared kaplan prafulla dhariwal arvind neelakan- tan pranav shyam girish sastry amanda askell sand- hini agarwal ariel herbert-voss gretchen krueger tom henighan rewon child aditya ramesh daniel m. ziegler jeffrey wu clemens winter christopher hesse mark chen eric sigler mateusz litwin scott gray benjamin chess jack clark christopher berner sam mccandlish alec rad- ford ilya sutskever and dario amodei. language models are few-shot learners 2020. 1 3 7 xu cao tong zhou yunsheng ma wenqian ye can cui kun tang zhipeng cao kaizhao liang ziran wang james m. rehg and chao zheng. maplm a real-world large-scale vision-language benchmark for map and traf- \ufb01c scene understanding. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr pages 21819 21830 2024. 3 8 mathilde caron hugo touvron ishan misra herv e j egou julien mairal piotr bojanowski and armand joulin. emerg- ing properties in self-supervised vision transformers. in pro- ceedings of the international conference on computer vi- sion iccv 2021. 1 3 8 9 jiaqi chen bingqian lin ran xu zhenhua chai xi- aodan liang and kwan-yee wong. mapgpt map- guided prompting with adaptive path planning for vision- and-language navigation. in proceedings of the 62nd an- nual meeting of the association for computational linguis- tics volume 1 long papers pages 9796 9810 bangkok thailand 2024. association for computational linguistics. 3 10 xuweiyi chen ziqiao ma xuejun zhang sihan xu shengyi qian jianing yang david fouhey and joyce chai. multi-object hallucination in vision language models. in the thirty-eighth annual conference on neural information processing systems 2024. 4 11 yongchao chen jacob arkin charles dawson yang zhang nicholas roy and chuchu fan. autotamp autoregressive task and motion planning with llms as translators and check- ers. in 2024 ieee international conference on robotics and automation icra pages 6695 6702 2024. 1 3 12 zhe chen jiannan wu wenhai wang weijie su guo chen sen xing muyan zhong qinglong zhang xizhou zhu lewei lu bin li ping luo tong lu yu qiao and jifeng dai. internvl scaling up vision foundation models and aligning for generic visual-linguistic tasks. arxiv preprint arxiv 2312.14238 2023. 3 5 13 wei-lin chiang zhuohan li zi lin ying sheng zhang- hao wu hao zhang lianmin zheng siyuan zhuang yong- hao zhuang joseph e. gonzalez ion stoica and eric p. xing. vicuna an open-source chatbot impressing gpt-4 with 90 chatgpt quality 2023. 4 14 yan ding xiaohan zhang chris paxton and shiqi zhang. task and motion planning with large language models for ob- ject rearrangement. in 2023 ieee rsj international confer- ence on intelligent robots and systems iros pages 2086 2092. ieee 2023. 3 15 danny driess fei xia mehdi s. m. sajjadi corey lynch aakanksha chowdhery brian ichter ayzaan wahid jonathan tompson quan vuong tianhe yu wenlong huang yevgen chebotar pierre sermanet daniel duck- worth sergey levine vincent vanhoucke karol hausman marc toussaint klaus greff andy zeng igor mordatch and pete florence. palm-e an embodied multimodal lan- guage model. in proceedings of the 40th international con- ference on machine learning. jmlr.org 2023. 3 16 yilun du sherry yang pete florence fei xia ayzaan wahid brian ichter pierre sermanet tianhe yu pieter abbeel joshua b. tenenbaum leslie pack kaelbling andy zeng and jonathan tompson. video language planning. in the twelfth international conference on learning represen- tations 2024. 3 17 abhimanyu dubey et. al. the llama 3 herd of models 2024. 3 5 18 difei gao ruiping wang shiguang shan and xilin chen. cric a vqa dataset for compositional reasoning on vision and commonsense. ieee transactions on pattern analysis and machine intelligence 45 5 5561 5578 2022. 3 19 yash goyal tejas khot douglas summers-stay dhruv ba- tra and devi parikh. making the v in vqa matter elevating 6894 page 10 the role of image understanding in visual question answer- ing. in proceedings of the ieee conference on computer vision and pattern recognition cvpr 2017. 3 20 jing gu eliana stefani qi wu jesse thomason and xin wang. vision-and-language navigation a survey of tasks methods and future directions. in proceedings of the 60th annual meeting of the association for computational lin- guistics volume 1 long papers . association for computa- tional linguistics 2022. 3 21 lin guan yifan zhou denis liu yantian zha heni ben amor and subbarao kambhampati. task success is not enough investigating the use of video-language models as behavior critics for catching undesirable agent behaviors. in first conference on language modeling 2024. 1 3 22 tianrui guan fuxiao liu xiyang wu ruiqi xian zongxia li xiaoyu liu xijun wang lichang chen furong huang yaser yacoob dinesh manocha and tianyi zhou. hal- lusionbench an advanced diagnostic suite for entangled language hallucination visual illusion in large vision- language models 2023. 4 23 xu han qiannan yang xianda chen xiaowen chu and meixin zhu. generating and evolving reward functions for highway driving with large language models 2024. 3 24 drew a. hudson and christopher d. manning. gqa a new dataset for real-world visual reasoning and compositional question answering. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr 2019. 3 25 gabriel ilharco mitchell wortsman ross wightman cade gordon nicholas carlini rohan taori achal dave vaishaal shankar hongseok namkoong john miller han- naneh hajishirzi ali farhadi and ludwig schmidt. open- clip 2021. if you use this software please cite it as below. 3 8 26 amita kamath jack hessel and kai-wei chang. what s up with vision-language models investigating their strug- gle with spatial reasoning. in the 2023 conference on em- pirical methods in natural language processing 2023. 1 3 27 subbarao kambhampati karthik valmeekam lin guan mudit verma kaya stechly siddhant bhambri lucas paul saldyt and anil b murthy. position llms can t plan but can help planning in llm-modulo frameworks. in forty- \ufb01rst international conference on machine learning 2024. 1 3 28 l.e. kavraki p. svestka j.-c. latombe and m.h. over- mars. probabilistic roadmaps for path planning in high- dimensional con\ufb01guration spaces. ieee transactions on robotics and automation 12 4 566 580 1996. 4 29 j.j. kuffner and s.m. lavalle. rrt-connect an ef\ufb01cient ap- proach to single-query path planning. in proceedings 2000 icra. millennium conference. ieee international confer- ence on robotics and automation. symposia proceedings cat. no.00ch37065 pages 995 1001 vol.2 2000. 4 30 steven m. lavalle. rapidly-exploring random trees a new tool for path planning. the annual research report 1998. 4 31 jan leike. why i m optimistic about our align- ment approach evaluation is easier than generation. https aligned.substack.com i 88447351 evaluation - is - easier - than - generation 2022. accessed 2024-11. 1 32 bo li yuanhan zhang dong guo renrui zhang feng li hao zhang kaichen zhang yanwei li ziwei liu and chunyuan li. llava-onevision easy visual task trans- fer. arxiv preprint arxiv 2408.03326 2024. 3 5 33 hao li xue yang zhaokai wang xizhou zhu jie zhou yu qiao xiaogang wang hongsheng li lewei lu and jifeng dai. auto mc-reward automated dense reward design with large language models for minecraft. in ieee cvf confer- ence on computer vision and pattern recognition 2024. 3 34 yifan li yifan du kun zhou jinpeng wang wayne xin zhao and ji-rong wen. evaluating object hallucination in large vision-language models. in the 2023 conference on empirical methods in natural language processing 2023. 4 35 yizhi li ge zhang yinghao ma ruibin yuan kang zhu hangyu guo yiming liang jiaheng liu zekun wang jian yang siwei wu xingwei qu jinjie shi xinyue zhang zhenzhu yang xiangzhou wang zhaoxiang zhang zachary liu emmanouil benetos wenhao huang and chenghua lin. omnibench towards the future of uni- versal omni-language models 2024. 4 36 hui sheng lim shuangshuang fan christopher k.h. chin shuhong chai neil bose and eonjoo kim. con- strained path planning of autonomous underwater vehicle us- ing selectively-hybridized particle swarm optimization algo- rithms. ifac-papersonline 52 21 315 322 2019. 12th ifac conference on control applications in marine sys- tems robotics and vehicles cams 2019. 2 37 haotian liu chunyuan li qingyang wu and yong jae lee. visual instruction tuning. in advances in neural information processing systems pages 34892 34916. curran associates inc. 2023. 3 38 haotian liu chunyuan li yuheng li and yong jae lee. improved baselines with visual instruction tuning 2024. 3 5 39 haotian liu chunyuan li yuheng li bo li yuanhan zhang sheng shen and yong jae lee. llava-next im- proved reasoning ocr and world knowledge 2024. 3 5 40 yuan liu haodong duan yuanhan zhang bo li songyang zhang wangbo zhao yike yuan jiaqi wang conghui he ziwei liu kai chen and dahua lin. mmbench is your multi-modal model an all-around player in computer vi- sion eccv 2024 pages 216 233 cham 2025. springer nature switzerland. 4 41 tom as lozano-p erez and michael a. wesley. an algorithm for planning collision-free paths among polyhedral obsta- cles. communications of the acm 22 10 560 570 1979. 4 42 pan lu hritik bansal tony xia jiacheng liu chunyuan li hannaneh hajishirzi hao cheng kai-wei chang michel galley and jianfeng gao. mathvista evaluating mathe- matical reasoning of foundation models in visual contexts. in the twelfth international conference on learning repre- sentations 2024. 3 6895 page 11 43 kenneth marino mohammad rastegari ali farhadi and roozbeh mottaghi. ok-vqa a visual question answering benchmark requiring external knowledge. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr 2019. 3 44 silin meng yiwei wang cheng-fu yang nanyun peng and kai-wei chang. llm-a large language model en- hanced incremental heuristic search on path planning. in findings of the association for computational linguistics emnlp 2024 pages 1087 1102 miami florida usa 2024. association for computational linguistics. 3 45 shota onohara atsuyuki miyai yuki imajuku kazuki egashira jeonghun baek xiang yue graham neubig and kiyoharu aizawa. jmmmu a japanese massive multi- discipline multimodal understanding benchmark for culture- aware evaluation 2024. 4 46 openai. gpt-4v ision system card. 2023. 7 47 openai. gpt-4o system card 2024. 3 5 48 openai. gpt-4 technical report 2024. 3 49 maxime oquab timoth ee darcet theo moutakanni huy v. vo marc szafraniec vasil khalidov pierre fernandez daniel haziza francisco massa alaaeldin el-nouby rus- sell howes po-yao huang hu xu vasu sharma shang- wen li wojciech galuba mike rabbat mido assran nico- las ballas gabriel synnaeve ishan misra herve jegou julien mairal patrick labatut armand joulin and piotr bo- janowski. dinov2 learning robust visual features without supervision 2023. 3 8 50 bowen pan rameswar panda souyoung jin rogerio feris aude oliva phillip isola and yoon kim. langnav lan- guage as a perceptual representation for navigation. in findings of the association for computational linguistics naacl 2024 pages 950 974 mexico city mexico 2024. association for computational linguistics. 1 3 51 chenbin pan burhaneddin yaman tommaso nesti abhirup mallik alessandro g allievi senem velipasalar and liu ren. vlp vision language planning for autonomous driving . in 2024 ieee cvf conference on computer vi- sion and pattern recognition cvpr pages 14760 14769 los alamitos ca usa 2024. ieee computer society. 1 3 52 clment petres yan pailhas pedro patron yvan petil- lot jonathan evans and david lane. path planning for autonomous underwater vehicles. ieee transactions on robotics 23 2 331 341 2007. 2 53 evis plaku erion plaku and patricio simari. clearance- driven motion planning for mobile robots with differential constraints. robotica 36 7 971 993 2018. 4 54 patrick a. plonski pratap tokekar and volkan isler. energy- ef\ufb01cient path planning for solar-powered mobile robots pages 717 731. springer international publishing heidel- berg 2013. 2 55 alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sas- try amanda askell pam mishkin jack clark gretchen krueger and ilya sutskever. learning transferable visual models from natural language supervision. proceedings of the 38th international conference on machine learning 2021. 3 8 56 christoph schuhmann romain beaumont richard vencu cade w gordon ross wightman mehdi cherti theo coombes aarush katta clayton mullis mitchell worts- man patrick schramowski srivatsa r kundurthy katherine crowson ludwig schmidt robert kaczmarczyk and jenia jitsev. laion-5b an open large-scale dataset for train- ing next generation image-text models. in thirty-sixth con- ference on neural information processing systems datasets and benchmarks track 2022. 3 8 57 jie-jing shao jiang-xin shi xiao-wen yang lan-zhe guo and yu-feng li. investigating the limitation of clip mod- els the worst-performing categories 2023. 3 58 tom silver soham dan kavitha srinivas joshua b. tenen- baum leslie pack kaelbling and michael katz. general- ized planning in pddl domains with pretrained large language models 2023. 3 59 ioan a. sucan mark moll and lydia e. kavraki. the open motion planning library. ieee robotics automa- tion magazine 19 4 72 82 2012. 4 60 yinan sun zicheng zhang haoning wu xiaohong liu weisi lin guangtao zhai and xiongkuo min. explore the hallucination on low-level perception for mllms 2024. 4 61 hao tan licheng yu and mohit bansal. learning to nav- igate unseen environments back translation with environ- mental dropout. in proceedings of the 2019 conference of the north american chapter of the association for computa- tional linguistics human language technologies volume 1 long and short papers pages 2610 2621 minneapolis minnesota 2019. association for computational linguis- tics. 3 62 gemini team. gemini a family of highly capable multi- modal models 2024. 3 63 shengbang tong erik jones and jacob steinhardt. mass- producing failures of multimodal systems with language models. in proceedings of the 37th international confer- ence on neural information processing systems red hook ny usa 2024. curran associates inc. 4 64 shengbang tong zhuang liu yuexiang zhai yi ma yann lecun and saining xie. eyes wide shut exploring the visual shortcomings of multimodal llms 2024. 1 3 4 65 hugo touvron louis martin kevin stone peter al- bert amjad almahairi yasmine babaei nikolay bash- lykov soumya batra prajjwal bhargava shruti bhos- ale dan bikel lukas blecher cristian canton ferrer moya chen guillem cucurull david esiobu jude fer- nandes jeremy fu wenyin fu brian fuller cynthia gao vedanuj goswami naman goyal anthony hartshorn saghar hosseini rui hou hakan inan marcin kardas vik- tor kerkez madian khabsa isabel kloumann artem ko- renev punit singh koura marie-anne lachaux thibaut lavril jenya lee diana liskovich yinghai lu yuning mao xavier martinet todor mihaylov pushkar mishra igor molybog yixin nie andrew poulton jeremy reizen- stein rashi rungta kalyan saladi alan schelten ruan silva eric michael smith ranjan subramanian xiao- qing ellen tan binh tang ross taylor adina williams 6896 page 12 jian xiang kuan puxin xu zheng yan iliyan zarov yuchen zhang angela fan melanie kambadur sharan narang aurelien rodriguez robert stojnic sergey edunov and thomas scialom. llama 2 open foundation and \ufb01ne- tuned chat models 2023. 3 66 miles turpin julian michael ethan perez and samuel r. bowman. language models don t always say what they think unfaithful explanations in chain-of-thought prompt- ing. in proceedings of the 37th international conference on neural information processing systems red hook ny usa 2024. curran associates inc. 6 67 karthik valmeekam alberto olmo sarath sreedharan and subbarao kambhampati. large language models still can t plan a benchmark for llms on planning and reasoning about change . in neurips 2022 foundation models for de- cision making workshop 2022. 3 68 karthik valmeekam kaya stechly and subbarao kamb- hampati. llms still can t plan can lrms a preliminary evaluation of openai s o1 on planbench 2024. 3 69 peng wang shuai bai sinan tan shijie wang zhihao fan jinze bai keqin chen xuejing liu jialin wang wenbin ge yang fan kai dang mengfei du xuancheng ren rui men dayiheng liu chang zhou jingren zhou and jun- yang lin. qwen2-vl enhancing vision-language model s perception of the world at any resolution. arxiv preprint arxiv 2409.12191 2024. 3 5 70 xin wang qiuyuan huang asli celikyilmaz jianfeng gao dinghan shen yuan-fang wang william yang wang and lei zhang. reinforced cross-modal matching and self- supervised imitation learning for vision-language navigation. in 2019 ieee cvf conference on computer vision and pat- tern recognition cvpr pages 6622 6631 2019. 3 71 jason wei xuezhi wang dale schuurmans maarten bosma brian ichter fei xia ed h. chi quoc v le and denny zhou. chain of thought prompting elicits reasoning in large language models. in advances in neural informa- tion processing systems 2022. 1 72 monika wysocza nska oriane sim eoni micha el ramamon- jisoa andrei bursuc tomasz trzci nski and patrick p erez. clip-dinoiser teaching clip a few dino tricks for open- vocabulary semantic segmentation. eccv 2024. 3 73 tianbao xie siheng zhao chen henry wu yitao liu qian luo victor zhong yanchao yang and tao yu. text2reward reward shaping with language models for reinforcement learning. in the twelfth international con- ference on learning representations 2024. 1 3 74 yutaro yamada yihan bao andrew kyle lampinen jungo kasai and ilker yildirim. evaluating spatial understanding of large language models. transactions on machine learn- ing research 2024. 3 75 zhutian yang caelan garrett dieter fox tom as lozano- p erez and leslie pack kaelbling. guiding long-horizon task and motion planning with vision language models 2024. 1 3 76 jiahui yu zirui wang vijay vasudevan legg yeung mo- jtaba seyedhosseini and yonghui wu. coca contrastive captioners are image-text foundation models. transactions on machine learning research 2022. 3 77 weihao yu zhengyuan yang linjie li jianfeng wang kevin lin zicheng liu xinchao wang and lijuan wang. mm-vet evaluating large multimodal models for integrated capabilities. in international conference on machine learn- ing. pmlr 2024. 4 78 xiang yue yuansheng ni kai zhang tianyu zheng ruoqi liu ge zhang samuel stevens dongfu jiang weiming ren yuxuan sun cong wei botao yu ruibin yuan ren- liang sun ming yin boyuan zheng zhenzhu yang yibo liu wenhao huang huan sun yu su and wenhu chen. mmmu a massive multi-discipline multimodal under- standing and reasoning benchmark for expert agi. in pro- ceedings of cvpr 2024. 4 79 xiang yue tianyu zheng yuansheng ni yubo wang kai zhang shengbang tong yuxuan sun botao yu ge zhang huan sun yu su wenhu chen and graham neubig. mmmu-pro a more robust multi-discipline multimodal understanding benchmark. arxiv preprint arxiv 2409.02813 2024. 4 80 xiaohua zhai basil mustafa alexander kolesnikov and lucas beyer. sigmoid loss for language image pre-training. in proceedings of the ieee cvf international conference on computer vision pages 11975 11986 2023. 3 8 81 ge zhang xinrun du bei chen yiming liang tongxu luo tianyu zheng kang zhu yuyang cheng chunpu xu shuyue guo haoran zhang xingwei qu junjie wang ruibin yuan yizhi li zekun wang yudong liu yu-hsuan tsai fengji zhang chenghua lin wenhao huang and jie fu. cmmmu a chinese massive multi-discipline multi- modal understanding benchmark 2024. 4 82 zhuosheng zhang aston zhang mu li hai zhao george karypis and alex smola. multimodal chain-of-thought rea- soning in language models 2024. 1 83 victor zhong dipendra misra xingdi yuan and marc- alexandre c\u02c6ot e. policy improvement using language feed- back models 2024. 1 3 84 yuke zhu oliver groth michael bernstein and li fei-fei. visual7w grounded question answering in images. in ieee conference on computer vision and pattern recog- nition 2016. 3 6897",
      "keywords": [
        "page",
        "evaluating",
        "vision-language",
        "models",
        "evaluators",
        "path",
        "planning",
        "mohamed",
        "aghzal1",
        "xiang",
        "yue2",
        "erion",
        "plaku3",
        "ziyu",
        "yao1",
        "1george",
        "mason",
        "university",
        "2carnegie",
        "mellon",
        "3national",
        "science",
        "foundation",
        "maghzal",
        "ziyuyao",
        "gmu.edu",
        "xyue2",
        "andrew.cmu.edu",
        "eplaku",
        "nsf.gov",
        "abstract",
        "despite",
        "promise",
        "perform",
        "complex",
        "reasoning",
        "large",
        "language",
        "llms",
        "shown",
        "limited",
        "effectiveness",
        "end-to-end",
        "planning.",
        "in-",
        "spired",
        "intriguing",
        "question",
        "cannot",
        "plan",
        "well",
        "still",
        "contribute",
        "framework",
        "helpful",
        "evaluator",
        "work",
        "generalize",
        "consider",
        "augmented",
        "visual",
        "under-",
        "standing",
        "i.e.",
        "vlms",
        "intro-",
        "duce",
        "patheval",
        "novel",
        "benchmark",
        "path-planning",
        "scenarios.",
        "suc-",
        "ceeding",
        "requires",
        "vlm",
        "able",
        "ab-",
        "stract",
        "traits",
        "optimal",
        "paths",
        "from",
        "scenario",
        "description",
        "demonstrate",
        "precise",
        "low-level",
        "perception",
        "each",
        "integrate",
        "information",
        "decide",
        "better",
        "path.",
        "analysis",
        "state-of-the-art",
        "reveals",
        "mod-",
        "els"
      ],
      "metadata": {
        "filename": "Aghzal_Evaluating_Vision-Language_Models_as_Evaluators_in_Path_Planning_CVPR_2025_paper.pdf",
        "original_filename": "Aghzal_Evaluating_Vision-Language_Models_as_Evaluators_in_Path_Planning_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 1520642,
        "upload_file_id": "upload_1752168249226_c3eiyvsah",
        "upload_timestamp": "2025-07-10T17:24:09.290882",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T17:24:09.483271+00:00",
      "content_length": 63431,
      "content_type": ".pdf",
      "filename": "Aghzal_Evaluating_Vision-Language_Models_as_Evaluators_in_Path_Planning_CVPR_2025_paper.pdf"
    },
    "19": {
      "file_id": "19",
      "content": "\n=== Page 1 ===\nMAD: Memory-Augmented Detection of 3D Objects\nBen Agro\u2020\nSergio Casas\u2020\nPatrick Wang\u2020\nThomas Gilles\nRaquel Urtasun\n{tgilles, urtasun}@waabi.ai\n(a) 3D Detector\n(b) Object Memory\n(c) Memory Augmented Detections\n0.0s\n-2.4s\nMemory Age\nFalse Negative\nFigure 1.\nDetectors without long-term temporal fusion (a) miss heavily occluded objects. Our approach enhances detectors (b) to\nremember past predictions, (c) recovering from occlusion. Detections are in green , labels are in black , lidar points are in \u2022 gray.\nAbstract\nTo perceive, humans use memory to \ufb01ll in gaps caused by\nour limited visibility, whether due to occlusion or our nar-\nrow \ufb01eld of view. However, most 3D object detectors are\nlimited to using sensor evidence from a short temporal win-\ndow (0.1s-0.3s). In this work, we present a simple and ef-\nfective add-on for enhancing any existing 3D object detec-\ntor with long-term memory regardless of its sensor modality\n(e.g., LiDAR, camera) and network architecture. We pro-\npose a model to effectively align and fuse object proposals\nfrom a detector with object proposals from a memory bank\nof past predictions, exploiting trajectory forecasts to align\nproposals across time. We propose a novel schedule to train\nour model on temporal data that balances data diversity\nand the gap between training and inference. By applying\nour method to existing LiDAR and camera-based detectors\non the Waymo Open Dataset (WOD) and Argoverse 2 Sen-\nsor (AV2) dataset, we demonstrate signi\ufb01cant improvements\nin detection performance (+2.5 to +7.6 AP points). Our\nmethod attains the best performance on the WOD 3D de-\ntection leaderboard among online methods (excluding en-\nsembles or test-time augmentation).\n1\nIntroduction\nMost self-driving vehicles (SDVs) utilize a 3D object detec-\ntor to recognize and localize objects in 3D space. This task\nis challenging due to occlusion, large intra-class variability,\nand distant objects, which typically have limited sensor ob-\nservations. To overcome these challenges, human drivers\nrely on their memory. For example, they may drive more\ncautiously when remembering a previously observed but\nnow occluded cyclist, who may suddenly enter the road.\nA common approach for improving 3D object detectors\nis to aggregate a short temporal window of past sensor ob-\nservations. Towards this goal, most existing LiDAR-based\nmethods transform a short buffer of sensor data into the cur-\nrent ego (SDV) coordinate frame to align past and current\nevidence [1, 39, 58, 74, 75]. Similarly, camera-based meth-\nods stack multiple images [46, 78] as input to existing ar-\nchitectures. These methods cannot handle long temporal\nsequences due to computational and memory constraints.\nMoreover, temporal stacks of 3D/Bird\u2019s-Eye-View (BEV)\nrepresentations like point clouds or lifted camera features\nrequire a large receptive \ufb01eld, especially for fast-moving\nobjects [30], further increasing computational burden.\nThere is a growing interest in long-term temporal fusion.\nScene-level memory approaches [14, 17, 31] recurrently\nfuse scene-level features, but they can struggle to capture\nrelevant foreground objects. Other approaches associate ob-\njects in memory over time via tracking [8, 20, 30, 32], ag-\ngregating past information for each particular object. How-\never, the associations from the tracker may contain mis-\ntakes that can compound over time and lead to informa-\ntion loss. Other methods leverage attention from current\ndetection proposals to the past sensor or object informa-\ntion [15, 18, 74]. Still, they can be challenging to scale\nto long histories and suffer from false negatives as the pro-\nposals re\ufb01ned into the \ufb01nal detections only come from the\n\u2020Work done while at Waabi\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1449\n\n=== Page 2 ===\npresent time.\nIn this paper, we present a simple and sensor-agnostic\nadd-on for enhancing any existing 3D object detector with\nlong-term memory. We refer to it as MAD \u2014 short for\nMemory-Augmented Detection, and Fig. 1 illustrates the\nhigh-level idea. MAD is a transformer-based model that\nfuses proposals from a detector with proposals from a mem-\nory bank representing past beliefs. Inspired by recent devel-\nopments [4], we exploit joint detection and trajectory fore-\ncasting. By storing explicit trajectory forecasts in the mem-\nory bank, we can estimate object poses at arbitrary future\ntimestamps for all the objects in the memory. This enables\nus to enrich the set of proposals by aligning memory pro-\nposals with the current observations.\nTraining with temporal data can be challenging: back-\npropogation through consecutive training examples con-\nsumes prohibitive amounts of memory, training on long se-\nquences can cause over-\ufb01tting when back-propagating on\nevery example, and using memory warm-up can slow down\ntraining. We design a more effective training schedule that\nbegins with short temporal sequences and progressively in-\ncreases the length, exploiting high data diversity early and\nclosing the gap with inference towards the end.\nTo en-\nsure the model learns to trust the memory when training\non short sequences, we use cached model outputs from pre-\nvious training iterations.\nWe demonstrate the generality of our approach by en-\nhancing existing LiDAR-based and camera-based 3D ob-\nject detection networks with MAD, and show considerable\nimprovements over the base detectors on two large-scale\ndatasets: Waymo Open dataset (WOD) [57] and Argoverse\n2 Sensor dataset (AV2) [68]. Notably, SAFDNet [80] en-\nhanced with MAD achieves state-of-the-art performance on\nWOD for online detection methods without requiring en-\nsembles or test-time augmentation.\n2\nRelated Work\n3D Object Detection:\nWe can categorize 3D detectors by\ntheir input modality (e.g., LiDAR, camera), scene repre-\nsentation (e.g., point clouds, voxels), and number of stages\n(e.g., single-stage or multi-stage)\nLiDAR-based methods commonly represent the input as\nvoxels [12, 27, 56, 71, 80, 81], pillars [23, 25, 55, 61, 66,\n72], or point clouds [43, 53, 54, 69, 73]. A widely used ap-\nproach for including temporal LiDAR information is point\naggregation, which involves transforming past point clouds\ninto a common coordinate frame and processing the ag-\ngregated point cloud. These approaches are usually lim-\nited to \u21925 past LiDAR frames due to computational con-\nstraints [30] in online applications like autonomous driving.\nAnother drawback is that point aggregation does not align\nmoving objects, requiring a larger receptive \ufb01eld in the de-\ntector backbone the longer the temporal horizon is [30, 80].\nCamera-based 3D detection is challenging because of\nmissing depth information. One approach is to produce 3D\nbounding boxes from image features by estimating depth,\n3D size, and orientation [7, 41, 64, 70]. Other methods\nleverage voxel [47, 48, 51] or point cloud [65, 76] repre-\nsentations by predicting pixel depth distributions to lift 2D\nfeatures to 3D. Stacking and processing past camera images\nor feature maps is a common but expensive method for tem-\nporal fusion [2, 19, 46, 52, 78].\nRegardless of modality, we can further categorize 3D de-\ntectors as single-stage or multi-stage. Single-stage methods\nproduce detections from sensor data [6, 12, 64, 71, 80, 81]\nwith a single deep neural network. Multi-stage methods\nuse bounding box proposals from a \ufb01rst stage (or ran-\ndomly initialized proposals) to gather features (e.g., with\nRoIPool [49], RoIAlign [16], interpolations [75], or atten-\ntion [4, 28, 33\u201335, 67, 82]) and iteratively re\ufb01ne the bound-\ning boxes.\nMAD is a sensor-modality-agnostic module that we can\nadd to any detector as a subsequent re\ufb01nement stage. In\nour work we utilize CenterPoint [75], SAFDNet [80], HED-\nNet [81], FCOS3D [64], and BEVMap [6] as proposal net-\nworks.\nLong-Term Temporal Fusion for 3D Detection:\nVari-\nous works attempt to solve the shortcomings of sensor ag-\ngregation by learning to use multiple seconds of sensor ev-\nidence to improve object detection.\nIn this line of work, the scene-based paradigm uses\nrecurrent fusion of scene-level features [14, 17, 31, 75],\nwith some methods relying on multiple traversals of the\nscene [77]. A challenge of this approach is focusing and\naligning features from relevant and dynamic foreground\nobjects, which past works addressed by transforming fea-\nture maps, using segmentation to focus on foreground ob-\njects [17], and using deformable attention or convolution\nto align features of moving objects [22]. Processing both\nforeground and background areas can be computationally\nexpensive. It is worth noting that many of these methods\nare single-stage detectors [14, 17] and which we could use\nas a detection proposal network with MAD.\nAlternatively, the object-based paradigm focuses on the\nforeground by using detection proposals. Detect-track-fuse\nmethods are a sub-family of object-based methods that as-\nsociate previous detections over time to create tracks, and\nthese tracks summarize information from past sensor evi-\ndence [8, 20, 30, 30, 32, 79]. However, in complex situ-\nations like pedestrian crowds, association over time can be\ndif\ufb01cult due to heavy occlusions and erratic behavior, po-\ntentially leading to false negatives or identity switches in\nthe tracks. Object-to-scene approaches mitigate the short-\ncomings of association by directly using current detec-\ntion proposals to aggregate historical scene-level informa-\ntion using hand-crafted feature aggregation modules [15]\n1450\n\n=== Page 3 ===\nMemory Bank\nRetrieval\nAlignment\nFeature \nExtraction\n3D \nDetector\nDetection Proposals\nMemory Proposals\nProposal \nMerging\nRefinement Transformer\nMemory \nCross \nAttention\nFactorized \nSelf \nAttention\nProposal \nUpdate\nModel Outputs\n(Refined Proposals)\nMemory Bank Update\nMerged Proposals\nIn preparation for future inferences\nSensor Data\nTimestamp\nSDV pose\nFigure 2. MAD is a plug-and-play module that enhances any off-the-shelf 3D detector (kept frozen) with long-term memory.\nor attention mechanisms [82].\nThese approaches can be\ndif\ufb01cult to scale to long history horizons as they require\nre-processing past sensor evidence or dense feature maps\nbased on the current proposals (e.g., [15, 82] only use 0.7s\nof history). Finally, object-to-object methods use past ob-\nject detections to improve current object detections without\nexplicit tracking, e.g., by cross-attending from current de-\ntection proposals to past detections [63, 74], or using hand-\ncoded attention matrices based on distance [18]. Overall,\nmost object-based methods share some de\ufb01ciencies: Many\nonly re\ufb01ne detection proposals produced by current sen-\nsor evidence and struggle to recover from missing propos-\nals [8, 15, 18, 20, 74].\nOthers naively concatenate past\ndetections with current proposals [63], which can lead to\nalignment issues for dynamic objects and miss-calibration\nin the proposal scores, as the model should trust historical\nproposals less than current proposals.\nOur proposed method, MAD, performs object-based\ntemporal fusion without requiring explicit object associa-\ntion, aligns the memory in space and time by with trajectory\nforecasting, and can recover from missing proposals by us-\ning and rescoring proposals from the memory bank.\n3\nMemory Augmented 3D Object Detection\n3D object detectors take a short temporal window of sensor\ndata as input and produce a set of detections. Existing ap-\nproaches typically struggle to perceive occluded and distant\nobjects with limited sensor observations. To tackle these\nchallenges, we propose MAD, a plug-and-play module to\nenhance existing 3D object detectors with the ability to per-\nform long-horizon temporal fusion. Our only requirement\nfrom the detector is that each detection includes an object\nbounding box, multi-class con\ufb01dence scores, and a feature\nvector capturing local context. We demonstrate the general-\nity of MAD by augmenting and improving various LiDAR-\nbased and camera-based detectors.\nWe enable long-horizon temporal understanding through\na memory bank that captures all the relevant information on\nobjects, including where we expect them to move. These\ntrajectory forecasts allow us to align the memory objects\nwith the current detector proposals in space and time. Im-\nportantly, we do not require the object detector to provide\nmotion forecasts; instead, MAD computes them. To com-\npensate for ego-motion, we assume the ego is localized \u2014\nwhich is the norm in modern self-driving platforms [57, 68]\n\u2014 and store the ego pose in the memory along with the\nmodel outputs.\n3.1. Model\nWe start with an overview of our model; refer to Figure 2\nfor an illustration.\nAt every inference step, MAD takes\nas input the detection proposals, the current timestamp t\n(e.g., LiDAR sweep-end time or camera capture time), and\nthe ego pose Et in a global coordinate frame. It then re-\ntrieves objects from memory, aligns them spatially with Et\nand temporally to t, and extracts high-dimensional features\nfrom the aligned boxes and trajectory forecasts. We refer\nto the aligned boxes and trajectory forecasts with the ex-\ntracted features as memory proposals. A proposal merg-\ning mechanism then fuses detection and memory proposals\nby rescoring their con\ufb01dence scores and applying standard\npost-processing. Finally, our re\ufb01nement transformer itera-\ntive re\ufb01nes the object detections and trajectory forecasts in\nthe merged proposals with cross-attention to the memory\nand factorized self-attention. In preparation for future in-\nferences, the memory bank is then updated by appending\nthe model outputs (a.k.a. re\ufb01ned proposals) and removing\nolder model outputs to keep the memory bounded in size.\nProposal representation:\nWe de\ufb01ne object proposals\nP = (B, C, T, Q) with N bounding boxes B \u2191RN\u21927,\nwhere the last dimension corresponds to (x, y, z, l, w, h, \u03c9)\nwith object 3D centroids (x, y, z), headings \u03c9 in a BEV\nego-relative coordinate frame, and the 3D box dimensions\n(w, l, h); multi-class con\ufb01dence scores C \u2191[0, 1]N\u2192C,\nwhere C is the number of actor classes; trajectory fore-\ncasts T\n\u2191\nRN\u2192Tf \u21923 describing objects\u2019 BEV pose\n{(x, y, \u03c9)t+sf , . . . , (x, y, \u03c9)t+Tf sf } over Tf future way-\npoints at a time interval sf; and an object feature Q \u2191\nRN\u2192(Tf +1)\u2192d encoding both local and global features for\nevery object at the present and future timestamps, where d\n1451\n\n=== Page 4 ===\nis the feature dimensionality. We use superscripts to de-\nnote the source of the proposals: detection proposals Pdet\nfrom the 3D detector, memory proposals Pmem from the\nmemory bank, merged proposals Pmerge from the proposal\nmerging module, and re\ufb01ned proposals Pref from the out-\nput of the re\ufb01nement transformer. For detection proposals\nPdet, we generate Tdet by assuming the object is static over\ntime since detectors do not provide forecasts (and this is\njust an initialization before re\ufb01nement). The object features\nQdet are obtained by interpolating the feature map from be-\nfore the detector header at the projected object centroids,\nrepeating (Tf +1) times to get the features for future times-\ntamps, and adding a learned embedding of Bdet and Cdet.\nIn the paragraphs below we describe how we obtain Pmem,\nPmerge and Pref.\nMemory Bank and Retrieval:\nThe memory bank is a set\nof tuples\n!\ntm, Etm, Pref\ntm\n\"\nwith timestamped past model out-\nputs and ego pose, sorted by the timestamp tm at which the\noutputs were generated. During inference at timestamp t,\nwe retrieve memory entries Pref\ntm at a set of past target times-\ntamps tm \u2191Tm, where Tm = {t \u2193sm, t \u21932sm, . . . , t \u2193\nTmsm}. Tm is the number of past target timestamps, and\nsm is the time stride of the retrieved entries. To be precise,\nwe retrieve the closest memory entry to each timestamp in\nTm to be robust to small sensor delays.\nExtracting Memory Proposals:\nFor effective use of the\nmemory at inference, we should align each retrieved entry\n(tm, Etm, Pref\ntm ) in space and time with the current detec-\ntion proposals at time t. We handle ego-motion by applying\nthe relative transform Etm\u2191t = E\u21931\nt Etm to Bref\ntm and Tref\ntm.\nTo handle object motion, we linearly interpolate the stored\ntrajectory forecast to the current timestamp t to obtain the\nproposal box Bmem. To obtain the proposal forecast Tmem,\nwe also interpolate/extrapolate the stored trajectories as re-\nquired to obtain waypoints at Tf = {t + sf, . . . , t + Tfsf}\nfrom stored waypoints at {tm + sf, . . . , tm + Tfsf}.\nFinally, we extract latent features Qmem at t and every\nfuture time step tf \u2191Tf: First, we compute sinusoidal posi-\ntional embeddings [60] for the centroid coordinates Bmem\nx,y,z\nand encode them with a lightweight MLP. Separately, we\nconcatenate other features including Bmem\nl,w,h,\u03c9 (box dimen-\nsions and heading), con\ufb01dence scores Cmem, the memory\nage t \u2193tm, and a 2D vector pointing to where the pro-\nposal was in the current ego coordinate frame at the time\ntm. Finally, we encode the concatenated features with an-\nother MLP and add the features from both MLPs together.\nProposal Merging:\nThe memory and detection propos-\nals can be redundant, particularly in areas with good sen-\nsor coverage. To merge proposals, we learn to rescore their\nmulti-class con\ufb01dence scores. Rescoring is essential as the\ncon\ufb01dence the model should put in a memory proposal not\nonly depends on the con\ufb01dence score at a past timestamp\nCmem\ntm\n, but also on the proposal age t \u2193tm, as the fore-\ncasting uncertainty grows with the time horizon and other\nfactors. For example, the model should trust a fast-moving\ndetection less than a stationary object, or it should trust an\nobject observed 0.5 seconds ago more than one observed\n5 seconds ago. Furthermore, the detection proposals come\nfrom the 3D detector, while the memory proposals are pro-\nduced by MAD, and detectors have been found to be mis-\ncalibrated [24, 40, 42].\nTo make the scores comparable, we learn two small\nMLPs that separately map the features of the detection pro-\nposals Qdet and the memory proposals Qmem to new multi-\nclass scores Cmerge.\nAs explained in Section 3.2, these\nrescoring MLPs are trained under a single detection loss ap-\nplied to the merged proposals so that the model can decide\nwhich proposals to trust from both sources. Finally, we \ufb01lter\nproposals with score thresholding, non-maximum suppres-\nsion (NMS), and keep the top K merged proposals sorted\nby score (maximum over actor classes). Post-processing en-\nables the re\ufb01nement transformer to process a smaller num-\nber of queries.\nFinally, we add learned time positional embeddings to\nthe merged proposal features Qmerge to indicate the time of\nthe trajectory forecast. At this point, we have N merge def\n=\nN ref merged proposals Pmerge ready for re\ufb01nement.\nRe\ufb01nement Transformer:\nWe utilize a transformer de-\ncoder to re\ufb01ne the merged proposals Pmerge = Pref(0) it-\neratively over I blocks into Pref(1) ...Pref(I), where the\n\ufb01nal model outputs are Pref = Pref(I). We propose a novel\nmemory cross-attention mechanism to allow the queries \u2014\nproposal features Qref(i) \u2014 to aggregate information from\nall the memory proposals Qmem, including those that pro-\nposal merging \ufb01ltered out. We want to use this informa-\ntion in the re\ufb01nement transformer because multiple overlap-\nping memory proposals provide signi\ufb01cant evidence about\nan object\u2019s presence and location. To achieve this, we per-\nform cross attention from the object queries Qref(i) to the\nmemory proposal features Qmem. For ef\ufb01ciency, we limit\nthe cross attention to the nearest k keys to each object query\n(computing the nearest neighbors of Bref(i)\nx,y,z in Bmem\nx,y,z).\nSimilar to many works [4, 44], we perform factorized\nself-attention in each re\ufb01nement block, which separates\ntime self-attention and object self-attention for ef\ufb01ciency,\nwhere the former attends only to queries from the same ob-\nject (sequence length Tf + 1) and the latter only attends to\nqueries from the same time step (sequence length N). The\nupdated queries Qref(i+1) are input to the next block.\nFinally, we update the explicit proposal information\nas described in DeTra [4], by using a simple MLP to\nproduce Bref(i+1) and Cref(i+1) and a gated recurrent\nunit (GRU) [9] to update the future trajectory waypoints\nTref(i+1).\n1452\n\n=== Page 5 ===\nMemory Bank Update:\nWe post-process the re\ufb01ned pro-\nposals Pref as we did to the merged proposals: score thresh-\nolding, NMS, and top K based on con\ufb01dence score, adding\nthe result to the memory bank, along with the correspond-\ning timestamp t and ego pose Et. To limit the size of the\nmemory bank when running on long sequences, we remove\nany memory entries older than t\u2193Tmsm \u2193\u03b5 (the past time-\nhorizon used in memory retrieval with a small buffer \u03b5).\n3.2. Training\nWe \ufb01rst train an off-the-shelf 3D detector following their\noriginal training strategy. This stage can be omitted if a\npre-trained 3D detector is available. Then, we train all the\nparameters in MAD as a subsequent stage, with the 3D de-\ntector weights frozen. Pre-training and freezing the 3D de-\ntector is important to ensure the detection proposals do not\nchange throughout MAD training. Note that we train a sep-\narate MAD for each 3D detector, as each detector has differ-\nent features Qdet and detection distribution and calibration.\nBefore detailing our proposed MAD training, we discuss\nsome possibilities and trade-offs when training temporal fu-\nsion models. Training on unordered examples has the ad-\nvantage of satisfying the assumption of i.i.d examples (bet-\nter learning dynamics) [21, 50]. However, it differs from\nevaluation, where the model is rolled out on long sequences\nand consumes its previous outputs. Training on long se-\nquences of ordered data has the advantage of being closer to\nevaluation, but it has worse learning dynamics since consec-\nutive examples are heavily correlated (there are few changes\nin the scene from one frame to the next). If, instead, gradi-\nents are accumulated over a long sequence and used to up-\ndate the model parameters once per sequence, a sequence\nbecomes one example (satisfying the i.i.d assumption), but\nthe training duration is multiplied by the sequence length\nif the number of model updates is kept constant. Despite\nthis large space of possibilities and the importance of such\nchoices, prior works on learned temporal fusion neglect de-\ntails and discussion of their training recipe [15, 17, 20, 30].\nTo tackle these challenges, we design a novel training\nschedule. We propose to train MAD on increasingly long\nchunks of ordered data, using single frames1 at the begin-\nning and entire sequences at the end of training. To train ob-\nject memory on short chunks (or even single frames) of data\nwhile maintaining a reasonable amount of memory inputs,\nwe propose to maintain a cache of memory banks across\ntraining and using it to build the memory proposals for each\ntraining example. Below, we detail this proposed schedule,\nour cache of memory banks, how we handle augmentations\nwith memory, and our loss function.\nTraining Schedule:\nThe datasets we use (WOD [57],\nAV2 [68]) organize their data into driving logs, each around\n1We slightly abuse the term \u201cframe\u201d here, as some detectors use a win-\ndow of multiple past frames as input.\n20s in duration with data captured at 10Hz, meaning each\nlog has around 200 frames. Each log has a unique identi-\n\ufb01er (logID). For the \ufb01rst 25% of training, we sample single\nframes (that is, consecutive training examples are random\nframes from random logs). Throughout the rest of training,\nwe sample sequential chunks of gradually increasing size:\n48 frames for (25%, 50%] of training, 96 frames for (50%,\n75%], and 144 frames for (75%, 100%]. We train with a\nsingle cosine decay learning rate schedule with no resets.\nThe intuition behind this is that when the learning rate is\nhigh, and the model weights change the most, the model is\nexposed to more diverse data. Then, when the learning rate\nis lower, the model is tuned to be closer to the evaluation\nsetting, where it consumes its previous outputs.\nExploiting a Cache of Memory Banks:\nIf the schedule\ndescribed above is followed naively during the individual\nframe and short chunk training, the model cannot consume\nits previous outputs and thus would not learn to use memory\nduring this phase of training. To address this problem, we\nintroduce a cache of previous memory banks. This cache is\na mapping from the unique driving log identi\ufb01er logID to\na memory bank. At the start of training, we initialize the\ncache with empty memory banks for all logIDs. On a given\ntraining iteration, we index the cache with the logID of the\ncurrent training example to obtain the memory bank.\nIf\navailable, we retrieve the memory proposals from this mem-\nory bank as described in Sec. 3.1. We update the retrieved\nmemory bank at the end of the training iteration with the\nmodel outputs, replacing any existing entry with the same\ntimestamp. Note that during training we do not limit the\nsize of the memory bank.\nThere are a few challenges to training with the object\nmemory cache that we address:\n\u2022 To train these models ef\ufb01ciently on large datasets, we\nuse a distributed data training scheme, meaning we split\nexamples in the minibatch across multiple GPUs. Each\nGPU has a unique index called a rank. Each rank main-\ntains a separate cache to prevent the cache from \ufb01lling\nup the RAM and avoid synchronization costs. To guaran-\ntee high cache hit rates, we ensure that training examples\nfrom a given logID are always put on the same rank dur-\ning training.\n\u2022 The cache is \ufb01lled with MAD outputs, which are inac-\ncurate at the beginning of training. We do not want er-\nroneous model outputs to \ufb01ll the cache; otherwise, the\nmodel may not learn to trust the memory proposals. To\nmitigate this, we only start \ufb01lling the cache (and train-\ning with memory proposals) after 2.5% of training, after\nwhich performance is reasonable.\n\u2022 To make the model robust to variable latency and the pres-\nence and absence of memory proposals, we randomize the\ntarget timestamps Tm that we retrieve memory elements\nfor during training by randomly sampling the time stride\n1453\n\n=== Page 6 ===\nOverall L1\nOverall L2\nVehicle L1\nVehicle L2\nPedestrian L1\nPedestrian L2\nCyclist L1\nCyclist L2\nMethod\nAP\nAPH\nAP\nAPH\nAP\nAPH\nAP\nAPH\nAP\nAPH\nAP\nAPH\nAP\nAPH\nAP\nAPH\nCenterpoint 1f [75]\n76.1\n73.5\n70.0\n67.6\n75.7\n75.2\n67.9\n67.4\n77.6\n71.6\n70.1\n64.4\n74.9\n73.8\n72.1\n71.0\n+ MAD (Ours)\n82.9\n81.0\n77.6\n75.8\n81.1\n80.5\n74.0\n73.4\n83.8\n80.0\n77.2\n73.5\n83.8\n82.6\n81.6\n80.4\nCenterpoint 2f [75]\n77.5\n75.8\n71.7\n70.1\n76.4\n75.9\n68.7\n68.2\n79.2\n75.6\n71.9\n68.5\n76.8\n75.9\n74.4\n73.5\n+ MAD (Ours)\n82.8\n81.2\n77.5\n76.0\n81.4\n80.8\n74.3\n73.7\n84.7\n82.1\n78.2\n75.6\n82.2\n80.8\n80.1\n78.7\nHEDNet 1f [81]\n81.6\n79.7\n75.6\n73.7\n80.9\n80.5\n73.1\n72.7\n84.6\n80.2\n77.1\n72.8\n79.4\n78.5\n76.6\n75.6\n+ MAD (Ours)\n85.2\n83.3\n80.2\n78.3\n83.6\n82.9\n76.6\n76.0\n87.0\n83.4\n81.0\n77.4\n85.1\n83.7\n83.0\n81.6\nHEDNet 4f [81]\n83.6\n82.3\n78.1\n76.8\n82.4\n81.9\n75.1\n74.6\n86.3\n83.6\n79.4\n76.8\n82.2\n81.4\n79.9\n79.1\n+ MAD (Ours)\n85.5\n83.8\n80.6\n79.0\n83.6\n82.9\n76.8\n76.1\n87.7\n85.0\n81.9\n79.2\n85.1\n83.5\n83.2\n81.6\nSAFDNet 1f [80]\n81.7\n79.7\n75.5\n73.6\n80.5\n80.0\n72.5\n72.1\n84.7\n80.2\n77.1\n72.9\n79.8\n78.8\n76.9\n75.9\n+ MAD (Ours)\n85.3\n83.5\n80.3\n78.4\n83.4\n82.8\n76.5\n75.9\n86.8\n82.9\n80.7\n76.8\n85.8\n84.7\n83.7\n82.6\nSAFDNet 4f [80]\n83.9\n82.6\n78.4\n77.1\n82.8\n82.3\n75.4\n74.9\n86.8\n84.2\n80.1\n77.5\n82.0\n81.1\n79.6\n78.8\n+ MAD (Ours)\n85.8\n84.2\n81.0\n79.4\n84.2\n83.6\n77.4\n76.8\n87.9\n85.4\n82.2\n79.7\n85.2\n83.7\n83.3\n81.7\nTable 1. Comparing the performance of various off-the-shelf LiDAR object detectors with and without MAD on the WOD validation set.\nBase detector results are reproduced using of\ufb01cial code. MAD consistently boosts the performance of all detectors across all metrics.\nVehicle AP IoU 10 In Camera Field of View\nMethod\nOverall\n[0, 40)m\n[40, 80)m\n[80, 120)m\nFCOS3D [64]\n37.6\n73.9\n34.3\n4.65\n+MAD (Ours)\n43.6\n82.6\n40.0\n8.11\nBEVMap [6]\n51.5\n86.5\n54.0\n13.9\n+MAD (Ours)\n53.4\n88.0\n55.1\n17.2\nTable 2. Adding MAD to camera-based 3D detectors on AV2.\nsm, and the number of target timestamps Tm.\nHandling\nAugmentations\nwith\nMemory:\nPrior\nworks [26, 75, 80, 81] \ufb01nd that data augmentations\n(e.g., translation, rotation, \ufb02ipping, and re-scaling) are\nimportant for detection performance. We apply augmenta-\ntions to the boxes Bmem and trajectories Tmem in memory\nproposals after the memory alignment step.\nWe apply\nthe inverse of the augmentations to Bref and Tref before\nstoring them in the memory bank.\nLoss function:\nWe optimize a multi-task objective\nL = Lrescore(Cmerge) + #I\ni=1 Ldet(Bref(i), Cref(i)) +\nLfor(Tref(i)), which is a combination of a rescoring loss\nLscore, a detection re\ufb01nement loss Ldet, and a forecasting\nre\ufb01nement loss Lfor, where the detection and forecasting\nlosses are computed at every re\ufb01nement block.\nFollow-\ning [4], Ldet includes a binary focal loss for classi\ufb01cation,\nan L1 loss for regression and an IoU loss. To calculate the\ntargets for these losses, we \ufb01rst match the detections to the\nground truth bounding boxes through bipartite matching as\nproposed in DETR [3]. The rescoring loss is similar, ex-\ncept it consists only of the focal loss as we are only train-\ning the multi-class scores Cmerge output by the rescoring\nmodule. The trajectory re\ufb01nement loss is an L1 loss against\nthe ground-truth trajectory, supervised only for true-positive\ndetections (with IoU with a ground truth box higher than\nMethod\nAP L1\nAPH L1\nAP L2\nAPH L2\nCenterFormer [82]\n82.3\n80.9\n77.6\n76.3\nBEVFusion [36]\n82.7\n81.4\n77.7\n76.3\nMSF [15]\n83.1\n81.7\n78.3\n77.0\nFSD++ [13]\n83.5\n82.1\n78.4\n77.1\nLoGoNet [29]\n83.1\n81.8\n78.4\n77.1\nOctopus Noah\n83.1\n81.7\n78.7\n77.3\nSEED-L [38]\n83.5\n82.2\n78.7\n77.3\nLION [37]\n83.7\n82.4\n78.7\n77.4\nVeuronNet3D\n83.7\n82.2\n79.1\n77.7\nHIAC\n84.0\n82.6\n79.2\n77.8\nInceptioLidar\n83.8\n82.5\n79.2\n77.8\nVADet\n84.1\n82.8\n79.4\n78.2\nMT3D\n85.0\n83.7\n80.1\n78.7\nLIVOX Detection\n84.8\n83.5\n80.2\n79.0\nMAD (Ours)\n86.0\n84.3\n81.8\n80.2\nTable 3.\nResults on the WOD test set, as reported on the\nleaderboard2. We exclude entries that state they use ensembles,\ntest-time augmentations, or are of\ufb02ine (use future sensor data).\n\u201cOurs\u201d is using SAFDNet 4f as the 3D detector. APH L2 is the\nranking metric.\n0.5). See our supplementary for more details.\n4\nExperiments\nThis section provides a comprehensive quantitative analy-\nsis of MAD from three perspectives. First, we add MAD to\nexisting 3D detectors, showing signi\ufb01cant improvements.\nWe use both LiDAR-based and camera-based detectors on\nWOD [57] and AV2 [68], respectively. Second, we com-\npare the best version of MAD to the state-of-the-art meth-\nods on WOD, setting a new record on the WOD leaderboard\namong online methods without ensembles or test-time aug-\nmentation and outperforming prior learned temporal fusion\nmethods by a large margin. Finally, we conduct thorough\n1454\n\n=== Page 7 ===\nMethod\nAP L1\nAPH L1\nAP L2\nAPH L2\nValidation\nLEF [17]\n79.6\n79.2\n71.4\n70.9\nMoDAR [30]\n-\n-\n-\n72.5\nMPPNet [8]\n81.6\n81.1\n76.0\n74.8\nMSF [15]\n82.2\n80.7\n76.8\n75.5\nPTT [20]\n82.7\n80.7\n77.7\n75.7\nMAD (Ours)\n85.8\n84.2\n81.0\n79.4\nTesting\n3D-MAN [74]\n49.6\n48.1\n44.8\n43.4\nMPPNet [8]\n81.8\n80.6\n76.9\n75.7\nMSF [15]\n83.1\n81.7\n78.3\n77.0\nMAD (Ours)\n86.0\n84.3\n81.8\n80.2\nTable 4. Comparison of our method against various methods for\nlearned temporal fusion on WOD. \u201cOurs\u201d is using SAFDNet 4f.\nablation studies to understand the architectural choices that\nmake MAD effective and the impact of different training\nprocedures. Refer to our supplementary for more imple-\nmentation details, experimental results, and ablations.\nImplementation Details:\nThe re\ufb01nement transformer\nuses I = 3 re\ufb01nement blocks, and the dimension of all\nembeddings is d = 128.\nWe forecast Tf = 10 future\ntimestamps at stride of sf = 0.5s, yielding a 5s prediction\nhorizon. Unless otherwise stated, we use target timestamps\nof Tm = {\u21930.3s, \u21930.6s, . . . , \u21932.4s} (i.e., sm = 0.3s,\nTm = 8) for reading from the memory bank at inference.\nIn the memory cross attention we use the nearest k = 4\nneighbors.\nFollowing prior works [75, 80, 81], for any\ndetection post-processing, we use a 0.1 con\ufb01dence thresh-\nold; per-class NMS IoU thresholds of {0.75, 0.6, 0.55} for\nvehicles, pedestrians, and cyclists, respectively; and a top\nK = 500.\nMAD has 3.8M parameters, while the base\ndetectors have anywhere from 8M (Centerpoint [75]) to\n53M (BEVMap [6]) parameters. For each base detector,\nwe train MAD for 60k update steps (roughly equivalent to\n6 epochs on WOD and AV2), with batch size 16. We use\na cosine learning rate decay with a max learning rate of\n8 \u219410\u21934, and a linear warm-up for the \ufb01rst 1000 steps,\nbeginning with a learning rate of 8 \u219410\u21935. During train-\ning, we use a variable set of memory target timestamps\nTm \u2197uniform({6, 7, 8, 9, 10}) with a variable stride\nsm \u2197uniform({0.2s, 0.3s, 0.4s}).\nMetrics:\nWe report the detection metrics from the of\ufb01-\ncial WOD leaderboard [57], which include average preci-\nsion (AP) and AP weighted by heading error (APH) for ve-\nhicles (Veh.), pedestrians (Ped.), and cyclists (Cyc.). These\nmetrics use intersection-over-union (IoU) thresholds of 0.7,\n0.5, and 0.5, respectively. The metrics are broken down into\ntwo levels of dif\ufb01culty: Level 1 (L1) includes only labels\nthat have > 5 LiDAR points and are not marked as \u201chard\u201d,\nand Level (L2) includes all boxes that have > 0 LiDAR\npoints (a superset of L1). For camera experiments on AV2,\nwe report the mean average precision (AP) for vehicles in\nthe camera \ufb01eld of view at an IoU threshold of 0.1. We re-\nport the macro-average over all classes if the actor class is\nnot speci\ufb01ed.\nAugmenting off-the-shelf 3D Detectors with MAD:\nTab. 1 and Tab. 2 show the performance of MAD ap-\nplied to off-the-shelf 3D detectors on WOD and AV2, re-\nspectively.To show the generality of our approach, we ex-\nperiment with multiple base detectors trained on different\ndatasets and sensor modalities. We enhanced three LiDAR-\nbased methods with MAD on WOD: CenterPoint [75] with\nboth 1 LiDAR frame (1f) and 2 LiDAR frames (2f) as in-\nput, HEDNet [81] (1f and 4f), and SAFDNet [80] (1f and\n4f). We follow their of\ufb01cial protocols to train and eval-\nuate all models from scratch (due to the Waymo Dataset\nLicense Agreement, we cannot simply re-use pre-trained\nmodels). We also enhance two camera-based methods on\nAV2, FCOS3D [64] and BEVMap [6], which takes the\nmost recent image from the front camera as input.\nWe\nuse the of\ufb01cial implementation for both FCOS3D [64] and\nBEVMap [6]. Training details are in the supplementary.\nOur model brings signi\ufb01cant improvements to all detec-\ntors on both datasets. These gains are largest for single-\nframe detectors, where the memory provides the most ad-\nditional information.\nThe fact that the MAD-augmented\nsingle-frame detectors are better than the multi-frame detec-\ntors clearly shows the effectiveness of our method relative\nto the common point aggregation approach. Please visit the\nsupplementary materials for qualitative comparisons.\nComparison against SOTA:\nBy augmenting SAFDNet\n4f with MAD, we show in Tab. 3 that we achieve the best\nperformance on the WOD leaderboard2, among all online\nmethods that do not use ensembles or test-time augmenta-\ntion. Table 4 compares MAD to prior learned temporal fu-\nsion methods on the WOD validation and test set, where we\nachieve substantial gains. Please refer to our supplementary\nfor full Tabs. 3 and 4 with metrics for all actor classes.\nEffect of memory proposals and memory attention:\nWe ablate the different components of our memory pipeline\nin Tab. 5. Comparing rows 1, 2, and 5 shows that both the\nproposed memory attention and memory proposals have a\npositive effect. This is intuitive as the memory proposals\nlet MAD recover from false negative detection proposals,\nwhich is complementary to memory cross-attention that al-\nlows MAD to use all memory information for re\ufb01nement\n(bypassing the \ufb01ltering in proposal-merging).\nEffect of forecasting:\nComparing rows 3 and 5 in Tab. 5,\nwe \ufb01nd that using trajectory forecasting to align memory\nproposals to the current time is important, particularly for\nfast-moving objects. Without forecasting, the memory pro-\nposals from previous frames will be far from the current po-\n2https://waymo.com/open/challenges/2020/3d-detection/ as of submis-\nsion (14/11/2024)\n1455\n\n=== Page 8 ===\nMem. Prop.\nMem. Attn.\nForecast.\nRescore.\nVeh. AP\nPed. AP\nCyc. AP\nVeh. AP [20, 30) m/s\nCyc. AP [5, 10) m/s\n0\n\u2701\n\u2701\n\u2701\n\u2701\n75.4\n80.1\n79.6\n38.0\n72.4\n1\n\u2701\n\u2702\n\u2702\n\u2702\n76.5\n81.3\n82.0\n40.4\n82.1\n2\n\u2702\n\u2701\n\u2702\n\u2702\n75.8\n81.7\n81.6\n37.7\n75.9\n3\n\u2702\n\u2702\n\u2701\n\u2702\n76.9\n81.8\n81.9\n34.6\n78.7\n4\n\u2702\n\u2702\n\u2702\n\u2701\n72.7\n82.0\n81.2\n11.9\n73.9\n5\n\u2702\n\u2702\n\u2702\n\u2702\n77.0\n82.3\n83.3\n45.2\n86.2\nTable 5. Component ablation of MAD on the WOD validation set. All metrics are L2. Row 0 is the base 3D detector, SAFDNet 4f [80].\nAll ablations in this table (including the \ufb01nal method with all components) use a reduced training duration of 45k iterations to reduce costs.\nEvaluated Proposals\nAP L1\nAPH L1\nAP L2\nAPH L2\nDetection\n!\nPdet\"\n83.9\n82.6\n78.4\n77.1\nCombined\n!\nPmem \u2192Pdet\"\n18.1\n17.8\n16.8\n16.4\nMerged (Pmerge)\n84.1\n83.0\n78.8\n77.7\nAfter Block 0\n!\nPref(1)\"\n84.4\n83.2\n79.3\n78.1\nAfter Block 1\n!\nPref(2)\"\n85.8\n84.2\n80.9\n79.3\nAfter Block 2\n!\nPref\"\n85.8\n84.2\n81.0\n79.4\nTable 6. Evaluating various intermediate proposals from MAD.\nThe base detector is SAFDNet 4f.\nsition of those objects, making it challenging for the model\nto leverage the memory effectively.\nEffect of learned proposal merging:\nComparing rows 0,\n4, and 5 of Tab. 5 we \ufb01nd the proposed learned rescoring of\nthe merged detection and memory proposals is crucial for\ngood performance. Without it, MAD cannot enhance the\nbase detector (row 0) because the proposal scores from the\n3D detector and memory are miss-calibrated before being\npost-processed in the proposal merging step (i.e., NMS).\nWe illustrate this in Tab. 6, where we evaluate intermedi-\nate proposals of MAD: (1) the detection proposals Pdet,\n(2) naively taking the union of the detection proposals Pdet\nand memory proposals Pmem and post-processing them, (3)\nthe merged proposals Pmerge (which have been rescored),\nand (4) after each block of the re\ufb01nement transformer\nPref(1), . . . , Pref(I). Naively concatenating the combined\nproposals is much worse than the base detector because of\nthe miss-calibrated scores. After proposal merging, Pmerge\nalready improves over the base detector. Each re\ufb01nement\nblock brings further gains, illustrating the strength of our\nproposed re\ufb01nement transformer.\nTraining Procedure Study:\nTable 7 provides evidence\nsupporting the effectiveness of our proposed training sched-\nule. We \ufb01rst train MAD with three different chunk sizes\n(i.e., sequences with {144, 48, 1} frames), each with and\nwithout the memory bank cache. Training with long chunks\n(144 frames, Tab. 7.1a) provides good performance because\nthere is a low gap between training and evaluation. The\ncache provides no gains in this setting (Tab. 7.1b) because\nthe model already has memory proposals in most frames.\nTraining with shorter chunks (Tab. 7.2a,3a) performs worse\nbecause there is a more signi\ufb01cant gap between training\nand evaluation. Including the cache helps signi\ufb01cantly by\nChunk Length\nCache\nAP L1\nAPH L1\nAP L2\nAPH L2\n1a\n144\n\u2701\n85.4\n83.9\n80.5\n79.0\n1b\n144\n\u2702\n85.2\n83.7\n80.3\n78.9\n2a\n48\n\u2701\n85.1\n83.5\n80.2\n78.6\n2b\n48\n\u2702\n85.3\n83.8\n80.3\n78.9\n3a\n1\n\u2701\n83.9\n82.4\n78.4\n77.0\n3b\n1\n\u2702\n85.0\n83.3\n80.1\n78.5\n4a\n1)48)96)144\n\u2701\n84.9\n83.3\n79.9\n78.3\n4b\n1)48)96)144\n\u2702\n85.8\n84.2\n81.0\n79.4\nTable 7. Ablating chunk length and the memory cache on WOD,\nusing SAFDNet 4f.\nclosing the gap to evaluation but does not fully reach the\nlong chunk performance (Tab. 7.2b,3b). As hypothesized\nin Sec. 3.2, there is room for improvement by training with\nour proposed schedule and memory bank cache (Tab. 7.4b).\nThis strategy allows MAD to learn generalized patterns\nover a diverse set of examples quickly by training on short\nchunks (more i.i.d. data) at the beginning when the learn-\ning rate is higher while re\ufb01ning its understanding on long\nchunks (closer to the deployment setting) towards the end\nwhen the learning rate is lower. Table 7.4a shows the impor-\ntance of the cache when using this training schedule; other-\nwise, training with small chunks is ineffective as the model\nwould not learn to use the memory.\n5\nConclusion\nIn this paper, we propose MAD \u2014 a simple, effective, and\nsensor-modality-agnostic add-on for enhancing any existing\n3D object detector with long-term memory. To achieve this,\nwe design a transformer-based model that uses joint detec-\ntion and trajectory forecasting to populate a memory bank\nwith spatial-temporal object trajectories. Our model can ef-\nfectively fuse memory proposals with detection proposals\nby reading previous memory entries and aligning them with\nthe current time and ego pose. We also propose a novel\ntraining strategy that increases data diversity while keep-\ning the training-to-inference gap low. Our approach is very\ngeneral \u2014 bringing impressive improvements to a variety\nof LiDAR-based and camera-based detectors, and very ef-\nfective \u2014 achieving SOTA performance on Waymo Open\nDataset when paired to the base detector SAFDNet 4f [80].\n1456\n\n=== Page 9 ===\nReferences\n[1] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom.\nnuscenes: A multi-\nmodal dataset for autonomous driving. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11621\u201311631, 2020. 1\n[2] Yigit Baran Can, Alexander Liniger, Ozan Unal, Danda\nPaudel, and Luc Van Gool. Understanding bird\u2019s-eye view\nof road semantics using an onboard camera. IEEE Robotics\nand Automation Letters, 7(2):3302\u20133309, 2022. 2\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European confer-\nence on computer vision, pages 213\u2013229. Springer, 2020. 6,\n1\n[4] Sergio Casas, Ben Agro, Jiageng Mao, Thomas Gilles,\nAlexander Cui, Thomas Li, and Raquel Urtasun. Detra: A\nuni\ufb01ed model for object detection and trajectory forecasting.\n2024. 2, 4, 6, 5\n[5] Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir\nAnguelov.\nMultipath: Multiple probabilistic anchor tra-\njectory hypotheses for behavior prediction. arXiv preprint\narXiv:1910.05449, 2019. 2\n[6] Mincheol Chang, Seokha Moon, Reza Mahjourian, and\nJinkyu Kim. Bevmap: Map-aware bev modeling for 3d per-\nception. In Proceedings of the IEEE/CVF Winter Confer-\nence on Applications of Computer Vision, pages 7419\u20137428,\n2024. 2, 6, 7, 1\n[7] Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Andrew G\nBerneshawi, Huimin Ma, Sanja Fidler, and Raquel Urtasun.\n3d object proposals for accurate object class detection. Ad-\nvances in neural information processing systems, 28, 2015.\n2\n[8] Xuesong Chen, Shaoshuai Shi, Benjin Zhu, Ka Chun Che-\nung, Hang Xu, and Hongsheng Li. Mppnet: Multi-frame\nfeature intertwining with proxy points for 3d temporal ob-\nject detection. In European Conference on Computer Vision,\npages 680\u2013697. Springer, 2022. 1, 2, 3, 7\n[9] Kyunghyun Cho. Learning phrase representations using rnn\nencoder-decoder for statistical machine translation.\narXiv\npreprint arXiv:1406.1078, 2014. 4, 1\n[10] Alexander Cui, Sergio Casas, Kelvin Wong, Simon Suo, and\nRaquel Urtasun. Gorela: Go relative for viewpoint-invariant\nmotion forecasting.\nIn 2023 IEEE International Confer-\nence on Robotics and Automation (ICRA), pages 7801\u20137807.\nIEEE, 2023. 2, 5\n[11] Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou,\nTsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schnei-\nder, and Nemanja Djuric. Multimodal trajectory predictions\nfor autonomous driving using deep convolutional networks.\nIn 2019 international conference on robotics and automation\n(icra), pages 2090\u20132096. IEEE, 2019. 2\n[12] Martin Engelcke, Dushyant Rao, Dominic Zeng Wang,\nChi Hay Tong, and Ingmar Posner.\nVote3deep: Fast ob-\nject detection in 3d point clouds using ef\ufb01cient convolutional\nneural networks. In 2017 IEEE International Conference on\nRobotics and Automation (ICRA), pages 1355\u20131361. IEEE,\n2017. 2\n[13] Lue Fan, Yuxue Yang, Feng Wang, Naiyan Wang, and\nZhaoxiang Zhang. Super sparse 3d object detection. IEEE\ntransactions on pattern analysis and machine intelligence,\n45(10):12490\u201312505, 2023. 6, 3\n[14] Davi Frossard, Shun Da Suo, Sergio Casas, James Tu, and\nRaquel Urtasun. Strobe: Streaming object detection from li-\ndar packets. In Conference on Robot Learning, pages 1174\u2013\n1183. PMLR, 2021. 1, 2\n[15] Chenhang He, Ruihuang Li, Yabin Zhang, Shuai Li, and Lei\nZhang. Msf: Motion-guided sequential fusion for ef\ufb01cient\n3d object detection from point cloud sequences. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5196\u20135205, 2023. 1, 2, 3, 5, 6, 7\n[16] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961\u20132969, 2017. 2\n[17] Tong He, Pei Sun, Zhaoqi Leng, Chenxi Liu, Dragomir\nAnguelov, and Mingxing Tan.\nLef: Late-to-early tempo-\nral fusion for lidar 3d object detection. In 2023 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems\n(IROS), pages 1637\u20131644. IEEE, 2023. 1, 2, 5, 7, 3\n[18] Jinghua Hou, Zhe Liu, Zhikang Zou, Xiaoqing Ye, Xiang\nBai, et al. Query-based temporal fusion with explicit mo-\ntion for 3d object detection. Advances in Neural Information\nProcessing Systems, 36, 2024. 1, 3\n[19] Anthony Hu, Zak Murez, Nikhil Mohan, Sof\u00b4\u0131a Dudas, Jef-\nfrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and\nAlex Kendall. Fiery: Future instance prediction in bird\u2019s-\neye view from surround monocular cameras. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 15273\u201315282, 2021. 2\n[20] Kuan-Chih Huang, Weijie Lyu, Ming-Hsuan Yang, and\nYi-Hsuan Tsai.\nPtt: Point-trajectory transformer for ef-\n\ufb01cient temporal 3d object detection.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14938\u201314947, 2024. 1, 2, 3, 5, 7\n[21] Jack Kiefer and Jacob Wolfowitz. Stochastic estimation of\nthe maximum of a regression function. The Annals of Math-\nematical Statistics, pages 462\u2013466, 1952. 5\n[22] Junho Koh, Junhyung Lee, Youngwoo Lee, Jaekyum Kim,\nand Jun Won Choi.\nMgtanet: Encoding sequential lidar\npoints using long short-term motion-guided temporal atten-\ntion for 3d object detection. In Proceedings of the AAAI Con-\nference on Arti\ufb01cial Intelligence, pages 1179\u20131187, 2023. 2\n[23] Jason Ku, Melissa Mozi\ufb01an, Jungwook Lee, Ali Harakeh,\nand Steven L Waslander. Joint 3d proposal generation and\nobject detection from view aggregation. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems\n(IROS), pages 1\u20138. IEEE, 2018. 2\n[24] Fabian Kuppers, Jan Kronenberger, Amirhossein Shantia,\nand Anselm Haselhoff. Multivariate con\ufb01dence calibration\nfor object detection. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition workshops,\npages 326\u2013327, 2020. 4\n[25] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\n1457\n\n=== Page 10 ===\nfor object detection from point clouds. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 12697\u201312705, 2019. 2\n[26] Zhaoqi Leng, Guowang Li, Chenxi Liu, Ekin Dogus Cubuk,\nPei Sun, Tong He, Dragomir Anguelov, and Mingxing Tan.\nLidar augment: Searching for scalable 3d lidar data augmen-\ntations. In 2023 IEEE International Conference on Robotics\nand Automation (ICRA), pages 7039\u20137045. IEEE, 2023. 6\n[27] Bo Li. 3d fully convolutional network for vehicle detection\nin point cloud. In 2017 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), pages 1513\u20131518.\nIEEE, 2017. 2\n[28] Hongyang Li, Hao Zhang, Zhaoyang Zeng, Shilong Liu,\nFeng Li, Tianhe Ren, and Lei Zhang. Dfa3d: 3d deformable\nattention for 2d-to-3d feature lifting. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 6684\u20136693, 2023. 2\n[29] Xin Li, Tao Ma, Yuenan Hou, Botian Shi, Yuchen Yang,\nYouquan Liu, Xingjiao Wu, Qin Chen, Yikang Li, Yu Qiao,\net al.\nLogonet:\nTowards accurate 3d object detection\nwith local-to-global cross-modal fusion. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17524\u201317534, 2023. 6\n[30] Yingwei Li, Charles R Qi, Yin Zhou, Chenxi Liu, and\nDragomir Anguelov. Modar: Using motion forecasting for\n3d object detection in point cloud sequences. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9329\u20139339, 2023. 1, 2, 5, 7, 3\n[31] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-\nhao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:\nLearning bird\u2019s-eye-view representation from multi-camera\nimages via spatiotemporal transformers. In European con-\nference on computer vision, pages 1\u201318. Springer, 2022. 1,\n2\n[32] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu,\nSergio Casas, and Raquel Urtasun. Pnpnet: End-to-end per-\nception and prediction with tracking in the loop. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11553\u201311562, 2020. 1, 2\n[33] Xuewu Lin, Tianwei Lin, Zixiang Pei, Lichao Huang, and\nZhizhong Su.\nSparse4d:\nMulti-view 3d object detec-\ntion with sparse spatial-temporal fusion.\narXiv preprint\narXiv:2211.10581, 2022. 2\n[34] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.\nPetr: Position embedding transformation for multi-view 3d\nobject detection. In European Conference on Computer Vi-\nsion, pages 531\u2013548. Springer, 2022.\n[35] Yingfei Liu, Junjie Yan, Fan Jia, Shuailin Li, Aqi Gao, Tian-\ncai Wang, and Xiangyu Zhang. Petrv2: A uni\ufb01ed framework\nfor 3d perception from multi-camera images. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 3262\u20133272, 2023. 2\n[36] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,\nHuizi Mao, Daniela L Rus, and Song Han. Bevfusion: Multi-\ntask multi-sensor fusion with uni\ufb01ed bird\u2019s-eye view repre-\nsentation. In 2023 IEEE international conference on robotics\nand automation (ICRA), pages 2774\u20132781. IEEE, 2023. 6, 3\n[37] Zhe Liu, Jinghua Hou, Xinyu Wang, Xiaoqing Ye, Jingdong\nWang, Hengshuang Zhao, and Xiang Bai.\nLion: Linear\ngroup rnn for 3d object detection in point clouds.\narXiv\npreprint arXiv:2407.18232, 2024. 6, 3\n[38] Zhe Liu, Jinghua Hou, Xiaoqing Ye, Tong Wang, Jingdong\nWang, and Xiang Bai. Seed: A simple and effective 3d detr\nin point clouds. arXiv preprint arXiv:2407.10749, 2024. 6,\n3\n[39] Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furi-\nous: Real time end-to-end 3d detection, tracking and motion\nforecasting with a single convolutional net. In Proceedings of\nthe IEEE conference on Computer Vision and Pattern Recog-\nnition, pages 3569\u20133577, 2018. 1\n[40] Matthias Minderer,\nJosip Djolonga,\nRob Romijnders,\nFrances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran,\nand Mario Lucic. Revisiting the calibration of modern neu-\nral networks. Advances in Neural Information Processing\nSystems, 34:15682\u201315694, 2021. 4\n[41] Arsalan Mousavian, Dragomir Anguelov, John Flynn, and\nJana Kosecka. 3d bounding box estimation using deep learn-\ning and geometry. In Proceedings of the IEEE conference\non Computer Vision and Pattern Recognition, pages 7074\u2013\n7082, 2017. 2\n[42] Lukas Neumann, Andrew Zisserman, and Andrea Vedaldi.\nRelaxed softmax: Ef\ufb01cient con\ufb01dence auto-calibration for\nsafe pedestrian detection. 2018. 4\n[43] Jiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang,\nYuning Chai, Pei Sun, Yin Zhou, Xi Yi, Ouais Al-\nsharif, Patrick Nguyen, et al.\nStarnet: Targeted compu-\ntation for object detection in point clouds. arXiv preprint\narXiv:1908.11069, 2019. 2\n[44] Jiquan Ngiam, Benjamin Caine, Vijay Vasudevan, Zheng-\ndong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, Rebecca\nRoelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, et al.\nScene transformer: A uni\ufb01ed architecture for predicting mul-\ntiple agent trajectories.\narXiv preprint arXiv:2106.08417,\n2021. 4, 2\n[45] Tung Phan-Minh, Elena Corina Grigore, Freddy A Boulton,\nOscar Beijbom, and Eric M Wolff. Covernet: Multimodal\nbehavior prediction using trajectory sets. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 14074\u201314083, 2020. 2\n[46] AJ Piergiovanni, Vincent Casser, Michael S Ryoo, and\nAnelia Angelova. 4d-net for learned multi-modal alignment.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 15435\u201315445, 2021. 1, 2\n[47] Rui Qian, Divyansh Garg, Yan Wang, Yurong You, Serge\nBelongie, Bharath Hariharan, Mark Campbell, Kilian Q\nWeinberger, and Wei-Lun Chao.\nEnd-to-end pseudo-lidar\nfor image-based 3d object detection.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 5881\u20135890, 2020. 2\n[48] Cody Reading, Ali Harakeh, Julia Chae, and Steven L\nWaslander.\nCategorical depth distribution network for\nmonocular 3d object detection.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8555\u20138564, 2021. 2\n1458\n\n=== Page 11 ===\n[49] Shaoqing Ren.\nFaster r-cnn:\nTowards real-time object\ndetection with region proposal networks.\narXiv preprint\narXiv:1506.01497, 2015. 2\n[50] Herbert Robbins and Sutton Monro. A stochastic approxi-\nmation method. The annals of mathematical statistics, pages\n400\u2013407, 1951. 5\n[51] Thomas Roddick, Alex Kendall, and Roberto Cipolla. Ortho-\ngraphic feature transform for monocular 3d object detection.\narXiv preprint arXiv:1811.08188, 2018. 2\n[52] Avishkar Saha, Oscar Mendez, Chris Russell, and Richard\nBowden. Translating images into maps. In 2022 Interna-\ntional conference on robotics and automation (ICRA), pages\n9200\u20139206. IEEE, 2022. 2\n[53] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr-\ncnn: 3d object proposal generation and detection from point\ncloud. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 770\u2013779, 2019.\n2\n[54] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural net-\nwork for 3d object detection in a point cloud. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 1711\u20131719, 2020. 2\n[55] Martin Simony, Stefan Milzy, Karl Amendey, and Horst-\nMichael Gross. Complex-yolo: An euler-region-proposal for\nreal-time 3d object detection on point clouds. In Proceed-\nings of the European conference on computer vision (ECCV)\nworkshops, pages 0\u20130, 2018. 2\n[56] Shuran Song and Jianxiong Xiao. Deep sliding shapes for\namodal 3d object detection in rgb-d images.\nIn Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 808\u2013816, 2016. 2\n[57] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\nYuning Chai, Benjamin Caine, et al. Scalability in perception\nfor autonomous driving: Waymo open dataset. In CVPR,\n2020. 2, 3, 5, 6, 7\n[58] Pei Sun, Mingxing Tan, Weiyue Wang, Chenxi Liu, Fei Xia,\nZhaoqi Leng, and Dragomir Anguelov. Swformer: Sparse\nwindow transformer for 3d object detection in point clouds.\nIn European Conference on Computer Vision, pages 426\u2013\n442. Springer, 2022. 1\n[59] Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivas-\ntava, Khaled S Refaat, Nigamaa Nayakanti, Andre Cornman,\nKan Chen, Bertrand Douillard, Chi Pang Lam, Dragomir\nAnguelov, et al.\nMultipath++: Ef\ufb01cient information fu-\nsion and trajectory aggregation for behavior prediction. In\n2022 International Conference on Robotics and Automation\n(ICRA), pages 7814\u20137821. IEEE, 2022. 2\n[60] A Vaswani. Attention is all you need. Advances in Neural\nInformation Processing Systems, 2017. 4\n[61] Dominic Zeng Wang and Ingmar Posner. Voting for voting\nin online point cloud object detection. In Robotics: science\nand systems, pages 10\u201315. Rome, Italy, 2015. 2\n[62] Haiyang Wang, Chen Shi, Shaoshuai Shi, Meng Lei, Sen\nWang, Di He, Bernt Schiele, and Liwei Wang. Dsvt: Dy-\nnamic sparse voxel transformer with rotated sets. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 13520\u201313529, 2023. 1\n[63] Shihao Wang, Yingfei Liu, Tiancai Wang, Ying Li, and Xi-\nangyu Zhang. Exploring object-centric temporal modeling\nfor ef\ufb01cient multi-view 3d object detection. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 3621\u20133631, 2023. 3\n[64] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin.\nFcos3d: Fully convolutional one-stage monocular 3d object\ndetection.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 913\u2013922, 2021. 2, 6,\n7, 1\n[65] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hari-\nharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-\nlidar from visual depth estimation: Bridging the gap in 3d\nobject detection for autonomous driving. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8445\u20138453, 2019. 2\n[66] Yue Wang, Alireza Fathi, Abhijit Kundu, David A Ross,\nCaroline Pantofaru, Tom Funkhouser, and Justin Solomon.\nPillar-based object detection for autonomous driving.\nIn\nComputer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part XXII\n16, pages 18\u201334. Springer, 2020. 2\n[67] Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang,\nYilun Wang, Hang Zhao, and Justin Solomon.\nDetr3d:\n3d object detection from multi-view images via 3d-to-2d\nqueries. In Conference on Robot Learning, pages 180\u2013191.\nPMLR, 2022. 2\n[68] Benjamin Wilson, William Qi, Tanmay Agarwal, John\nLambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen\nPan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel\nPontes, et al.\nArgoverse 2: Next generation datasets for\nself-driving perception and forecasting.\narXiv preprint\narXiv:2301.00493, 2023. 2, 3, 5, 6\n[69] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xi-\nhui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang\nZhao. Point transformer v3: Simpler faster stronger. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4840\u20134851, 2024. 2\n[70] Bin Xu and Zhenzhong Chen. Multi-level fusion based 3d\nobject detection from monocular images. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 2345\u20132353, 2018. 2\n[71] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed-\nded convolutional detection. Sensors, 18(10):3337, 2018. 2,\n1\n[72] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-\ntime 3d object detection from point clouds. In Proceedings of\nthe IEEE conference on Computer Vision and Pattern Recog-\nnition, pages 7652\u20137660, 2018. 2\n[73] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia.\n3dssd:\nPoint-based 3d single stage object detector. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11040\u201311048, 2020. 2\n[74] Zetong Yang, Yin Zhou, Zhifeng Chen, and Jiquan Ngiam.\n3d-man: 3d multi-frame attention network for object de-\ntection.\nIn Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 1863\u20131872,\n2021. 1, 3, 7\n1459\n\n=== Page 12 ===\n[75] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-\nbased 3d object detection and tracking. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11784\u201311793, 2021. 1, 2, 6, 7\n[76] Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg,\nGeoff Pleiss, Bharath Hariharan, Mark Campbell, and Kil-\nian Q Weinberger.\nPseudo-lidar++: Accurate depth for\n3d object detection in autonomous driving. arXiv preprint\narXiv:1906.06310, 2019. 2\n[77] Yurong You, Katie Z Luo, Xiangyu Chen, Junan Chen, Wei-\nLun Chao, Wen Sun, Bharath Hariharan, Mark Campbell,\nand Kilian Q Weinberger.\nHindsight is 20/20: Leverag-\ning past traversals to aid 3d perception.\narXiv preprint\narXiv:2203.11405, 2022. 2\n[78] Yihan Zeng, Da Zhang, Chunwei Wang, Zhenwei Miao, Ting\nLiu, Xin Zhan, Dayang Hao, and Chao Ma. Lift: Learning\n4d lidar image fusion transformer for 3d object detection.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 17172\u201317181, 2022.\n1, 2\n[79] Ce Zhang, Chengjie Zhang, Yiluan Guo, Lingji Chen, and\nMichael Happold.\nMotiontrack: end-to-end transformer-\nbased multi-object tracking with lidar-camera fusion. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 151\u2013160, 2023. 2\n[80] Gang Zhang, Junnan Chen, Guohuan Gao, Jianmin Li, Si\nLiu, and Xiaolin Hu. Safdnet: A simple and effective net-\nwork for fully sparse 3d object detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14477\u201314486, 2024. 2, 6, 7, 8, 1\n[81] Gang Zhang, Chen Junnan, Guohuan Gao, Jianmin Li, and\nXiaolin Hu. Hednet: A hierarchical encoder-decoder net-\nwork for 3d object detection in point clouds. Advances in\nNeural Information Processing Systems, 36, 2024. 2, 6, 7, 1\n[82] Zixiang Zhou, Xiangchen Zhao, Yu Wang, Panqu Wang, and\nHassan Foroosh. Centerformer: Center-based transformer\nfor 3d object detection. In European Conference on Com-\nputer Vision, pages 496\u2013513. Springer, 2022. 2, 3, 6\n1460\n",
      "searchable_content": "page 1 mad memory-augmented detection of 3d objects ben agro sergio casas patrick wang thomas gilles raquel urtasun tgilles urtasun waabi.ai a 3d detector b object memory c memory augmented detections 0.0s -2.4s memory age false negative figure 1. detectors without long-term temporal fusion a miss heavily occluded objects. our approach enhances detectors b to remember past predictions c recovering from occlusion. detections are in green labels are in black lidar points are in gray. abstract to perceive humans use memory to \ufb01ll in gaps caused by our limited visibility whether due to occlusion or our nar- row \ufb01eld of view. however most 3d object detectors are limited to using sensor evidence from a short temporal win- dow 0.1s-0.3s . in this work we present a simple and ef- fective add-on for enhancing any existing 3d object detec- tor with long-term memory regardless of its sensor modality e.g. lidar camera and network architecture. we pro- pose a model to effectively align and fuse object proposals from a detector with object proposals from a memory bank of past predictions exploiting trajectory forecasts to align proposals across time. we propose a novel schedule to train our model on temporal data that balances data diversity and the gap between training and inference. by applying our method to existing lidar and camera-based detectors on the waymo open dataset wod and argoverse 2 sen- sor av2 dataset we demonstrate signi\ufb01cant improvements in detection performance 2.5 to 7.6 ap points . our method attains the best performance on the wod 3d de- tection leaderboard among online methods excluding en- sembles or test-time augmentation . 1 introduction most self-driving vehicles sdvs utilize a 3d object detec- tor to recognize and localize objects in 3d space. this task is challenging due to occlusion large intra-class variability and distant objects which typically have limited sensor ob- servations. to overcome these challenges human drivers rely on their memory. for example they may drive more cautiously when remembering a previously observed but now occluded cyclist who may suddenly enter the road. a common approach for improving 3d object detectors is to aggregate a short temporal window of past sensor ob- servations. towards this goal most existing lidar-based methods transform a short buffer of sensor data into the cur- rent ego sdv coordinate frame to align past and current evidence 1 39 58 74 75 . similarly camera-based meth- ods stack multiple images 46 78 as input to existing ar- chitectures. these methods cannot handle long temporal sequences due to computational and memory constraints. moreover temporal stacks of 3d bird s-eye-view bev representations like point clouds or lifted camera features require a large receptive \ufb01eld especially for fast-moving objects 30 further increasing computational burden. there is a growing interest in long-term temporal fusion. scene-level memory approaches 14 17 31 recurrently fuse scene-level features but they can struggle to capture relevant foreground objects. other approaches associate ob- jects in memory over time via tracking 8 20 30 32 ag- gregating past information for each particular object. how- ever the associations from the tracker may contain mis- takes that can compound over time and lead to informa- tion loss. other methods leverage attention from current detection proposals to the past sensor or object informa- tion 15 18 74 . still they can be challenging to scale to long histories and suffer from false negatives as the pro- posals re\ufb01ned into the \ufb01nal detections only come from the work done while at waabi this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 1449 page 2 present time. in this paper we present a simple and sensor-agnostic add-on for enhancing any existing 3d object detector with long-term memory. we refer to it as mad short for memory-augmented detection and fig. 1 illustrates the high-level idea. mad is a transformer-based model that fuses proposals from a detector with proposals from a mem- ory bank representing past beliefs. inspired by recent devel- opments 4 we exploit joint detection and trajectory fore- casting. by storing explicit trajectory forecasts in the mem- ory bank we can estimate object poses at arbitrary future timestamps for all the objects in the memory. this enables us to enrich the set of proposals by aligning memory pro- posals with the current observations. training with temporal data can be challenging back- propogation through consecutive training examples con- sumes prohibitive amounts of memory training on long se- quences can cause over-\ufb01tting when back-propagating on every example and using memory warm-up can slow down training. we design a more effective training schedule that begins with short temporal sequences and progressively in- creases the length exploiting high data diversity early and closing the gap with inference towards the end. to en- sure the model learns to trust the memory when training on short sequences we use cached model outputs from pre- vious training iterations. we demonstrate the generality of our approach by en- hancing existing lidar-based and camera-based 3d ob- ject detection networks with mad and show considerable improvements over the base detectors on two large-scale datasets waymo open dataset wod 57 and argoverse 2 sensor dataset av2 68 . notably safdnet 80 en- hanced with mad achieves state-of-the-art performance on wod for online detection methods without requiring en- sembles or test-time augmentation. 2 related work 3d object detection we can categorize 3d detectors by their input modality e.g. lidar camera scene repre- sentation e.g. point clouds voxels and number of stages e.g. single-stage or multi-stage lidar-based methods commonly represent the input as voxels 12 27 56 71 80 81 pillars 23 25 55 61 66 72 or point clouds 43 53 54 69 73 . a widely used ap- proach for including temporal lidar information is point aggregation which involves transforming past point clouds into a common coordinate frame and processing the ag- gregated point cloud. these approaches are usually lim- ited to 5 past lidar frames due to computational con- straints 30 in online applications like autonomous driving. another drawback is that point aggregation does not align moving objects requiring a larger receptive \ufb01eld in the de- tector backbone the longer the temporal horizon is 30 80 . camera-based 3d detection is challenging because of missing depth information. one approach is to produce 3d bounding boxes from image features by estimating depth 3d size and orientation 7 41 64 70 . other methods leverage voxel 47 48 51 or point cloud 65 76 repre- sentations by predicting pixel depth distributions to lift 2d features to 3d. stacking and processing past camera images or feature maps is a common but expensive method for tem- poral fusion 2 19 46 52 78 . regardless of modality we can further categorize 3d de- tectors as single-stage or multi-stage. single-stage methods produce detections from sensor data 6 12 64 71 80 81 with a single deep neural network. multi-stage methods use bounding box proposals from a \ufb01rst stage or ran- domly initialized proposals to gather features e.g. with roipool 49 roialign 16 interpolations 75 or atten- tion 4 28 33 35 67 82 and iteratively re\ufb01ne the bound- ing boxes. mad is a sensor-modality-agnostic module that we can add to any detector as a subsequent re\ufb01nement stage. in our work we utilize centerpoint 75 safdnet 80 hed- net 81 fcos3d 64 and bevmap 6 as proposal net- works. long-term temporal fusion for 3d detection vari- ous works attempt to solve the shortcomings of sensor ag- gregation by learning to use multiple seconds of sensor ev- idence to improve object detection. in this line of work the scene-based paradigm uses recurrent fusion of scene-level features 14 17 31 75 with some methods relying on multiple traversals of the scene 77 . a challenge of this approach is focusing and aligning features from relevant and dynamic foreground objects which past works addressed by transforming fea- ture maps using segmentation to focus on foreground ob- jects 17 and using deformable attention or convolution to align features of moving objects 22 . processing both foreground and background areas can be computationally expensive. it is worth noting that many of these methods are single-stage detectors 14 17 and which we could use as a detection proposal network with mad. alternatively the object-based paradigm focuses on the foreground by using detection proposals. detect-track-fuse methods are a sub-family of object-based methods that as- sociate previous detections over time to create tracks and these tracks summarize information from past sensor evi- dence 8 20 30 30 32 79 . however in complex situ- ations like pedestrian crowds association over time can be dif\ufb01cult due to heavy occlusions and erratic behavior po- tentially leading to false negatives or identity switches in the tracks. object-to-scene approaches mitigate the short- comings of association by directly using current detec- tion proposals to aggregate historical scene-level informa- tion using hand-crafted feature aggregation modules 15 1450 page 3 memory bank retrieval alignment feature extraction 3d detector detection proposals memory proposals proposal merging refinement transformer memory cross attention factorized self attention proposal update model outputs refined proposals memory bank update merged proposals in preparation for future inferences sensor data timestamp sdv pose figure 2. mad is a plug-and-play module that enhances any off-the-shelf 3d detector kept frozen with long-term memory. or attention mechanisms 82 . these approaches can be dif\ufb01cult to scale to long history horizons as they require re-processing past sensor evidence or dense feature maps based on the current proposals e.g. 15 82 only use 0.7s of history . finally object-to-object methods use past ob- ject detections to improve current object detections without explicit tracking e.g. by cross-attending from current de- tection proposals to past detections 63 74 or using hand- coded attention matrices based on distance 18 . overall most object-based methods share some de\ufb01ciencies many only re\ufb01ne detection proposals produced by current sen- sor evidence and struggle to recover from missing propos- als 8 15 18 20 74 . others naively concatenate past detections with current proposals 63 which can lead to alignment issues for dynamic objects and miss-calibration in the proposal scores as the model should trust historical proposals less than current proposals. our proposed method mad performs object-based temporal fusion without requiring explicit object associa- tion aligns the memory in space and time by with trajectory forecasting and can recover from missing proposals by us- ing and rescoring proposals from the memory bank. 3 memory augmented 3d object detection 3d object detectors take a short temporal window of sensor data as input and produce a set of detections. existing ap- proaches typically struggle to perceive occluded and distant objects with limited sensor observations. to tackle these challenges we propose mad a plug-and-play module to enhance existing 3d object detectors with the ability to per- form long-horizon temporal fusion. our only requirement from the detector is that each detection includes an object bounding box multi-class con\ufb01dence scores and a feature vector capturing local context. we demonstrate the general- ity of mad by augmenting and improving various lidar- based and camera-based detectors. we enable long-horizon temporal understanding through a memory bank that captures all the relevant information on objects including where we expect them to move. these trajectory forecasts allow us to align the memory objects with the current detector proposals in space and time. im- portantly we do not require the object detector to provide motion forecasts instead mad computes them. to com- pensate for ego-motion we assume the ego is localized which is the norm in modern self-driving platforms 57 68 and store the ego pose in the memory along with the model outputs. 3.1. model we start with an overview of our model refer to figure 2 for an illustration. at every inference step mad takes as input the detection proposals the current timestamp t e.g. lidar sweep-end time or camera capture time and the ego pose et in a global coordinate frame. it then re- trieves objects from memory aligns them spatially with et and temporally to t and extracts high-dimensional features from the aligned boxes and trajectory forecasts. we refer to the aligned boxes and trajectory forecasts with the ex- tracted features as memory proposals. a proposal merg- ing mechanism then fuses detection and memory proposals by rescoring their con\ufb01dence scores and applying standard post-processing. finally our re\ufb01nement transformer itera- tive re\ufb01nes the object detections and trajectory forecasts in the merged proposals with cross-attention to the memory and factorized self-attention. in preparation for future in- ferences the memory bank is then updated by appending the model outputs a.k.a. re\ufb01ned proposals and removing older model outputs to keep the memory bounded in size. proposal representation we de\ufb01ne object proposals p b c t q with n bounding boxes b rn 7 where the last dimension corresponds to x y z l w h \u03c9 with object 3d centroids x y z headings \u03c9 in a bev ego-relative coordinate frame and the 3d box dimensions w l h multi-class con\ufb01dence scores c 0 1 n c where c is the number of actor classes trajectory fore- casts t rn tf 3 describing objects bev pose x y \u03c9 t sf . . . x y \u03c9 t tf sf over tf future way- points at a time interval sf and an object feature q rn tf 1 d encoding both local and global features for every object at the present and future timestamps where d 1451 page 4 is the feature dimensionality. we use superscripts to de- note the source of the proposals detection proposals pdet from the 3d detector memory proposals pmem from the memory bank merged proposals pmerge from the proposal merging module and re\ufb01ned proposals pref from the out- put of the re\ufb01nement transformer. for detection proposals pdet we generate tdet by assuming the object is static over time since detectors do not provide forecasts and this is just an initialization before re\ufb01nement . the object features qdet are obtained by interpolating the feature map from be- fore the detector header at the projected object centroids repeating tf 1 times to get the features for future times- tamps and adding a learned embedding of bdet and cdet. in the paragraphs below we describe how we obtain pmem pmerge and pref. memory bank and retrieval the memory bank is a set of tuples tm etm pref tm with timestamped past model out- puts and ego pose sorted by the timestamp tm at which the outputs were generated. during inference at timestamp t we retrieve memory entries pref tm at a set of past target times- tamps tm tm where tm t sm t 2sm . . . t tmsm . tm is the number of past target timestamps and sm is the time stride of the retrieved entries. to be precise we retrieve the closest memory entry to each timestamp in tm to be robust to small sensor delays. extracting memory proposals for effective use of the memory at inference we should align each retrieved entry tm etm pref tm in space and time with the current detec- tion proposals at time t. we handle ego-motion by applying the relative transform etm t e 1 t etm to bref tm and tref tm. to handle object motion we linearly interpolate the stored trajectory forecast to the current timestamp t to obtain the proposal box bmem. to obtain the proposal forecast tmem we also interpolate extrapolate the stored trajectories as re- quired to obtain waypoints at tf t sf . . . t tfsf from stored waypoints at tm sf . . . tm tfsf . finally we extract latent features qmem at t and every future time step tf tf first we compute sinusoidal posi- tional embeddings 60 for the centroid coordinates bmem x y z and encode them with a lightweight mlp. separately we concatenate other features including bmem l w h \u03c9 box dimen- sions and heading con\ufb01dence scores cmem the memory age t tm and a 2d vector pointing to where the pro- posal was in the current ego coordinate frame at the time tm. finally we encode the concatenated features with an- other mlp and add the features from both mlps together. proposal merging the memory and detection propos- als can be redundant particularly in areas with good sen- sor coverage. to merge proposals we learn to rescore their multi-class con\ufb01dence scores. rescoring is essential as the con\ufb01dence the model should put in a memory proposal not only depends on the con\ufb01dence score at a past timestamp cmem tm but also on the proposal age t tm as the fore- casting uncertainty grows with the time horizon and other factors. for example the model should trust a fast-moving detection less than a stationary object or it should trust an object observed 0.5 seconds ago more than one observed 5 seconds ago. furthermore the detection proposals come from the 3d detector while the memory proposals are pro- duced by mad and detectors have been found to be mis- calibrated 24 40 42 . to make the scores comparable we learn two small mlps that separately map the features of the detection pro- posals qdet and the memory proposals qmem to new multi- class scores cmerge. as explained in section 3.2 these rescoring mlps are trained under a single detection loss ap- plied to the merged proposals so that the model can decide which proposals to trust from both sources. finally we \ufb01lter proposals with score thresholding non-maximum suppres- sion nms and keep the top k merged proposals sorted by score maximum over actor classes . post-processing en- ables the re\ufb01nement transformer to process a smaller num- ber of queries. finally we add learned time positional embeddings to the merged proposal features qmerge to indicate the time of the trajectory forecast. at this point we have n merge def n ref merged proposals pmerge ready for re\ufb01nement. re\ufb01nement transformer we utilize a transformer de- coder to re\ufb01ne the merged proposals pmerge pref 0 it- eratively over i blocks into pref 1 ...pref i where the \ufb01nal model outputs are pref pref i . we propose a novel memory cross-attention mechanism to allow the queries proposal features qref i to aggregate information from all the memory proposals qmem including those that pro- posal merging \ufb01ltered out. we want to use this informa- tion in the re\ufb01nement transformer because multiple overlap- ping memory proposals provide signi\ufb01cant evidence about an object s presence and location. to achieve this we per- form cross attention from the object queries qref i to the memory proposal features qmem. for ef\ufb01ciency we limit the cross attention to the nearest k keys to each object query computing the nearest neighbors of bref i x y z in bmem x y z . similar to many works 4 44 we perform factorized self-attention in each re\ufb01nement block which separates time self-attention and object self-attention for ef\ufb01ciency where the former attends only to queries from the same ob- ject sequence length tf 1 and the latter only attends to queries from the same time step sequence length n . the updated queries qref i 1 are input to the next block. finally we update the explicit proposal information as described in detra 4 by using a simple mlp to produce bref i 1 and cref i 1 and a gated recurrent unit gru 9 to update the future trajectory waypoints tref i 1 . 1452 page 5 memory bank update we post-process the re\ufb01ned pro- posals pref as we did to the merged proposals score thresh- olding nms and top k based on con\ufb01dence score adding the result to the memory bank along with the correspond- ing timestamp t and ego pose et. to limit the size of the memory bank when running on long sequences we remove any memory entries older than t tmsm \u03b5 the past time- horizon used in memory retrieval with a small buffer \u03b5 . 3.2. training we \ufb01rst train an off-the-shelf 3d detector following their original training strategy. this stage can be omitted if a pre-trained 3d detector is available. then we train all the parameters in mad as a subsequent stage with the 3d de- tector weights frozen. pre-training and freezing the 3d de- tector is important to ensure the detection proposals do not change throughout mad training. note that we train a sep- arate mad for each 3d detector as each detector has differ- ent features qdet and detection distribution and calibration. before detailing our proposed mad training we discuss some possibilities and trade-offs when training temporal fu- sion models. training on unordered examples has the ad- vantage of satisfying the assumption of i.i.d examples bet- ter learning dynamics 21 50 . however it differs from evaluation where the model is rolled out on long sequences and consumes its previous outputs. training on long se- quences of ordered data has the advantage of being closer to evaluation but it has worse learning dynamics since consec- utive examples are heavily correlated there are few changes in the scene from one frame to the next . if instead gradi- ents are accumulated over a long sequence and used to up- date the model parameters once per sequence a sequence becomes one example satisfying the i.i.d assumption but the training duration is multiplied by the sequence length if the number of model updates is kept constant. despite this large space of possibilities and the importance of such choices prior works on learned temporal fusion neglect de- tails and discussion of their training recipe 15 17 20 30 . to tackle these challenges we design a novel training schedule. we propose to train mad on increasingly long chunks of ordered data using single frames1 at the begin- ning and entire sequences at the end of training. to train ob- ject memory on short chunks or even single frames of data while maintaining a reasonable amount of memory inputs we propose to maintain a cache of memory banks across training and using it to build the memory proposals for each training example. below we detail this proposed schedule our cache of memory banks how we handle augmentations with memory and our loss function. training schedule the datasets we use wod 57 av2 68 organize their data into driving logs each around 1we slightly abuse the term frame here as some detectors use a win- dow of multiple past frames as input. 20s in duration with data captured at 10hz meaning each log has around 200 frames. each log has a unique identi- \ufb01er logid . for the \ufb01rst 25 of training we sample single frames that is consecutive training examples are random frames from random logs . throughout the rest of training we sample sequential chunks of gradually increasing size 48 frames for 25 50 of training 96 frames for 50 75 and 144 frames for 75 100 . we train with a single cosine decay learning rate schedule with no resets. the intuition behind this is that when the learning rate is high and the model weights change the most the model is exposed to more diverse data. then when the learning rate is lower the model is tuned to be closer to the evaluation setting where it consumes its previous outputs. exploiting a cache of memory banks if the schedule described above is followed naively during the individual frame and short chunk training the model cannot consume its previous outputs and thus would not learn to use memory during this phase of training. to address this problem we introduce a cache of previous memory banks. this cache is a mapping from the unique driving log identi\ufb01er logid to a memory bank. at the start of training we initialize the cache with empty memory banks for all logids. on a given training iteration we index the cache with the logid of the current training example to obtain the memory bank. if available we retrieve the memory proposals from this mem- ory bank as described in sec. 3.1. we update the retrieved memory bank at the end of the training iteration with the model outputs replacing any existing entry with the same timestamp. note that during training we do not limit the size of the memory bank. there are a few challenges to training with the object memory cache that we address to train these models ef\ufb01ciently on large datasets we use a distributed data training scheme meaning we split examples in the minibatch across multiple gpus. each gpu has a unique index called a rank. each rank main- tains a separate cache to prevent the cache from \ufb01lling up the ram and avoid synchronization costs. to guaran- tee high cache hit rates we ensure that training examples from a given logid are always put on the same rank dur- ing training. the cache is \ufb01lled with mad outputs which are inac- curate at the beginning of training. we do not want er- roneous model outputs to \ufb01ll the cache otherwise the model may not learn to trust the memory proposals. to mitigate this we only start \ufb01lling the cache and train- ing with memory proposals after 2.5 of training after which performance is reasonable. to make the model robust to variable latency and the pres- ence and absence of memory proposals we randomize the target timestamps tm that we retrieve memory elements for during training by randomly sampling the time stride 1453 page 6 overall l1 overall l2 vehicle l1 vehicle l2 pedestrian l1 pedestrian l2 cyclist l1 cyclist l2 method ap aph ap aph ap aph ap aph ap aph ap aph ap aph ap aph centerpoint 1f 75 76.1 73.5 70.0 67.6 75.7 75.2 67.9 67.4 77.6 71.6 70.1 64.4 74.9 73.8 72.1 71.0 mad ours 82.9 81.0 77.6 75.8 81.1 80.5 74.0 73.4 83.8 80.0 77.2 73.5 83.8 82.6 81.6 80.4 centerpoint 2f 75 77.5 75.8 71.7 70.1 76.4 75.9 68.7 68.2 79.2 75.6 71.9 68.5 76.8 75.9 74.4 73.5 mad ours 82.8 81.2 77.5 76.0 81.4 80.8 74.3 73.7 84.7 82.1 78.2 75.6 82.2 80.8 80.1 78.7 hednet 1f 81 81.6 79.7 75.6 73.7 80.9 80.5 73.1 72.7 84.6 80.2 77.1 72.8 79.4 78.5 76.6 75.6 mad ours 85.2 83.3 80.2 78.3 83.6 82.9 76.6 76.0 87.0 83.4 81.0 77.4 85.1 83.7 83.0 81.6 hednet 4f 81 83.6 82.3 78.1 76.8 82.4 81.9 75.1 74.6 86.3 83.6 79.4 76.8 82.2 81.4 79.9 79.1 mad ours 85.5 83.8 80.6 79.0 83.6 82.9 76.8 76.1 87.7 85.0 81.9 79.2 85.1 83.5 83.2 81.6 safdnet 1f 80 81.7 79.7 75.5 73.6 80.5 80.0 72.5 72.1 84.7 80.2 77.1 72.9 79.8 78.8 76.9 75.9 mad ours 85.3 83.5 80.3 78.4 83.4 82.8 76.5 75.9 86.8 82.9 80.7 76.8 85.8 84.7 83.7 82.6 safdnet 4f 80 83.9 82.6 78.4 77.1 82.8 82.3 75.4 74.9 86.8 84.2 80.1 77.5 82.0 81.1 79.6 78.8 mad ours 85.8 84.2 81.0 79.4 84.2 83.6 77.4 76.8 87.9 85.4 82.2 79.7 85.2 83.7 83.3 81.7 table 1. comparing the performance of various off-the-shelf lidar object detectors with and without mad on the wod validation set. base detector results are reproduced using of\ufb01cial code. mad consistently boosts the performance of all detectors across all metrics. vehicle ap iou 10 in camera field of view method overall 0 40 m 40 80 m 80 120 m fcos3d 64 37.6 73.9 34.3 4.65 mad ours 43.6 82.6 40.0 8.11 bevmap 6 51.5 86.5 54.0 13.9 mad ours 53.4 88.0 55.1 17.2 table 2. adding mad to camera-based 3d detectors on av2. sm and the number of target timestamps tm. handling augmentations with memory prior works 26 75 80 81 \ufb01nd that data augmentations e.g. translation rotation \ufb02ipping and re-scaling are important for detection performance. we apply augmenta- tions to the boxes bmem and trajectories tmem in memory proposals after the memory alignment step. we apply the inverse of the augmentations to bref and tref before storing them in the memory bank. loss function we optimize a multi-task objective l lrescore cmerge i i 1 ldet bref i cref i lfor tref i which is a combination of a rescoring loss lscore a detection re\ufb01nement loss ldet and a forecasting re\ufb01nement loss lfor where the detection and forecasting losses are computed at every re\ufb01nement block. follow- ing 4 ldet includes a binary focal loss for classi\ufb01cation an l1 loss for regression and an iou loss. to calculate the targets for these losses we \ufb01rst match the detections to the ground truth bounding boxes through bipartite matching as proposed in detr 3 . the rescoring loss is similar ex- cept it consists only of the focal loss as we are only train- ing the multi-class scores cmerge output by the rescoring module. the trajectory re\ufb01nement loss is an l1 loss against the ground-truth trajectory supervised only for true-positive detections with iou with a ground truth box higher than method ap l1 aph l1 ap l2 aph l2 centerformer 82 82.3 80.9 77.6 76.3 bevfusion 36 82.7 81.4 77.7 76.3 msf 15 83.1 81.7 78.3 77.0 fsd 13 83.5 82.1 78.4 77.1 logonet 29 83.1 81.8 78.4 77.1 octopus noah 83.1 81.7 78.7 77.3 seed-l 38 83.5 82.2 78.7 77.3 lion 37 83.7 82.4 78.7 77.4 veuronnet3d 83.7 82.2 79.1 77.7 hiac 84.0 82.6 79.2 77.8 inceptiolidar 83.8 82.5 79.2 77.8 vadet 84.1 82.8 79.4 78.2 mt3d 85.0 83.7 80.1 78.7 livox detection 84.8 83.5 80.2 79.0 mad ours 86.0 84.3 81.8 80.2 table 3. results on the wod test set as reported on the leaderboard2. we exclude entries that state they use ensembles test-time augmentations or are of\ufb02ine use future sensor data . ours is using safdnet 4f as the 3d detector. aph l2 is the ranking metric. 0.5 . see our supplementary for more details. 4 experiments this section provides a comprehensive quantitative analy- sis of mad from three perspectives. first we add mad to existing 3d detectors showing signi\ufb01cant improvements. we use both lidar-based and camera-based detectors on wod 57 and av2 68 respectively. second we com- pare the best version of mad to the state-of-the-art meth- ods on wod setting a new record on the wod leaderboard among online methods without ensembles or test-time aug- mentation and outperforming prior learned temporal fusion methods by a large margin. finally we conduct thorough 1454 page 7 method ap l1 aph l1 ap l2 aph l2 validation lef 17 79.6 79.2 71.4 70.9 modar 30 - - - 72.5 mppnet 8 81.6 81.1 76.0 74.8 msf 15 82.2 80.7 76.8 75.5 ptt 20 82.7 80.7 77.7 75.7 mad ours 85.8 84.2 81.0 79.4 testing 3d-man 74 49.6 48.1 44.8 43.4 mppnet 8 81.8 80.6 76.9 75.7 msf 15 83.1 81.7 78.3 77.0 mad ours 86.0 84.3 81.8 80.2 table 4. comparison of our method against various methods for learned temporal fusion on wod. ours is using safdnet 4f. ablation studies to understand the architectural choices that make mad effective and the impact of different training procedures. refer to our supplementary for more imple- mentation details experimental results and ablations. implementation details the re\ufb01nement transformer uses i 3 re\ufb01nement blocks and the dimension of all embeddings is d 128. we forecast tf 10 future timestamps at stride of sf 0.5s yielding a 5s prediction horizon. unless otherwise stated we use target timestamps of tm 0.3s 0.6s . . . 2.4s i.e. sm 0.3s tm 8 for reading from the memory bank at inference. in the memory cross attention we use the nearest k 4 neighbors. following prior works 75 80 81 for any detection post-processing we use a 0.1 con\ufb01dence thresh- old per-class nms iou thresholds of 0.75 0.6 0.55 for vehicles pedestrians and cyclists respectively and a top k 500. mad has 3.8m parameters while the base detectors have anywhere from 8m centerpoint 75 to 53m bevmap 6 parameters. for each base detector we train mad for 60k update steps roughly equivalent to 6 epochs on wod and av2 with batch size 16. we use a cosine learning rate decay with a max learning rate of 8 10 4 and a linear warm-up for the \ufb01rst 1000 steps beginning with a learning rate of 8 10 5. during train- ing we use a variable set of memory target timestamps tm uniform 6 7 8 9 10 with a variable stride sm uniform 0.2s 0.3s 0.4s . metrics we report the detection metrics from the of\ufb01- cial wod leaderboard 57 which include average preci- sion ap and ap weighted by heading error aph for ve- hicles veh. pedestrians ped. and cyclists cyc. . these metrics use intersection-over-union iou thresholds of 0.7 0.5 and 0.5 respectively. the metrics are broken down into two levels of dif\ufb01culty level 1 l1 includes only labels that have 5 lidar points and are not marked as hard and level l2 includes all boxes that have 0 lidar points a superset of l1 . for camera experiments on av2 we report the mean average precision ap for vehicles in the camera \ufb01eld of view at an iou threshold of 0.1. we re- port the macro-average over all classes if the actor class is not speci\ufb01ed. augmenting off-the-shelf 3d detectors with mad tab. 1 and tab. 2 show the performance of mad ap- plied to off-the-shelf 3d detectors on wod and av2 re- spectively.to show the generality of our approach we ex- periment with multiple base detectors trained on different datasets and sensor modalities. we enhanced three lidar- based methods with mad on wod centerpoint 75 with both 1 lidar frame 1f and 2 lidar frames 2f as in- put hednet 81 1f and 4f and safdnet 80 1f and 4f . we follow their of\ufb01cial protocols to train and eval- uate all models from scratch due to the waymo dataset license agreement we cannot simply re-use pre-trained models . we also enhance two camera-based methods on av2 fcos3d 64 and bevmap 6 which takes the most recent image from the front camera as input. we use the of\ufb01cial implementation for both fcos3d 64 and bevmap 6 . training details are in the supplementary. our model brings signi\ufb01cant improvements to all detec- tors on both datasets. these gains are largest for single- frame detectors where the memory provides the most ad- ditional information. the fact that the mad-augmented single-frame detectors are better than the multi-frame detec- tors clearly shows the effectiveness of our method relative to the common point aggregation approach. please visit the supplementary materials for qualitative comparisons. comparison against sota by augmenting safdnet 4f with mad we show in tab. 3 that we achieve the best performance on the wod leaderboard2 among all online methods that do not use ensembles or test-time augmenta- tion. table 4 compares mad to prior learned temporal fu- sion methods on the wod validation and test set where we achieve substantial gains. please refer to our supplementary for full tabs. 3 and 4 with metrics for all actor classes. effect of memory proposals and memory attention we ablate the different components of our memory pipeline in tab. 5. comparing rows 1 2 and 5 shows that both the proposed memory attention and memory proposals have a positive effect. this is intuitive as the memory proposals let mad recover from false negative detection proposals which is complementary to memory cross-attention that al- lows mad to use all memory information for re\ufb01nement bypassing the \ufb01ltering in proposal-merging . effect of forecasting comparing rows 3 and 5 in tab. 5 we \ufb01nd that using trajectory forecasting to align memory proposals to the current time is important particularly for fast-moving objects. without forecasting the memory pro- posals from previous frames will be far from the current po- 2https waymo.com open challenges 2020 3d-detection as of submis- sion 14 11 2024 1455 page 8 mem. prop. mem. attn. forecast. rescore. veh. ap ped. ap cyc. ap veh. ap 20 30 m s cyc. ap 5 10 m s 0 75.4 80.1 79.6 38.0 72.4 1 76.5 81.3 82.0 40.4 82.1 2 75.8 81.7 81.6 37.7 75.9 3 76.9 81.8 81.9 34.6 78.7 4 72.7 82.0 81.2 11.9 73.9 5 77.0 82.3 83.3 45.2 86.2 table 5. component ablation of mad on the wod validation set. all metrics are l2. row 0 is the base 3d detector safdnet 4f 80 . all ablations in this table including the \ufb01nal method with all components use a reduced training duration of 45k iterations to reduce costs. evaluated proposals ap l1 aph l1 ap l2 aph l2 detection pdet 83.9 82.6 78.4 77.1 combined pmem pdet 18.1 17.8 16.8 16.4 merged pmerge 84.1 83.0 78.8 77.7 after block 0 pref 1 84.4 83.2 79.3 78.1 after block 1 pref 2 85.8 84.2 80.9 79.3 after block 2 pref 85.8 84.2 81.0 79.4 table 6. evaluating various intermediate proposals from mad. the base detector is safdnet 4f. sition of those objects making it challenging for the model to leverage the memory effectively. effect of learned proposal merging comparing rows 0 4 and 5 of tab. 5 we \ufb01nd the proposed learned rescoring of the merged detection and memory proposals is crucial for good performance. without it mad cannot enhance the base detector row 0 because the proposal scores from the 3d detector and memory are miss-calibrated before being post-processed in the proposal merging step i.e. nms . we illustrate this in tab. 6 where we evaluate intermedi- ate proposals of mad 1 the detection proposals pdet 2 naively taking the union of the detection proposals pdet and memory proposals pmem and post-processing them 3 the merged proposals pmerge which have been rescored and 4 after each block of the re\ufb01nement transformer pref 1 . . . pref i . naively concatenating the combined proposals is much worse than the base detector because of the miss-calibrated scores. after proposal merging pmerge already improves over the base detector. each re\ufb01nement block brings further gains illustrating the strength of our proposed re\ufb01nement transformer. training procedure study table 7 provides evidence supporting the effectiveness of our proposed training sched- ule. we \ufb01rst train mad with three different chunk sizes i.e. sequences with 144 48 1 frames each with and without the memory bank cache. training with long chunks 144 frames tab. 7.1a provides good performance because there is a low gap between training and evaluation. the cache provides no gains in this setting tab. 7.1b because the model already has memory proposals in most frames. training with shorter chunks tab. 7.2a 3a performs worse because there is a more signi\ufb01cant gap between training and evaluation. including the cache helps signi\ufb01cantly by chunk length cache ap l1 aph l1 ap l2 aph l2 1a 144 85.4 83.9 80.5 79.0 1b 144 85.2 83.7 80.3 78.9 2a 48 85.1 83.5 80.2 78.6 2b 48 85.3 83.8 80.3 78.9 3a 1 83.9 82.4 78.4 77.0 3b 1 85.0 83.3 80.1 78.5 4a 1 48 96 144 84.9 83.3 79.9 78.3 4b 1 48 96 144 85.8 84.2 81.0 79.4 table 7. ablating chunk length and the memory cache on wod using safdnet 4f. closing the gap to evaluation but does not fully reach the long chunk performance tab. 7.2b 3b . as hypothesized in sec. 3.2 there is room for improvement by training with our proposed schedule and memory bank cache tab. 7.4b . this strategy allows mad to learn generalized patterns over a diverse set of examples quickly by training on short chunks more i.i.d. data at the beginning when the learn- ing rate is higher while re\ufb01ning its understanding on long chunks closer to the deployment setting towards the end when the learning rate is lower. table 7.4a shows the impor- tance of the cache when using this training schedule other- wise training with small chunks is ineffective as the model would not learn to use the memory. 5 conclusion in this paper we propose mad a simple effective and sensor-modality-agnostic add-on for enhancing any existing 3d object detector with long-term memory. to achieve this we design a transformer-based model that uses joint detec- tion and trajectory forecasting to populate a memory bank with spatial-temporal object trajectories. our model can ef- fectively fuse memory proposals with detection proposals by reading previous memory entries and aligning them with the current time and ego pose. we also propose a novel training strategy that increases data diversity while keep- ing the training-to-inference gap low. our approach is very general bringing impressive improvements to a variety of lidar-based and camera-based detectors and very ef- fective achieving sota performance on waymo open dataset when paired to the base detector safdnet 4f 80 . 1456 page 9 references 1 holger caesar varun bankiti alex h lang sourabh vora venice erin liong qiang xu anush krishnan yu pan gi- ancarlo baldan and oscar beijbom. nuscenes a multi- modal dataset for autonomous driving. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 11621 11631 2020. 1 2 yigit baran can alexander liniger ozan unal danda paudel and luc van gool. understanding bird s-eye view of road semantics using an onboard camera. ieee robotics and automation letters 7 2 3302 3309 2022. 2 3 nicolas carion francisco massa gabriel synnaeve nicolas usunier alexander kirillov and sergey zagoruyko. end-to- end object detection with transformers. in european confer- ence on computer vision pages 213 229. springer 2020. 6 1 4 sergio casas ben agro jiageng mao thomas gilles alexander cui thomas li and raquel urtasun. detra a uni\ufb01ed model for object detection and trajectory forecasting. 2024. 2 4 6 5 5 yuning chai benjamin sapp mayank bansal and dragomir anguelov. multipath multiple probabilistic anchor tra- jectory hypotheses for behavior prediction. arxiv preprint arxiv 1910.05449 2019. 2 6 mincheol chang seokha moon reza mahjourian and jinkyu kim. bevmap map-aware bev modeling for 3d per- ception. in proceedings of the ieee cvf winter confer- ence on applications of computer vision pages 7419 7428 2024. 2 6 7 1 7 xiaozhi chen kaustav kundu yukun zhu andrew g berneshawi huimin ma sanja fidler and raquel urtasun. 3d object proposals for accurate object class detection. ad- vances in neural information processing systems 28 2015. 2 8 xuesong chen shaoshuai shi benjin zhu ka chun che- ung hang xu and hongsheng li. mppnet multi-frame feature intertwining with proxy points for 3d temporal ob- ject detection. in european conference on computer vision pages 680 697. springer 2022. 1 2 3 7 9 kyunghyun cho. learning phrase representations using rnn encoder-decoder for statistical machine translation. arxiv preprint arxiv 1406.1078 2014. 4 1 10 alexander cui sergio casas kelvin wong simon suo and raquel urtasun. gorela go relative for viewpoint-invariant motion forecasting. in 2023 ieee international confer- ence on robotics and automation icra pages 7801 7807. ieee 2023. 2 5 11 henggang cui vladan radosavljevic fang-chieh chou tsung-han lin thi nguyen tzu-kuo huang jeff schnei- der and nemanja djuric. multimodal trajectory predictions for autonomous driving using deep convolutional networks. in 2019 international conference on robotics and automation icra pages 2090 2096. ieee 2019. 2 12 martin engelcke dushyant rao dominic zeng wang chi hay tong and ingmar posner. vote3deep fast ob- ject detection in 3d point clouds using ef\ufb01cient convolutional neural networks. in 2017 ieee international conference on robotics and automation icra pages 1355 1361. ieee 2017. 2 13 lue fan yuxue yang feng wang naiyan wang and zhaoxiang zhang. super sparse 3d object detection. ieee transactions on pattern analysis and machine intelligence 45 10 12490 12505 2023. 6 3 14 davi frossard shun da suo sergio casas james tu and raquel urtasun. strobe streaming object detection from li- dar packets. in conference on robot learning pages 1174 1183. pmlr 2021. 1 2 15 chenhang he ruihuang li yabin zhang shuai li and lei zhang. msf motion-guided sequential fusion for ef\ufb01cient 3d object detection from point cloud sequences. in proceed- ings of the ieee cvf conference on computer vision and pattern recognition pages 5196 5205 2023. 1 2 3 5 6 7 16 kaiming he georgia gkioxari piotr doll ar and ross gir- shick. mask r-cnn. in proceedings of the ieee international conference on computer vision pages 2961 2969 2017. 2 17 tong he pei sun zhaoqi leng chenxi liu dragomir anguelov and mingxing tan. lef late-to-early tempo- ral fusion for lidar 3d object detection. in 2023 ieee rsj international conference on intelligent robots and systems iros pages 1637 1644. ieee 2023. 1 2 5 7 3 18 jinghua hou zhe liu zhikang zou xiaoqing ye xiang bai et al. query-based temporal fusion with explicit mo- tion for 3d object detection. advances in neural information processing systems 36 2024. 1 3 19 anthony hu zak murez nikhil mohan sof \u0131a dudas jef- frey hawke vijay badrinarayanan roberto cipolla and alex kendall. fiery future instance prediction in bird s- eye view from surround monocular cameras. in proceedings of the ieee cvf international conference on computer vi- sion pages 15273 15282 2021. 2 20 kuan-chih huang weijie lyu ming-hsuan yang and yi-hsuan tsai. ptt point-trajectory transformer for ef- \ufb01cient temporal 3d object detection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 14938 14947 2024. 1 2 3 5 7 21 jack kiefer and jacob wolfowitz. stochastic estimation of the maximum of a regression function. the annals of math- ematical statistics pages 462 466 1952. 5 22 junho koh junhyung lee youngwoo lee jaekyum kim and jun won choi. mgtanet encoding sequential lidar points using long short-term motion-guided temporal atten- tion for 3d object detection. in proceedings of the aaai con- ference on arti\ufb01cial intelligence pages 1179 1187 2023. 2 23 jason ku melissa mozi\ufb01an jungwook lee ali harakeh and steven l waslander. joint 3d proposal generation and object detection from view aggregation. in 2018 ieee rsj international conference on intelligent robots and systems iros pages 1 8. ieee 2018. 2 24 fabian kuppers jan kronenberger amirhossein shantia and anselm haselhoff. multivariate con\ufb01dence calibration for object detection. in proceedings of the ieee cvf confer- ence on computer vision and pattern recognition workshops pages 326 327 2020. 4 25 alex h lang sourabh vora holger caesar lubing zhou jiong yang and oscar beijbom. pointpillars fast encoders 1457 page 10 for object detection from point clouds. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 12697 12705 2019. 2 26 zhaoqi leng guowang li chenxi liu ekin dogus cubuk pei sun tong he dragomir anguelov and mingxing tan. lidar augment searching for scalable 3d lidar data augmen- tations. in 2023 ieee international conference on robotics and automation icra pages 7039 7045. ieee 2023. 6 27 bo li. 3d fully convolutional network for vehicle detection in point cloud. in 2017 ieee rsj international conference on intelligent robots and systems iros pages 1513 1518. ieee 2017. 2 28 hongyang li hao zhang zhaoyang zeng shilong liu feng li tianhe ren and lei zhang. dfa3d 3d deformable attention for 2d-to-3d feature lifting. in proceedings of the ieee cvf international conference on computer vision pages 6684 6693 2023. 2 29 xin li tao ma yuenan hou botian shi yuchen yang youquan liu xingjiao wu qin chen yikang li yu qiao et al. logonet towards accurate 3d object detection with local-to-global cross-modal fusion. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 17524 17534 2023. 6 30 yingwei li charles r qi yin zhou chenxi liu and dragomir anguelov. modar using motion forecasting for 3d object detection in point cloud sequences. in proceed- ings of the ieee cvf conference on computer vision and pattern recognition pages 9329 9339 2023. 1 2 5 7 3 31 zhiqi li wenhai wang hongyang li enze xie chong- hao sima tong lu yu qiao and jifeng dai. bevformer learning bird s-eye-view representation from multi-camera images via spatiotemporal transformers. in european con- ference on computer vision pages 1 18. springer 2022. 1 2 32 ming liang bin yang wenyuan zeng yun chen rui hu sergio casas and raquel urtasun. pnpnet end-to-end per- ception and prediction with tracking in the loop. in proceed- ings of the ieee cvf conference on computer vision and pattern recognition pages 11553 11562 2020. 1 2 33 xuewu lin tianwei lin zixiang pei lichao huang and zhizhong su. sparse4d multi-view 3d object detec- tion with sparse spatial-temporal fusion. arxiv preprint arxiv 2211.10581 2022. 2 34 yingfei liu tiancai wang xiangyu zhang and jian sun. petr position embedding transformation for multi-view 3d object detection. in european conference on computer vi- sion pages 531 548. springer 2022. 35 yingfei liu junjie yan fan jia shuailin li aqi gao tian- cai wang and xiangyu zhang. petrv2 a uni\ufb01ed framework for 3d perception from multi-camera images. in proceedings of the ieee cvf international conference on computer vi- sion pages 3262 3272 2023. 2 36 zhijian liu haotian tang alexander amini xinyu yang huizi mao daniela l rus and song han. bevfusion multi- task multi-sensor fusion with uni\ufb01ed bird s-eye view repre- sentation. in 2023 ieee international conference on robotics and automation icra pages 2774 2781. ieee 2023. 6 3 37 zhe liu jinghua hou xinyu wang xiaoqing ye jingdong wang hengshuang zhao and xiang bai. lion linear group rnn for 3d object detection in point clouds. arxiv preprint arxiv 2407.18232 2024. 6 3 38 zhe liu jinghua hou xiaoqing ye tong wang jingdong wang and xiang bai. seed a simple and effective 3d detr in point clouds. arxiv preprint arxiv 2407.10749 2024. 6 3 39 wenjie luo bin yang and raquel urtasun. fast and furi- ous real time end-to-end 3d detection tracking and motion forecasting with a single convolutional net. in proceedings of the ieee conference on computer vision and pattern recog- nition pages 3569 3577 2018. 1 40 matthias minderer josip djolonga rob romijnders frances hubis xiaohua zhai neil houlsby dustin tran and mario lucic. revisiting the calibration of modern neu- ral networks. advances in neural information processing systems 34 15682 15694 2021. 4 41 arsalan mousavian dragomir anguelov john flynn and jana kosecka. 3d bounding box estimation using deep learn- ing and geometry. in proceedings of the ieee conference on computer vision and pattern recognition pages 7074 7082 2017. 2 42 lukas neumann andrew zisserman and andrea vedaldi. relaxed softmax ef\ufb01cient con\ufb01dence auto-calibration for safe pedestrian detection. 2018. 4 43 jiquan ngiam benjamin caine wei han brandon yang yuning chai pei sun yin zhou xi yi ouais al- sharif patrick nguyen et al. starnet targeted compu- tation for object detection in point clouds. arxiv preprint arxiv 1908.11069 2019. 2 44 jiquan ngiam benjamin caine vijay vasudevan zheng- dong zhang hao-tien lewis chiang jeffrey ling rebecca roelofs alex bewley chenxi liu ashish venugopal et al. scene transformer a uni\ufb01ed architecture for predicting mul- tiple agent trajectories. arxiv preprint arxiv 2106.08417 2021. 4 2 45 tung phan-minh elena corina grigore freddy a boulton oscar beijbom and eric m wolff. covernet multimodal behavior prediction using trajectory sets. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 14074 14083 2020. 2 46 aj piergiovanni vincent casser michael s ryoo and anelia angelova. 4d-net for learned multi-modal alignment. in proceedings of the ieee cvf international conference on computer vision pages 15435 15445 2021. 1 2 47 rui qian divyansh garg yan wang yurong you serge belongie bharath hariharan mark campbell kilian q weinberger and wei-lun chao. end-to-end pseudo-lidar for image-based 3d object detection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 5881 5890 2020. 2 48 cody reading ali harakeh julia chae and steven l waslander. categorical depth distribution network for monocular 3d object detection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 8555 8564 2021. 2 1458 page 11 49 shaoqing ren. faster r-cnn towards real-time object detection with region proposal networks. arxiv preprint arxiv 1506.01497 2015. 2 50 herbert robbins and sutton monro. a stochastic approxi- mation method. the annals of mathematical statistics pages 400 407 1951. 5 51 thomas roddick alex kendall and roberto cipolla. ortho- graphic feature transform for monocular 3d object detection. arxiv preprint arxiv 1811.08188 2018. 2 52 avishkar saha oscar mendez chris russell and richard bowden. translating images into maps. in 2022 interna- tional conference on robotics and automation icra pages 9200 9206. ieee 2022. 2 53 shaoshuai shi xiaogang wang and hongsheng li. pointr- cnn 3d object proposal generation and detection from point cloud. in proceedings of the ieee cvf conference on com- puter vision and pattern recognition pages 770 779 2019. 2 54 weijing shi and raj rajkumar. point-gnn graph neural net- work for 3d object detection in a point cloud. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 1711 1719 2020. 2 55 martin simony stefan milzy karl amendey and horst- michael gross. complex-yolo an euler-region-proposal for real-time 3d object detection on point clouds. in proceed- ings of the european conference on computer vision eccv workshops pages 0 0 2018. 2 56 shuran song and jianxiong xiao. deep sliding shapes for amodal 3d object detection in rgb-d images. in proceed- ings of the ieee conference on computer vision and pattern recognition pages 808 816 2016. 2 57 pei sun henrik kretzschmar xerxes dotiwalla aurelien chouard vijaysai patnaik paul tsui james guo yin zhou yuning chai benjamin caine et al. scalability in perception for autonomous driving waymo open dataset. in cvpr 2020. 2 3 5 6 7 58 pei sun mingxing tan weiyue wang chenxi liu fei xia zhaoqi leng and dragomir anguelov. swformer sparse window transformer for 3d object detection in point clouds. in european conference on computer vision pages 426 442. springer 2022. 1 59 balakrishnan varadarajan ahmed hefny avikalp srivas- tava khaled s refaat nigamaa nayakanti andre cornman kan chen bertrand douillard chi pang lam dragomir anguelov et al. multipath ef\ufb01cient information fu- sion and trajectory aggregation for behavior prediction. in 2022 international conference on robotics and automation icra pages 7814 7821. ieee 2022. 2 60 a vaswani. attention is all you need. advances in neural information processing systems 2017. 4 61 dominic zeng wang and ingmar posner. voting for voting in online point cloud object detection. in robotics science and systems pages 10 15. rome italy 2015. 2 62 haiyang wang chen shi shaoshuai shi meng lei sen wang di he bernt schiele and liwei wang. dsvt dy- namic sparse voxel transformer with rotated sets. in pro- ceedings of the ieee cvf conference on computer vision and pattern recognition pages 13520 13529 2023. 1 63 shihao wang yingfei liu tiancai wang ying li and xi- angyu zhang. exploring object-centric temporal modeling for ef\ufb01cient multi-view 3d object detection. in proceedings of the ieee cvf international conference on computer vi- sion pages 3621 3631 2023. 3 64 tai wang xinge zhu jiangmiao pang and dahua lin. fcos3d fully convolutional one-stage monocular 3d object detection. in proceedings of the ieee cvf international conference on computer vision pages 913 922 2021. 2 6 7 1 65 yan wang wei-lun chao divyansh garg bharath hari- haran mark campbell and kilian q weinberger. pseudo- lidar from visual depth estimation bridging the gap in 3d object detection for autonomous driving. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 8445 8453 2019. 2 66 yue wang alireza fathi abhijit kundu david a ross caroline pantofaru tom funkhouser and justin solomon. pillar-based object detection for autonomous driving. in computer vision eccv 2020 16th european conference glasgow uk august 23 28 2020 proceedings part xxii 16 pages 18 34. springer 2020. 2 67 yue wang vitor campagnolo guizilini tianyuan zhang yilun wang hang zhao and justin solomon. detr3d 3d object detection from multi-view images via 3d-to-2d queries. in conference on robot learning pages 180 191. pmlr 2022. 2 68 benjamin wilson william qi tanmay agarwal john lambert jagjeet singh siddhesh khandelwal bowen pan ratnesh kumar andrew hartnett jhony kaesemodel pontes et al. argoverse 2 next generation datasets for self-driving perception and forecasting. arxiv preprint arxiv 2301.00493 2023. 2 3 5 6 69 xiaoyang wu li jiang peng-shuai wang zhijian liu xi- hui liu yu qiao wanli ouyang tong he and hengshuang zhao. point transformer v3 simpler faster stronger. in pro- ceedings of the ieee cvf conference on computer vision and pattern recognition pages 4840 4851 2024. 2 70 bin xu and zhenzhong chen. multi-level fusion based 3d object detection from monocular images. in proceedings of the ieee conference on computer vision and pattern recog- nition pages 2345 2353 2018. 2 71 yan yan yuxing mao and bo li. second sparsely embed- ded convolutional detection. sensors 18 10 3337 2018. 2 1 72 bin yang wenjie luo and raquel urtasun. pixor real- time 3d object detection from point clouds. in proceedings of the ieee conference on computer vision and pattern recog- nition pages 7652 7660 2018. 2 73 zetong yang yanan sun shu liu and jiaya jia. 3dssd point-based 3d single stage object detector. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 11040 11048 2020. 2 74 zetong yang yin zhou zhifeng chen and jiquan ngiam. 3d-man 3d multi-frame attention network for object de- tection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 1863 1872 2021. 1 3 7 1459 page 12 75 tianwei yin xingyi zhou and philipp krahenbuhl. center- based 3d object detection and tracking. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 11784 11793 2021. 1 2 6 7 76 yurong you yan wang wei-lun chao divyansh garg geoff pleiss bharath hariharan mark campbell and kil- ian q weinberger. pseudo-lidar accurate depth for 3d object detection in autonomous driving. arxiv preprint arxiv 1906.06310 2019. 2 77 yurong you katie z luo xiangyu chen junan chen wei- lun chao wen sun bharath hariharan mark campbell and kilian q weinberger. hindsight is 20 20 leverag- ing past traversals to aid 3d perception. arxiv preprint arxiv 2203.11405 2022. 2 78 yihan zeng da zhang chunwei wang zhenwei miao ting liu xin zhan dayang hao and chao ma. lift learning 4d lidar image fusion transformer for 3d object detection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 17172 17181 2022. 1 2 79 ce zhang chengjie zhang yiluan guo lingji chen and michael happold. motiontrack end-to-end transformer- based multi-object tracking with lidar-camera fusion. in pro- ceedings of the ieee cvf conference on computer vision and pattern recognition pages 151 160 2023. 2 80 gang zhang junnan chen guohuan gao jianmin li si liu and xiaolin hu. safdnet a simple and effective net- work for fully sparse 3d object detection. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 14477 14486 2024. 2 6 7 8 1 81 gang zhang chen junnan guohuan gao jianmin li and xiaolin hu. hednet a hierarchical encoder-decoder net- work for 3d object detection in point clouds. advances in neural information processing systems 36 2024. 2 6 7 1 82 zixiang zhou xiangchen zhao yu wang panqu wang and hassan foroosh. centerformer center-based transformer for 3d object detection. in european conference on com- puter vision pages 496 513. springer 2022. 2 3 6 1460",
      "keywords": [
        "page",
        "mad",
        "memory-augmented",
        "detection",
        "objects",
        "ben",
        "agro",
        "sergio",
        "casas",
        "patrick",
        "wang",
        "thomas",
        "gilles",
        "raquel",
        "urtasun",
        "tgilles",
        "waabi.ai",
        "detector",
        "object",
        "memory",
        "augmented",
        "detections",
        "0.0s",
        "-2.4s",
        "age",
        "false",
        "negative",
        "figure",
        "detectors",
        "without",
        "long-term",
        "temporal",
        "fusion",
        "miss",
        "heavily",
        "occluded",
        "objects.",
        "approach",
        "enhances",
        "remember",
        "past",
        "predictions",
        "recovering",
        "from",
        "occlusion.",
        "green",
        "labels",
        "black",
        "lidar",
        "points",
        "gray.",
        "abstract",
        "perceive",
        "humans",
        "use",
        "\ufb01ll",
        "gaps",
        "caused",
        "limited",
        "visibility",
        "whether",
        "due",
        "occlusion",
        "nar-",
        "row",
        "\ufb01eld",
        "view.",
        "however",
        "most",
        "using",
        "sensor",
        "evidence",
        "short",
        "win-",
        "dow",
        "0.1s-0.3s",
        "work",
        "present",
        "simple",
        "ef-",
        "fective",
        "add-on",
        "enhancing",
        "any",
        "existing",
        "detec-",
        "tor",
        "regardless",
        "modality",
        "e.g.",
        "camera",
        "network",
        "architecture.",
        "pro-",
        "pose",
        "model",
        "effectively",
        "align",
        "fuse",
        "proposals"
      ],
      "metadata": {
        "filename": "Agro_MAD_Memory-Augmented_Detection_of_3D_Objects_CVPR_2025_paper.pdf",
        "original_filename": "Agro_MAD_Memory-Augmented_Detection_of_3D_Objects_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 2862588,
        "upload_file_id": "upload_1752168286257_bcj87gzif",
        "upload_timestamp": "2025-07-10T17:24:46.326758",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T17:24:46.824042+00:00",
      "content_length": 61240,
      "content_type": ".pdf",
      "filename": "Agro_MAD_Memory-Augmented_Detection_of_3D_Objects_CVPR_2025_paper.pdf"
    },
    "20": {
      "file_id": "20",
      "content": "\n=== Page 1 ===\nDeepCompress-ViT: Rethinking Model Compression to Enhance\nEfficiency of Vision Transformers at the Edge\nSabbir Ahmed1\u2217, Abdullah Al Arafat2\u2217, Deniz Najafi3\u2217, Akhlak Mahmood4, Mamshad Nayeem Rizve5,\nMohaiminul Al Nahian1, Ranyang Zhou3, Shaahin Angizi3, Adnan Siraj Rakin1\n1Binghamton University (SUNY), 2North Carolina State University,\n3New Jersey Institute of Technology, 4Georgia Institute of Technology, 5Adobe Inc.\nAbstract\nVision Transformers (ViTs) excel in tackling complex vi-\nsion tasks, yet their substantial size poses significant chal-\nlenges for applications on resource-constrained edge de-\nvices. The increased size of these models leads to higher\noverhead (e.g., energy, latency) when transmitting model\nweights between the edge device and the server. Hence,\nViTs are not ideal for edge devices where the entire model\nmay not fit on the device. Current model compression tech-\nniques often achieve high compression ratios at the expense\nof performance degradation, particularly for ViTs. To over-\ncome the limitations of existing works, we rethink model\ncompression strategy for ViTs from first principle approach\nand develop an orthogonal strategy called DeepCompress-\nViT. The objective of the DeepCompress-ViT is to encode\nthe model weights to a highly compressed encoded repre-\nsentation using a novel training method, denoted as Uni-\nfied Compression Training (UCT). Proposed UCT is accom-\npanied by a decoding mechanism during inference, which\nhelps to gain any loss of accuracy due to high compres-\nsion ratio. We further optimize this decoding step by re-\nordering the decoding operation using associative prop-\nerty of matrix multiplication, ensuring that the compressed\nweights can be decoded during inference without incurring\nany computational overhead.\nOur extensive experiments\nacross multiple ViT models on modern edge devices show\nthat DeepCompress-ViT can successfully compress ViTs at\nhigh compression ratios (> 14\u00d7). DeepCompress-ViT en-\nables the entire model to be stored on edge device, re-\nsulting in unprecedented reductions in energy consumption\n(> 1470\u00d7) and latency (> 68\u00d7) for edge ViT inference.\nOur code is available at https://github.com/ML-Security-\nResearch-LAB/DeepCompress-ViT.\n1. Introduction\nThe integration of computer vision and deep learning (DL)\ninto the Industrial Internet of Things (IIoT) and edge de-\n\u2217These authors contributed equally\n10\n20\n30\n40\n50\n60\n70\n80\nPerformance Comparison with SOTA ViT Compression Methods\nModel Size (MB)\nAccuracy (%)\nDeepCompress-ViT (\nLSQ (\n14.9x)\nDeepCompress-ViT\nQ-ViT (13.6x)\nDeiT-S (1x)\n13.6x)\nWDPruning (2.5x)\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\nUncompressed Model\nPruned Model\nQuantized Model\nFigure 1. Comparison of DeepCompress-ViT with SOTA ViT com-\npression methods [8, 11, 31] for compressing DeiT-S [27] trained\non Imagenet-1k [22]. Existing compression methods achieve a\nlarge compression ratio at the expense of large performance loss,\nwhereas the DeepCompress-ViT achieves the highest compression\nratio with the best accuracy among the competition.\nvices has revolutionized data processing, analysis, and ac-\ntion in these systems [3].\nBy deploying advanced mod-\nels, such as Vision Transformers (ViTs), on resource-\nconstrained devices, IIoT systems can directly manage\ncomplex tasks, including object detection, anomaly detec-\ntion, and facial recognition on the device. This local pro-\ncessing capability on edge minimizes data transmission to\ncentralized servers, reducing latency and enhancing data\nprivacy by keeping information closer to its source [35].\nIn recent years, Vision Transformers (ViTs) [7] have\ngained significant attention among deep neural network\n(DNN) models for their remarkable performance [20].\nTheir potential to tackle various real-world vision problems,\nsuch as object detection [4] and segmentation [26], makes\nthem especially appealing for edge applications. However,\nthe challenge of achieving reliable, real-time inference on\nresource-constrained devices cannot be underestimated, pri-\nmarily due to the large model sizes of ViTs in comparison\nto models like Convolutional Neural Networks [23].\nThe architectural design of ViTs often exceeds the on-\nchip memory limits of edge devices (e.g., Espressif ESP32-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n30147\n\n=== Page 2 ===\nS3 [1] has 8 MB pseudo-static RAM on-chip memory, but\nDeiT-S model size is approximately 85 MB), leading to a\nreliance on off-chip memory from servers to store the en-\ntire model. This dependency introduces two critical chal-\nlenges that significantly impede the use of large ViTs in\nedge environments [10]. Firstly, computations performed\non-chip are estimated to be 10 to 100 times faster than\nthose conducted off-chip (see Table 1 for a detailed numeri-\ncal comparison of various metrics between on- and off-chip\nmemories). Secondly, transferring sizable model weights\ndemands high communication bandwidth, resulting in ele-\nvated energy consumption. As a result, for edge applica-\ntions (e.g., real-time patient monitoring, autonomous vehi-\ncle perception, quality control in production) where latency\nand power efficiency are critical, the deployment of ViT\nmodels is severely limited. Therefore, addressing model\nsize and enhancing memory management are not just chal-\nlenges; they are essential for unlocking the full potential of\nViT models in IoT devices.\nTo effectively deploy large Vision Transformer (ViT)\nmodels at the edge, it is crucial to aggressively compress\nthem to fit into on-chip memory.\nAggressive compres-\nsion reduces reliance on off-chip memory, decreasing la-\ntency and energy consumption. However, techniques such\nas pruning [31] and quantization [11] often face challenges\nat high compression ratios, leading to significant drops in\nperformance. For instance, the DeiT-Small model, which is\napproximately 85 MB, would require over 10\u00d7 compression\nto fit into an 8 MB edge device. This extreme compression\nlevel can result in severe accuracy loss (please refer to Fig-\nure 1). Therefore, it is essential to carefully reconsider and\nrethink model compression strategies for ViTs to maintain\nboth performance and efficiency at the edge.\nIn this work, we begin by formulating the popular com-\npression strategies from the perspective of weight encod-\ning, as illustrated in Figure 2. Both pruning and quantiza-\ntion are techniques that encode model weights into a com-\npressed format. However, at high compression ratios, this\nweight encoding mechanism can become lossy, leading to\na decline in model performance. Our hypothesis is that to\nmitigate the performance loss due to encoded weights, it\nis essential to implement a decoding method during infer-\nence that can reconstruct or remap the weights back to their\noriginal (decompressed) state. This approach could enhance\nthe model\u2019s accuracy during inference by reversing the ef-\nfects of a lossy compression. Nonetheless, developing an\neffective compression strategy that achieves high compres-\nsion while also enabling accurate weight decoding for in-\nference involves overcoming several challenges, which our\nwork aims to resolve.\nTo this end, we propose a novel and unique perspective\non model compression, which enables aggressive compres-\nsion of ViT models while maintaining a decoding frame-\nFP\nPruning\nW\nOriginal\nWeights\nWP\nWP\nQ\nPruned weights\n        Moderate \nCompression Ratio\n        Very High \nCompression Ratio\n    Only encoded weights \n   and Decoder are stored \n      in on-chip memory\n            High \nCompression Ratio\nFQ\nQuantization\nW\nOriginal\nWeights\nW\nQ\nW\nQuantized\nWeights\nF\nDeepCompress-ViT\nW\nOriginal\nWeights\nW\nEncoded\nWeights\nDecoder\nNo Decoder\nNo Decoder\nF\n-1\nW\nDecoded\nWeights\nW\nEncoder\n         ViT Model\n         ViT Model\n         ViT Model\nInput Images\nInput Images\nInput Images\nLow Accuracy\nHigh Accuracy\nLow Accuracy\nFigure 2.\nExisting compression methods can be viewed as a\nform of weight encoding.\nHowever, inference with lossy en-\ncoded weights results in performance dorp whereas proposed\nDeepCompress-ViT maintains an optimized decoding method at\ntest time to maintain model performance.\nwork to reconstruct the weights back. The key idea is that\nthe compressed version of the weight and a lightweight de-\ncoder will be stored in memory to reduce memory costs.\nHowever, these weights will be reconstructed back to their\noriginal form using the decoder in a layer-by-layer sequence\nfor model inference and thus help to maintain accuracy.\nOur proposed DeepCompress-ViT consists of two phases\u2014\nUnified Compression Training and Optimized Test-Time De-\ncoding. In Unified Compression Training, we train an en-\ncoder and decoder alongside the model, learning a highly\ncompressed representation of its parameters through en-\ncoded embeddings. After training, we retain only these en-\ncoded weights and the decoder, discarding the encoder.\nNext, during inference, the goal is to Decode the com-\npressed parameters layer-by-layer systematically to recon-\nstruct the original parameters for each layer to compute\nthe next layers output. Nevertheless, the proposed layer-\nby-layer decoding approach comes with two notable draw-\nbacks.\nFirst, decoding the compressed representation of\neach layer back to its original form incurs extra compu-\ntational costs, such as increased energy use and process-\ning time. Additionally, this can lead to significant on-chip\nmemory usage, particularly in larger layers, when decoded\nweights will be stored on-chip temporarily to compute out-\nput of a given layer. Hence, we propose Optimized Test-\nTime Decoding, eliminating the need to decode embed-\ndings to their original parameters. This optimized decod-\ning step leverages the associative property of matrix mul-\ntiplication to reorder the decoding operation. This innova-\ntion eliminates both computation and memory usage draw-\nback associated with the decoding step, making proposed\n30148\n\n=== Page 3 ===\nDeepCompress-ViT best of all worlds (accuracy, memory,\nenergy and latency). Our experiments on SOTA ViTs and\nedge devices show that DeepCompress-ViT achieves com-\npression ratios exceeding 14\u00d7 while achieving unprece-\ndented reductions in energy (> 1470\u00d7) and latency (>\n68\u00d7), all with the least amount of accuracy loss among the\nSOTA compression methods.\n2. Related Work\nVision Transformers.\nThe introduction of Transformer\narchitecture [28] has marked a paradigm shift in machine\nlearning, initially transforming natural language process-\ning. Building on this success, the architecture was adapted\nfor computer vision, leading to the development of Vision\nTransformer (ViT) [7]. This success has led to a surge in\ntransformer-based models for computer vision. For exam-\nple, DeiT [27] introduced advanced training techniques and\na novel distillation approach to reduce ViT\u2019s dependence\non large datasets. Further architectural innovations, such as\nPyramid Vision Transformer (PVT) [29] and Swin Trans-\nformer [17], incorporated hierarchical feature representa-\ntions enabling transformers to serve as versatile frameworks\nfor diverse computer vision applications. Even though ViTs\nhave demonstrated strong performance, they require signif-\nicant memory that limits their application in edge devices.\nCompressing ViTs has thus emerged as an important area\nof research to address this large memory requirement.\nViT Compression.\nA widely used approach for com-\npressing Vision Transformers (ViT) is pruning [16, 30\u2013\n33, 36], which focuses on reducing the number of unimpor-\ntant weights in the model. Pruning methods for ViT models\nare commonly divided into two categories: unstructured and\nstructured pruning. Unstructured pruning techniques, such\nas magnitude-based and Hessian-based methods [24], re-\nmove individual weights based on certain importance met-\nrics. However, these techniques result in irregular sparsity\npatterns, which complicate tensor computations and limit\nhardware efficiency. Structured pruning mitigates this chal-\nlenge by removing larger, contiguous structures within the\nmodel, such as entire attention heads or rows of weight ma-\ntrices, which are more compatible with hardware require-\nments.\nFor instance, [31] proposed a structured pruning\nmethod for ViTs that uses a binary mask to distinguish im-\nportant from unimportant parameters based on magnitude.\nAnother effective approach, introduced in [33], combines\npruning, layer skipping, and knowledge distillation within a\nunified framework to create a compact ViT model.\nAnother widely adopted approach for compressing ViT\nmodels is quantization [8, 11\u201313, 15, 18], which converts\nfloating-point parameters to lower precision values. Sev-\neral quantization methods tailored to ViT\u2019s unique archi-\ntecture have been proposed.\nFor instance, Q-ViT [11]\nintroduced differentiable quantization for ViTs, treating\nquantization bit-widths and scales as learnable parameters.\nPTQ4ViT [18] utilizes twin uniform quantization and ap-\nplies a Hessian-guided metric to determine optimal scal-\ning factors. FQ-ViT [15] incorporates powers-of-two scale\nquantization and log-int quantization specifically for Layer-\nNorm and Softmax operations, respectively.\nWhile both pruning and quantization methods have\nshown great promise in reducing computational demands,\nthey still experience significant performance degradation at\nhigh compression ratios [11, 31], also shown in Table 2,\nseverely limiting their applicability in edge IoT devices.\n3. Challenges and Motivation\nTo motivate our compression technique, we identify the\ncritical bottleneck hindering efficient inference of Vision\nTransformers (ViTs) at the edge. We will also assess the\nlimitations of current compression methods, establishing\nthe necessary foundation for our proposed method.\n3.1. Challenges of Efficient Edge Inference\nThe deployment of ViTs on edge devices presents signif-\nicant challenges due to their substantial memory require-\nments, which often exceed the on-chip memory capac-\nity of these resource-constrained environments.\nConse-\nquently, model parameters must be stored in off-chip mem-\nory, necessitating a sequential processing approach during\ninference. This approach involves iteratively loading each\nlayer\u2019s weights into on-chip memory and performing com-\nputations on that layer before proceeding to the next. While\nthis method enables the inference of ViTs on devices with\nlimited on-chip memory, it introduces significant latency\nand increased energy consumption due to frequent off-chip\nmemory accesses, both of which are critical concerns in\nresource-constrained edge environments (e.g., autonomous\nvehicle perception).\nTable 1. Comparison of On-chip and Off-chip Memory Access\nCost in Edge Devices\nMemory Type\nAccess Time (ns)\nEnergy per Access (pJ)\nAvailable Storage (MB)\nOn-chip\n1-10\n5-20\u2020\n1-8\nOff-chip\n70-200\n1300-2600\u00a7\n1024-8192\u2217\n\u202032b SRAM Read (32KB), \u00a732b DRAM [10], \u2217For edge AI devices.\nOne potential solution to mitigate this frequent off-chip\nmemory access is model compression. Model compression\ncan reduce the overall memory footprint and allow the en-\ntire model to be stored in on-chip memory, which can min-\nimize/eliminate the need for costly memory access. How-\never, as shown in Table 1, the available on-chip memory\nis often severely constrained (e.g., \u223c1-8 MB), and achiev-\ning this would require aggressive compression (e.g., greater\nthan 10\u00d7).\nOne of the widely explored approaches for model com-\npression is pruning. Numerous studies [24, 31, 33] have\napplied pruning techniques on ViTs to reduce their memory\nand computation costs. While pruning has demonstrated\nsignificant success in lowering the computational demands\n30149\n\n=== Page 4 ===\nof these models, our investigation reveals that achieving\nhigh compression through pruning in ViTs is challenging\nwithout severely impacting model performance. As shown\nin Table 2, even with a 2.5 \u00d7 compression ratio, the model\nsuffers \u223c19 % accuracy loss.\nAnother widely adopted approach for improving model\nefficiency is quantization\n[11, 15, 18].\nQuantization\nhas proven particularly effective in reducing the memory\nfootprint of large models, such as large language models\n(LLMs). The existing literature demonstrates that extreme\nquantization can be successfully applied to LLMs [19] with\nminimal performance loss. However, quantization applied\nto ViTs presents a different challenge. Studies focusing on\nViTs [11, 34] have shown that these models suffer from\nsignificant performance degradation when subjected to ex-\ntreme quantization (see Table 2).\nTable 2. Evaluating existing compression methods by aggressively\ncompressing a DeiT-S model and highlighting their shortcoming\nin achieving our ideal objective (4th row) for edge inference.\nMethod\nOriginal Size (MB)\nCompression Ratio\nAccuracy (%)\nOriginal Model\n84.1\n1\u00d7\n79.72\nPruning [11]\n33.7\n2.5\u00d7\n60.51\nQuantization [31]\n6.18\n13.6\u00d7\n71.90\nOur Objective\n1-8 ( constraint at edge)\n10-80\u00d7 (required)\n79.72 (ideally)\nNext, we dissect these two popular model compres-\nsion strategies to determine the fundamental limitation that\nhurts the state-of-the-art model compression technique and\nwhether we can develop an orthogonal approach to over-\ncome them at our targeting edge applications.\n3.2. Motivation\nAs shown in Figure 2, both pruning and quantization can\nbe viewed as a form of encoding original model weights to\na compressed representation. The fundamental challenge\nlies in the way these compression techniques encode model\nweights. In this work, we postulate that the primary factor\ncontributing to this performance loss can be attributed to the\nlarge weight perturbation error introduced by the encoding\nmechanism of these compression techniques, as formalized\nin the following lemma.\nLemma 3.1. (Error Bound for Compression in DNNs).\nLet w represent the weights of a deep neural network (DNN)\nwith an associated loss function L(w). Consider a com-\npression technique that encodes the weights using a trans-\nformation F resulting in w\u2032 = F(w) and introducing a\nperturbation \u03b4w = w\u2032 \u2212w. The change in loss due to this\ncompression can be upper bounded as follows:\n|L(w\u2032) \u2212L(w)| \u2264\u2225\u2207wL(w)\u2225\u2225\u03b4w\u2225+ 1\n2\u2225H\u2225\u2225\u03b4w\u22252,\nwhere \u2207wL(w) is the gradient of the loss function with re-\nspect to the original weights w, \u2225H\u2225denotes the norm of\nthe Hessian matrix of L(w), and \u2225\u03b4w\u2225is the norm of the\nweight perturbation introduced by the compression.\nFrom Lemma 3.1, it is evident that a smaller weight per-\nturbation \u2225\u03b4w\u2225leads to a tighter upper bound on the loss\ndifference |L(w\u2032) \u2212L(w)|. Therefore, an ideal compres-\nsion method should minimize the weight perturbation error.\nHowever, for edge application of ViTs, we are forced to\nperform aggressive compression that will inevitably lead to\na higher \u2225\u03b4w\u2225.\nNeither pruning nor quantization has a countermeasure\nagainst this high \u2225\u03b4w\u2225for aggressive compression, laying\ndown the foundation for our proposed method as to whether\nwe can device a technique to reduce \u2225\u03b4w\u2225at the infer-\nence stage even at a high compression ratio. The research\nquestion that motivates our work: Given that weight encod-\ning for compression inevitably introduces significant weight\nperturbation error at high compression ratio, an effective\napproach to mitigate this error may lie in coupling the en-\ncoding mechanism with a complementary decoding mecha-\nnism that can reverse the lossy encoding, as formalized in\nthe following proposition.\nProposition 3.2. (Lower Error Bound with Weight Decod-\ning). Let w represent the weights of a deep neural net-\nwork (DNN) and consider a compression method that en-\ncodes these weights using a transformation F, introducing a\nweight perturbation \u03b4w = w\u2032 \u2212w. Now consider a weight\ndecoding mechanism with transformation G, where G ap-\nproximates the inverse of F. Then, the weight perturbation\nerror after decoding the weights can be bounded as follows:\n\u2225G(F(w)) \u2212w\u2225\u2264\u2225\u03b4w\u2225.\nThe above proposition demonstrates the potential of a\nweight decoding strategy to reduce weight perturbation and\nminimize the impact on model performance at inference.\nInspired by Proposition 3.2, we aim to design a novel\nEncoder-Decoder-based model compression approach that\ncan enable the potential of achieving a high compression\nratio while minimizing inference loss.\n4. Proposed Method: DeepCompress-ViT\nWe propose DeepCompress-ViT, a novel compression\nframework designed for achieving a high compression ra-\ntio while preserving inference accuracy. As shown in Fig-\nure 3, our novel DeepCompress-ViT framework consists of\ntwo stages. In the first stage, we propose Unified Com-\npression Training, which jointly trains the encoder and de-\ncoder networks (see Figure 3). The encoder network, de-\nnoted as F, learns to encode the original model weights w\nfrom Rd to Rr, where r \u226ad. And the decoder network,\nG, learns to decode the encoded weights back to Rd. Af-\nter training is complete, only the encoded weights and the\ndecoder network are retained for inference, while the en-\ncoder network is discarded. During inference, the proposed\nDeepCompress-ViT reconstructs each layer\u2019s weight one by\n30150\n\n=== Page 5 ===\nF\nW\nG\n         ViT Model\nOriginal \nWeights\n    Encoder\n    Decoder\nUnified Compression Training\nz\n    Encoded \n     Weights\nW\n        \n    Decoded \n     Weights\n        \nLoad\nInput Images\nPrediction\nW\nG\n    Only Encoded weights \n   and Decoder are stored \n      in on-chip memory \n           for Inference\nFigure 3. Unified Compression Training. The Encoder and De-\ncoder parameters are optimized jointly to minimize the loss. After\ntraining, only encoded weights and decoder network is retained\nfor inference\none and performs inference using the decoded version of the\nweight to help maintain inference accuracy.\nWhile the decoding step is necessary to maintain accu-\nracy, it has two drawbacks. To understand this, lets consider\na simplified form the output computation for a given layer:\ninput \u00d7 encoded weight \u00d7 decoder weight. First, the de-\ncoding step will be performed using encoded weight \u00d7\ndecoder weight which introduces additional computa-\ntion at runtime, and second, the reconstructed version of\nthe weight will increase temporary on-chip memory con-\nsumption, especially for larger layers.\nTo address this,\nwe propose optimizing the decoding step by designing a\nlightweight decoder implemented as a single linear layer.\nIn addition, we leverage the associative property of ma-\ntrix multiplication to transform the inefficient weight decod-\ning input \u00d7 (encoded weight \u00d7 decoder weight) to opti-\nmized test-time decoding as (input \u00d7 encoded weight) \u00d7\ndecoder weight, which would ensure the computation cost\nremains unchanged for an optimum level of decoder input\n(compressed weight size r outlined in equation 4 in de-\ntails) is selected. This optimization ensures the proposed\nDeepCompress-ViT does not introduce any computational\noverhead (Figure 4) in pursuit of compression and accuracy.\n4.1. Unified Compression Training (UCT)\nIn this stage, an encoder-decoder network is trained to\nachieve aggressive compression. The encoder network F,\nlearns to encode the model\u2019s weights to a compressed rep-\nresentation as the encoded embedding, while the decoder\nnetwork, G, learns to decode the weights back to their orig-\ninal dimension as shown in Figure 3.\nConsider a ViT model HW(\u00b7) consisting of L layers,\nwhere the weights of the layers are denoted by the set\nW = {w1, w2, . . . , wL}. Let the parameters of the en-\ncoder network F(.) be represented as WE, and the param-\neters of the decoder network G(.) be represented as WD.\nDuring training, each layer\u2019s weight wi is passed\nthrough the encoder network F, which encodes it to a lower-\ndimensional representation, given by zi = F(wi). The de-\ncoder network G then decodes zi back to its original di-\nmensionality as \u02c6wi = G(zi). The encoder and decoder\nnetworks are trained jointly to minimize the mean squared\nerror (MSE) between the original weight set (W) and the\nreconstructed weight set ( \u02c6\nW). The weight reconstruction\nloss is calculated as:\nLmse = 1\nL\nL\nX\ni=1\n\u2225wi \u2212G(F(wi))\u22252\nTo optimize this training step further, we unify the\nencoder-decoder training step with the original ViT-model.\nOur hypothesis is that only MSE loss-based weight recon-\nstruction may not provide optimal performance [2]. Even\nthough minimizing MSE loss results in the minimization of\nweight perturbation error, different sets of decoded weights\nwith the same MSE value can have varying weight distri-\nbutions, which can lead to a performance drop in the ViT\nmodel. Hence, it is reasonable to incorporate the ViT model\ninto this training loop to align the ViT model\u2019s weight dis-\ntribution with the decoded version of the weight.\nHence, we propose this end-to-end unified framework\nwhere the encoder-decoder networks are jointly trained with\nthe ViT model. Our method will incorporate cross-entropy\n(CE) loss and knowledge distillation (KD) from a pre-\ntrained model in training, where CE ensures the ViT model\nperformance is optimized on the decoded weight values\nw.r.t ground truth, and KD ensures the decoded weight pa-\nrameters do not drift far apart from the initial weight distri-\nbution of a pre-trained ViT model.\nThe forward pass for the whole end-to-end training af-\nter obtaining the decoded weight sets\n\u02c6\nW from the de-\ncoder is formulated as HW(\u00b7) \u2192H \u02c6\nW(\u00b7), which implies\nthat the ViT model inference will always use the decoded\nweights, i.e., the forward pass of the ViT model becomes\n\u02c6y = H \u02c6\nW(x), where x are the input images and \u02c6y are the\npredictions from the ViT model. Next, using \u02c6y, we com-\npute the cross-entropy loss and knowledge distillation loss\nas follows:\nLCE = \u2212\nX\ni\nyilog(\u02c6yi),\nLKD =\nX\ni\n\u02c6yt,i log\n\u0012 \u02c6yt,i\n\u02c6yi\n\u0013\nwhere \u02c6yt represents the prediction from the pretrained\nteacher model. The total loss function is then defined as:\nLtotal = Lmse + \u03b1 \u00b7 LCE + \u03b2 \u00b7 LKD,\nwhere \u03b1 and \u03b2 are scaling factors controlling the contri-\nbutions of cross-entropy and knowledge distillation losses,\nrespectively. Finally, the overall loss function is minimized\nby jointly optimizing the encoder and decoder parameters\nWE and WD as follows:\nmin\nWE,WD Ltotal\n30151\n\n=== Page 6 ===\n              Inefficient Weight Decoding\n    x\n N x C\n   \n   w\nC x d\n   z\nC x r\n  w\nr x d\nMatmul (FLOPs: 2Crd)\nAdditional FLOPs\nTotal FLOPs: 2NCd + 2Crd\nMatmul (FLOPs: 2NCd)\n       Weight \n     Decoding\n      Total FLOPs <= 2NCd\n      2NCd\n      2N(C + d)\n      r <= \nForward Pass\n  \nw\nxi\nxi+1\ni\ni\n    x\n N x C\n   \ni\ni\n   z\nC x r\ni\ni\nCompressed \n     weight\n      Weight of  \n       Decoder\nz\nwD\nD\n  w\nr x d\nD\ni\n    Input\nActivation\n   Output\n activation\n    x'\n N x r\n   \n \n         \nMatmul (FLOPs: 2NCr)\nMatmul (FLOPs: 2Nrd)\n         Input  \n  Compression \n    Optimized\n  forward pass\n                              Optimized Test-Time Decoding\nx'\nxi+1\ni\ni\nCompressed \n     weight\nWe set\nz\n     Weight of \n      Decoder\nwD\ni\n    Input \nactivation\nxi\n    Output\n  activation\nFigure 4. Proposed Optimized Test-Time Decoding overview. Decoding the encoded (compressed) weights first and then matrix multiplying\nwith input activation increases FLOPs. The proposed method eliminates extra FLOPs by multiplying input activation with encoded weights\nfirst, then passing through decoder. The chosen value of encoding dimension (r) prevents any FLOP increase.\nAfter training is complete, for inference, we keep only\nthe encoded weight parameters {zi} and a lightweight de-\ncoder WD in memory while discarding the encoder WE.\nAt inference, the decoder needs to map compressed\nweights to their original form layer by layer, which may\nhelp preserve accuracy. However, this can introduce two\nmajor drawbacks. First, the decoding step will inevitably\nlead to additional computation at runtime, and second, the\ndecoded weights will increase temporary on-chip memory\nconsumption, particularly for larger layers. Next, we out-\nline our optimized test-time decoding strategy, which en-\nsures these two bottlenecks remain in check.\n4.2. Optimized Test-Time Decoding\nTo mitigate the overhead associated with the decoding step,\nfirst design level optimization we propose is to design the\ndecoder G as a single linear layer. This design choice min-\nimizes repetitive decoding operations, reducing the compu-\ntational overhead. Although a single-layer decoder network\nlowers the computational load relative to a multi-layer de-\ncoder, the computation needed for decoding weights using\na single layer still persists.\nTo illustrate the overhead of weight decoding step, con-\nsider a fully connected layer in a ViT model with original\nweights wi \u2208RC\u00d7d, whose weights are encoded and the\nencoded weights are represented by zi \u2208RC\u00d7r. During in-\nference of this layer, the weights are first decoded using the\nsingle-layer decoder network G, where weights of the de-\ncoder network is represented by wD \u2208Rr\u00d7d. This weight\ndecoding computation can be represented as follows:\n\u02c6wi = zi \u00d7 wD,\n(1)\nwhere \u02c6wi \u2208RC\u00d7d denotes the decoded weights. One of the\nmost widely used metrics to evaluate computational com-\nplexity is Floating-point Operations (FLOPs) [6], which\ncounts the total number of multiplications and additions\nperformed in a given operation. The FLOPs incurred for\nthe weight decoding operation is 2 \u00b7 C \u00b7 r \u00b7 d, corresponding\nto the matrix multiplication in eqn. (1).\nNext, the usual forward pass of fully connected layer in\nthe ViT model with input activations xi \u2208RN\u00d7C and de-\ncoded weights \u02c6wi is computed as follows:\nxi+1 = xi \u00d7 \u02c6wi,\n(2)\nwhere xi+1 represents the output activations of the layer.\nThe FLOPs incurred by this forward pass is 2 \u00b7 N \u00b7 C \u00b7 d,\nresulting from the matrix multiplication in eqn. (2). There-\nfore, the total FLOPs incurred in the forward pass with this\nstrategy is (2 \u00b7 N \u00b7 C \u00b7 d + 2 \u00b7 C \u00b7 r \u00b7 d) where the additional\n2 \u00b7 C \u00b7 r \u00b7 d FLOPs is incurred because of this inefficient\nweight decoding strategy.\nNext, we optimize the decoding step to eliminate this\nadditional 2 \u00b7 C \u00b7 r \u00b7 d FLOPs, we propose Optimized Test-\nTime Decoding. Our proposed decoding approach lever-\nages the associative property of matrix multiplication to re-\norder computation sequence to maintain or improve overall\nFLOPs. To illustrate, recall that the overall computation for\ninefficient decoding strategy is as follows:\nxi+1 = xi \u00d7 \u02c6wi = xi \u00d7 (zi \u00d7 wD),\n(3)\nwhere it shows weight decoding first and then perform the\nforward pass in the fully connected layer of ViT model. We\noptimize the decoding step by simply reorder the matrix\nmultiplications as follows:\nxi+1 = x\u2032\ni \u00d7 wD = (xi \u00d7 zi) \u00d7 wD,\n30152\n\n=== Page 7 ===\nwhere we first perform input compression (x \u2208RN\u00d7C \u2192\nx\u2032 \u2208RN\u00d7r ) and then do the optimized forward pass (x\u2032 \u00d7\nwD). The proposed strategy is also illustrated in Figure 4.\nThe total FLOPs for this optimized approach is 2 \u00b7 N \u00b7\nr \u00b7 (C + d). However, with an arbitrary value of r, the\noverall FLOPs can still go up. Therefore, to ensure that\nthe total FLOPs of the proposed decoding strategy do not\nexceed the baseline models FLOPs, we derive the optimal r\nfor efficiency by solving the following inequality,\n2 \u00b7 N \u00b7 r \u00b7 (C + d) \u22642 \u00b7 N \u00b7 C \u00b7 d,\nwhere 2 \u00b7 N \u00b7 C \u00b7 d is the FLOPs incurred due to normal\nforward pass in eqn. (2). Solving the inequality, we get:\nr \u2264\n2 \u00b7 N \u00b7 C \u00b7 d\n2 \u00b7 N \u00b7 (C + d)\n(4)\nIn summary, by reordering the computation sequence and\nusing optimal r for compression derived in eqn. (4), our\ninnovative decoding optimization does not incur any addi-\ntional computation.\n5. Experimental Results\n5.1. Datasets and Implementation Details\nDatasets. We conduct our experiments on the ILSVRC12\nImageNet classification dataset [22] consisting 1,000\nclasses with 1.2 million training images and 50k validation\nimages. Additionally, we evaluate DeepCompress-ViT on\nthe widely used COCO benchmark dataset [14] for object\ndetection and instance segmentation task.\nEncoder-Decoder Architecture. We use four encoder net-\nwork each with 2 linear layers to encode four types of lay-\ners: attn qkv, attn proj, mlp fc1 and mlp fc2.\nEach encoder has an affine transformation before and after\nthe 2 linear layers. Similarly to encoders, we use four sin-\ngle linear layer decoders to decode the four types of layers.\nNote that each decoder is a shared decoder which decodes a\nparticular type of layer of all ViT blocks. Note that, we ex-\nclude the first layer, last layer, and LayerNorm parameters\nfrom compression.\nExperimental Settings. The encoder and decoder models\nare trained for 275 epochs with a batch size of 256 and a\nbase learning rate of 1 \u00d7 10\u22124, using a cosine learning rate\nscheduler. For data augmentation, we apply the standard\ntechniques used in [27] and for knowledge distillation, we\nuse pretrained distilled DeiT models [27] with temperature\n\u03c4 = 1. For UCT, we set the value of \u03b1 = 1 and \u03b2 = 3\u00d7103.\nThe encoded weights are quantized using 4-bit asymmet-\nric quantization, while decoder weights are quantized us-\ning 4-bit symmetric quantization, both trained using Quan-\ntization Aware Training. Additionally, the patch embedding\n(first) and classification (last) layers are quantized using 8-\nbit symmetric quantization. The optimal value of r from\nsolving (4) is set to 277 for DeiT-S and 502 for DeiT-B.\nFor object detection and instance segmentation, we use the\nmmdetection library with the standard 1\u00d7 training sched-\nuler and adopt an edge-friendly resolution of 384 \u00d7 384 for\nboth training and evaluation.\n5.2. Results\nImage Classification.\nPerformance of the compressed\nDeiT models with proposed DeepCompress-ViT on the\nclassification task is shown in Table 3. The results demon-\nstrates substantial reductions in model size and energy con-\nsumption resulting from DeepCompress-ViT. For instance,\nthe DeepCompress-DeiT-S model achieves a compression\nratio of 14.9\u00d7, reducing the model size from 84.1 MB to\njust 5.64 MB while maintaining an accuracy of 78.74%\non ImageNet. Similarly, DeepCompress-DeiT-B achieves a\ncompression ratio of 17.4\u00d7, decreasing the model size from\n330.2 MB to 18.9 MB. Remarkably, both models show an\nunprecedented energy reduction of 673\u00d7 and 1475\u00d7 and\nlatency reduction of 44.5\u00d7 and 68.4\u00d7 respectively (please\nrefer to section 5.3 and Figure 5 for more hardware energy\nand latency evaluation details). Proposed DeepCompress-\nViT achieves this unprecedented level of model compres-\nsion, energy, and latency reduction, enabling the deploy-\nment of ViT models at the edge while only sacrificing a\nmarginal level of accuracy (e.g., for DeiT-B only \u223c0.1%)\nObject Detection and Instance Segmentation. Next, ta-\nble 4 compares the performance of DeepCompress-ViT\nwith baseline (no compression) DeiT-S model on object de-\ntection and instance segmentation tasks. We trained both\nthe models on COCO dataset integrating the respective ViT\nbackbones with Mask R-CNN [9].\nWe evaluate both of\nthe models for these tasks at an edge-friendly resolution of\n384 \u00d7 384. Notably, DeepCompress-ViT reduces the ViT\nbackbone (DeiT-S) size from 82.65MB to 5.27MB (15.7\u00d7\ncompression) with a slight drop in APb and APm, further\nhighlighting the effectiveness of our compression method\non other complex vision tasks.\nComparison with SOTA compression methods. Finally,\nTable 5 provides a comparative analysis of DeiT-Small\nmodels compressed using different pruning and quantiza-\ntion methods.\nAmong the methods, DeepCompress-ViT\nachieves the highest compression ratio of 14.9\u00d7, reducing\nthe model size to 5.64 MB while maintaining a top-1 ac-\ncuracy of 78.74%. In contrast, popular WDPruning [31],\nLSQ [8] and Q-ViT [11] achieves achieve much lower accu-\nracies of 60.51%, 68.00% and 71.9%, respectively. Our pro-\nposed DeepCompress-ViT consistently outperforms them\nby large margins of \u223c18 %, \u223c10 %, and \u223c7%, respec-\ntively, making DeepCompress-ViT the only practical choice\nto maintain accuracy while performing aggressive compres-\nsion.\n30153\n\n=== Page 8 ===\nTable 3. Performance of DeepCompress-ViT on ImageNet-1K. DeepCompress-ViT achieves memory (\u226515\u00d7) and energy consumption\n(\u22651531\u00d7), while marginal accuracy drop (<1 %).\nMethod\nAccuracy (%)\nModel Size (MB)\nCompression Ratio\nGFLOPs\nExecution Time (s)\nEnergy Consumption (J)\nDeiT-S [27]\n79.72\n84.1\n1x\n9.20\n24.02\n9979.52\nDeepCompress-DeiT-S\n78.74\n5.64\n14.9x\n8.88\n0.54\n14.83\nDeiT-B [27]\n81.74\n330.2\n1x\n35.14\n129.32\n150895\nDeepCompress-DeiT-B\n81.62\n18.9\n17.4x\n30.84\n1.89\n102.29\nTable 4. Performance of DeepCompress-ViT for Object Detection\non the COCO validation set with Mask R-CNN evaluated at an\nedge-friendly resolution of 384\u00d7384.\nMethod\nAPb\nAPm\nViT Backbone Size (MB)\nDeiT-S [27]\n26.2\n23.7\n82.65\nDeepCompress-ViT\n23.0\n20.9\n5.27 (15.7\u00d7)\nTable 5. Performance comparison of DeiT-Small with SOTA com-\npression methods on ImageNet-1K\nModel\nAccuracy (%)\nModel Size (MB)\nCompression Ratio\nDeiT-S [27]\n79.72 (0)\n84.1\n1x\nWDPruning [31]\n60.51 (\u219319.21)\n33.7\n2.5x\nLSQ [8]\n68.00 (\u219311.72)\n6.18\n13.6x\nQ-ViT [11]\n71.90 (\u21937.82)\n6.18\n13.6x\nDeepCompress-ViT\n78.74 (\u21930.98)\n5.64\n14.9x\n5.3. Hardware Evaluation\nHigh compression gain gives an understanding of the effec-\ntiveness of a compression method in terms of memory sav-\nings. But to successfully realize the full potential of our pro-\nposed method, we evaluate the energy and latency gain on\nhardware evaluation platforms. For hardware deployment,\nwe use the gem5-Aladdin platform [25], which effectively\ncaptures dynamic interactions between accelerators and the\nSoC platform, enabling detailed architectural analysis of the\nproposed compression method. Our setup incorporates a\ncycle-accurate in-house-developed model based on the Ey-\neriss accelerator [5] (0.0029 DRAM access/multiply and\naccumulation (MAC), 33.6 GMAC/s). Performance met-\nrics from gem5-Aladdin are then analyzed using DRAM-\nSim2 [21] to model realistic off-chip memory latency and\nenergy characteristics. The total on-chip weight memory\nis capped at three configurations (2 MB, 4 MB, 8 MB).\nThe analyzed DeiT-S model initially requires 84.1MB of\nmemory, which exceeds the on-chip memory capacity. By\napplying DeepCompress-ViT algorithm, this requirement is\nreduced to 5.64 MB, which can readily fit into the 8MB on-\nchip memory. However, with a smaller memory capacity, it\nis not feasible to transfer the entire set of weight parameters\nto the accelerator in a single batch due to the model\u2019s mem-\nory demands. Therefore, we implemented a weight parser\nthat transmits weight parameters in variable batch sizes to\nthe accelerator. Figure 5 shows the breakdown of energy\nconsumption and latency across MAC, off-chip, and on-\nchip memory access components for various on-chip mem-\nory sizes. For both DeiT-S and DeiT-B, the DeepCompress-\nViT algorithm achieves significant energy savings, reduc-\n8MB\n4MB\n2MB\nOn-chip Memory Size\n10-4\n10-2\n100\n102\n104\n106\nEnergy Consumption (J)\nMAC\nOn-chip Memory Access\nOff-chip Memory Access\nDeiT-S\nDeiT-B\nDC-DeiT-B\nDC-DeiT-S\n9979.52\n14.83\n8MB\n4MB\n2MB\nOn-chip Memory Size\n10-1\n100\n101\n102\n103\nLatency (s)\nMAC\nOn-chip Memory Access\nOff-chip Memory Access\n(a)\n(b)\n24.02\n0.54\n129.32\n1.89\n150895\n102.29\nFigure 5. Breakdown of (a) Energy consumption, (b) Latency vs.\non-chip memory size.\nFor every off-chip memory size, DeiT-S,\nDeepCompress-ViT-S, DeiT-B, and DeepCompress-ViT-B results\nare plotted, respectively.\ning consumption by approximately 673\u00d7 and 1475\u00d7, re-\nspectively, with an 8MB on-chip memory. This remarkable\nreduction is primarily due to the DeepCompress-ViT algo-\nrithm\u2019s ability to minimize off-chip memory access, as visi-\nble through the high difference in the red bar plots. This re-\nduction becomes even more pronounced with smaller mem-\nory sizes, enabling for the first time the possibility of de-\nploying large ViT models at the edge and performing time-\nsensitive applications.\n6. Conclusion\nIn this paper, we present DeepCompress-ViT, a novel ap-\nproach to achieve highly compressed Vision Transformers\noptimized for efficient edge deployment. We first analyze\nthe limitations of existing model compression techniques\nand show that they lead to large weight perturbation error\nbecause of their lossy encoding mechanism. Then, to re-\nsolve this challenge, we develop an orthogonal compression\ntechnique that consists of an encoding for compression and\na decoding step to help maintain performance. However,\ndecoding weights repeatedly during inference can lead to\nadditional computation and increase on-chip memory us-\nage, both of which are critical concerns for edge applica-\ntions. To address these challenges, we propose a novel opti-\nmized Test-Time Decoding scheme that enables efficient in-\nference without increased computation and avoids high on-\nchip memory usage. Our extensive experiments and hard-\nware evaluation show that the proposed DeepCompress-ViT\ncan lead to an unprecedented reduction in energy and execu-\ntion time with minimal loss of performance, unlocking the\npossibilities of deploying large-scale ViT models at edge\nIIoT devices.\n30154\n\n=== Page 9 ===\nReferences\n[1] Esp32-s3 designed for aiot applications, 2022. 2\n[2] Sabbir Ahmed, Uday Kamal, and Md Kamrul Hasan. Dfr-\ntsd: A deep learning based framework for robust traffic\nsign detection under challenging weather conditions. IEEE\nTransactions on Intelligent Transportation Systems, 23(6):\n5150\u20135162, 2021. 5\n[3] Jiang Bian, Abdullah Al Arafat, Haoyi Xiong, Jing Li, Li\nLi, Hongyang Chen, Jun Wang, Dejing Dou, and Zhishan\nGuo. Machine learning in real-time internet of things (iot)\nsystems: A survey. IEEE Internet of Things Journal, 9(11):\n8364\u20138386, 2022. 1\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European confer-\nence on computer vision, pages 213\u2013229. Springer, 2020. 1\n[5] Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne\nSze. Eyeriss: An energy-efficient reconfigurable accelera-\ntor for deep convolutional neural networks. IEEE journal of\nsolid-state circuits, 52(1):127\u2013138, 2016. 8\n[6] Radosvet Desislavov, Fernando Mart\u00b4\u0131nez-Plumed, and Jos\u00b4e\nHern\u00b4andez-Orallo. Compute and energy consumption trends\nin deep learning inference. arXiv preprint arXiv:2109.05472,\n2021. 6\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. arXiv preprint arXiv:2010.11929, 2020. 1, 3\n[8] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani,\nRathinakumar Appuswamy, and Dharmendra S Modha.\nLearned\nstep\nsize\nquantization.\narXiv\npreprint\narXiv:1902.08153, 2019. 1, 3, 7, 8\n[9] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961\u20132969, 2017. 7\n[10] Mark Horowitz. 1.1 computing\u2019s energy problem (and what\nwe can do about it). In 2014 IEEE international solid-state\ncircuits conference digest of technical papers (ISSCC), pages\n10\u201314. IEEE, 2014. 2, 3\n[11] Yanjing Li, Sheng Xu, Baochang Zhang, Xianbin Cao, Peng\nGao, and Guodong Guo. Q-vit: Accurate and fully quantized\nlow-bit vision transformer. Advances in neural information\nprocessing systems, 35:34451\u201334463, 2022. 1, 2, 3, 4, 7, 8\n[12] Zhikai Li, Liping Ma, Mengjuan Chen, Junrui Xiao, and\nQingyi Gu. Patch similarity aware data-free quantization for\nvision transformers. In European conference on computer\nvision, pages 154\u2013170. Springer, 2022.\n[13] Zhikai Li, Junrui Xiao, Lianwei Yang, and Qingyi Gu. Repq-\nvit: Scale reparameterization for post-training quantization\nof vision transformers. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 17227\u2013\n17236, 2023. 3\n[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean Conference on Computer Vision, pages 740\u2013755.\nSpringer, 2014. 7\n[15] Yang Lin, Tianyu Zhang, Peiqin Sun, Zheng Li, and\nShuchang Zhou.\nFq-vit:\nPost-training quantization\nfor fully quantized vision transformer.\narXiv preprint\narXiv:2111.13824, 2021. 3, 4\n[16] Ji Liu, Dehua Tang, Yuanxian Huang, Li Zhang, Xiaocheng\nZeng, Dong Li, Mingjie Lu, Jinzhang Peng, Yu Wang, Fan\nJiang, et al. Updp: A unified progressive depth pruner for\ncnn and vision transformer. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, pages 13891\u201313899, 2024.\n3\n[17] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 10012\u201310022, 2021. 3\n[18] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma,\nand Wen Gao. Post-training quantization for vision trans-\nformer. Advances in Neural Information Processing Systems,\n34:28092\u201328103, 2021. 3, 4\n[19] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wen-\nhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong\nXue, and Furu Wei. The era of 1-bit llms: All large language\nmodels are in 1.58 bits. arXiv preprint arXiv:2402.17764,\n2024. 4\n[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 1\n[21] Paul Rosenfeld, Elliott Cooper-Balis, and Bruce Jacob.\nDramsim2:\nA cycle accurate memory system simulator.\nIEEE computer architecture letters, 10(1):16\u201319, 2011. 8\n[22] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International Journal of\nComputer Vision, 115(3):211\u2013252, 2015. 1, 7\n[23] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen.\nMobilenetv2: Inverted\nresiduals and linear bottlenecks.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 4510\u20134520, 2018. 1\n[24] Victor Sanh, Thomas Wolf, and Alexander Rush. Movement\npruning: Adaptive sparsity by fine-tuning. Advances in neu-\nral information processing systems, 33:20378\u201320389, 2020.\n3\n[25] Yakun Sophia Shao, Sam Likun Xi, Vijayalakshmi Srini-\nvasan, Gu-Yeon Wei, and David Brooks. Co-designing ac-\ncelerators and soc interfaces using gem5-aladdin. In 2016\n49th Annual IEEE/ACM International Symposium on Mi-\ncroarchitecture (MICRO), pages 1\u201312. IEEE, 2016. 8\n[26] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia\nSchmid. Segmenter: Transformer for semantic segmenta-\ntion. In Proceedings of the IEEE/CVF international confer-\nence on computer vision, pages 7262\u20137272, 2021. 1\n30155\n\n=== Page 10 ===\n[27] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv\u00b4e J\u00b4egou. Training\ndata-efficient image transformers & distillation through at-\ntention. In Proceedings of the 38th International Conference\non Machine Learning, pages 10347\u201310357. PMLR, 2021. 1,\n3, 7, 8\n[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 5998\u20136008, 2017. 3\n[29] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for dense\nprediction without convolutions.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 568\u2013578, 2021. 3\n[30] Huanrui\nYang,\nHongxu\nYin,\nMaying\nShen,\nPavlo\nMolchanov, Hai Li, and Jan Kautz.\nGlobal vision trans-\nformer pruning with hessian-aware saliency. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 18547\u201318557, 2023. 3\n[31] Fang Yu, Kun Huang, Meng Wang, Yuan Cheng, Wei Chu,\nand Li Cui. Width & depth pruning for vision transformers.\nIn Proceedings of the AAAI Conference on Artificial Intelli-\ngence, pages 3143\u20133151, 2022. 1, 2, 3, 4, 7, 8\n[32] Lu Yu and Wei Xiang. X-pruner: explainable pruning for\nvision transformers. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n24355\u201324363, 2023.\n[33] Shixing Yu, Tianlong Chen, Jiayi Shen, Huan Yuan, Jianchao\nTan, Sen Yang, Ji Liu, and Zhangyang Wang. Unified visual\ntransformer compression. arXiv preprint arXiv:2203.08243,\n2022. 3\n[34] Zhengqing Yuan, Rong Zhou, Hongyi Wang, Lifang He,\nYanfang Ye, and Lichao Sun.\nVit-1.58 b:\nMobile\nvision transformers in the 1-bit era.\narXiv preprint\narXiv:2406.18051, 2024. 4\n[35] Jiale Zhang, Bing Chen, Yanchao Zhao, Xiang Cheng, and\nFeng Hu. Data security and privacy-preserving in edge com-\nputing paradigm: Survey and open issues. IEEE access, 6:\n18209\u201318237, 2018. 1\n[36] Chuanyang Zheng, Kai Zhang, Zhi Yang, Wenming Tan, Jun\nXiao, Ye Ren, Shiliang Pu, et al. Savit: Structure-aware vi-\nsion transformer pruning via collaborative optimization. Ad-\nvances in Neural Information Processing Systems, 35:9010\u2013\n9023, 2022. 3\n30156\n",
      "searchable_content": "page 1 deepcompress-vit rethinking model compression to enhance efficiency of vision transformers at the edge sabbir ahmed1 abdullah al arafat2 deniz najafi3 akhlak mahmood4 mamshad nayeem rizve5 mohaiminul al nahian1 ranyang zhou3 shaahin angizi3 adnan siraj rakin1 1binghamton university suny 2north carolina state university 3new jersey institute of technology 4georgia institute of technology 5adobe inc. abstract vision transformers vits excel in tackling complex vi- sion tasks yet their substantial size poses significant chal- lenges for applications on resource-constrained edge de- vices. the increased size of these models leads to higher overhead e.g. energy latency when transmitting model weights between the edge device and the server. hence vits are not ideal for edge devices where the entire model may not fit on the device. current model compression tech- niques often achieve high compression ratios at the expense of performance degradation particularly for vits. to over- come the limitations of existing works we rethink model compression strategy for vits from first principle approach and develop an orthogonal strategy called deepcompress- vit. the objective of the deepcompress-vit is to encode the model weights to a highly compressed encoded repre- sentation using a novel training method denoted as uni- fied compression training uct . proposed uct is accom- panied by a decoding mechanism during inference which helps to gain any loss of accuracy due to high compres- sion ratio. we further optimize this decoding step by re- ordering the decoding operation using associative prop- erty of matrix multiplication ensuring that the compressed weights can be decoded during inference without incurring any computational overhead. our extensive experiments across multiple vit models on modern edge devices show that deepcompress-vit can successfully compress vits at high compression ratios 14 . deepcompress-vit en- ables the entire model to be stored on edge device re- sulting in unprecedented reductions in energy consumption 1470 and latency 68 for edge vit inference. our code is available at https github.com ml-security- research-lab deepcompress-vit. 1. introduction the integration of computer vision and deep learning dl into the industrial internet of things iiot and edge de- these authors contributed equally 10 20 30 40 50 60 70 80 performance comparison with sota vit compression methods model size mb accuracy deepcompress-vit lsq 14.9x deepcompress-vit q-vit 13.6x deit-s 1x 13.6x wdpruning 2.5x 60.0 62.5 65.0 67.5 70.0 72.5 75.0 77.5 80.0 uncompressed model pruned model quantized model figure 1. comparison of deepcompress-vit with sota vit com- pression methods 8 11 31 for compressing deit-s 27 trained on imagenet-1k 22 . existing compression methods achieve a large compression ratio at the expense of large performance loss whereas the deepcompress-vit achieves the highest compression ratio with the best accuracy among the competition. vices has revolutionized data processing analysis and ac- tion in these systems 3 . by deploying advanced mod- els such as vision transformers vits on resource- constrained devices iiot systems can directly manage complex tasks including object detection anomaly detec- tion and facial recognition on the device. this local pro- cessing capability on edge minimizes data transmission to centralized servers reducing latency and enhancing data privacy by keeping information closer to its source 35 . in recent years vision transformers vits 7 have gained significant attention among deep neural network dnn models for their remarkable performance 20 . their potential to tackle various real-world vision problems such as object detection 4 and segmentation 26 makes them especially appealing for edge applications. however the challenge of achieving reliable real-time inference on resource-constrained devices cannot be underestimated pri- marily due to the large model sizes of vits in comparison to models like convolutional neural networks 23 . the architectural design of vits often exceeds the on- chip memory limits of edge devices e.g. espressif esp32- this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 30147 page 2 s3 1 has 8 mb pseudo-static ram on-chip memory but deit-s model size is approximately 85 mb leading to a reliance on off-chip memory from servers to store the en- tire model. this dependency introduces two critical chal- lenges that significantly impede the use of large vits in edge environments 10 . firstly computations performed on-chip are estimated to be 10 to 100 times faster than those conducted off-chip see table 1 for a detailed numeri- cal comparison of various metrics between on- and off-chip memories . secondly transferring sizable model weights demands high communication bandwidth resulting in ele- vated energy consumption. as a result for edge applica- tions e.g. real-time patient monitoring autonomous vehi- cle perception quality control in production where latency and power efficiency are critical the deployment of vit models is severely limited. therefore addressing model size and enhancing memory management are not just chal- lenges they are essential for unlocking the full potential of vit models in iot devices. to effectively deploy large vision transformer vit models at the edge it is crucial to aggressively compress them to fit into on-chip memory. aggressive compres- sion reduces reliance on off-chip memory decreasing la- tency and energy consumption. however techniques such as pruning 31 and quantization 11 often face challenges at high compression ratios leading to significant drops in performance. for instance the deit-small model which is approximately 85 mb would require over 10 compression to fit into an 8 mb edge device. this extreme compression level can result in severe accuracy loss please refer to fig- ure 1 . therefore it is essential to carefully reconsider and rethink model compression strategies for vits to maintain both performance and efficiency at the edge. in this work we begin by formulating the popular com- pression strategies from the perspective of weight encod- ing as illustrated in figure 2. both pruning and quantiza- tion are techniques that encode model weights into a com- pressed format. however at high compression ratios this weight encoding mechanism can become lossy leading to a decline in model performance. our hypothesis is that to mitigate the performance loss due to encoded weights it is essential to implement a decoding method during infer- ence that can reconstruct or remap the weights back to their original decompressed state. this approach could enhance the model s accuracy during inference by reversing the ef- fects of a lossy compression. nonetheless developing an effective compression strategy that achieves high compres- sion while also enabling accurate weight decoding for in- ference involves overcoming several challenges which our work aims to resolve. to this end we propose a novel and unique perspective on model compression which enables aggressive compres- sion of vit models while maintaining a decoding frame- fp pruning w original weights wp wp q pruned weights moderate compression ratio very high compression ratio only encoded weights and decoder are stored in on-chip memory high compression ratio fq quantization w original weights w q w quantized weights f deepcompress-vit w original weights w encoded weights decoder no decoder no decoder f -1 w decoded weights w encoder vit model vit model vit model input images input images input images low accuracy high accuracy low accuracy figure 2. existing compression methods can be viewed as a form of weight encoding. however inference with lossy en- coded weights results in performance dorp whereas proposed deepcompress-vit maintains an optimized decoding method at test time to maintain model performance. work to reconstruct the weights back. the key idea is that the compressed version of the weight and a lightweight de- coder will be stored in memory to reduce memory costs. however these weights will be reconstructed back to their original form using the decoder in a layer-by-layer sequence for model inference and thus help to maintain accuracy. our proposed deepcompress-vit consists of two phases unified compression training and optimized test-time de- coding. in unified compression training we train an en- coder and decoder alongside the model learning a highly compressed representation of its parameters through en- coded embeddings. after training we retain only these en- coded weights and the decoder discarding the encoder. next during inference the goal is to decode the com- pressed parameters layer-by-layer systematically to recon- struct the original parameters for each layer to compute the next layers output. nevertheless the proposed layer- by-layer decoding approach comes with two notable draw- backs. first decoding the compressed representation of each layer back to its original form incurs extra compu- tational costs such as increased energy use and process- ing time. additionally this can lead to significant on-chip memory usage particularly in larger layers when decoded weights will be stored on-chip temporarily to compute out- put of a given layer. hence we propose optimized test- time decoding eliminating the need to decode embed- dings to their original parameters. this optimized decod- ing step leverages the associative property of matrix mul- tiplication to reorder the decoding operation. this innova- tion eliminates both computation and memory usage draw- back associated with the decoding step making proposed 30148 page 3 deepcompress-vit best of all worlds accuracy memory energy and latency . our experiments on sota vits and edge devices show that deepcompress-vit achieves com- pression ratios exceeding 14 while achieving unprece- dented reductions in energy 1470 and latency 68 all with the least amount of accuracy loss among the sota compression methods. 2. related work vision transformers. the introduction of transformer architecture 28 has marked a paradigm shift in machine learning initially transforming natural language process- ing. building on this success the architecture was adapted for computer vision leading to the development of vision transformer vit 7 . this success has led to a surge in transformer-based models for computer vision. for exam- ple deit 27 introduced advanced training techniques and a novel distillation approach to reduce vit s dependence on large datasets. further architectural innovations such as pyramid vision transformer pvt 29 and swin trans- former 17 incorporated hierarchical feature representa- tions enabling transformers to serve as versatile frameworks for diverse computer vision applications. even though vits have demonstrated strong performance they require signif- icant memory that limits their application in edge devices. compressing vits has thus emerged as an important area of research to address this large memory requirement. vit compression. a widely used approach for com- pressing vision transformers vit is pruning 16 30 33 36 which focuses on reducing the number of unimpor- tant weights in the model. pruning methods for vit models are commonly divided into two categories unstructured and structured pruning. unstructured pruning techniques such as magnitude-based and hessian-based methods 24 re- move individual weights based on certain importance met- rics. however these techniques result in irregular sparsity patterns which complicate tensor computations and limit hardware efficiency. structured pruning mitigates this chal- lenge by removing larger contiguous structures within the model such as entire attention heads or rows of weight ma- trices which are more compatible with hardware require- ments. for instance 31 proposed a structured pruning method for vits that uses a binary mask to distinguish im- portant from unimportant parameters based on magnitude. another effective approach introduced in 33 combines pruning layer skipping and knowledge distillation within a unified framework to create a compact vit model. another widely adopted approach for compressing vit models is quantization 8 11 13 15 18 which converts floating-point parameters to lower precision values. sev- eral quantization methods tailored to vit s unique archi- tecture have been proposed. for instance q-vit 11 introduced differentiable quantization for vits treating quantization bit-widths and scales as learnable parameters. ptq4vit 18 utilizes twin uniform quantization and ap- plies a hessian-guided metric to determine optimal scal- ing factors. fq-vit 15 incorporates powers-of-two scale quantization and log-int quantization specifically for layer- norm and softmax operations respectively. while both pruning and quantization methods have shown great promise in reducing computational demands they still experience significant performance degradation at high compression ratios 11 31 also shown in table 2 severely limiting their applicability in edge iot devices. 3. challenges and motivation to motivate our compression technique we identify the critical bottleneck hindering efficient inference of vision transformers vits at the edge. we will also assess the limitations of current compression methods establishing the necessary foundation for our proposed method. 3.1. challenges of efficient edge inference the deployment of vits on edge devices presents signif- icant challenges due to their substantial memory require- ments which often exceed the on-chip memory capac- ity of these resource-constrained environments. conse- quently model parameters must be stored in off-chip mem- ory necessitating a sequential processing approach during inference. this approach involves iteratively loading each layer s weights into on-chip memory and performing com- putations on that layer before proceeding to the next. while this method enables the inference of vits on devices with limited on-chip memory it introduces significant latency and increased energy consumption due to frequent off-chip memory accesses both of which are critical concerns in resource-constrained edge environments e.g. autonomous vehicle perception . table 1. comparison of on-chip and off-chip memory access cost in edge devices memory type access time ns energy per access pj available storage mb on-chip 1-10 5-20 1-8 off-chip 70-200 1300-2600 1024-8192 32b sram read 32kb 32b dram 10 for edge ai devices. one potential solution to mitigate this frequent off-chip memory access is model compression. model compression can reduce the overall memory footprint and allow the en- tire model to be stored in on-chip memory which can min- imize eliminate the need for costly memory access. how- ever as shown in table 1 the available on-chip memory is often severely constrained e.g. 1-8 mb and achiev- ing this would require aggressive compression e.g. greater than 10 . one of the widely explored approaches for model com- pression is pruning. numerous studies 24 31 33 have applied pruning techniques on vits to reduce their memory and computation costs. while pruning has demonstrated significant success in lowering the computational demands 30149 page 4 of these models our investigation reveals that achieving high compression through pruning in vits is challenging without severely impacting model performance. as shown in table 2 even with a 2.5 compression ratio the model suffers 19 accuracy loss. another widely adopted approach for improving model efficiency is quantization 11 15 18 . quantization has proven particularly effective in reducing the memory footprint of large models such as large language models llms . the existing literature demonstrates that extreme quantization can be successfully applied to llms 19 with minimal performance loss. however quantization applied to vits presents a different challenge. studies focusing on vits 11 34 have shown that these models suffer from significant performance degradation when subjected to ex- treme quantization see table 2 . table 2. evaluating existing compression methods by aggressively compressing a deit-s model and highlighting their shortcoming in achieving our ideal objective 4th row for edge inference. method original size mb compression ratio accuracy original model 84.1 1 79.72 pruning 11 33.7 2.5 60.51 quantization 31 6.18 13.6 71.90 our objective 1-8 constraint at edge 10-80 required 79.72 ideally next we dissect these two popular model compres- sion strategies to determine the fundamental limitation that hurts the state-of-the-art model compression technique and whether we can develop an orthogonal approach to over- come them at our targeting edge applications. 3.2. motivation as shown in figure 2 both pruning and quantization can be viewed as a form of encoding original model weights to a compressed representation. the fundamental challenge lies in the way these compression techniques encode model weights. in this work we postulate that the primary factor contributing to this performance loss can be attributed to the large weight perturbation error introduced by the encoding mechanism of these compression techniques as formalized in the following lemma. lemma 3.1. error bound for compression in dnns . let w represent the weights of a deep neural network dnn with an associated loss function l w . consider a com- pression technique that encodes the weights using a trans- formation f resulting in w f w and introducing a perturbation \u03b4w w w. the change in loss due to this compression can be upper bounded as follows l w l w wl w \u03b4w 1 2 h \u03b4w 2 where wl w is the gradient of the loss function with re- spect to the original weights w h denotes the norm of the hessian matrix of l w and \u03b4w is the norm of the weight perturbation introduced by the compression. from lemma 3.1 it is evident that a smaller weight per- turbation \u03b4w leads to a tighter upper bound on the loss difference l w l w . therefore an ideal compres- sion method should minimize the weight perturbation error. however for edge application of vits we are forced to perform aggressive compression that will inevitably lead to a higher \u03b4w . neither pruning nor quantization has a countermeasure against this high \u03b4w for aggressive compression laying down the foundation for our proposed method as to whether we can device a technique to reduce \u03b4w at the infer- ence stage even at a high compression ratio. the research question that motivates our work given that weight encod- ing for compression inevitably introduces significant weight perturbation error at high compression ratio an effective approach to mitigate this error may lie in coupling the en- coding mechanism with a complementary decoding mecha- nism that can reverse the lossy encoding as formalized in the following proposition. proposition 3.2. lower error bound with weight decod- ing . let w represent the weights of a deep neural net- work dnn and consider a compression method that en- codes these weights using a transformation f introducing a weight perturbation \u03b4w w w. now consider a weight decoding mechanism with transformation g where g ap- proximates the inverse of f. then the weight perturbation error after decoding the weights can be bounded as follows g f w w \u03b4w . the above proposition demonstrates the potential of a weight decoding strategy to reduce weight perturbation and minimize the impact on model performance at inference. inspired by proposition 3.2 we aim to design a novel encoder-decoder-based model compression approach that can enable the potential of achieving a high compression ratio while minimizing inference loss. 4. proposed method deepcompress-vit we propose deepcompress-vit a novel compression framework designed for achieving a high compression ra- tio while preserving inference accuracy. as shown in fig- ure 3 our novel deepcompress-vit framework consists of two stages. in the first stage we propose unified com- pression training which jointly trains the encoder and de- coder networks see figure 3 . the encoder network de- noted as f learns to encode the original model weights w from rd to rr where r d. and the decoder network g learns to decode the encoded weights back to rd. af- ter training is complete only the encoded weights and the decoder network are retained for inference while the en- coder network is discarded. during inference the proposed deepcompress-vit reconstructs each layer s weight one by 30150 page 5 f w g vit model original weights encoder decoder unified compression training z encoded weights w decoded weights load input images prediction w g only encoded weights and decoder are stored in on-chip memory for inference figure 3. unified compression training. the encoder and de- coder parameters are optimized jointly to minimize the loss. after training only encoded weights and decoder network is retained for inference one and performs inference using the decoded version of the weight to help maintain inference accuracy. while the decoding step is necessary to maintain accu- racy it has two drawbacks. to understand this lets consider a simplified form the output computation for a given layer input encoded weight decoder weight. first the de- coding step will be performed using encoded weight decoder weight which introduces additional computa- tion at runtime and second the reconstructed version of the weight will increase temporary on-chip memory con- sumption especially for larger layers. to address this we propose optimizing the decoding step by designing a lightweight decoder implemented as a single linear layer. in addition we leverage the associative property of ma- trix multiplication to transform the inefficient weight decod- ing input encoded weight decoder weight to opti- mized test-time decoding as input encoded weight decoder weight which would ensure the computation cost remains unchanged for an optimum level of decoder input compressed weight size r outlined in equation 4 in de- tails is selected. this optimization ensures the proposed deepcompress-vit does not introduce any computational overhead figure 4 in pursuit of compression and accuracy. 4.1. unified compression training uct in this stage an encoder-decoder network is trained to achieve aggressive compression. the encoder network f learns to encode the model s weights to a compressed rep- resentation as the encoded embedding while the decoder network g learns to decode the weights back to their orig- inal dimension as shown in figure 3. consider a vit model hw consisting of l layers where the weights of the layers are denoted by the set w w1 w2 . . . wl . let the parameters of the en- coder network f . be represented as we and the param- eters of the decoder network g . be represented as wd. during training each layer s weight wi is passed through the encoder network f which encodes it to a lower- dimensional representation given by zi f wi . the de- coder network g then decodes zi back to its original di- mensionality as \u02c6wi g zi . the encoder and decoder networks are trained jointly to minimize the mean squared error mse between the original weight set w and the reconstructed weight set \u02c6 w . the weight reconstruction loss is calculated as lmse 1 l l x i 1 wi g f wi 2 to optimize this training step further we unify the encoder-decoder training step with the original vit-model. our hypothesis is that only mse loss-based weight recon- struction may not provide optimal performance 2 . even though minimizing mse loss results in the minimization of weight perturbation error different sets of decoded weights with the same mse value can have varying weight distri- butions which can lead to a performance drop in the vit model. hence it is reasonable to incorporate the vit model into this training loop to align the vit model s weight dis- tribution with the decoded version of the weight. hence we propose this end-to-end unified framework where the encoder-decoder networks are jointly trained with the vit model. our method will incorporate cross-entropy ce loss and knowledge distillation kd from a pre- trained model in training where ce ensures the vit model performance is optimized on the decoded weight values w.r.t ground truth and kd ensures the decoded weight pa- rameters do not drift far apart from the initial weight distri- bution of a pre-trained vit model. the forward pass for the whole end-to-end training af- ter obtaining the decoded weight sets \u02c6 w from the de- coder is formulated as hw h \u02c6 w which implies that the vit model inference will always use the decoded weights i.e. the forward pass of the vit model becomes \u02c6y h \u02c6 w x where x are the input images and \u02c6y are the predictions from the vit model. next using \u02c6y we com- pute the cross-entropy loss and knowledge distillation loss as follows lce x i yilog \u02c6yi lkd x i \u02c6yt i log \u02c6yt i \u02c6yi where \u02c6yt represents the prediction from the pretrained teacher model. the total loss function is then defined as ltotal lmse \u03b1 lce \u03b2 lkd where \u03b1 and \u03b2 are scaling factors controlling the contri- butions of cross-entropy and knowledge distillation losses respectively. finally the overall loss function is minimized by jointly optimizing the encoder and decoder parameters we and wd as follows min we wd ltotal 30151 page 6 inefficient weight decoding x n x c w c x d z c x r w r x d matmul flops 2crd additional flops total flops 2ncd 2crd matmul flops 2ncd weight decoding total flops 2ncd 2ncd 2n c d r forward pass w xi xi 1 i i x n x c i i z c x r i i compressed weight weight of decoder z wd d w r x d d i input activation output activation x n x r matmul flops 2ncr matmul flops 2nrd input compression optimized forward pass optimized test-time decoding x xi 1 i i compressed weight we set z weight of decoder wd i input activation xi output activation figure 4. proposed optimized test-time decoding overview. decoding the encoded compressed weights first and then matrix multiplying with input activation increases flops. the proposed method eliminates extra flops by multiplying input activation with encoded weights first then passing through decoder. the chosen value of encoding dimension r prevents any flop increase. after training is complete for inference we keep only the encoded weight parameters zi and a lightweight de- coder wd in memory while discarding the encoder we. at inference the decoder needs to map compressed weights to their original form layer by layer which may help preserve accuracy. however this can introduce two major drawbacks. first the decoding step will inevitably lead to additional computation at runtime and second the decoded weights will increase temporary on-chip memory consumption particularly for larger layers. next we out- line our optimized test-time decoding strategy which en- sures these two bottlenecks remain in check. 4.2. optimized test-time decoding to mitigate the overhead associated with the decoding step first design level optimization we propose is to design the decoder g as a single linear layer. this design choice min- imizes repetitive decoding operations reducing the compu- tational overhead. although a single-layer decoder network lowers the computational load relative to a multi-layer de- coder the computation needed for decoding weights using a single layer still persists. to illustrate the overhead of weight decoding step con- sider a fully connected layer in a vit model with original weights wi rc d whose weights are encoded and the encoded weights are represented by zi rc r. during in- ference of this layer the weights are first decoded using the single-layer decoder network g where weights of the de- coder network is represented by wd rr d. this weight decoding computation can be represented as follows \u02c6wi zi wd 1 where \u02c6wi rc d denotes the decoded weights. one of the most widely used metrics to evaluate computational com- plexity is floating-point operations flops 6 which counts the total number of multiplications and additions performed in a given operation. the flops incurred for the weight decoding operation is 2 c r d corresponding to the matrix multiplication in eqn. 1 . next the usual forward pass of fully connected layer in the vit model with input activations xi rn c and de- coded weights \u02c6wi is computed as follows xi 1 xi \u02c6wi 2 where xi 1 represents the output activations of the layer. the flops incurred by this forward pass is 2 n c d resulting from the matrix multiplication in eqn. 2 . there- fore the total flops incurred in the forward pass with this strategy is 2 n c d 2 c r d where the additional 2 c r d flops is incurred because of this inefficient weight decoding strategy. next we optimize the decoding step to eliminate this additional 2 c r d flops we propose optimized test- time decoding. our proposed decoding approach lever- ages the associative property of matrix multiplication to re- order computation sequence to maintain or improve overall flops. to illustrate recall that the overall computation for inefficient decoding strategy is as follows xi 1 xi \u02c6wi xi zi wd 3 where it shows weight decoding first and then perform the forward pass in the fully connected layer of vit model. we optimize the decoding step by simply reorder the matrix multiplications as follows xi 1 x i wd xi zi wd 30152 page 7 where we first perform input compression x rn c x rn r and then do the optimized forward pass x wd . the proposed strategy is also illustrated in figure 4. the total flops for this optimized approach is 2 n r c d . however with an arbitrary value of r the overall flops can still go up. therefore to ensure that the total flops of the proposed decoding strategy do not exceed the baseline models flops we derive the optimal r for efficiency by solving the following inequality 2 n r c d 2 n c d where 2 n c d is the flops incurred due to normal forward pass in eqn. 2 . solving the inequality we get r 2 n c d 2 n c d 4 in summary by reordering the computation sequence and using optimal r for compression derived in eqn. 4 our innovative decoding optimization does not incur any addi- tional computation. 5. experimental results 5.1. datasets and implementation details datasets. we conduct our experiments on the ilsvrc12 imagenet classification dataset 22 consisting 1 000 classes with 1.2 million training images and 50k validation images. additionally we evaluate deepcompress-vit on the widely used coco benchmark dataset 14 for object detection and instance segmentation task. encoder-decoder architecture. we use four encoder net- work each with 2 linear layers to encode four types of lay- ers attn qkv attn proj mlp fc1 and mlp fc2. each encoder has an affine transformation before and after the 2 linear layers. similarly to encoders we use four sin- gle linear layer decoders to decode the four types of layers. note that each decoder is a shared decoder which decodes a particular type of layer of all vit blocks. note that we ex- clude the first layer last layer and layernorm parameters from compression. experimental settings. the encoder and decoder models are trained for 275 epochs with a batch size of 256 and a base learning rate of 1 10 4 using a cosine learning rate scheduler. for data augmentation we apply the standard techniques used in 27 and for knowledge distillation we use pretrained distilled deit models 27 with temperature \u03c4 1. for uct we set the value of \u03b1 1 and \u03b2 3 103. the encoded weights are quantized using 4-bit asymmet- ric quantization while decoder weights are quantized us- ing 4-bit symmetric quantization both trained using quan- tization aware training. additionally the patch embedding first and classification last layers are quantized using 8- bit symmetric quantization. the optimal value of r from solving 4 is set to 277 for deit-s and 502 for deit-b. for object detection and instance segmentation we use the mmdetection library with the standard 1 training sched- uler and adopt an edge-friendly resolution of 384 384 for both training and evaluation. 5.2. results image classification. performance of the compressed deit models with proposed deepcompress-vit on the classification task is shown in table 3. the results demon- strates substantial reductions in model size and energy con- sumption resulting from deepcompress-vit. for instance the deepcompress-deit-s model achieves a compression ratio of 14.9 reducing the model size from 84.1 mb to just 5.64 mb while maintaining an accuracy of 78.74 on imagenet. similarly deepcompress-deit-b achieves a compression ratio of 17.4 decreasing the model size from 330.2 mb to 18.9 mb. remarkably both models show an unprecedented energy reduction of 673 and 1475 and latency reduction of 44.5 and 68.4 respectively please refer to section 5.3 and figure 5 for more hardware energy and latency evaluation details . proposed deepcompress- vit achieves this unprecedented level of model compres- sion energy and latency reduction enabling the deploy- ment of vit models at the edge while only sacrificing a marginal level of accuracy e.g. for deit-b only 0.1 object detection and instance segmentation. next ta- ble 4 compares the performance of deepcompress-vit with baseline no compression deit-s model on object de- tection and instance segmentation tasks. we trained both the models on coco dataset integrating the respective vit backbones with mask r-cnn 9 . we evaluate both of the models for these tasks at an edge-friendly resolution of 384 384. notably deepcompress-vit reduces the vit backbone deit-s size from 82.65mb to 5.27mb 15.7 compression with a slight drop in apb and apm further highlighting the effectiveness of our compression method on other complex vision tasks. comparison with sota compression methods. finally table 5 provides a comparative analysis of deit-small models compressed using different pruning and quantiza- tion methods. among the methods deepcompress-vit achieves the highest compression ratio of 14.9 reducing the model size to 5.64 mb while maintaining a top-1 ac- curacy of 78.74 . in contrast popular wdpruning 31 lsq 8 and q-vit 11 achieves achieve much lower accu- racies of 60.51 68.00 and 71.9 respectively. our pro- posed deepcompress-vit consistently outperforms them by large margins of 18 10 and 7 respec- tively making deepcompress-vit the only practical choice to maintain accuracy while performing aggressive compres- sion. 30153 page 8 table 3. performance of deepcompress-vit on imagenet-1k. deepcompress-vit achieves memory 15 and energy consumption 1531 while marginal accuracy drop 1 . method accuracy model size mb compression ratio gflops execution time s energy consumption j deit-s 27 79.72 84.1 1x 9.20 24.02 9979.52 deepcompress-deit-s 78.74 5.64 14.9x 8.88 0.54 14.83 deit-b 27 81.74 330.2 1x 35.14 129.32 150895 deepcompress-deit-b 81.62 18.9 17.4x 30.84 1.89 102.29 table 4. performance of deepcompress-vit for object detection on the coco validation set with mask r-cnn evaluated at an edge-friendly resolution of 384 384. method apb apm vit backbone size mb deit-s 27 26.2 23.7 82.65 deepcompress-vit 23.0 20.9 5.27 15.7 table 5. performance comparison of deit-small with sota com- pression methods on imagenet-1k model accuracy model size mb compression ratio deit-s 27 79.72 0 84.1 1x wdpruning 31 60.51 19.21 33.7 2.5x lsq 8 68.00 11.72 6.18 13.6x q-vit 11 71.90 7.82 6.18 13.6x deepcompress-vit 78.74 0.98 5.64 14.9x 5.3. hardware evaluation high compression gain gives an understanding of the effec- tiveness of a compression method in terms of memory sav- ings. but to successfully realize the full potential of our pro- posed method we evaluate the energy and latency gain on hardware evaluation platforms. for hardware deployment we use the gem5-aladdin platform 25 which effectively captures dynamic interactions between accelerators and the soc platform enabling detailed architectural analysis of the proposed compression method. our setup incorporates a cycle-accurate in-house-developed model based on the ey- eriss accelerator 5 0.0029 dram access multiply and accumulation mac 33.6 gmac s . performance met- rics from gem5-aladdin are then analyzed using dram- sim2 21 to model realistic off-chip memory latency and energy characteristics. the total on-chip weight memory is capped at three configurations 2 mb 4 mb 8 mb . the analyzed deit-s model initially requires 84.1mb of memory which exceeds the on-chip memory capacity. by applying deepcompress-vit algorithm this requirement is reduced to 5.64 mb which can readily fit into the 8mb on- chip memory. however with a smaller memory capacity it is not feasible to transfer the entire set of weight parameters to the accelerator in a single batch due to the model s mem- ory demands. therefore we implemented a weight parser that transmits weight parameters in variable batch sizes to the accelerator. figure 5 shows the breakdown of energy consumption and latency across mac off-chip and on- chip memory access components for various on-chip mem- ory sizes. for both deit-s and deit-b the deepcompress- vit algorithm achieves significant energy savings reduc- 8mb 4mb 2mb on-chip memory size 10-4 10-2 100 102 104 106 energy consumption j mac on-chip memory access off-chip memory access deit-s deit-b dc-deit-b dc-deit-s 9979.52 14.83 8mb 4mb 2mb on-chip memory size 10-1 100 101 102 103 latency s mac on-chip memory access off-chip memory access a b 24.02 0.54 129.32 1.89 150895 102.29 figure 5. breakdown of a energy consumption b latency vs. on-chip memory size. for every off-chip memory size deit-s deepcompress-vit-s deit-b and deepcompress-vit-b results are plotted respectively. ing consumption by approximately 673 and 1475 re- spectively with an 8mb on-chip memory. this remarkable reduction is primarily due to the deepcompress-vit algo- rithm s ability to minimize off-chip memory access as visi- ble through the high difference in the red bar plots. this re- duction becomes even more pronounced with smaller mem- ory sizes enabling for the first time the possibility of de- ploying large vit models at the edge and performing time- sensitive applications. 6. conclusion in this paper we present deepcompress-vit a novel ap- proach to achieve highly compressed vision transformers optimized for efficient edge deployment. we first analyze the limitations of existing model compression techniques and show that they lead to large weight perturbation error because of their lossy encoding mechanism. then to re- solve this challenge we develop an orthogonal compression technique that consists of an encoding for compression and a decoding step to help maintain performance. however decoding weights repeatedly during inference can lead to additional computation and increase on-chip memory us- age both of which are critical concerns for edge applica- tions. to address these challenges we propose a novel opti- mized test-time decoding scheme that enables efficient in- ference without increased computation and avoids high on- chip memory usage. our extensive experiments and hard- ware evaluation show that the proposed deepcompress-vit can lead to an unprecedented reduction in energy and execu- tion time with minimal loss of performance unlocking the possibilities of deploying large-scale vit models at edge iiot devices. 30154 page 9 references 1 esp32-s3 designed for aiot applications 2022. 2 2 sabbir ahmed uday kamal and md kamrul hasan. dfr- tsd a deep learning based framework for robust traffic sign detection under challenging weather conditions. ieee transactions on intelligent transportation systems 23 6 5150 5162 2021. 5 3 jiang bian abdullah al arafat haoyi xiong jing li li li hongyang chen jun wang dejing dou and zhishan guo. machine learning in real-time internet of things iot systems a survey. ieee internet of things journal 9 11 8364 8386 2022. 1 4 nicolas carion francisco massa gabriel synnaeve nicolas usunier alexander kirillov and sergey zagoruyko. end-to- end object detection with transformers. in european confer- ence on computer vision pages 213 229. springer 2020. 1 5 yu-hsin chen tushar krishna joel s emer and vivienne sze. eyeriss an energy-efficient reconfigurable accelera- tor for deep convolutional neural networks. ieee journal of solid-state circuits 52 1 127 138 2016. 8 6 radosvet desislavov fernando mart \u0131nez-plumed and jos e hern andez-orallo. compute and energy consumption trends in deep learning inference. arxiv preprint arxiv 2109.05472 2021. 6 7 alexey dosovitskiy lucas beyer alexander kolesnikov dirk weissenborn xiaohua zhai thomas unterthiner mostafa dehghani matthias minderer georg heigold syl- vain gelly jakob uszkoreit and neil houlsby. an image is worth 16x16 words transformers for image recognition at scale. arxiv preprint arxiv 2010.11929 2020. 1 3 8 steven k esser jeffrey l mckinstry deepika bablani rathinakumar appuswamy and dharmendra s modha. learned step size quantization. arxiv preprint arxiv 1902.08153 2019. 1 3 7 8 9 kaiming he georgia gkioxari piotr doll ar and ross gir- shick. mask r-cnn. in proceedings of the ieee international conference on computer vision pages 2961 2969 2017. 7 10 mark horowitz. 1.1 computing s energy problem and what we can do about it . in 2014 ieee international solid-state circuits conference digest of technical papers isscc pages 10 14. ieee 2014. 2 3 11 yanjing li sheng xu baochang zhang xianbin cao peng gao and guodong guo. q-vit accurate and fully quantized low-bit vision transformer. advances in neural information processing systems 35 34451 34463 2022. 1 2 3 4 7 8 12 zhikai li liping ma mengjuan chen junrui xiao and qingyi gu. patch similarity aware data-free quantization for vision transformers. in european conference on computer vision pages 154 170. springer 2022. 13 zhikai li junrui xiao lianwei yang and qingyi gu. repq- vit scale reparameterization for post-training quantization of vision transformers. in proceedings of the ieee cvf in- ternational conference on computer vision pages 17227 17236 2023. 3 14 tsung-yi lin michael maire serge belongie james hays pietro perona deva ramanan piotr doll ar and c lawrence zitnick. microsoft coco common objects in context. in european conference on computer vision pages 740 755. springer 2014. 7 15 yang lin tianyu zhang peiqin sun zheng li and shuchang zhou. fq-vit post-training quantization for fully quantized vision transformer. arxiv preprint arxiv 2111.13824 2021. 3 4 16 ji liu dehua tang yuanxian huang li zhang xiaocheng zeng dong li mingjie lu jinzhang peng yu wang fan jiang et al. updp a unified progressive depth pruner for cnn and vision transformer. in proceedings of the aaai con- ference on artificial intelligence pages 13891 13899 2024. 3 17 ze liu yutong lin yue cao han hu yixuan wei zheng zhang stephen lin and baining guo. swin transformer hierarchical vision transformer using shifted windows. in proceedings of the ieee cvf international conference on computer vision pages 10012 10022 2021. 3 18 zhenhua liu yunhe wang kai han wei zhang siwei ma and wen gao. post-training quantization for vision trans- former. advances in neural information processing systems 34 28092 28103 2021. 3 4 19 shuming ma hongyu wang lingxiao ma lei wang wen- hui wang shaohan huang li dong ruiping wang jilong xue and furu wei. the era of 1-bit llms all large language models are in 1.58 bits. arxiv preprint arxiv 2402.17764 2024. 4 20 alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sastry amanda askell pamela mishkin jack clark et al. learning transferable visual models from natural language supervi- sion. in international conference on machine learning pages 8748 8763. pmlr 2021. 1 21 paul rosenfeld elliott cooper-balis and bruce jacob. dramsim2 a cycle accurate memory system simulator. ieee computer architecture letters 10 1 16 19 2011. 8 22 olga russakovsky jia deng hao su jonathan krause san- jeev satheesh sean ma zhiheng huang andrej karpathy aditya khosla michael bernstein et al. imagenet large scale visual recognition challenge. international journal of computer vision 115 3 211 252 2015. 1 7 23 mark sandler andrew howard menglong zhu andrey zh- moginov and liang-chieh chen. mobilenetv2 inverted residuals and linear bottlenecks. in proceedings of the ieee conference on computer vision and pattern recogni- tion pages 4510 4520 2018. 1 24 victor sanh thomas wolf and alexander rush. movement pruning adaptive sparsity by fine-tuning. advances in neu- ral information processing systems 33 20378 20389 2020. 3 25 yakun sophia shao sam likun xi vijayalakshmi srini- vasan gu-yeon wei and david brooks. co-designing ac- celerators and soc interfaces using gem5-aladdin. in 2016 49th annual ieee acm international symposium on mi- croarchitecture micro pages 1 12. ieee 2016. 8 26 robin strudel ricardo garcia ivan laptev and cordelia schmid. segmenter transformer for semantic segmenta- tion. in proceedings of the ieee cvf international confer- ence on computer vision pages 7262 7272 2021. 1 30155 page 10 27 hugo touvron matthieu cord matthijs douze francisco massa alexandre sablayrolles and herv e j egou. training data-efficient image transformers distillation through at- tention. in proceedings of the 38th international conference on machine learning pages 10347 10357. pmlr 2021. 1 3 7 8 28 ashish vaswani noam shazeer niki parmar jakob uszko- reit llion jones aidan n gomez \u0142ukasz kaiser and illia polosukhin. attention is all you need. in advances in neural information processing systems pages 5998 6008 2017. 3 29 wenhai wang enze xie xiang li deng-ping fan kaitao song ding liang tong lu ping luo and ling shao. pyramid vision transformer a versatile backbone for dense prediction without convolutions. in proceedings of the ieee cvf international conference on computer vision pages 568 578 2021. 3 30 huanrui yang hongxu yin maying shen pavlo molchanov hai li and jan kautz. global vision trans- former pruning with hessian-aware saliency. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 18547 18557 2023. 3 31 fang yu kun huang meng wang yuan cheng wei chu and li cui. width depth pruning for vision transformers. in proceedings of the aaai conference on artificial intelli- gence pages 3143 3151 2022. 1 2 3 4 7 8 32 lu yu and wei xiang. x-pruner explainable pruning for vision transformers. in proceedings of the ieee cvf con- ference on computer vision and pattern recognition pages 24355 24363 2023. 33 shixing yu tianlong chen jiayi shen huan yuan jianchao tan sen yang ji liu and zhangyang wang. unified visual transformer compression. arxiv preprint arxiv 2203.08243 2022. 3 34 zhengqing yuan rong zhou hongyi wang lifang he yanfang ye and lichao sun. vit-1.58 b mobile vision transformers in the 1-bit era. arxiv preprint arxiv 2406.18051 2024. 4 35 jiale zhang bing chen yanchao zhao xiang cheng and feng hu. data security and privacy-preserving in edge com- puting paradigm survey and open issues. ieee access 6 18209 18237 2018. 1 36 chuanyang zheng kai zhang zhi yang wenming tan jun xiao ye ren shiliang pu et al. savit structure-aware vi- sion transformer pruning via collaborative optimization. ad- vances in neural information processing systems 35 9010 9023 2022. 3 30156",
      "keywords": [
        "page",
        "deepcompress-vit",
        "rethinking",
        "model",
        "compression",
        "enhance",
        "efficiency",
        "vision",
        "transformers",
        "edge",
        "sabbir",
        "ahmed1",
        "abdullah",
        "arafat2",
        "deniz",
        "najafi3",
        "akhlak",
        "mahmood4",
        "mamshad",
        "nayeem",
        "rizve5",
        "mohaiminul",
        "nahian1",
        "ranyang",
        "zhou3",
        "shaahin",
        "angizi3",
        "adnan",
        "siraj",
        "rakin1",
        "1binghamton",
        "university",
        "suny",
        "2north",
        "carolina",
        "state",
        "3new",
        "jersey",
        "institute",
        "technology",
        "4georgia",
        "5adobe",
        "inc.",
        "abstract",
        "vits",
        "excel",
        "tackling",
        "complex",
        "vi-",
        "sion",
        "tasks",
        "yet",
        "substantial",
        "size",
        "poses",
        "significant",
        "chal-",
        "lenges",
        "applications",
        "resource-constrained",
        "de-",
        "vices.",
        "increased",
        "models",
        "leads",
        "higher",
        "overhead",
        "e.g.",
        "energy",
        "latency",
        "when",
        "transmitting",
        "weights",
        "between",
        "device",
        "server.",
        "hence",
        "not",
        "ideal",
        "devices",
        "where",
        "entire",
        "fit",
        "device.",
        "current",
        "tech-",
        "niques",
        "often",
        "achieve",
        "high",
        "ratios",
        "expense",
        "performance",
        "degradation",
        "particularly",
        "vits.",
        "over-",
        "come",
        "limitations",
        "existing"
      ],
      "metadata": {
        "filename": "Ahmed_DeepCompress-ViT_Rethinking_Model_Compression_to_Enhance_Efficiency_of_Vision_Transformers_CVPR_2025_paper.pdf",
        "original_filename": "Ahmed_DeepCompress-ViT_Rethinking_Model_Compression_to_Enhance_Efficiency_of_Vision_Transformers_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 719225,
        "upload_file_id": "upload_1752168860381_0hz470p8p",
        "upload_timestamp": "2025-07-10T17:34:20.425670",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T17:34:20.610985+00:00",
      "content_length": 49383,
      "content_type": ".pdf",
      "filename": "Ahmed_DeepCompress-ViT_Rethinking_Model_Compression_to_Enhance_Efficiency_of_Vision_Transformers_CVPR_2025_paper.pdf"
    },
    "21": {
      "file_id": "21",
      "content": "\n=== Page 1 ===\nEfficient Event-Based Object Detection: A Hybrid Neural Network with Spatial\nand Temporal Attention\nSoikat Hasan Ahmed*, Jan Finkbeiner\u2020, Emre Neftci\nForschungszentrum J\u00a8ulich, RWTH Aachen University\n{s.ahmed, j.finkbeiner, e.neftci}@fz-juelich.de\nAbstract\nEvent cameras offer high temporal resolution and dy-\nnamic range with minimal motion blur, making them\npromising for robust object detection. While Spiking Neu-\nral Networks (SNNs) on neuromorphic hardware are often\nconsidered for energy efficient and low latency event-based\ndata processing, they often fall short of Artificial Neural\nNetworks (ANNs) in accuracy and flexibility. Here, we in-\ntroduce Attention-based Hybrid SNN-ANN backbones for\nevent-based object detection to leverage the strengths of\nboth SNN and ANN architectures. A novel Attention-based\nSNN-ANN bridge module captures sparse spatial and tem-\nporal relations from the SNN layer and converts them into\ndense feature maps for the ANN part of the backbone. Ad-\nditionally, we present a variant that integrates DWConvL-\nSTMs to the ANN blocks to capture slower dynamics. This\nmulti-timescale network combines fast SNN processing for\nshort timesteps with long-term dense RNN processing, ef-\nfectively capturing both fast and slow dynamics. Experi-\nmental results demonstrate that our proposed method sur-\npasses SNN-based approaches by significant margins, with\nresults comparable to existing ANN and RNN-based meth-\nods. Unlike ANN-only networks, the hybrid setup allows us\nto implement the SNN blocks on digital neuromorphic hard-\nware to investigate the feasibility of our approach. Exten-\nsive ablation studies and implementation on neuromorphic\nhardware confirm the effectiveness of our proposed modules\nand architectural choices. Our hybrid SNN-ANN architec-\ntures pave the way for ANN-like performance at a drasti-\ncally reduced parameter, latency, and power budget.\n1. Introduction\nOver the past decade, deep learning has made significant\nadvances in object detection. State-of-the-art approaches\npredominantly rely on frame-based cameras which cap-\n*Conceptual design, algorithm development and experimentation.\n\u2020Conceptual design, hardware analysis and deployment.\nture frames at a fixed rate. Frame cameras provide dense\nintensity data but have limitations in dynamic range and\nframe rates, leading to motion blur.\nDynamic Vision\nSensors (DVS), or event cameras, offer an alternative by\nasynchronously capturing pixel-level illumination changes,\nachieving low latency (\u223c10\u00b5s), higher temporal resolu-\ntion, and an extended dynamic range (140 dB vs. 60 dB)\n[12].\nThese characteristics make them well-suited for\nlow-light and fast-motion scenarios. However, due to the\nsparse, high-temporal-resolution data they generate, effec-\ntively processing event data for object detection remains a\nchallenging and emerging research area. Early adopters of\nevent-based object detection ANN models often naively re-\npurpose architectures originally designed for frame-based\ncameras [5, 18, 30, 35, 44]. ANN models generally achieve\ngood accuracy but tend to be large in terms of parameter\ncount and MAC operations, making them less suitable for\ndeployment on power-efficient edge or neuromorphic de-\nvices. Furthermore, the high sparsity and temporal reso-\nlution is often discarded in favor of dense representations\nto leverage GPUs\u2019 dense, vector-based representations. In\ncontrast, SNNs implemented in neuromorphic hardware are\nideally suited to leverage the sparsity of event-based inputs,\noffering significant reductions in computational cost, power\nconsumption, and latency [4, 8, 34]. However, SNNs tend\nto be less accurate at the task level compared to their ANN\ncounterparts.\nIn this work, we create a hybrid SNN-ANN-based back-\nbone architecture to combine the efficient, event-driven pro-\ncessing of SNNs on neuromorphic hardware with the effi-\ncient learning and representation capabilities of ANNs. The\nSNN extracts low-level features with high temporal resolu-\ntion from the event-based sensors and converts them into in-\ntermediate features, which then change to slower timescales\nbefore being processed by the ANN with dense activations.\nAdditionally, we feature a variant that adds DWConvL-\nSTMs [14, 36] to the ANN block.\nThis multi-timescale\nRNN variant combines the sparse SNN processing of short\ntimesteps with long time horizon processing via the dense\nRNN with the extracted long timesteps to efficiently cap-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n13970\n\n=== Page 2 ===\nture both fast and slow dynamics of the data. For hybrid\nmodels, the SNN can be efficiently deployed at the edge in\na power efficient manner, as we demonstrate with an imple-\nmentation on a digital neuromorphic chip. ANN processing\ncan occur either at the edge or, with reduced data rates, in\na cloud setting. By training the network jointly, the SNN\ncomponent can leverage backpropagated errors for efficient\ntraining via the surrogate gradient approach [31].\nInformation in SNNs is communicated by spike events.\nIn our hybrid models, these must be efficiently converted\ninto dense representations without discarding valuable spa-\ntiotemporal features. In our model, this is achieved by an\nattention-based SNN-ANN bridge module, which bridges\nSNN representations to ANN representations.\nThe attention module contains two attention modules\nnamed Event-Rate Spatial (ERS) attention and Spatial\nAware Temporal (SAT) attention. The SAT attention mod-\nule addresses the challenge of sparse event inputs by en-\nhancing the model\u2019s understanding of irregular structures\nand temporal attention to discern temporal relationships\nwithin the data. On the other hand, the ERS attention mod-\nule focuses on highlighting spatial areas by leveraging the\nactivity of events. Moreover, we implement the SNN blocks\nin digital neuromorphic hardware to demonstrate the feasi-\nbility of our approach. We report the performance of our\nmodel on large-scale event-based object detection bench-\nmarks. The contributions of our work can be summarized\nas follows:\n\u2022 A novel hybrid backbone-based event object detection\nmodel.\nTo the best of our knowledge, this is the first\nwork to propose a hybrid object detection approach for\nbenchmark object detection task. Evaluation on the Gen1\nand Gen4 Automotive Detection datasets [9, 33] shows\nthat the proposed method outperforms SNN-based meth-\nods and achieves comparable results to ANN and RNN-\nbased approaches.\n\u2022 An attention-based SNN-ANN bridge module (\u03b2asab) to\nconvert spatiotemporal spikes into a dense representation,\nenabling the ANN part of the network to process it effec-\ntively while preserving valuable spatial and temporal fea-\ntures through the Event-Rate Spatial (ERS) and Spatial-\nAware Temporal (SAT) attention mechanisms.\n\u2022 A multi-timescale RNN variant that includes both the\nhigh-temporal resolution SNN block followed by a\nslower, long time horizon DWConvLSTMs in the ANN\nblock, operating on larger timesteps extracted via the\n\u03b2asab module.\n\u2022 Implementation of the SNN blocks on digital neuromor-\nphic hardware to validate its performance and efficiency.\n2. Related Work\nRecent studies demonstrated the potential of event cameras\nin object detection tasks. In the earlier stages of adopting\nevent cameras, the focus primarily revolved around adapt-\ning existing frame-based feature extractors and a detection\nhead for object detection using event data [5, 18]. In [18],\nresearchers integrated event-based data into off-the-shelf\nframe-based object detection networks. They employed an\nInceptionNet-based backbone for feature extraction and a\nsingle-shot detector (SSD) for detection [29, 40]. Similarly,\n[33] utilized a frame-based object detection model called\nRetinaNet, which incorporates a spatial pooling-based fea-\nture extractor [27] along with a detection head, applied to\nevent data.\nAdditionally, methods such as [14, 25, 33] have incorpo-\nrated recurrent neural networks (RNNs) as feature extrac-\ntors for event data. [48] uses SSM to improve training time,\nand [42] proposes a training schema with efficient ground\ntruth label utilization. [30] introduced sparse convolution\nas a method for event feature extraction. To address the\nchallenges of efficiently extracting spatiotemporal features,\n[35] investigates the usability of a graph neural network-\nbased approach as a feature extractor.\nRecently, SNN-based methods have become popular for\nevent data processing due to their spike-based working prin-\nciple, similar to event cameras, which enables efficient\nprocessing.\nResearch conducted by [6] and [39] show-\ncases the effective utilization of SNNs in object detection\ntasks. Specifically, [6] and [39] delve into assessing the\nperformance of converting widely-used ANN-based back-\nbone architectures such as SqueezeNet [19], VGG [37],\nMobileNet [16], DenseNet [17], and ResNet [15] into SNN\narchitecture for event-based object detection. Nonetheless,\noptimizing intermediate and high-level features for detec-\ntion with SNNs results in a significant drop in accuracy.\nRecognizing the distinct advantages offered by both\nSNNs and ANNs, researchers have explored merging these\nnetworks into hybrid architectures [23].\nFor instance,\n[47] presents a framework that leverages hierarchical infor-\nmation abstraction for meta-continual learning with inter-\npretable multimodal reasoning. Building on this idea, [43]\nintroduces DashNet, which integrates SNNs with ANN-\nbased feature extraction for high-speed object tracking.\nSimilarly, [28] improves SNN performance through a hy-\nbrid top-down attention mechanism, while [24] demon-\nstrates that hybrid models can achieve energy-efficient op-\ntical flow estimation with enhanced robustness. Comple-\nmenting these advances, [2, 23] develops an architecture\nthat fuses SNN backbones with ANN heads for event-based\nvision tasks. By leveraging the complementary strengths of\neach, these hybrid networks show promise for simpler tasks.\nHowever, the bridge between SNNs and ANNs is still over-\nlooked to harness the best of both worlds.\nMoreover, the full extent of their capabilities remains\nlargely unexplored, especially in tackling state-of-the-art\nbenchmark vision tasks, such as object detection on popu-\n13971\n\n=== Page 3 ===\nlar datasets like Gen1 Automotive Detection dataset [9] and\nGen4 Automotive Detection dataset [33].\n3. Hybrid Object Detection Network\nThe overall hybrid network as shown in Figure 1 com-\nprises two key parts: an attention-based hybrid backbone\ndesigned to extract spatio-temporal features, and detection\nheads tasked with identifying objects. In the following sec-\ntion, we will delve into the details of the core components\nof the network.\n3.1. Event Representation\nAn event is represented as en = (xn, yn, tn, pn), where\n(xn, yn) is the pixel location, tn is the time, and pn is polar-\nity which indicates the change in light intensity (i.e., posi-\ntive or negative). The event data is pre-processed to convert\nit into a time-space tensor format. Following [14], we start\nby creating a 4D tensor Events[tk\u22121, tk] \u2208RT \u00d72\u00d7H\u00d7W ,\nwhere T represents number of time discretization steps, 2\ndenotes polarity features which contain the count of posi-\ntive and negative events in each discretized time step, and\nH and W signify the height and width of the event cam-\nera, respectively. Given the event set E = {e1, e2, . . . , eN},\nthe event tensor Events[tk\u22121, tk] is constructed from the\ndiscretized time variable t\u2032\nn =\nj\ntn\u2212ta\ntb\u2212ta \u00b7 T\nk\nas follows:\nEvents[tk\u22121, tk](t, p, x, y) =\nX\nen\u2208E\n\u03b4 (p \u2212pn) \u03b4 (x \u2212xn)\n\u03b4 (y \u2212yn) \u03b4 (t \u2212t\u2032\nn) .\n(1)\nWhile training, event tensors are created. However, during\ninference, given an input sparsity of \u223c98% (for Gen 1),\nthis results in significant efficiency gains due to sparse pro-\ncessing in neuromorphic hardware compared to the dense\nprocessing in a GPU.\n3.2. Attention-based Hybrid Backbone\nThe proposed hybrid backbone architecture, as shown in\nFigure 1, consists of three fundamental components: a\nlow-level spatio-temporal feature extractor fsnn, an ANN-\nbased high-level spatial feature extractor fann, and a novel\nAttention-based SNN-ANN Bridge (ASAB) module \u03b2asab.\nThe first module, denoted as fsnn, is an event-level\nfeature extractor operating in the spatio-temporal domain\nand consists of multiple convolutional SNN blocks. Each\nblock follows a structured sequence of operations: stan-\ndard convolution, batch normalization [20], and Paramet-\nric Leaky Integration and Fire (PLIF) spiking neuron [10].\nThe neural dynamics of PILF with trainable time constant\n\u03c4 = sigmoid(w)\u22121 given input X[t] can be expressed as\nfollows:\nV [t] = V [t \u22121] + 1\n\u03c4 (X[t] \u2212(V [t \u22121] \u2212Vreset).\n(2)\nThe fsnn module receives Events[tk\u22121, tk] as its input\nand generates events Espike = fsnn( Events[tk\u22121, tk]) \u2208\nRT \u00d7C\u00d7H\u2032\u00d7W \u2032.\nAs SNNs operate on a faster timescale\nand utilize sparse representations and ANNs operate\non dense representations, efficiently translating valuable\nspatio-temporal information into dense representations is\nessential. To achieve this translation, the Espike is subse-\nquently fed into a proposed \u03b2asab module, which bridges\nthe SNN and the ANN parts. The events Espike are con-\nverted into dense, non-binary features while preserving spa-\ntial and temporal information in the form of spatial fea-\nture maps. The output of \u03b2asab is represented by Fout =\n\u03b2asab(Espike), with dimensions C\u00d7H\u2032\u00d7W \u2032 which is com-\npatible with traditional 2D convolution-based networks, al-\nlowing for smooth processing and integration of informa-\ntion across both spatial and temporal dimensions. The at-\ntention module is further described in Section 3.3.\nThe third component, fann, extracts high-level spatial\nfeatures using multiple ANN blocks with standard ANN\ncomponents. Each ANN block consists of standard convo-\nlution operations, normalization [3, 20], and ReLU activa-\ntion functions, enabling the extraction of detailed high-level\nspatial features from the densely encoded Fout.\nIn addition to the proposed model, we explore a vari-\nant that features an added RNN module, incorporating two\nDepth-Wise separable Convolutional LSTM (DWConvL-\nSTM) units similar to those in [14], as illustrated in Figure\n1.\nThe fsnn processes fast dynamics with small timesteps\nfrom the event-based camera, while the DWConvLSTM op-\nerates on larger timesteps extracted from the \u03b2asab-module\nto capture slower dynamics. The resulting outputs from the\nANN blocks are then fed to the detection head for the final\nobject detection output.\n3.3. Attention-based SNN-ANN Bridge Module\nThe bridge module \u03b2asab comprises two attention modules:\ni) Spatial-aware Temporal (SAT) attention and ii) Event-\nRate Spatial (ERS) attention. The SAT attention module dy-\nnamically captures local spatial context within the irregular\nspatial spike-structure to uncover temporal relations. Mean-\nwhile, the ERS attention submodule focuses on attending to\nspatial areas utilizing the spatial event activities. Below, we\ndescribe these two submodules.\n3.3.1\nSpatial-aware Temporal (SAT) Attention\nThe SAT attention contains three crucial operations:\ni)\nChannel-wise Temporal Grouping to group relevant fea-\ntures from different time dimensions, ii) Time-wise Separa-\nble Deformable Convolution (TSDC) denoted as \u03a6tsdc for\ncapturing channel-independent local spatial context from\nsparse spike features, and iii) Temporal attention module\n13972\n\n=== Page 4 ===\nSNN Block\nSNN Block\nSNN Block\nSNN Block\nANN Block\nANN Block\nANN Block\nANN Block\nDWConvLSTM\nDWConvLSTM\nDetection\nFPN + Head\nTo Tensor\nTime\nFigure 1. Architecture of the hybrid model with object detection head and SNN-ANN hybrid backbone, including the SNN part, \u03b2asab\nbridge module and ANN part. The DWConvLSTM modules and dashed blue arrows are only part of the proposed hybrid + RNN variant.\nSoftmax\nMatmul\nMatmul\noffsets\nlearned\noffset field\noffset learning\nConv 1x1 + ReLu\nConv 1x1\nConv 1x1\nConv 1x1\n(a) Spatial-aware Temporal Attention.\nSigmoid\nSpatial-Aware Temporal\u00a0\nAttenton\nHadamard\nproduct\nSum(dim=T)\n(b) Event-rate Spatial Attention.\nFigure 2. Visualization of the proposed attention module\u2019s components. (a) Spatial-aware Temporal Attention: Highlights relevant temporal\nfeatures in spatial regions to enhance temporal coherence in event-based data. (b) Event-rate Spatial Attention: Emphasizes spatial regions\nbased on event rates, allowing for adaptive focus on areas with significant event activity. Together, these components improve feature\nextraction in spatiotemporal data processing.\n\u03a6ta, which uses local spatial context features to extract tem-\nporal relations to accumulate and translate temporal infor-\nmation into spatial information.\nTime-wise\nSeparable\nDeformable\nConvolutions\n(TSDC):\nAt first, we apply the channel-wise tempo-\nral grouping operation to the input data so that each\nfeature channel is processed separately while capturing\nspatial and temporal relations. This operation transforms\nthe input spike tensor Espike\n\u2208\nRT \u00d7C\u00d7H\u2032\u00d7W \u2032 into\nAin \u2208RC\u00d7T \u00d7H\u2032\u00d7W \u2032.\nAs shown in Figure 2a, the \u03a6tsdc operation operates on\nindividual channel-wise temporal groups Ain[ci, :, :, :], de-\nnoted as Ac, where i is the channel index. This operation\nextracts the local spatial context of the sparse, irregularly\nshaped spike-based representations. We posit that this irreg-\nular representation is better extracted using a deformed ker-\nnel rather than a standard square grid kernel, as discussed in\nSection 4.3. We implemented the TSDC as a time-wise sep-\narable convolution to capture spatial details independently\nof time-based changes, as motion occurs over time. Isolat-\ning spatial aspects enables a clearer understanding of the\nstructure and layout of features, separate from their move-\nment.\nThe time-wise separated spatial context is then passed\nto the \u03a6ta module for further processing to determine the\ntemporal relation of different time dimensions.\nFor the implementation of TSDC, we utilize deformable\nconvolution introduced by [7] which adjusts sampling\npoints on the standard grid by dynamically predicting ker-\nnel offsets based on input features. During training, an ad-\nditional convolutional layer called \u201doffset learning\u201d (refer to\nFigure 2a) is trained to predict these offsets. Moreover, to\nindependently process each temporal dimension, we set the\ngroup of deformable convolution kernels equal to the num-\nber of time steps T. This is done to encourage the network\nto focus on the spatial context of the data while maintaining\ntemporal relations intact for further processing.\nTemporal Attention:\nTo learn relationships between dif-\nferent time steps, we pass the local spatial context Asc =\n\u03a6tsdc(Ac) through a temporal attention module. This mod-\nule leverages the multi-head softmax self-attention mecha-\nnism introduced in [41]. In our case, we apply self-attention\nalong the temporal dimension to extract temporal relations.\nFirstly, we calculate the keys, queries, and values for\ntemporal self-attention by employing 1 \u00d7 1 convolutions,\nfollowed by a reshape operation, which we denote as Ak,\n13973\n\n=== Page 5 ===\nAq, and Av, respectively. These operations output tensors of\nshapes RH\u2032W \u2032\u00d7T , RT \u00d7H\u2032W \u2032, and RT \u00d7H\u2032W \u2032, respectively.\nAk = \u03c9k(Asc),\nAq = \u03a8(\u03c9q(Asc)),\nAv = \u03a8(\u03c9v(Asc))\n(3)\nWhere \u03a8 denotes the reshape operation and \u03c9 denotes the\n1 \u00d7 1 convolution operation.\nNext, the temporal atten-\ntion scores denoted as Ascore \u2208RT \u00d7T are computed by\nperforming matrix multiplication between Aq and Ak, fol-\nlowed by applying a softmax operation:\nAscore = softmax (AqAk) .\n(4)\nTo obtain the attended temporal features, Av is multiplied\nwith Ascore, followed by a reshape operation to output\nAattended \u2208RT \u00d7H\u2032\u00d7W \u2032:\nAattended = \u03a8(AvAscore).\n(5)\nFinally, a weighted-sum along the temporal dimension us-\ning a 1 \u00d7 1 convolution produces the output Aout[c, :, :] \u2208\nRH\u2032\u00d7W \u2032. This operation effectively combines the attended\ntemporal features to produce the final output.\n3.3.2\nEvent-rate Spatial Attention\nThis attention module extracts spatial correlation as spatial\nweights, utilizing dynamic event activity from intermediate\nspikes generated by the fsnn module. To identify active\nregions, an Event-rate Spatial Attention mechanism takes\nthe input Espike, and sums the time dimension to calculate\nthe event rates Srate, resulting in a shape of RC\u00d7H\u2032\u00d7W \u2032:\nSrate = PT\nt=1 Espike(t, :, :, :). The Srate is first normal-\nized using a sigmoid function to provide a spatial attention\nscore based on the event activity. This attention score is then\nutilized as a weight to adjust the output of the SAT module\nthrough a Hadamard product, as visualized in Figure 2b:\nEfeature = sigmoid(Srate) \u2299Aout\n(6)\nThe resulting tensor Fout is then fed into ANN blocks,\nwhich are subsequently utilized to predict the object detec-\ntion bounding box by a detection head [13].\n4. Experiments\n4.1. Setup\nDatasets:\nTo conduct the training and evaluation of our\nnetwork, we utilized two event-based object detection\ndatasets: Gen1 Automotive Detection dataset [9] and Gen4\nAutomotive Detection dataset [33]. The Gen1 and Gen4\ndatasets comprise 39 and 15 hours of event camera record-\nings at a resolution of 304 \u00d7 240 and 720 \u00d7 1280, respec-\ntively, with bounding box annotations for car, pedestrian,\nand two-wheeler (Gen4 only) classes.\nImplementation Details:\nThe model is implemented in\nPyTorch [32] with the SpikingJelly library [11] and trained\nend-to-end for 50 epochs on the Gen 1 dataset and 10\nepochs on the Gen 4 dataset. The ADAM optimizer [22]\nis used with a OneCycle learning rate schedule [38], which\ndecays linearly from a set maximum learning rate. The ker-\nnel size for \u03a6tsdc is set to 5. The training pipeline incorpo-\nrates data augmentation methods such as random horizon-\ntal flips, zoom, and crop, based on [14]. Event representa-\ntions for the SNN are constructed from 5 ms bins. During\ntraining, object detections are generated every 50 ms, us-\ning the SNN\u2019s output from the last 10 time bins, while in-\nference allows higher temporal resolution, bounded by the\nSNN timestep. The YOLOX framework [13] is used for\nobject detection, incorporating IOU loss, class loss, and re-\ngression loss. For the Gen 1 dataset, models are trained with\na batch size of 24 and a learning rate of 2 \u00d7 10\u22124, requiring\napproximately 8 hours on four 3090 GPUs. On the Gen 4\ndataset, the batch size is 8 with a learning rate of 3.5\u00d710\u22124,\ntaking around 1.5 days on four 3090 GPUs.\nWhen using an RNN variant, we follow previous meth-\nods with a sequence length of 21 for fair comparison. This\nRNN-based network, trained for 400,000 steps with a batch\nsize of 2, requires approximately 6 days to complete train-\ning.\n4.2. Benchmark comparisons\nComparison Design\nTo the best of our knowledge, this\nwork presents the first hybrid object detection model imple-\nmented in large-scale benchmark datasets, rendering com-\nparisons to other work challenging. Therefore, we design\nour comparison in three setups - (i) comparison with exist-\ning ANN-based methods (ii) comparison with SNN-based\nobject detection methods, and (iii) comparison with RNN-\nbased models.\nEvaluation Procedure:\nFollowing the evaluation proto-\ncol established in prior studies [6, 14, 33], the mean aver-\nage precision (mAP) [26] is used as the primary evaluation\nmetric to compare the proposed methods\u2019 effectiveness with\nexisting approaches. Since most methods do not offer open-\nsource code, the reported numbers from the corresponding\npapers were used.\nComparison design with ANN-based methods:\nThe ef-\nficacy of the proposed method was evaluated against ANN-\nbased models.\nThe results presented in Table 1 provide\na compelling comparison of various ANN-based networks\nand performance on the event-based Gen 1 dataset. No-\ntably, the Proposed hybrid model stands out with only 6.6M\nparameters, significantly smaller than other models such\nas SparseConv (133M) and RRC-Events (100M). Despite\n13974\n\n=== Page 6 ===\nTable 1. Comparative analysis of various ANN-based models for event-based object detection on the Gen1 [9] and Gen4 [33] Automotive\nDetection datasets, where mAP denotes mAP(.5:.05:.95). A\u2217suggests that this information was not directly available and estimated based\non the publication.\nGen 1\nGen 4\nModels\nType\nParams\nmAP\nmAP\nAEGNN [35]\nGNN\n20M\n0.16\n-\nSparseConv [30]\nANN\n133M\n0.15\n-\nInception + SSD [18]\nANN\n> 60M \u2217\n0.3\n0.34\nRRC-Events [5]\nANN\n> 100M \u2217\n0.31\n0.34\nEvents-RetinaNet [33]\nANN\n33M\n0.34\n0.18\nE2Vid-RetinaNet [33]\nANN\n44M\n0.27\n.25\nRVT-B W/O LSTM [14]\nTransformer\n16.2M \u2217\n0.32\n-\nProposed\nHybrid\n6.6M\n0.35\n.27\nTable 2. Comparative analysis of various SNN-based models for\nevent-based object detection on the Gen1 Automotive Detection\ndataset .\nModels\nType\nParams\nmAP\nVGG-11+SDD [6]\nSNN\n13M\n0.17\nMobileNet-64+SSD [6]\nSNN\n24M\n0.15\nDenseNet121-24+SSD [6]\nSNN\n8M\n0.19\nFP-DAGNet[45]\nSNN\n22M\n0.22\nEMS-RES10 [39]\nSNN\n6.20M\n0.27\nEMS-RES18 [39]\nSNN\n9.34M\n0.29\nEMS-RES34 [39]\nSNN\n14.4M\n0.31\nSpikeFPN [46]\nSNN\n22M\n0.22\nProposed\nHybrid\n6.6M\n0.35\nTable 3. Comprehensive evaluation of different RNN-based mod-\nels for event-based object detection tasks on the Gen1 Automotive\nDetection dataset. Here \u2019TF\u2019 denotes Transformer.\nModels\nType\nParams\nmAP\nS4D-ViT-B [48]\nTF + SSM\n16.5M\n0.46\nS5-ViT-B [48]\nTF + SSM\n18.2M\n0.48\nS5-ViT-S [48]\nTF + SSM\n9.7M\n0.47\nRVT-B [14]\nTF + RNN\n19M\n0.47\nRVT-S [14]\nTF + RNN\n10M\n0.46\nRVT-T [14]\nTF + RNN\n4M\n0.44\nASTMNet [25]\n(T)CNN + RNN\n100M\n0.48\nRED [33]\nCNN + RNN\n24M\n0.40\nProposed+RNN\nHybrid + RNN\n7.7M\n0.43\nits compact size, our proposed model achieves an accu-\nracy of 0.35, outperforming larger models like SparseC-\nonv, which achieves 0.15, and closely matching the perfor-\nmance of Events-RetinaNet (33M, 0.34). In contrast, for the\nGen 4 dataset, the analysis also includes architectures like\nEvents-RetinaNet and E2Vid-RetinaNet. Events-RetinaNet\nachieves a lower mean Average Precision (mAP) of 0.18,\nwhile E2Vid-RetinaNet performs slightly better with an\nmAP of 0.25. The AEGNN model, which utilizes a graph\nneural network approach, achieves an mAP of 0.16 with\n20 million parameters; however, its performance is over-\nshadowed by the proposed hybrid model, which achieves\nan mAP of 0.27 while maintaining a compact size of only\n6.6 million parameters. We observe that larger models tend\nto achieve higher accuracy due to their increased parame-\nter counts. However, their significant size makes them less\nsuitable for deployment on hardware with limited resources,\nsuch as edge devices or neuromorphic systems.\nComparison Design with SNN-based Methods:\nThe\nproposed hybrid method was compared against sev-\neral SNN-based methods, specifically, VGG-11+SDD [6],\nMobileNet-64+SSD [6], DenseNet121-24+SSD [6], FP-\nDAGNet[45], EMS-RES10 [39], EMS-RES18 [39], EMS-\nRES34 [39] and SpikeFPN [46].\nTable 2 displays the comparison results with vari-\nous state-of-the-art SNN-based object detection methods.\nMore specifically, VGG-11+SSD and MobileNet-64+SSD\nachieve mAP values of 0.17 and 0.15 with parameter counts\nof 13M and 24M, respectively.\nDenseNet121-24+SSD,\nwith a smaller parameter size of 8M, slightly outper-\nforms these models with an mAP of 0.19.\nFP-DAGNet\nand SpikeFPN, both at 22M parameters, attain an mAP\nof 0.22.\nThe EMS-RES series showcases incremental\nimprovements, with EMS-RES10 (6.2M) achieving 0.27,\nEMS-RES18 (9.34M) at 0.29, and EMS-RES34 (14.4M)\nachieving the highest mAP among SNNs at 0.31. In con-\n13975\n\n=== Page 7 ===\ntrast, the Proposed Hybrid Model surpasses all these SNN\nmodels with an mAP of 0.35 while maintaining an efficient\nparameter size of only 6.6M. This superiority can be at-\ntributed to our method\u2019s incorporation of a hybrid feature\nextraction approach with both spatial and temporal atten-\ntion modules, which are lacking in other methods.\nComparison design with RNN and SSM-based methods:\nAlthough the performance of the RNN-based models gen-\nerally outperforms models with spiking components, this\ncomparison aims to investigate how the proposed hybrid\nmodel is comparable to the RNN. We provide comparisons\nwith RNN-based methods such as RED [33], ASTMNet\n[25], RVT-B [14], RVT-S [14], RVT-T [14], S4D-ViT-B\n[48], S5-ViT-B [48], and S5-ViT-S [48]. Table 3 presents\na comparison of results obtained with RNN-based models.\nIt can be seen that two of the works, RED [33] and ASTM-\nNet [25], have substantially larger parameter counts and are\ntherefore expected to perform better.\nRVT [14] demon-\nstrates good accuracy at a parameter count comparable to\nthe proposed hybrid network.\nOur fully recurrent back-\nbone allows high-frequency detection without re-computing\nrecurrent layers, unlike RVT\u2019s non-causal attention which\nrequires re-computation for every prediction.\nCNNs are\nmore efficient than MLPs at small batch sizes due to higher\narithmetic intensity; in our method, less than 5% of MACs\nare from attention and MLPs versus 67% in RVT which is\nharder to deploy on energy-efficient edge and neuromorphic\nhardware.\n4.3. Ablation Study\nTable 4. Ablation study for ASAB module.\nModels\nmAP(.5)\nmAP\nVariant 1(w/o - \u03a6ta)\n0.57\n0.33\nVariant 2 (w/o deform)\n0.59\n0.34\nVariant 3 (w/o - ESA)\n0.59\n0.34\nVariant 4 (w/o - ASAB)\n0.53\n0.30\nVariant 5 (Proposed)\n0.61\n0.35\nASAB module:\nThe ablation study highlights the im-\nportance of each ASAB module component in enhancing\nmodel accuracy on the Gen1 Automotive Detection dataset.\nIn Table 4, Variant 1 (excluding \u03a8ta) achieves an mAP of\n0.33, revealing reduced temporal capture. In variant 2, we\nreplaced the deformable convolution with a standard con-\nvolution, which shows irregular sampling helps with sparse\ndata. In variant 3, removing the ERS module shows some\naccuracy drop, indicating limited spatial flexibility and at-\ntention. Variant 4, replacing ASAB with a simple accumu-\nlation operation, results in the lowest mAP of 0.30. The\ncomplete model (Variant 5) reaches the highest mAP of\n0.35, emphasizing the value of each component. Figure 3\nillustrates how the bridge module enhances detection by re-\nducing false predictions. Additionally, we performed an ab-\nlation study on various DWConvLSTM configurations by\ntoggling layers. The proposed setting achieved a mAP of\n0.43. Please refer to the supplementary (Table ??) for more\ndetails .\n(a) W/O ASAB module (b) W/ ASAB module\n(c) Ground Truth\nFigure 3. Visual comparison of object detection outputs between\nthe baseline hybrid event object detection method (left) and the\nproposed method (right) for the Gen 4 dataset. From left to right:\n(a) object detection output without the ASAB module, (b) object\ndetection output with the ASAB module, and (c) ground truth ob-\nject boundaries. More samples are in the Supplementary docu-\nment.\n5. Hardware Implementation, Energy and\nComputational Efficiency Analysis\n5.1. Hardware Implementation\nTable 5. Power and time measurements of the SNN block on Loihi\n2 for several input sizes and number of weight bits. The power is\nmeasured in Watts and the execution time per step is in millisec-\nonds. The mean and standard deviation of the measurements aver-\naged over 12 inputs for a total of 100k steps are reported.\nWeight\nquant.\n# chips\nPower [W]\nTime/Step\nint8\n6\n1.73 \u00b1 0.10\n2.06\nint6\n6\n1.71 \u00b1 0.11\n2.06\nint4\n6\n1.95 \u00b1 0.33\n1.16\nIn order to demonstrate the suitability of the chosen hy-\nbrid SNN-ANN approach for energy efficient inference on\nthe edge, we implemented the SNN backbone in hardware.\nIn the proposed architecture the SNN block transforms sen-\nsor data into intermediate representations and therefore un-\nderlies the strictest latency requirements. Due to the clear\n13976\n\n=== Page 8 ===\nTable 6. Comparison of different baselines complexities.\nModels\nmAP(.5)\nMACs\nACs\nBaselineann\n0.61\n15.34e9\n0.0\nBaselinew/o \u03b2asab\n0.53\n1.18e9\n0.97e9\nProposedw/\u03b2asab\n0.61\n1.63e9\n0.97e9\nProposedsnn+\n0.58\n0.87e9\n1.59e9\nseparation between SNN and ANN parts in the model\u2019s\narchitecture, the SNN blocks can be implemented in spe-\ncialized hardware.\nAs hardware, we chose Intel\u2019s Loihi\n2 [8], a digital, programmable chip based on event-based\ncommunication and computation. Only minor adjustments\nare necessary for execution on Loihi 2: The kernel-weights\nof the convolutional layers are quantized to int8 via a\nper-output-channel quantization scheme showing no result-\ning loss in accuracy (mAP: 0.348 (float16) vs 0.343\n(int8)). The batchnorm (BN) operations and quantization\nscaling are fused into the LIF-neuron dynamics by scaling\nand shifting the inputs according to the follwing equations:\nscale =\nqscale weightBN\n\u03c4 \u221aVarBN + \u03b5BN\n(7)\nshift = (biasconv \u2212meanBN)\nweightBN\n\u03c4 \u221aVarBN + \u03b5BN\n+ biasBN\n\u03c4\n(8)\nwhere qscale is the scaling factor introduced by the quan-\ntization and \u03c4 is the PLIF neurons time constant. Given\nthis approach, spike times are almost exactly reproduced\non Loihi 2 compared to the PyTorch int8 implementa-\ntion. For benchmarking purposes, the inputs to the network\nare simulated with an additional neuron population, due to\nthe current IO limitations of the chip. With this approach\nthe spiking statistics in the input and SNN layers are repro-\nduced.\nTable 5 reports power and time measurement results of\nthe 4-layer SNN block running on Loihi 2 for inputs of\nsize (2, 256, 160). The network runs at (1.7 \u00b1 0.1) W and\n(1.9 \u00b1 0.8) ms per step, which is faster than real-time in\nthe currently chosen architecture (5 ms per step).\nThese\nresults compare favorably to commercially available chips\nfor edge computing like the NVIDIA Jetson Orin Nano\n(7 W\u221215 W) [1] and demonstrate the suitability of an SNN\nbackbone for event-based data processing.\n5.2. Computational Analysis\nWe trained a variant of our model where the modified PLIF\nneuron acts like a ReLU with proposed attention mod-\nThis research was funded by the German Federal Ministry of\nEducation and Research (BMBF) under the projects \u201dGreenEdge-\nFuE\u201d (16ME0521) and \u201dCluster4Future\u201d (03ZU1106CB). Access to\nJUWELS [21] has been granted by GCS (www.gauss-centre.eu) under\nproject neuroml. We thank Intel for access to Loihi 2.\nule to investigate a comparison between a similar artifi-\ncial neural network (ANN) and the hybrid network. This\nvariant, Baselineann, with a mean 15.34 \u00d7 109 multiply-\naccumulate operations (MACs), makes it resource-intensive\nand unsuitable for energy-constrained hardware. In con-\ntrast, our Proposed hybrid model computes only 1.63 \u00d7\n109 MACs, making it more practical for edge devices.\nThe \u03b2asab module does incur additional operations but\nleads to significant accuracy improvement (compare to\nBaselinew/o \u03b2asab). Additionally, the spiking neural net-\nwork (SNN) variant, Proposedsnn+ (Increasing one SNN\nlayer and reducing one ANN layer), reduces computational\ndemands further to 0.87\u00d7109 MACs and is highly efficient\non neuromorphic hardware, running significantly faster and\nwith less energy on Intel\u2019s Loihi 2 compared to the dense-\nactivation ANNs.\nWe analyze power consumption across methods follow-\ning [2].\nAmong SNNs, DenseNet121+SSD uses 0.9 mJ\n(0.0 MACs, 2.3 \u00d7 109 ACs), while VGG-11+SSD requires\n4.2 mJ (11.1 \u00d7 109 ACs). In ANNs, Inception+SSD is the\nmost demanding at 19.3 mJ (11.4 \u00d7 109 MACs). Events-\nRetinaNet consumes 5.4 mJ (3.2 \u00d7 109 MACs), and RVT-B\nW/O LSTM requires 3.9 mJ (2.3\u00d7109 MACs). Our method\nachieves 1.6\u00d7109 MACs, 1.0\u00d7109 ACs, and 3.1 mJ, signif-\nicantly reducing energy and computational costs compared\nto most ANNs.\n6. Conclusion\nIn this work, we introduced a hybrid attention-based SNN-\nANN backbone for event-based visual object detection.\nA novel attention-based SNN-ANN bridge module is\nproposed to capture sparse spatial and temporal relations\nfrom the SNN layer and convert them into dense feature\nmaps for the ANN part of the backbone.\nAdditionally,\nwe demonstrate the effectiveness of combining RNNs\non multiple timescales: hardware-efficient SNNs for fast\ndynamics on short timescales with ConvLSTMs for longer\ntimescales that operate on the extracted features of the\nbridge-module. Experimental results demonstrate that our\nproposed method surpasses baseline hybrid and SNN-based\napproaches by significant margins, with results comparable\nto existing ANN-based methods.\nThe efficacy of our\nproposed modules and architectural choices is confirmed\nthrough extensive ablation studies.\nAdditionally, we\ndemonstrate the effectiveness of our architectural choice\nwith separate SNN and ANN blocks by implementing the\nSNN blocks on digital neuromorphic hardware, Intel\u2019s\nLoihi 2.\nThe neuromorphic hardware implementation\nachieves sub-real-time processing and improved power\nconsumption compared to commercially available edge\ncomputing hardware. The achieved accuracy and hardware\nimplementation results pave the way toward a hybrid\nSNN-ANN architecture that achieves ANN-like perfor-\nmance at a drastically reduced parameter and power budget.\n13977\n\n=== Page 9 ===\nReferences\n[1] Nvidia jetson orin nano datasheet, revision 4. https://\nopenzeka.com/wp-content/uploads/2023/03/\njetson- orin- nano- datasheet- r4- web.pdf,\n2023. Accessed: 2025-03-09. 8\n[2] Asude Aydin, Mathias Gehrig, Daniel Gehrig, and Da-\nvide Scaramuzza.\nA hybrid ann-snn architecture for low-\npower and low-latency visual perception. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops (CVPRW), 2024. 2, 8\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 3\n[4] Francesco Barchi, Gianvito Urgese, Alessandro Siino, Santa\nDi Cataldo, Enrico Macii, and Andrea Acquaviva.\nFlexi-\nble on-line reconfiguration of multi-core neuromorphic plat-\nforms. IEEE Transactions on Emerging Topics in Comput-\ning, 9(2):915\u2013927, 2019. 1\n[5] Nicholas FY Chen. Pseudo-labels for supervised learning\non dynamic vision sensor data, applied to object detection\nunder ego-motion. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition workshops, pages\n644\u2013653, 2018. 1, 2, 6\n[6] Lo\u00a8\u0131c Cordone, Beno\u02c6\u0131t Miramond, and Philippe Thierion. Ob-\nject detection with spiking neural networks on automotive\nevent data. In 2022 International Joint Conference on Neu-\nral Networks (IJCNN), pages 1\u20138. IEEE, 2022. 2, 5, 6\n[7] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In Proceedings of the IEEE international confer-\nence on computer vision, pages 764\u2013773, 2017. 4\n[8] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham\nChinya, Yongqiang Cao, Sri Harsha Choday, Georgios Di-\nmou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi:\nA neuromorphic manycore processor with on-chip learning.\nIeee Micro, 38(1):82\u201399, 2018. 1, 8\n[9] Pierre De Tournemire, Davide Nitti, Etienne Perot, Da-\nvide Migliore, and Amos Sironi.\nA large scale event-\nbased detection dataset for automotive.\narXiv preprint\narXiv:2001.08499, 2020. 2, 3, 5, 6\n[10] Wei Fang, Zhaofei Yu, Yanqi Chen, Timoth\u00b4ee Masquelier,\nTiejun Huang, and Yonghong Tian. Incorporating learnable\nmembrane time constant to enhance learning of spiking neu-\nral networks. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pages 2661\u20132671,\n2021. 3\n[11] Wei Fang, Yanqi Chen, Jianhao Ding, Zhaofei Yu, Timoth\u00b4ee\nMasquelier, Ding Chen, Liwei Huang, Huihui Zhou, Guoqi\nLi, and Yonghong Tian. Spikingjelly: An open-source ma-\nchine learning infrastructure platform for spike-based intel-\nligence. Science Advances, 9(40):eadi1480, 2023. 5\n[12] Guillermo Gallego, Tobi Delbr\u00a8uck, Garrick Orchard, Chiara\nBartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger,\nAndrew J Davison, J\u00a8org Conradt, Kostas Daniilidis, et al.\nEvent-based vision: A survey. IEEE transactions on pattern\nanalysis and machine intelligence, 44(1):154\u2013180, 2020. 1\n[13] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\nSun. Yolox: Exceeding yolo series in 2021. arXiv preprint\narXiv:2107.08430, 2021. 5\n[14] Mathias Gehrig and Davide Scaramuzza. Recurrent vision\ntransformers for object detection with event cameras. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 13884\u201313893, 2023. 1, 2, 3,\n5, 6, 7\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 2\n[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017. 2\n[17] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger.\nDensely connected convolutional net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4700\u20134708, 2017. 2\n[18] Massimiliano Iacono, Stefan Weber, Arren Glover, and\nChiara Bartolozzi.\nTowards event-driven object detection\nwith off-the-shelf deep learning. In 2018 IEEE/RSJ Interna-\ntional Conference on Intelligent Robots and Systems (IROS),\npages 1\u20139. IEEE, 2018. 1, 2, 6\n[19] Forrest N Iandola, Song Han, Matthew W Moskewicz,\nKhalid Ashraf,\nWilliam J Dally,\nand Kurt Keutzer.\nSqueezenet:\nAlexnet-level accuracy with 50x fewer pa-\nrameters and\u00a1 0.5 mb model size.\narXiv preprint\narXiv:1602.07360, 2016. 2\n[20] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In International conference on machine learn-\ning, pages 448\u2013456. pmlr, 2015. 3\n[21] J\u00a8ulich Supercomputing Centre.\nJUWELS Cluster and\nBooster: Exascale Pathfinder with Modular Supercomputing\nArchitecture at Juelich Supercomputing Centre. Journal of\nlarge-scale research facilities, 7(A138), 2021. 8\n[22] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 5\n[23] Alexander Kugele, Thomas Pfeil, Michael Pfeiffer, and Elis-\nabetta Chicca. Hybrid snn-ann: Energy-efficient classifica-\ntion and object detection for event-based vision. In DAGM\nGerman Conference on Pattern Recognition, pages 297\u2013312.\nSpringer, 2021. 2\n[24] Chankyu Lee, Adarsh Kumar Kosta, Alex Zihao Zhu, Ken-\nneth Chaney, Kostas Daniilidis, and Kaushik Roy. Spike-\nflownet: event-based optical flow estimation with energy-\nefficient hybrid neural networks. In European Conference\non Computer Vision, pages 366\u2013382. Springer, 2020. 2\n[25] Jianing Li, Jia Li, Lin Zhu, Xijie Xiang, Tiejun Huang, and\nYonghong Tian. Asynchronous spatio-temporal memory net-\nwork for continuous event-based object detection.\nIEEE\nTransactions on Image Processing, 31:2975\u20132987, 2022. 2,\n6, 7\n13978\n\n=== Page 10 ===\n[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 5\n[27] Tsung-Yi Lin, Piotr Doll\u00b4ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie.\nFeature pyra-\nmid networks for object detection.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 2117\u20132125, 2017. 2\n[28] Faqiang Liu and Rong Zhao. Enhancing spiking neural net-\nworks with hybrid top-down attention. Frontiers in Neuro-\nscience, 16:949142, 2022. 2\n[29] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\nSzegedy, Scott Reed, Cheng-Yang Fu, and Alexander C\nBerg.\nSsd: Single shot multibox detector.\nIn Computer\nVision\u2013ECCV 2016: 14th European Conference, Amster-\ndam, The Netherlands, October 11\u201314, 2016, Proceedings,\nPart I 14, pages 21\u201337. Springer, 2016. 2\n[30] Nico Messikommer, Daniel Gehrig, Antonio Loquercio, and\nDavide Scaramuzza. Event-based asynchronous sparse con-\nvolutional networks. In Computer Vision\u2013ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part VIII 16, pages 415\u2013431. Springer, 2020.\n1, 2, 6\n[31] E. O. Neftci, H. Mostafa, and F. Zenke. Surrogate gradient\nlearning in spiking neural networks: Bringing the power of\ngradientbased optimization to spiking neural networks. IEEE\nSignal Processing Magazine, 36(6):5163, 2019. 2\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library. Ad-\nvances in neural information processing systems, 32, 2019.\n5\n[33] Etienne Perot, Pierre De Tournemire, Davide Nitti, Jonathan\nMasci, and Amos Sironi. Learning to detect objects with a\n1 megapixel event camera. Advances in Neural Information\nProcessing Systems, 33:16639\u201316652, 2020. 2, 3, 5, 6, 7\n[34] Jun Sawada, Filipp Akopyan, Andrew S Cassidy, Brian Taba,\nMichael V Debole, Pallab Datta, Rodrigo Alvarez-Icaza,\nArnon Amir, John V Arthur, Alexander Andreopoulos, et al.\nTruenorth ecosystem for brain-inspired computing: scalable\nsystems, software, and applications. In SC\u201916: Proceedings\nof the International Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 130\u2013141.\nIEEE, 2016. 1\n[35] Simon Schaefer, Daniel Gehrig, and Davide Scaramuzza.\nAegnn: Asynchronous event-based graph neural networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 12371\u201312381, 2022. 1,\n2, 6\n[36] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung,\nWai-Kin Wong, and Wang-chun Woo. Convolutional lstm\nnetwork:\nA machine learning approach for precipitation\nnowcasting. Advances in neural information processing sys-\ntems, 28, 2015. 1\n[37] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 2\n[38] Leslie N Smith and Nicholay Topin.\nSuper-convergence:\nVery fast training of neural networks using large learn-\ning rates.\nIn Artificial intelligence and machine learning\nfor multi-domain operations applications, pages 369\u2013386.\nSPIE, 2019. 5\n[39] Qiaoyi Su, Yuhong Chou, Yifan Hu, Jianing Li, Shijie Mei,\nZiyang Zhang, and Guoqi Li. Deep directly-trained spik-\ning neural networks for object detection.\nIn Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 6555\u20136565, 2023. 2, 6\n[40] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich.\nGoing deeper with\nconvolutions.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1\u20139, 2015.\n2\n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 4\n[42] Ziyi Wu, Mathias Gehrig, Qing Lyu, Xudong Liu, and Igor\nGilitschenski.\nLeod: Label-efficient object detection for\nevent cameras. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16933\u2013\n16943, 2024. 2\n[43] Zheyu Yang, Yujie Wu, Guanrui Wang, Yukuan Yang, Guoqi\nLi, Lei Deng, Jun Zhu, and Luping Shi. Dashnet: A hybrid\nartificial and spiking neural network for high-speed object\ntracking. arXiv preprint arXiv:1909.12942, 2019. 2\n[44] Jason Yik, Soikat Hasan Ahmed, Zergham Ahmed, Brian\nAnderson, Andreas G Andreou, Chiara Bartolozzi, Arindam\nBasu, Douwe den Blanken, Petrut Bogdan, Sander Bohte,\net al.\nNeurobench: Advancing neuromorphic computing\nthrough collaborative, fair and representative benchmarking.\narXiv preprint arXiv:2304.04640, 2023. 1\n[45] Hu Zhang, Luziwei Leng, Kaiwei Che, Qian Liu, Jie\nCheng, Qinghai Guo, Jiangxing Liao, and Ran Cheng.\nAutomotive object detection via learning sparse events by\ntemporal dynamics of spiking neurons.\narXiv preprint\narXiv:2307.12900, 2023. 6\n[46] Hu Zhang, Yanchen Li, Luziwei Leng, Kaiwei Che, Qian\nLiu, Qinghai Guo, Jianxing Liao, and Ran Cheng. Auto-\nmotive object detection via learning sparse events by spiking\nneurons. IEEE Transactions on Cognitive and Developmen-\ntal Systems, 2024. 6\n[47] Rong Zhao, Zheyu Yang, Hao Zheng, Yujie Wu, Faqiang\nLiu, Zhenzhi Wu, Lukai Li, Feng Chen, Seng Song, Jun Zhu,\net al. A framework for the general design and computation\nof hybrid neural networks. Nature communications, 13(1):\n3427, 2022. 2\n[48] Nikola Zubic, Mathias Gehrig, and Davide Scaramuzza.\nState space models for event cameras.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5819\u20135828, 2024. 2, 6, 7\n13979\n",
      "searchable_content": "page 1 efficient event-based object detection a hybrid neural network with spatial and temporal attention soikat hasan ahmed jan finkbeiner emre neftci forschungszentrum j ulich rwth aachen university s.ahmed j.finkbeiner e.neftci fz-juelich.de abstract event cameras offer high temporal resolution and dy- namic range with minimal motion blur making them promising for robust object detection. while spiking neu- ral networks snns on neuromorphic hardware are often considered for energy efficient and low latency event-based data processing they often fall short of artificial neural networks anns in accuracy and flexibility. here we in- troduce attention-based hybrid snn-ann backbones for event-based object detection to leverage the strengths of both snn and ann architectures. a novel attention-based snn-ann bridge module captures sparse spatial and tem- poral relations from the snn layer and converts them into dense feature maps for the ann part of the backbone. ad- ditionally we present a variant that integrates dwconvl- stms to the ann blocks to capture slower dynamics. this multi-timescale network combines fast snn processing for short timesteps with long-term dense rnn processing ef- fectively capturing both fast and slow dynamics. experi- mental results demonstrate that our proposed method sur- passes snn-based approaches by significant margins with results comparable to existing ann and rnn-based meth- ods. unlike ann-only networks the hybrid setup allows us to implement the snn blocks on digital neuromorphic hard- ware to investigate the feasibility of our approach. exten- sive ablation studies and implementation on neuromorphic hardware confirm the effectiveness of our proposed modules and architectural choices. our hybrid snn-ann architec- tures pave the way for ann-like performance at a drasti- cally reduced parameter latency and power budget. 1. introduction over the past decade deep learning has made significant advances in object detection. state-of-the-art approaches predominantly rely on frame-based cameras which cap- conceptual design algorithm development and experimentation. conceptual design hardware analysis and deployment. ture frames at a fixed rate. frame cameras provide dense intensity data but have limitations in dynamic range and frame rates leading to motion blur. dynamic vision sensors dvs or event cameras offer an alternative by asynchronously capturing pixel-level illumination changes achieving low latency 10\u00b5s higher temporal resolu- tion and an extended dynamic range 140 db vs. 60 db 12 . these characteristics make them well-suited for low-light and fast-motion scenarios. however due to the sparse high-temporal-resolution data they generate effec- tively processing event data for object detection remains a challenging and emerging research area. early adopters of event-based object detection ann models often naively re- purpose architectures originally designed for frame-based cameras 5 18 30 35 44 . ann models generally achieve good accuracy but tend to be large in terms of parameter count and mac operations making them less suitable for deployment on power-efficient edge or neuromorphic de- vices. furthermore the high sparsity and temporal reso- lution is often discarded in favor of dense representations to leverage gpus dense vector-based representations. in contrast snns implemented in neuromorphic hardware are ideally suited to leverage the sparsity of event-based inputs offering significant reductions in computational cost power consumption and latency 4 8 34 . however snns tend to be less accurate at the task level compared to their ann counterparts. in this work we create a hybrid snn-ann-based back- bone architecture to combine the efficient event-driven pro- cessing of snns on neuromorphic hardware with the effi- cient learning and representation capabilities of anns. the snn extracts low-level features with high temporal resolu- tion from the event-based sensors and converts them into in- termediate features which then change to slower timescales before being processed by the ann with dense activations. additionally we feature a variant that adds dwconvl- stms 14 36 to the ann block. this multi-timescale rnn variant combines the sparse snn processing of short timesteps with long time horizon processing via the dense rnn with the extracted long timesteps to efficiently cap- this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 13970 page 2 ture both fast and slow dynamics of the data. for hybrid models the snn can be efficiently deployed at the edge in a power efficient manner as we demonstrate with an imple- mentation on a digital neuromorphic chip. ann processing can occur either at the edge or with reduced data rates in a cloud setting. by training the network jointly the snn component can leverage backpropagated errors for efficient training via the surrogate gradient approach 31 . information in snns is communicated by spike events. in our hybrid models these must be efficiently converted into dense representations without discarding valuable spa- tiotemporal features. in our model this is achieved by an attention-based snn-ann bridge module which bridges snn representations to ann representations. the attention module contains two attention modules named event-rate spatial ers attention and spatial aware temporal sat attention. the sat attention mod- ule addresses the challenge of sparse event inputs by en- hancing the model s understanding of irregular structures and temporal attention to discern temporal relationships within the data. on the other hand the ers attention mod- ule focuses on highlighting spatial areas by leveraging the activity of events. moreover we implement the snn blocks in digital neuromorphic hardware to demonstrate the feasi- bility of our approach. we report the performance of our model on large-scale event-based object detection bench- marks. the contributions of our work can be summarized as follows a novel hybrid backbone-based event object detection model. to the best of our knowledge this is the first work to propose a hybrid object detection approach for benchmark object detection task. evaluation on the gen1 and gen4 automotive detection datasets 9 33 shows that the proposed method outperforms snn-based meth- ods and achieves comparable results to ann and rnn- based approaches. an attention-based snn-ann bridge module \u03b2asab to convert spatiotemporal spikes into a dense representation enabling the ann part of the network to process it effec- tively while preserving valuable spatial and temporal fea- tures through the event-rate spatial ers and spatial- aware temporal sat attention mechanisms. a multi-timescale rnn variant that includes both the high-temporal resolution snn block followed by a slower long time horizon dwconvlstms in the ann block operating on larger timesteps extracted via the \u03b2asab module. implementation of the snn blocks on digital neuromor- phic hardware to validate its performance and efficiency. 2. related work recent studies demonstrated the potential of event cameras in object detection tasks. in the earlier stages of adopting event cameras the focus primarily revolved around adapt- ing existing frame-based feature extractors and a detection head for object detection using event data 5 18 . in 18 researchers integrated event-based data into off-the-shelf frame-based object detection networks. they employed an inceptionnet-based backbone for feature extraction and a single-shot detector ssd for detection 29 40 . similarly 33 utilized a frame-based object detection model called retinanet which incorporates a spatial pooling-based fea- ture extractor 27 along with a detection head applied to event data. additionally methods such as 14 25 33 have incorpo- rated recurrent neural networks rnns as feature extrac- tors for event data. 48 uses ssm to improve training time and 42 proposes a training schema with efficient ground truth label utilization. 30 introduced sparse convolution as a method for event feature extraction. to address the challenges of efficiently extracting spatiotemporal features 35 investigates the usability of a graph neural network- based approach as a feature extractor. recently snn-based methods have become popular for event data processing due to their spike-based working prin- ciple similar to event cameras which enables efficient processing. research conducted by 6 and 39 show- cases the effective utilization of snns in object detection tasks. specifically 6 and 39 delve into assessing the performance of converting widely-used ann-based back- bone architectures such as squeezenet 19 vgg 37 mobilenet 16 densenet 17 and resnet 15 into snn architecture for event-based object detection. nonetheless optimizing intermediate and high-level features for detec- tion with snns results in a significant drop in accuracy. recognizing the distinct advantages offered by both snns and anns researchers have explored merging these networks into hybrid architectures 23 . for instance 47 presents a framework that leverages hierarchical infor- mation abstraction for meta-continual learning with inter- pretable multimodal reasoning. building on this idea 43 introduces dashnet which integrates snns with ann- based feature extraction for high-speed object tracking. similarly 28 improves snn performance through a hy- brid top-down attention mechanism while 24 demon- strates that hybrid models can achieve energy-efficient op- tical flow estimation with enhanced robustness. comple- menting these advances 2 23 develops an architecture that fuses snn backbones with ann heads for event-based vision tasks. by leveraging the complementary strengths of each these hybrid networks show promise for simpler tasks. however the bridge between snns and anns is still over- looked to harness the best of both worlds. moreover the full extent of their capabilities remains largely unexplored especially in tackling state-of-the-art benchmark vision tasks such as object detection on popu- 13971 page 3 lar datasets like gen1 automotive detection dataset 9 and gen4 automotive detection dataset 33 . 3. hybrid object detection network the overall hybrid network as shown in figure 1 com- prises two key parts an attention-based hybrid backbone designed to extract spatio-temporal features and detection heads tasked with identifying objects. in the following sec- tion we will delve into the details of the core components of the network. 3.1. event representation an event is represented as en xn yn tn pn where xn yn is the pixel location tn is the time and pn is polar- ity which indicates the change in light intensity i.e. posi- tive or negative . the event data is pre-processed to convert it into a time-space tensor format. following 14 we start by creating a 4d tensor events tk 1 tk rt 2 h w where t represents number of time discretization steps 2 denotes polarity features which contain the count of posi- tive and negative events in each discretized time step and h and w signify the height and width of the event cam- era respectively. given the event set e e1 e2 . . . en the event tensor events tk 1 tk is constructed from the discretized time variable t n j tn ta tb ta t k as follows events tk 1 tk t p x y x en e \u03b4 p pn \u03b4 x xn \u03b4 y yn \u03b4 t t n . 1 while training event tensors are created. however during inference given an input sparsity of 98 for gen 1 this results in significant efficiency gains due to sparse pro- cessing in neuromorphic hardware compared to the dense processing in a gpu. 3.2. attention-based hybrid backbone the proposed hybrid backbone architecture as shown in figure 1 consists of three fundamental components a low-level spatio-temporal feature extractor fsnn an ann- based high-level spatial feature extractor fann and a novel attention-based snn-ann bridge asab module \u03b2asab. the first module denoted as fsnn is an event-level feature extractor operating in the spatio-temporal domain and consists of multiple convolutional snn blocks. each block follows a structured sequence of operations stan- dard convolution batch normalization 20 and paramet- ric leaky integration and fire plif spiking neuron 10 . the neural dynamics of pilf with trainable time constant \u03c4 sigmoid w 1 given input x t can be expressed as follows v t v t 1 1 \u03c4 x t v t 1 vreset . 2 the fsnn module receives events tk 1 tk as its input and generates events espike fsnn events tk 1 tk rt c h w . as snns operate on a faster timescale and utilize sparse representations and anns operate on dense representations efficiently translating valuable spatio-temporal information into dense representations is essential. to achieve this translation the espike is subse- quently fed into a proposed \u03b2asab module which bridges the snn and the ann parts. the events espike are con- verted into dense non-binary features while preserving spa- tial and temporal information in the form of spatial fea- ture maps. the output of \u03b2asab is represented by fout \u03b2asab espike with dimensions c h w which is com- patible with traditional 2d convolution-based networks al- lowing for smooth processing and integration of informa- tion across both spatial and temporal dimensions. the at- tention module is further described in section 3.3. the third component fann extracts high-level spatial features using multiple ann blocks with standard ann components. each ann block consists of standard convo- lution operations normalization 3 20 and relu activa- tion functions enabling the extraction of detailed high-level spatial features from the densely encoded fout. in addition to the proposed model we explore a vari- ant that features an added rnn module incorporating two depth-wise separable convolutional lstm dwconvl- stm units similar to those in 14 as illustrated in figure 1. the fsnn processes fast dynamics with small timesteps from the event-based camera while the dwconvlstm op- erates on larger timesteps extracted from the \u03b2asab-module to capture slower dynamics. the resulting outputs from the ann blocks are then fed to the detection head for the final object detection output. 3.3. attention-based snn-ann bridge module the bridge module \u03b2asab comprises two attention modules i spatial-aware temporal sat attention and ii event- rate spatial ers attention. the sat attention module dy- namically captures local spatial context within the irregular spatial spike-structure to uncover temporal relations. mean- while the ers attention submodule focuses on attending to spatial areas utilizing the spatial event activities. below we describe these two submodules. 3.3.1 spatial-aware temporal sat attention the sat attention contains three crucial operations i channel-wise temporal grouping to group relevant fea- tures from different time dimensions ii time-wise separa- ble deformable convolution tsdc denoted as \u03c6tsdc for capturing channel-independent local spatial context from sparse spike features and iii temporal attention module 13972 page 4 snn block snn block snn block snn block ann block ann block ann block ann block dwconvlstm dwconvlstm detection fpn head to tensor time figure 1. architecture of the hybrid model with object detection head and snn-ann hybrid backbone including the snn part \u03b2asab bridge module and ann part. the dwconvlstm modules and dashed blue arrows are only part of the proposed hybrid rnn variant. softmax matmul matmul offsets learned offset field offset learning conv 1x1 relu conv 1x1 conv 1x1 conv 1x1 a spatial-aware temporal attention. sigmoid spatial-aware temporal attenton hadamard product sum dim t b event-rate spatial attention. figure 2. visualization of the proposed attention module s components. a spatial-aware temporal attention highlights relevant temporal features in spatial regions to enhance temporal coherence in event-based data. b event-rate spatial attention emphasizes spatial regions based on event rates allowing for adaptive focus on areas with significant event activity. together these components improve feature extraction in spatiotemporal data processing. \u03c6ta which uses local spatial context features to extract tem- poral relations to accumulate and translate temporal infor- mation into spatial information. time-wise separable deformable convolutions tsdc at first we apply the channel-wise tempo- ral grouping operation to the input data so that each feature channel is processed separately while capturing spatial and temporal relations. this operation transforms the input spike tensor espike rt c h w into ain rc t h w . as shown in figure 2a the \u03c6tsdc operation operates on individual channel-wise temporal groups ain ci de- noted as ac where i is the channel index. this operation extracts the local spatial context of the sparse irregularly shaped spike-based representations. we posit that this irreg- ular representation is better extracted using a deformed ker- nel rather than a standard square grid kernel as discussed in section 4.3. we implemented the tsdc as a time-wise sep- arable convolution to capture spatial details independently of time-based changes as motion occurs over time. isolat- ing spatial aspects enables a clearer understanding of the structure and layout of features separate from their move- ment. the time-wise separated spatial context is then passed to the \u03c6ta module for further processing to determine the temporal relation of different time dimensions. for the implementation of tsdc we utilize deformable convolution introduced by 7 which adjusts sampling points on the standard grid by dynamically predicting ker- nel offsets based on input features. during training an ad- ditional convolutional layer called offset learning refer to figure 2a is trained to predict these offsets. moreover to independently process each temporal dimension we set the group of deformable convolution kernels equal to the num- ber of time steps t. this is done to encourage the network to focus on the spatial context of the data while maintaining temporal relations intact for further processing. temporal attention to learn relationships between dif- ferent time steps we pass the local spatial context asc \u03c6tsdc ac through a temporal attention module. this mod- ule leverages the multi-head softmax self-attention mecha- nism introduced in 41 . in our case we apply self-attention along the temporal dimension to extract temporal relations. firstly we calculate the keys queries and values for temporal self-attention by employing 1 1 convolutions followed by a reshape operation which we denote as ak 13973 page 5 aq and av respectively. these operations output tensors of shapes rh w t rt h w and rt h w respectively. ak \u03c9k asc aq \u03c8 \u03c9q asc av \u03c8 \u03c9v asc 3 where \u03c8 denotes the reshape operation and \u03c9 denotes the 1 1 convolution operation. next the temporal atten- tion scores denoted as ascore rt t are computed by performing matrix multiplication between aq and ak fol- lowed by applying a softmax operation ascore softmax aqak . 4 to obtain the attended temporal features av is multiplied with ascore followed by a reshape operation to output aattended rt h w aattended \u03c8 avascore . 5 finally a weighted-sum along the temporal dimension us- ing a 1 1 convolution produces the output aout c rh w . this operation effectively combines the attended temporal features to produce the final output. 3.3.2 event-rate spatial attention this attention module extracts spatial correlation as spatial weights utilizing dynamic event activity from intermediate spikes generated by the fsnn module. to identify active regions an event-rate spatial attention mechanism takes the input espike and sums the time dimension to calculate the event rates srate resulting in a shape of rc h w srate pt t 1 espike t . the srate is first normal- ized using a sigmoid function to provide a spatial attention score based on the event activity. this attention score is then utilized as a weight to adjust the output of the sat module through a hadamard product as visualized in figure 2b efeature sigmoid srate aout 6 the resulting tensor fout is then fed into ann blocks which are subsequently utilized to predict the object detec- tion bounding box by a detection head 13 . 4. experiments 4.1. setup datasets to conduct the training and evaluation of our network we utilized two event-based object detection datasets gen1 automotive detection dataset 9 and gen4 automotive detection dataset 33 . the gen1 and gen4 datasets comprise 39 and 15 hours of event camera record- ings at a resolution of 304 240 and 720 1280 respec- tively with bounding box annotations for car pedestrian and two-wheeler gen4 only classes. implementation details the model is implemented in pytorch 32 with the spikingjelly library 11 and trained end-to-end for 50 epochs on the gen 1 dataset and 10 epochs on the gen 4 dataset. the adam optimizer 22 is used with a onecycle learning rate schedule 38 which decays linearly from a set maximum learning rate. the ker- nel size for \u03c6tsdc is set to 5. the training pipeline incorpo- rates data augmentation methods such as random horizon- tal flips zoom and crop based on 14 . event representa- tions for the snn are constructed from 5 ms bins. during training object detections are generated every 50 ms us- ing the snn s output from the last 10 time bins while in- ference allows higher temporal resolution bounded by the snn timestep. the yolox framework 13 is used for object detection incorporating iou loss class loss and re- gression loss. for the gen 1 dataset models are trained with a batch size of 24 and a learning rate of 2 10 4 requiring approximately 8 hours on four 3090 gpus. on the gen 4 dataset the batch size is 8 with a learning rate of 3.5 10 4 taking around 1.5 days on four 3090 gpus. when using an rnn variant we follow previous meth- ods with a sequence length of 21 for fair comparison. this rnn-based network trained for 400 000 steps with a batch size of 2 requires approximately 6 days to complete train- ing. 4.2. benchmark comparisons comparison design to the best of our knowledge this work presents the first hybrid object detection model imple- mented in large-scale benchmark datasets rendering com- parisons to other work challenging. therefore we design our comparison in three setups - i comparison with exist- ing ann-based methods ii comparison with snn-based object detection methods and iii comparison with rnn- based models. evaluation procedure following the evaluation proto- col established in prior studies 6 14 33 the mean aver- age precision map 26 is used as the primary evaluation metric to compare the proposed methods effectiveness with existing approaches. since most methods do not offer open- source code the reported numbers from the corresponding papers were used. comparison design with ann-based methods the ef- ficacy of the proposed method was evaluated against ann- based models. the results presented in table 1 provide a compelling comparison of various ann-based networks and performance on the event-based gen 1 dataset. no- tably the proposed hybrid model stands out with only 6.6m parameters significantly smaller than other models such as sparseconv 133m and rrc-events 100m . despite 13974 page 6 table 1. comparative analysis of various ann-based models for event-based object detection on the gen1 9 and gen4 33 automotive detection datasets where map denotes map .5 .05 .95 . a suggests that this information was not directly available and estimated based on the publication. gen 1 gen 4 models type params map map aegnn 35 gnn 20m 0.16 - sparseconv 30 ann 133m 0.15 - inception ssd 18 ann 60m 0.3 0.34 rrc-events 5 ann 100m 0.31 0.34 events-retinanet 33 ann 33m 0.34 0.18 e2vid-retinanet 33 ann 44m 0.27 .25 rvt-b w o lstm 14 transformer 16.2m 0.32 - proposed hybrid 6.6m 0.35 .27 table 2. comparative analysis of various snn-based models for event-based object detection on the gen1 automotive detection dataset . models type params map vgg-11 sdd 6 snn 13m 0.17 mobilenet-64 ssd 6 snn 24m 0.15 densenet121-24 ssd 6 snn 8m 0.19 fp-dagnet 45 snn 22m 0.22 ems-res10 39 snn 6.20m 0.27 ems-res18 39 snn 9.34m 0.29 ems-res34 39 snn 14.4m 0.31 spikefpn 46 snn 22m 0.22 proposed hybrid 6.6m 0.35 table 3. comprehensive evaluation of different rnn-based mod- els for event-based object detection tasks on the gen1 automotive detection dataset. here tf denotes transformer. models type params map s4d-vit-b 48 tf ssm 16.5m 0.46 s5-vit-b 48 tf ssm 18.2m 0.48 s5-vit-s 48 tf ssm 9.7m 0.47 rvt-b 14 tf rnn 19m 0.47 rvt-s 14 tf rnn 10m 0.46 rvt-t 14 tf rnn 4m 0.44 astmnet 25 t cnn rnn 100m 0.48 red 33 cnn rnn 24m 0.40 proposed rnn hybrid rnn 7.7m 0.43 its compact size our proposed model achieves an accu- racy of 0.35 outperforming larger models like sparsec- onv which achieves 0.15 and closely matching the perfor- mance of events-retinanet 33m 0.34 . in contrast for the gen 4 dataset the analysis also includes architectures like events-retinanet and e2vid-retinanet. events-retinanet achieves a lower mean average precision map of 0.18 while e2vid-retinanet performs slightly better with an map of 0.25. the aegnn model which utilizes a graph neural network approach achieves an map of 0.16 with 20 million parameters however its performance is over- shadowed by the proposed hybrid model which achieves an map of 0.27 while maintaining a compact size of only 6.6 million parameters. we observe that larger models tend to achieve higher accuracy due to their increased parame- ter counts. however their significant size makes them less suitable for deployment on hardware with limited resources such as edge devices or neuromorphic systems. comparison design with snn-based methods the proposed hybrid method was compared against sev- eral snn-based methods specifically vgg-11 sdd 6 mobilenet-64 ssd 6 densenet121-24 ssd 6 fp- dagnet 45 ems-res10 39 ems-res18 39 ems- res34 39 and spikefpn 46 . table 2 displays the comparison results with vari- ous state-of-the-art snn-based object detection methods. more specifically vgg-11 ssd and mobilenet-64 ssd achieve map values of 0.17 and 0.15 with parameter counts of 13m and 24m respectively. densenet121-24 ssd with a smaller parameter size of 8m slightly outper- forms these models with an map of 0.19. fp-dagnet and spikefpn both at 22m parameters attain an map of 0.22. the ems-res series showcases incremental improvements with ems-res10 6.2m achieving 0.27 ems-res18 9.34m at 0.29 and ems-res34 14.4m achieving the highest map among snns at 0.31. in con- 13975 page 7 trast the proposed hybrid model surpasses all these snn models with an map of 0.35 while maintaining an efficient parameter size of only 6.6m. this superiority can be at- tributed to our method s incorporation of a hybrid feature extraction approach with both spatial and temporal atten- tion modules which are lacking in other methods. comparison design with rnn and ssm-based methods although the performance of the rnn-based models gen- erally outperforms models with spiking components this comparison aims to investigate how the proposed hybrid model is comparable to the rnn. we provide comparisons with rnn-based methods such as red 33 astmnet 25 rvt-b 14 rvt-s 14 rvt-t 14 s4d-vit-b 48 s5-vit-b 48 and s5-vit-s 48 . table 3 presents a comparison of results obtained with rnn-based models. it can be seen that two of the works red 33 and astm- net 25 have substantially larger parameter counts and are therefore expected to perform better. rvt 14 demon- strates good accuracy at a parameter count comparable to the proposed hybrid network. our fully recurrent back- bone allows high-frequency detection without re-computing recurrent layers unlike rvt s non-causal attention which requires re-computation for every prediction. cnns are more efficient than mlps at small batch sizes due to higher arithmetic intensity in our method less than 5 of macs are from attention and mlps versus 67 in rvt which is harder to deploy on energy-efficient edge and neuromorphic hardware. 4.3. ablation study table 4. ablation study for asab module. models map .5 map variant 1 w o - \u03c6ta 0.57 0.33 variant 2 w o deform 0.59 0.34 variant 3 w o - esa 0.59 0.34 variant 4 w o - asab 0.53 0.30 variant 5 proposed 0.61 0.35 asab module the ablation study highlights the im- portance of each asab module component in enhancing model accuracy on the gen1 automotive detection dataset. in table 4 variant 1 excluding \u03c8ta achieves an map of 0.33 revealing reduced temporal capture. in variant 2 we replaced the deformable convolution with a standard con- volution which shows irregular sampling helps with sparse data. in variant 3 removing the ers module shows some accuracy drop indicating limited spatial flexibility and at- tention. variant 4 replacing asab with a simple accumu- lation operation results in the lowest map of 0.30. the complete model variant 5 reaches the highest map of 0.35 emphasizing the value of each component. figure 3 illustrates how the bridge module enhances detection by re- ducing false predictions. additionally we performed an ab- lation study on various dwconvlstm configurations by toggling layers. the proposed setting achieved a map of 0.43. please refer to the supplementary table for more details . a w o asab module b w asab module c ground truth figure 3. visual comparison of object detection outputs between the baseline hybrid event object detection method left and the proposed method right for the gen 4 dataset. from left to right a object detection output without the asab module b object detection output with the asab module and c ground truth ob- ject boundaries. more samples are in the supplementary docu- ment. 5. hardware implementation energy and computational efficiency analysis 5.1. hardware implementation table 5. power and time measurements of the snn block on loihi 2 for several input sizes and number of weight bits. the power is measured in watts and the execution time per step is in millisec- onds. the mean and standard deviation of the measurements aver- aged over 12 inputs for a total of 100k steps are reported. weight quant. chips power w time step int8 6 1.73 0.10 2.06 int6 6 1.71 0.11 2.06 int4 6 1.95 0.33 1.16 in order to demonstrate the suitability of the chosen hy- brid snn-ann approach for energy efficient inference on the edge we implemented the snn backbone in hardware. in the proposed architecture the snn block transforms sen- sor data into intermediate representations and therefore un- derlies the strictest latency requirements. due to the clear 13976 page 8 table 6. comparison of different baselines complexities. models map .5 macs acs baselineann 0.61 15.34e9 0.0 baselinew o \u03b2asab 0.53 1.18e9 0.97e9 proposedw \u03b2asab 0.61 1.63e9 0.97e9 proposedsnn 0.58 0.87e9 1.59e9 separation between snn and ann parts in the model s architecture the snn blocks can be implemented in spe- cialized hardware. as hardware we chose intel s loihi 2 8 a digital programmable chip based on event-based communication and computation. only minor adjustments are necessary for execution on loihi 2 the kernel-weights of the convolutional layers are quantized to int8 via a per-output-channel quantization scheme showing no result- ing loss in accuracy map 0.348 float16 vs 0.343 int8 . the batchnorm bn operations and quantization scaling are fused into the lif-neuron dynamics by scaling and shifting the inputs according to the follwing equations scale qscale weightbn \u03c4 varbn \u03b5bn 7 shift biasconv meanbn weightbn \u03c4 varbn \u03b5bn biasbn \u03c4 8 where qscale is the scaling factor introduced by the quan- tization and \u03c4 is the plif neurons time constant. given this approach spike times are almost exactly reproduced on loihi 2 compared to the pytorch int8 implementa- tion. for benchmarking purposes the inputs to the network are simulated with an additional neuron population due to the current io limitations of the chip. with this approach the spiking statistics in the input and snn layers are repro- duced. table 5 reports power and time measurement results of the 4-layer snn block running on loihi 2 for inputs of size 2 256 160 . the network runs at 1.7 0.1 w and 1.9 0.8 ms per step which is faster than real-time in the currently chosen architecture 5 ms per step . these results compare favorably to commercially available chips for edge computing like the nvidia jetson orin nano 7 w 15 w 1 and demonstrate the suitability of an snn backbone for event-based data processing. 5.2. computational analysis we trained a variant of our model where the modified plif neuron acts like a relu with proposed attention mod- this research was funded by the german federal ministry of education and research bmbf under the projects greenedge- fue 16me0521 and cluster4future 03zu1106cb . access to juwels 21 has been granted by gcs www.gauss-centre.eu under project neuroml. we thank intel for access to loihi 2. ule to investigate a comparison between a similar artifi- cial neural network ann and the hybrid network. this variant baselineann with a mean 15.34 109 multiply- accumulate operations macs makes it resource-intensive and unsuitable for energy-constrained hardware. in con- trast our proposed hybrid model computes only 1.63 109 macs making it more practical for edge devices. the \u03b2asab module does incur additional operations but leads to significant accuracy improvement compare to baselinew o \u03b2asab . additionally the spiking neural net- work snn variant proposedsnn increasing one snn layer and reducing one ann layer reduces computational demands further to 0.87 109 macs and is highly efficient on neuromorphic hardware running significantly faster and with less energy on intel s loihi 2 compared to the dense- activation anns. we analyze power consumption across methods follow- ing 2 . among snns densenet121 ssd uses 0.9 mj 0.0 macs 2.3 109 acs while vgg-11 ssd requires 4.2 mj 11.1 109 acs . in anns inception ssd is the most demanding at 19.3 mj 11.4 109 macs . events- retinanet consumes 5.4 mj 3.2 109 macs and rvt-b w o lstm requires 3.9 mj 2.3 109 macs . our method achieves 1.6 109 macs 1.0 109 acs and 3.1 mj signif- icantly reducing energy and computational costs compared to most anns. 6. conclusion in this work we introduced a hybrid attention-based snn- ann backbone for event-based visual object detection. a novel attention-based snn-ann bridge module is proposed to capture sparse spatial and temporal relations from the snn layer and convert them into dense feature maps for the ann part of the backbone. additionally we demonstrate the effectiveness of combining rnns on multiple timescales hardware-efficient snns for fast dynamics on short timescales with convlstms for longer timescales that operate on the extracted features of the bridge-module. experimental results demonstrate that our proposed method surpasses baseline hybrid and snn-based approaches by significant margins with results comparable to existing ann-based methods. the efficacy of our proposed modules and architectural choices is confirmed through extensive ablation studies. additionally we demonstrate the effectiveness of our architectural choice with separate snn and ann blocks by implementing the snn blocks on digital neuromorphic hardware intel s loihi 2. the neuromorphic hardware implementation achieves sub-real-time processing and improved power consumption compared to commercially available edge computing hardware. the achieved accuracy and hardware implementation results pave the way toward a hybrid snn-ann architecture that achieves ann-like perfor- mance at a drastically reduced parameter and power budget. 13977 page 9 references 1 nvidia jetson orin nano datasheet revision 4. https openzeka.com wp-content uploads 2023 03 jetson- orin- nano- datasheet- r4- web.pdf 2023. accessed 2025-03-09. 8 2 asude aydin mathias gehrig daniel gehrig and da- vide scaramuzza. a hybrid ann-snn architecture for low- power and low-latency visual perception. in proceedings of the ieee cvf conference on computer vision and pattern recognition workshops cvprw 2024. 2 8 3 jimmy lei ba jamie ryan kiros and geoffrey e hin- ton. layer normalization. arxiv preprint arxiv 1607.06450 2016. 3 4 francesco barchi gianvito urgese alessandro siino santa di cataldo enrico macii and andrea acquaviva. flexi- ble on-line reconfiguration of multi-core neuromorphic plat- forms. ieee transactions on emerging topics in comput- ing 9 2 915 927 2019. 1 5 nicholas fy chen. pseudo-labels for supervised learning on dynamic vision sensor data applied to object detection under ego-motion. in proceedings of the ieee conference on computer vision and pattern recognition workshops pages 644 653 2018. 1 2 6 6 lo \u0131c cordone beno\u02c6\u0131t miramond and philippe thierion. ob- ject detection with spiking neural networks on automotive event data. in 2022 international joint conference on neu- ral networks ijcnn pages 1 8. ieee 2022. 2 5 6 7 jifeng dai haozhi qi yuwen xiong yi li guodong zhang han hu and yichen wei. deformable convolutional networks. in proceedings of the ieee international confer- ence on computer vision pages 764 773 2017. 4 8 mike davies narayan srinivasa tsung-han lin gautham chinya yongqiang cao sri harsha choday georgios di- mou prasad joshi nabil imam shweta jain et al. loihi a neuromorphic manycore processor with on-chip learning. ieee micro 38 1 82 99 2018. 1 8 9 pierre de tournemire davide nitti etienne perot da- vide migliore and amos sironi. a large scale event- based detection dataset for automotive. arxiv preprint arxiv 2001.08499 2020. 2 3 5 6 10 wei fang zhaofei yu yanqi chen timoth ee masquelier tiejun huang and yonghong tian. incorporating learnable membrane time constant to enhance learning of spiking neu- ral networks. in proceedings of the ieee cvf international conference on computer vision iccv pages 2661 2671 2021. 3 11 wei fang yanqi chen jianhao ding zhaofei yu timoth ee masquelier ding chen liwei huang huihui zhou guoqi li and yonghong tian. spikingjelly an open-source ma- chine learning infrastructure platform for spike-based intel- ligence. science advances 9 40 eadi1480 2023. 5 12 guillermo gallego tobi delbr uck garrick orchard chiara bartolozzi brian taba andrea censi stefan leutenegger andrew j davison j org conradt kostas daniilidis et al. event-based vision a survey. ieee transactions on pattern analysis and machine intelligence 44 1 154 180 2020. 1 13 zheng ge songtao liu feng wang zeming li and jian sun. yolox exceeding yolo series in 2021. arxiv preprint arxiv 2107.08430 2021. 5 14 mathias gehrig and davide scaramuzza. recurrent vision transformers for object detection with event cameras. in pro- ceedings of the ieee cvf conference on computer vision and pattern recognition pages 13884 13893 2023. 1 2 3 5 6 7 15 kaiming he xiangyu zhang shaoqing ren and jian sun. deep residual learning for image recognition. in proceed- ings of the ieee conference on computer vision and pattern recognition pages 770 778 2016. 2 16 andrew g howard menglong zhu bo chen dmitry kalenichenko weijun wang tobias weyand marco an- dreetto and hartwig adam. mobilenets efficient convolu- tional neural networks for mobile vision applications. arxiv preprint arxiv 1704.04861 2017. 2 17 gao huang zhuang liu laurens van der maaten and kil- ian q weinberger. densely connected convolutional net- works. in proceedings of the ieee conference on computer vision and pattern recognition pages 4700 4708 2017. 2 18 massimiliano iacono stefan weber arren glover and chiara bartolozzi. towards event-driven object detection with off-the-shelf deep learning. in 2018 ieee rsj interna- tional conference on intelligent robots and systems iros pages 1 9. ieee 2018. 1 2 6 19 forrest n iandola song han matthew w moskewicz khalid ashraf william j dally and kurt keutzer. squeezenet alexnet-level accuracy with 50x fewer pa- rameters and 0.5 mb model size. arxiv preprint arxiv 1602.07360 2016. 2 20 sergey ioffe and christian szegedy. batch normalization accelerating deep network training by reducing internal co- variate shift. in international conference on machine learn- ing pages 448 456. pmlr 2015. 3 21 j ulich supercomputing centre. juwels cluster and booster exascale pathfinder with modular supercomputing architecture at juelich supercomputing centre. journal of large-scale research facilities 7 a138 2021. 8 22 diederik p kingma and jimmy ba. adam a method for stochastic optimization. arxiv preprint arxiv 1412.6980 2014. 5 23 alexander kugele thomas pfeil michael pfeiffer and elis- abetta chicca. hybrid snn-ann energy-efficient classifica- tion and object detection for event-based vision. in dagm german conference on pattern recognition pages 297 312. springer 2021. 2 24 chankyu lee adarsh kumar kosta alex zihao zhu ken- neth chaney kostas daniilidis and kaushik roy. spike- flownet event-based optical flow estimation with energy- efficient hybrid neural networks. in european conference on computer vision pages 366 382. springer 2020. 2 25 jianing li jia li lin zhu xijie xiang tiejun huang and yonghong tian. asynchronous spatio-temporal memory net- work for continuous event-based object detection. ieee transactions on image processing 31 2975 2987 2022. 2 6 7 13978 page 10 26 tsung-yi lin michael maire serge belongie james hays pietro perona deva ramanan piotr doll ar and c lawrence zitnick. microsoft coco common objects in context. in computer vision eccv 2014 13th european conference zurich switzerland september 6-12 2014 proceedings part v 13 pages 740 755. springer 2014. 5 27 tsung-yi lin piotr doll ar ross girshick kaiming he bharath hariharan and serge belongie. feature pyra- mid networks for object detection. in proceedings of the ieee conference on computer vision and pattern recogni- tion pages 2117 2125 2017. 2 28 faqiang liu and rong zhao. enhancing spiking neural net- works with hybrid top-down attention. frontiers in neuro- science 16 949142 2022. 2 29 wei liu dragomir anguelov dumitru erhan christian szegedy scott reed cheng-yang fu and alexander c berg. ssd single shot multibox detector. in computer vision eccv 2016 14th european conference amster- dam the netherlands october 11 14 2016 proceedings part i 14 pages 21 37. springer 2016. 2 30 nico messikommer daniel gehrig antonio loquercio and davide scaramuzza. event-based asynchronous sparse con- volutional networks. in computer vision eccv 2020 16th european conference glasgow uk august 23 28 2020 proceedings part viii 16 pages 415 431. springer 2020. 1 2 6 31 e. o. neftci h. mostafa and f. zenke. surrogate gradient learning in spiking neural networks bringing the power of gradientbased optimization to spiking neural networks. ieee signal processing magazine 36 6 5163 2019. 2 32 adam paszke sam gross francisco massa adam lerer james bradbury gregory chanan trevor killeen zeming lin natalia gimelshein luca antiga et al. pytorch an im- perative style high-performance deep learning library. ad- vances in neural information processing systems 32 2019. 5 33 etienne perot pierre de tournemire davide nitti jonathan masci and amos sironi. learning to detect objects with a 1 megapixel event camera. advances in neural information processing systems 33 16639 16652 2020. 2 3 5 6 7 34 jun sawada filipp akopyan andrew s cassidy brian taba michael v debole pallab datta rodrigo alvarez-icaza arnon amir john v arthur alexander andreopoulos et al. truenorth ecosystem for brain-inspired computing scalable systems software and applications. in sc 16 proceedings of the international conference for high performance com- puting networking storage and analysis pages 130 141. ieee 2016. 1 35 simon schaefer daniel gehrig and davide scaramuzza. aegnn asynchronous event-based graph neural networks. in proceedings of the ieee cvf conference on computer vi- sion and pattern recognition pages 12371 12381 2022. 1 2 6 36 xingjian shi zhourong chen hao wang dit-yan yeung wai-kin wong and wang-chun woo. convolutional lstm network a machine learning approach for precipitation nowcasting. advances in neural information processing sys- tems 28 2015. 1 37 karen simonyan and andrew zisserman. very deep convo- lutional networks for large-scale image recognition. arxiv preprint arxiv 1409.1556 2014. 2 38 leslie n smith and nicholay topin. super-convergence very fast training of neural networks using large learn- ing rates. in artificial intelligence and machine learning for multi-domain operations applications pages 369 386. spie 2019. 5 39 qiaoyi su yuhong chou yifan hu jianing li shijie mei ziyang zhang and guoqi li. deep directly-trained spik- ing neural networks for object detection. in proceedings of the ieee cvf international conference on computer vi- sion pages 6555 6565 2023. 2 6 40 christian szegedy wei liu yangqing jia pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke and andrew rabinovich. going deeper with convolutions. in proceedings of the ieee conference on computer vision and pattern recognition pages 1 9 2015. 2 41 ashish vaswani noam shazeer niki parmar jakob uszko- reit llion jones aidan n gomez \u0142ukasz kaiser and illia polosukhin. attention is all you need. advances in neural information processing systems 30 2017. 4 42 ziyi wu mathias gehrig qing lyu xudong liu and igor gilitschenski. leod label-efficient object detection for event cameras. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 16933 16943 2024. 2 43 zheyu yang yujie wu guanrui wang yukuan yang guoqi li lei deng jun zhu and luping shi. dashnet a hybrid artificial and spiking neural network for high-speed object tracking. arxiv preprint arxiv 1909.12942 2019. 2 44 jason yik soikat hasan ahmed zergham ahmed brian anderson andreas g andreou chiara bartolozzi arindam basu douwe den blanken petrut bogdan sander bohte et al. neurobench advancing neuromorphic computing through collaborative fair and representative benchmarking. arxiv preprint arxiv 2304.04640 2023. 1 45 hu zhang luziwei leng kaiwei che qian liu jie cheng qinghai guo jiangxing liao and ran cheng. automotive object detection via learning sparse events by temporal dynamics of spiking neurons. arxiv preprint arxiv 2307.12900 2023. 6 46 hu zhang yanchen li luziwei leng kaiwei che qian liu qinghai guo jianxing liao and ran cheng. auto- motive object detection via learning sparse events by spiking neurons. ieee transactions on cognitive and developmen- tal systems 2024. 6 47 rong zhao zheyu yang hao zheng yujie wu faqiang liu zhenzhi wu lukai li feng chen seng song jun zhu et al. a framework for the general design and computation of hybrid neural networks. nature communications 13 1 3427 2022. 2 48 nikola zubic mathias gehrig and davide scaramuzza. state space models for event cameras. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 5819 5828 2024. 2 6 7 13979",
      "keywords": [
        "page",
        "efficient",
        "event-based",
        "object",
        "detection",
        "hybrid",
        "neural",
        "network",
        "spatial",
        "temporal",
        "attention",
        "soikat",
        "hasan",
        "ahmed",
        "jan",
        "finkbeiner",
        "emre",
        "neftci",
        "forschungszentrum",
        "ulich",
        "rwth",
        "aachen",
        "university",
        "s.ahmed",
        "j.finkbeiner",
        "e.neftci",
        "fz-juelich.de",
        "abstract",
        "event",
        "cameras",
        "offer",
        "high",
        "resolution",
        "dy-",
        "namic",
        "range",
        "minimal",
        "motion",
        "blur",
        "making",
        "promising",
        "robust",
        "detection.",
        "while",
        "spiking",
        "neu-",
        "ral",
        "networks",
        "snns",
        "neuromorphic",
        "hardware",
        "often",
        "considered",
        "energy",
        "low",
        "latency",
        "data",
        "processing",
        "fall",
        "short",
        "artificial",
        "anns",
        "accuracy",
        "flexibility.",
        "here",
        "in-",
        "troduce",
        "attention-based",
        "snn-ann",
        "backbones",
        "leverage",
        "strengths",
        "both",
        "snn",
        "ann",
        "architectures.",
        "novel",
        "bridge",
        "module",
        "captures",
        "sparse",
        "tem-",
        "poral",
        "relations",
        "from",
        "layer",
        "converts",
        "into",
        "dense",
        "feature",
        "maps",
        "part",
        "backbone.",
        "ad-",
        "ditionally",
        "present",
        "variant",
        "integrates",
        "dwconvl-",
        "stms"
      ],
      "metadata": {
        "filename": "Ahmed_Efficient_Event-Based_Object_Detection_A_Hybrid_Neural_Network_with_Spatial_CVPR_2025_paper.pdf",
        "original_filename": "Ahmed_Efficient_Event-Based_Object_Detection_A_Hybrid_Neural_Network_with_Spatial_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 1154949,
        "upload_file_id": "upload_1752168963632_bctslojqy",
        "upload_timestamp": "2025-07-10T17:36:03.699651",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T17:36:03.957590+00:00",
      "content_length": 48705,
      "content_type": ".pdf",
      "filename": "Ahmed_Efficient_Event-Based_Object_Detection_A_Hybrid_Neural_Network_with_Spatial_CVPR_2025_paper.pdf"
    },
    "22": {
      "file_id": "22",
      "content": "\n=== Page 1 ===\nCLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object\nRepresentation\nReza Abbasi, Ali Nazari, Aminreza Se\ufb01d, Mohammadali Banayeeanzade,\nMohammad Hossein Rohban, Mahdieh Soleymani Baghshah\nSharif University of Technology, Tehran, Iran\n{reza.abbasi, ali.nazari02, aminreza.sefid, a.banayeean, rohban, soleymani}@sharif.edu\nAbstract\nContrastive Language-Image Pre-training (CLIP) mod-\nels excel in zero-shot classi\ufb01cation, yet face challenges in\ncomplex multi-object scenarios. This study offers a com-\nprehensive analysis of CLIP\u2019s limitations in these contexts\nusing a specialized dataset, ComCO, designed to evaluate\nCLIP\u2019s encoders in diverse multi-object scenarios.\nOur\n\ufb01ndings reveal signi\ufb01cant biases: the text encoder prior-\nitizes \ufb01rst-mentioned objects, and the image encoder fa-\nvors larger objects.\nThrough retrieval and classi\ufb01cation\ntasks, we quantify these biases across multiple CLIP vari-\nants and trace their origins to CLIP\u2019s training process, sup-\nported by analyses of the LAION dataset and training pro-\ngression. Our image-text matching experiments show sub-\nstantial performance drops when object size or token order\nchanges, underscoring CLIP\u2019s instability with rephrased\nbut semantically similar captions. Extending this to longer\ncaptions and text-to-image models like Stable Diffusion,\nwe demonstrate how prompt order in\ufb02uences object promi-\nnence in generated images. For more details and access to\nour dataset and analysis code, visit our project repository:\nhttps://clip-oscope.github.io/.\n1. Introduction\nThe convergence of vision and language in arti\ufb01cial in-\ntelligence has led to the development of Vision-Language\nModels (VLMs) that can interpret and generate multimodal\ncontent. Among these, OpenAI\u2019s Contrastive Language-\nImage Pre-training (CLIP) model [13] has been particu-\nlarly in\ufb02uential, demonstrating remarkable capabilities in\nzero-shot image classi\ufb01cation and setting new standards for\nmultimodal understanding [3, 5, 18, 20]. The success of\nCLIP has catalyzed a wide array of applications\u2014from im-\nage retrieval and visual question answering to text-to-image\ngeneration\u2014signifying a paradigm shift in how models per-\nceive and relate visual and linguistic information.\nVisual Language Models like CLIP face signi\ufb01cant\nchallenges in understanding and reasoning about complex\nscenes with multiple objects and intricate relationships.\nCLIP struggles to identify distinct objects and model their\nrelationships accurately, especially when captions contain\nthe same objects but differ in their relationships. This re-\nsults in dif\ufb01culty distinguishing between similar captions\nwith different object relationships.\nSeveral benchmark\ndatasets have been introduced to elucidate the limitations of\nexisting models in capturing subtle relational nuances. No-\ntably, Winoground [20], VL-CheckList [23], ARO [21], and\nCREPE [10] have been instrumental in evaluating models\u2019\ncapacities to accurately match images with semantically ap-\npropriate captions.\nNumerous studies have addressed compositionality chal-\nlenges in multi-object scenarios, often through end-to-end\nmethods like \ufb01ne-tuning with hard-negative samples [21] to\nimprove model performance. However, these approaches\nhave faced criticism and subsequent re\ufb01nement, as seen in\nmethods like SUGARCREPE [8] and [17], which generate\nnegative captions with minor structural changes or LLMs\nto highlight semantic distinctions.\nWhile most focus on\nCLIP\u2019s ability to distinguish structurally similar yet concep-\ntually different captions, few studies, such as Dumpala et al.\n[4], explore CLIP\u2019s performance on semantically equivalent\nbut structurally distinct captions, revealing a gap in under-\nstanding CLIP\u2019s inconsistency with such prompts.\nWhile previous studies have advanced our understanding\nof CLIP\u2019s limitations, our work uniquely focuses on CLIP\u2019s\nperformance with semantically equivalent but structurally\nvaried captions rather than simply distinguishing conceptu-\nally different captions. This shift enables a deeper exam-\nination of the model\u2019s grasp of language and visual con-\ntent, where systematic errors reveal potential biases. Un-\nlike prior works that primarily propose benchmarks or end-\nto-end solutions, we investigate the root causes of CLIP\u2019s\nbehavior, delving into the mechanisms of both image and\ntext encoders to uncover why the model displays biases\nand lacks robustness to certain linguistic and visual varia-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n9308\n\n=== Page 2 ===\nFigure 1. Overview of our key contributions. Step 1: We create ComCO dataset for controlled multi-object experiments. Step 2: We\nidentify biases in CLIP\u2019s image encoder (favoring larger objects) and text encoder (prioritizing \ufb01rst-mentioned objects). Step 3: We\ninvestigate the origin of these biases, \ufb01nding a connection to training data characteristics. Step 4: We demonstrate the practical impacts of\nthese biases on image-text matching task, showing how they affect model performance in multi-object scenarios.\ntions. To support this analysis, we introduce the ComCO\ndataset, purpose-built for examining CLIP\u2019s performance\nunder controlled multi-object scenarios. Our study spans\nmultiple versions of CLIP trained on diverse datasets and ar-\nchitectures, ensuring the broad applicability of our \ufb01ndings.\nThis comprehensive approach aims to deepen our under-\nstanding of CLIP\u2019s limitations and pave the way for more\nadaptable vision-language models. Beyond CLIP, our in-\nsights have signi\ufb01cant implications for text-to-image (T2I)\ngenerative models and multimodal large language models\n(MLLMs), where decoding CLIP\u2019s encoding intricacies can\ninform advancements in arti\ufb01cial intelligence across do-\nmains. As shown in Figure 1, our key contributions are as\nfollows:\n\u2022 Development of Novel Dataset: We introduce ComCO,\na specialized dataset for creating controlled multi-object\nscenarios.\nUnlike previous benchmarks, ComCO al-\nlows control over object size and caption order, enabling\nprecise analysis of model performance across composi-\ntional challenges and enhancing understanding of VLMs\u2019\nstrengths and weaknesses.\n\u2022 Encoder Analysis: We conduct an in-depth examination\nof CLIP\u2019s image and text encoders in multi-object scenes,\nrevealing weaknesses in preserving information for object\ndistinction and identifying where compositional informa-\ntion is lost.\n\u2022 Bias Identi\ufb01cation: Our study reveals that CLIP\u2019s im-\nage encoder prefers larger objects, while the text encoder\nfavors \ufb01rst-mentioned and visually larger objects, high-\nlighting biases in CLIP\u2019s handling of visual and linguistic\ninformation.\n\u2022 Investigation of Bias Origins: We explore the origins of\nthese biases, showing that larger objects are often men-\ntioned earlier in CLIP\u2019s training captions, and are favored\nin embeddings due to the abundance of their visual to-\nkens. We substantiate this with analyses of the LAION\ndataset and CLIP\u2019s training progression.\n\u2022 Practical Impact: We show how these biases affect per-\nformance in multi-object tasks, with signi\ufb01cant drops in\nimage-text matching accuracy in ComCO and COCO [9].\nThese biases also extend to text-to-image models, in\ufb02u-\nencing object prominence based on prompt order.\nThese \ufb01ndings reveal how biases in CLIP\u2019s text and im-\nage encoders signi\ufb01cantly reduce its performance in multi-\nobject scenarios, emphasizing the need to address these bi-\nases to enhance vision-language models\u2019 robustness. Our\nwork offers key insights into CLIP\u2019s behavior and lays\ngroundwork for improving model performance in real-\nworld applications.\n2. Methodology\n2.1. Dataset Design\nTo thoroughly evaluate the performance of CLIP models\nin multi-object scenarios under controlled conditions, we\nconstructed the ComCO (Complex COCO Objects) dataset.\nUtilizing Blender software allowed us precise control over\nthe number, location, and dimensions of objects in the im-\nages (see Appendix 7.1). The ComCO dataset comprises\n72 objects derived from the COCO dataset. We generated\n9309\n\n=== Page 3 ===\nimages containing 2, 3, 4, and 5 objects. Each image is\npaired with a speci\ufb01c caption that accurately describes the\nobjects present. This approach ensures high control over\nthe dataset and minimizes confounding factors, providing a\nrobust platform for evaluating the CLIP models.\nWe deliberately chose not to use text-to-image models\nfor generating these datasets due to two main reasons. First,\nthese models often lack the capability to produce high-\nquality, fully controlled multi-object images. Second, since\nCLIP is used in many of these models, utilizing them could\nintroduce unwanted biases into our evaluations.\n2.2. Experimental Framework for Encoder Analy-\nsis\nThe main goal of this study is to evaluate the performance of\nCLIP\u2019s text and image encoders separately in multi-object\nscenarios. We aim to analyze the impact and contribution of\neach object in the \ufb01nal output of the encoders. To achieve\nthis, we conducted experiments using our designed ComCO\ndataset, with images and captions containing two to \ufb01ve ob-\njects. To ensure the generalizability of our \ufb01ndings, we also\nvalidated our results on the widely-used COCO dataset. We\ndesigned two sets of experiments: retrieval-based experi-\nments and classi\ufb01cation-based experiments. Given the con-\nsistency of the results in both types of experiments, we have\nincluded the classi\ufb01cation results in the appendix 7.2 and\n7.4 and explain the retrieval-based experiments bellow.\n2.2.1. TEXT-BASED OBJECT RETRIEVAL (TOR)\nThe Text-based Object Retrieval task evaluates how well\nCLIP\u2019s text encoder can identify individual objects within\nmulti-object captions. As illustrated in Figure 2a, this ex-\nperiment involves several steps: First, we use CLIP\u2019s text\nencoder to create embeddings for both multi-object captions\nand single-object captions. We then measure the similar-\nity between each multi-object caption embedding and all\nsingle-object caption embeddings. The single-object cap-\ntion with the highest similarity score is considered the \u201dre-\ntrieved\u201d object. To assess performance, we calculate re-\ntrieval accuracy for each object position in the multi-object\ncaptions. This helps us identify any biases related to an\nobject\u2019s position within a caption, such as favoring objects\nmentioned \ufb01rst or last.\n2.2.2. IMAGE-BASED OBJECT RETRIEVAL (IOR)\nThe Image-based Object Retrieval task is similar to TOR\nbut focuses on CLIP\u2019s image encoder. As shown in Fig-\nure 2b, this experiment involves several steps: We begin\nby using CLIP\u2019s image encoder to generate embeddings\nfor multi-object images and single-object images. We then\ncompute similarity scores between each multi-object image\nembedding and all single-object image embeddings. The\nsingle-object image with the highest similarity score is con-\nsidered the \u201dretrieved\u201d object. To evaluate performance, we\ncalculate retrieval accuracy for different object size cate-\ngories (e.g., large, small) within the multi-object images.\nThis allows us to determine if the image encoder shows any\npreference for objects of a particular size.\nWe also experimented with a variation of ComCO, called\nSimCO, where objects were replaced with simple geometric\nshapes from the CLEVR dataset. This was done to con\ufb01rm\nthat bias persists even with non-natural, geometric objects.\nFurther details are provided in Appendix 7.1.\n3. Results and Analysis\nOur experiments revealed signi\ufb01cant biases in both the\ntext and image encoders of the CLIP model. This section\npresents our \ufb01ndings, organized by encoder type and focus-\ning on retrieval tasks.\n3.1. Text Encoder Biases\nWe observed a consistent bias in the text encoder towards\nthe \ufb01rst object mentioned in descriptions. In the TOR ex-\nperiment, the retrieval accuracy (as shown in Table 1) was\nhighest for the \ufb01rst object, indicating its dominant in\ufb02uence\non the overall text representation. This suggests that the\ntext encoder prioritizes the initial object, leading to its more\naccurate retrieval compared to subsequent objects. The de-\ntailed results for the scenarios involving 2, 3, and 5 objects\ncan be found in the appendix 7.3, and experiments on longer\ncaption templates are in Appendix 7.6 and 7.7.\n3.2. Image Encoder Biases\nIn multi-object images, the image encoder exhibited a\nstrong bias towards larger objects. The Image-based Ob-\nject Retrieval IOR experiment, detailed in Table 2, shows\nthat larger objects were more frequently and accurately re-\ntrieved during single-object image searches. This \ufb01nding\nhighlights the image encoder\u2019s bias towards larger objects,\nwhich receive disproportionate emphasis in the \ufb01nal image\nrepresentation. Further detailed results, speci\ufb01cally for sce-\nnarios with 2, 3, and 5 objects, are provided in the appendix\n7.5.\n3.3. COCO Dataset Experiments\nTo validate the generalizability of our \ufb01ndings from the\nsynthetic dataset, we conducted similar experiments on the\nCOCO dataset, which comprises real images with accom-\npanying captions. This real-world dataset allowed us to in-\nvestigate whether the previously observed biases persist in\nmore naturalistic settings.\nDue to the absence of single-object images for COCO\nobjects, we approached the IOR experiment in two ways.\nFirst, we used single-object images from the DomainNet\ndataset [11] as retrieval targets. Second, we introduced an\nalternative approach called Image-to-Text Object Retrieval\n(I2TOR). In I2TOR, we used the textual names of COCO\n9310\n\n=== Page 4 ===\n%DVH\u0003,PDJH\u001db\n7KUHH\u00032EMHFWV\u0003,PDJH\n6LQJOH\u00032EMHFW\u0003,PDJH\u0003ZKLFK\nPDWFKLQJb%DVH\u0003,PDJH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n2WKHUb6LQJOHb2EMHFW\n,PDJHV\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n\u0011\u0011\u0011\n&/,3\u00036FRUH\n\u0013\u0011\u001a\u0013\u0018\n\u0013\u0011\u001b\u0015\u0017\n\u0013\u0011\u0019\u0015\u0014\n\u0013\u0011\u0018\u0015\u001a\n\u0013\u0011\u0018\u001a\u001c\n\u0013\u0011\u0018\u0018\u0017\nE\f\nD\f\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n\u0011\u0011\u0011\n&/,3\u00036FRUH\n\u0013\u0011\u0016\u0017\u0019\n\u0013\u0011\u0019\u0017\u001a\n\u0013\u0011\u0017\u001b\u0014\n\u0013\u0011\u0015\u0014\u001c\n\u0013\u0011\u0015\u0017\u0015\n\u0013\u0011\u0015\u0016\u0019\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n%DVH\u00037H[W\u001db\n7KUHH\u00032EMHFWV\u00037H[W\n6LQJOH\u00032EMHFW\u00037H[W\u0003ZKLFK\nPDWFKLQJb%DVH\u00037H[W\n2WKHUb6LQJOHb2EMHFW\n7H[WV\nSL]]D\u0003DQGbDSSOH\nDQGbGHVN\nDSSOH\nSL]]D\nGHVN\nKDW\nFDU\nD[H\n\u0011\u0011\u0011\n\u0011\u0011\u0011\n725\n,25\nFigure 2. Experimental setup for Text-based Object Retrieval (TOR) and Image-based Object Retrieval (IOR) tasks. a) TOR: The CLIP\ntext encoder generates embeddings for multi-object and single-object texts. Cosine similarity scores are calculated between the base text\nembedding and single-object text embeddings to identify the most similar object. b) IOR: The CLIP image encoder generates embeddings\nfor multi-object and single-object images. Cosine similarity scores are calculated between the base image embedding and single-object\nimage embeddings to identify the most similar object.\nTable 1. Performance on TOR for ComCO datasets\nTask\nModel\nFirst Obj\nSecond Obj\nThird Obj\nFourth Obj\nTOR\nCLIP LAION\n63.96\n21.59\n10.68\n3.76\nCLIP Datacomp\n71.13\n16.26\n8.74\n3.87\nCLIP Roberta\n44.03\n23.73\n18.07\n14.18\nSIGLIP\n58.11\n21.16\n10.99\n9.73\nCLIP openAI\n50.31\n20.74\n14.45\n6.79\nNegCLIP\n51.63\n28.92\n14.86\n4.59\nSugarCrepe\n44.29\n30.32\n18.73\n6.66\nTable 2. Performance on IOR for ComCO datasets\nTask\nModel\nLarge Object\nSmall Obj 1\nSmall Obj 2\nSmall Obj 3\nIOR\nCLIP LAION\n85.45\n6.36\n5.45\n2.73\nCLIP Datacomp\n85.16\n5.65\n4.95\n4.24\nCLIP Roberta\n87.40\n8.66\n2.36\n1.57\nSIGLIP\n77.66\n10.11\n6.38\n5.85\nCLIP openAI\n65.22\n17.39\n8.70\n8.70\nNegCLIP\n61.67\n15.00\n13.33\n10.00\nSugarCrepe\n60.0\n18.38\n16.85\n4.7\nobjects instead of single-object images. These object names\nwere embedded using CLIP\u2019s text encoder, allowing us to\nperform a retrieval task consistent with the IOR methodol-\nogy while adapting to the constraints of the COCO dataset.\nTables 3 and 4 present the results of our COCO dataset\nexperiments. In TOR, the \ufb01rst-mentioned object in COCO\ncaptions was retrieved with higher accuracy, which aligns\nwith our earlier \ufb01ndings of bias in the text encoder. Simi-\nlarly, in IOR, larger objects in COCO images were retrieved\nmore accurately, consistent with the trends observed in our\nsynthetic dataset experiments. The I2TOR results further\ncon\ufb01rmed this bias, demonstrating that even when using\ntextual object representations, the bias towards larger ob-\nTable 3. Performance on TOR for coco dataset\nTask\nModel\nFirst Obj\nSecond Obj\nThird Obj\nFourth Obj\nTOR\nCLIP openAI\n35.24\n21.90\n20.48\n22.38\nCLIP LAION\n67.89\n13.76\n8.26\n10.09\nCLIP Datacomp\n57.68\n17.68\n12.75\n11.88\nCLIP Roberta\n40.78\n23.30\n20.39\n15.53\nSIGLIP\n49.47\n26.84\n12.11\n11.58\nNegCLIP\n38.69\n22.11\n17.09\n22.11\nTable 4. Performance on IOR for coco dataset\nTask\nModel\nLarge Object\nSmall Obj 1\nSmall Obj 2\nSmall Obj 3\nIOR\nCLIP openAI\n43.02\n28.82\n17.13\n11.03\nCLIP LAION\n39.44\n28.45\n17.70\n14.41\nCLIP Datacomp\n36.71\n29.55\n19.13\n14.61\nCLIP Roberta\n36.71\n28.61\n19.82\n14.86\nSIGLIP\n36.63\n28.29\n20.02\n15.06\nNegCLIP\n44.04\n28.86\n16.48\n10.62\nI2TOR\nCLIP openAI\n51.49\n24.87\n13.68\n9.97\nCLIP LAION\n45.50\n27.02\n15.91\n11.56\nCLIP Datacomp\n46.64\n26.82\n14.53\n12.01\nCLIP Roberta\n44.69\n26.98\n16.04\n12.29\nSIGLIP\n47.09\n27.07\n15.10\n10.74\nNegCLIP\n49.04\n27.07\n14.08\n9.81\njects persists.\nOur experiments reveal two signi\ufb01cant biases in the\nCLIP model: the text encoder shows a strong preference for\nthe \ufb01rst mentioned object in textual descriptions, while the\nimage encoder exhibits greater sensitivity to larger objects\nin images. These biases can signi\ufb01cantly impact the overall\nsystem performance in various vision-language tasks, par-\nticularly in multi-object scenarios.\n9311\n\n=== Page 5 ===\n4. Origin of Bias in CLIP Models\nIn this section, we investigate the potential origins of the\nbiases observed in CLIP models and provide evidence sup-\nporting our hypotheses.\n4.1. Bias in the Image Encoder\nThe observed bias favoring larger objects within the image\ndomain can be attributed to the architectural characteristics\nof Vision Transformers (ViT) [2] utilized in CLIP\u2019s image\nencoder. Our hypothesis is that larger objects, which occupy\na greater number of patches in the ViT\u2019s patch-based image\nrepresentation, exert a more signi\ufb01cant in\ufb02uence on the \ufb01-\nnal class (CLS) token representation. This bias is not exclu-\nsive to CLIP; it appears to be a consistent feature across ViT\nmodels, as demonstrated by our experiments detailed in the\nappendix.\nTo substantiate this hypothesis, we designed an experi-\nment to quantify the attention allocated by the CLS token\nto each image patch. By calculating the cumulative atten-\ntion received by each object from the CLS token, we could\nassess the in\ufb02uence of object size on attention allocation.\nWe applied this analysis to our three-object ComCO dataset,\nand the results are illustrated in Figure 3. The \ufb01ndings con-\n\ufb01rm our hypothesis: larger objects indeed receive more at-\ntention from the CLS token.\n4.2. Bias in the Text Encoder\nWe explore the bias present in the text encoder from two\nperspectives: the attention mechanism in the model struc-\nture and the model\u2019s training method.\n4.2.1. Impact of Attention Mechanism\nText encoder models can be categorized based on their at-\ntention mechanisms: uni-directional (causal) attention and\nbi-directional attention. In models with causal attention,\neach token attends only to preceding tokens, whereas in bi-\ndirectional models, each token attends to all tokens in the\nsequence.\nWhen OpenAI introduced the CLIP model, its text en-\ncoder employed causal attention, meaning each token could\nonly attend to tokens before it and itself.\nThis differs\nfrom typical self-attention mechanisms, where tokens at-\ntend to all other tokens. Most CLIP models use causal self-\nattention, with the exception of the variant using the XLM-\nRoberta text encoder, which also employs self-attention.\nHowever, as shown in Table 1, even this model exhibits the\nmentioned bias. This indicates that the bias does not origi-\nnate from the attention mechanism itself.\n4.2.2. Role of Training Method\nTo determine whether the observed bias is speci\ufb01c to CLIP\nmodels, we compared CLIP\u2019s text encoder with two other\nTable 5. Performance on TOC and TOR for ComCO datasets\nTask\nModel\nFirst Obj\nSecond Obj\nThird Obj\nFourth Obj\nTOR\nCLIP\n56.28\n22.71\n13.17\n7.48\nSBERT\n29.02\n19.80\n17.50\n33.57\nSimCSE [7]\n27.59\n19.07\n17.76\n34.83\nmodels designed to embed sentences into a meaningful se-\nmantic space: Sentence-BERT (SBERT) [14] and SimCSE\n[7]. The primary distinction is that CLIP\u2019s embedding space\nis shared between images and text, whereas SBERT and\nSimCSE operate solely in the text domain.\nWe conducted the TOR experiment on our dataset using\nthese models. As presented in Table 5, the bias observed in\nCLIP differs from that in the other models. This suggests\nthat CLIP\u2019s unique training method, which aligns images\nand text in a shared embedding space through contrastive\nlearning, contributes to the bias. Therefore, to uncover the\nroot cause of the bias, we focus on the speci\ufb01cs of CLIP\u2019s\ntraining procedure.\n4.3. Hypothesized Origin of Text-Side Bias in CLIP\nWe hypothesize that the text-side bias in CLIP, which fa-\nvors objects mentioned earlier in text descriptions, origi-\nnates from the image-side bias toward larger objects and is\ntransferred to the text encoder during contrastive training.\nWe present evidence supporting this hypothesis through two\nkey claims and an analysis of the training progression.\nClaim 1: Larger Objects Have More In\ufb02uence on Text\nEmbeddings.\nBuilding upon the established image-side\nbias discussed earlier, we posit that objects with larger\nphysical sizes exert more in\ufb02uence on CLIP\u2019s text em-\nbeddings due to the alignment enforced during contrastive\ntraining. To test this, we categorized objects in the Domain-\nNet dataset into large, medium, and small groups based on\ntheir relative physical sizes in real-world (with the full list of\nobjects provided in the appendix 7.10). Speci\ufb01cally, objects\nsmaller than a school bag were categorized as small, objects\nsized between a school bag and a medium-sized car were\nclassi\ufb01ed as medium, and objects larger than a car\u2014up to\nsigni\ufb01cantly larger items\u2014were considered large. We then\nconstructed two sets of sentences, each containing four ob-\njects: one set with a large object mentioned \ufb01rst followed by\nthree medium-sized objects, and another with a small object\nmentioned \ufb01rst followed by three medium-sized objects.\nFigure 4.a compares the TOR accuracy for the \ufb01rst ob-\nject in these two groups. The higher TOR accuracy for sen-\ntences beginning with large objects supports our hypothe-\nsis that larger objects, when mentioned \ufb01rst, have a more\nsigni\ufb01cant impact on the text embeddings due to the cross-\nmodal alignment with their prominent representation in im-\nages.\n9312\n\n=== Page 6 ===\na)\nb)\nFigure 3. Attention allocation from the CLS token to objects of different sizes in the ComCO dataset. a) Qualitative results showing the\nCLS token\u2019s attention to each object. b) Quantitative analysis of attention distribution across 8,000 images, with each image containing one\nlarge and two small objects. The bar chart shows the average attention allocated to the large object versus the smaller ones, demonstrating\na bias towards larger objects.\na)\nb)\nc)\nFigure 4. a) Top-1 Object Retrieval accuracy comparison for sentences where the \ufb01rst object is either large or small. The higher TOR\naccuracy for sentences beginning with large objects supports the hypothesis that larger objects, when mentioned \ufb01rst, exert a stronger\nin\ufb02uence on text embeddings due to cross-modal alignment with their prominent visual representation in images. b) Distribution of the\nposition of the largest object within image captions from the LAION datasets. The results show a consistent bias where larger objects\ntend to be mentioned earlier in text descriptions. c) Progression of TOR rates across different training stages, indicating that text-side bias\nstrengthens as the model is exposed to more data, suggesting the cumulative effect of image-side bias being transferred to the text encoder\nthrough contrastive learning.\nClaim 2: Caption Bias in Training Datasets.\nTo inves-\ntigate potential biases in CLIP\u2019s training data, we analyzed\nboth the LAION [19] and COCO datasets. Due to limited\ncomputational resources and the large size of the LAION\ndataset, which contains over 2 billion image-text pairs, we\nrandomly selected a subset of 200,000 samples for our anal-\nysis. Using the Llama3 model, we extracted objects from\nthe image captions and employed the Language Segment-\nAnything tool to generate object masks in the correspond-\ning images, calculating their areas based on these masks. A\ndetailed description of our LAION dataset analysis method-\nology can be found in Appendix 7.8.\nFigure4.b shows the position of the largest object within\neach caption. The results indicate that, in the majority of\ncases, the largest object in an image is mentioned earlier\nin its caption. The same experiment was conducted on the\nCOCO dataset, with detailed results and the distribution for\ntwo to \ufb01ve object scenarios provided in Appendix 7.9. This\ndemonstrates a consistent bias in the training data, where\nlarger objects are not only more visually prominent but are\nalso described earlier in text annotations.\nAnalysis of Bias Development During Training.\nTo fur-\nther validate our hypothesis, we examined the progression\nof text-side bias during CLIP\u2019s training. We utilized model\ncheckpoints from the LAION dataset at \ufb01ve training stages,\ncorresponding to exposure to 2, 4, 6, 8, and 10 billion sam-\nples. We conducted TOR experiments at each stage, focus-\n9313\n\n=== Page 7 ===\ning on the retrieval accuracy for the \ufb01rst object mentioned\nin text descriptions.\nFigure4.c depicts the evolution of the TOR rate across\ndifferent training stages for scenarios with varying numbers\nof objects (from 3 to 8). The consistent upward trend in\nthe TOR rate as the model is exposed to more training data\nsuggests that the text-side bias strengthens over time, likely\ndue to the cumulative effect of the image-side bias being\ntransferred to the text encoder through contrastive learning.\nIncomplete Text Representation of CLIP\nHere we want\nto theoretically highlight why the CLIP text encoder could\nlearn an incomplete representation of the text. Let z and\nw represent a latent representation of an image content de-\nscribed in the caption, and such visual content not men-\ntioned in the text, respectively. For example, z represents\nthe fact that an image contains \u201ca horse that is eating the\ngrass.\u201d In this case, w might represent other details in the\nimage, like the \u201chorse color,\u201d \u201cwhere the horse is located,\u201d\netc. We assume a data generative process as follows:\nI := g(z,w)\nT := h(z),\nwhere I is the image, and T is its corresponding caption.\nNow we want to learn a joint embedding of the image\nand text through the CLIP. Here, we assume that f\u03b8(.) and\ni\u03c9(.) as learnable functions that map the image and text into\nthe joint embedding space, respectively.\nTheorem 1 Let elements of z be independent, zero-mean,\nand unit-variance. The contrastive loss for the ideal text en-\ncoder, i\u03c9(T) = z converges to that of a non-ideal incomplete\none, i.e. i\u03c9\u2032(T) = zs, where zs is the \ufb01rst d \u2212k dimensions\nof z, with k being a constant, and d \u2192\u221e.\nProof: The contrastive loss in making this learning hap-\npen can be written as:\nEz,z\u2032,w\n\u0002\nexp(sim(z,z))\nexp(sim(z,z))+\u2211k exp(sim(z,z\u2032\nk))\n\u0003\n(1)\nwith\nsim(z,z\u2032) = S(f\u03b8(g(z,w),i\u03c9(h(z\u2032)))),\nand z and {z\u2032\nk|1 \u2264k \u2264b} are b + 1 i.i.d. samples of the\ncontent in the representation space, and S is some normal-\nized similarity metric, e.g. cosine similarity, and b+1 is the\nbatch size. We assume that elements of z are independent,\nunit-variance, and zero mean. We further assume that the\ndimensionality of z, denoted as d, goes to in\ufb01nity.\nUnder such conditions, and based on Law of Large\nNumbers, \u2225z\u2225\np\u2212\u2192\n\u221a\nd, when d is large.\nTherefore, for\nany two independent copies of z, z\u2032\nk, we have sim(z,z\u2032\nk) =\nz\u22a4z\u2032\nk/(\u2225z\u2225\u2225z\u2032\nk\u2225)\np\u2212\u21920.\nIt is evident that in the ideal case, f\u03b8(g(z,w)) = z and\nalso i\u03c9(h(z)) = z, so the contrastive loss would converge\nto e/(e + b), as the numerator is e, and the second term in\nthe denominator converges to exp(0) = 1, according to the\nMann-Wald\u2019s theorem.\nHowever, we show that other learning of this representa-\ntion could achieve the same amount of loss. For instance, let\nzs be the \ufb01rst d \u2212k elements of z, with k being a constant.\nWe show that if f\u03b8\u2032(I) = zs and i\u03c9\u2032(T) = zs, the same loss\nwould be achieved in the limit of large d. To see this, note\nthat the numerator stays the same, i.e. e, while the second\nterm in the denominator still converges to bexp(0) = b.\nThis means that even if the image and text encoder of\nthe CLIP only partially recover the content embedding, they\nreach an excellent loss. But such possible incomplete rep-\nresentations of z are combinatorially large, making conver-\ngence of the CLIP to such local minima pretty likely. This\nmakes the text encoding of CLIP be far from ideal. Fur-\nthermore, the text encoder would become biased, depend-\ning on which of such local minima it converges to. Based\non this explanation, we would expect a text encoder that has\nlearned a complete representation to exhibit such biases to a\nlesser degree. As mentioned earlier, the subject of learning\ntext representations in VLMs that are discriminative of hard\nnegatives (e.g. NegCLIP) has been around for few years.\nWe tested one of strongest such models, [8], in our bench-\nmark to validate the hypothesis that an incomplete text rep-\nresentation is one of the causes of the bias in the VLMs.\nWe noticed that this model shows lower bias based on our\nbenchmark (see the SugarCrepe model in tables 1 and 2).\nWe have developed an initial approach to address the\nidenti\ufb01ed bias in the CLIP model, which is presented in\nAppendix 7.12. While this method is speci\ufb01c to our cur-\nrent dataset, it represents a promising step toward address-\ning these challenges and can inspire further advancements.\nThis work demonstrates our commitment to exploring prac-\ntical solutions while maintaining the primary focus of this\nstudy on the analysis of bias and its implications.\n5. Practical Impacts of Encoder Biases\nThe biases observed in CLIP\u2019s image and text encoders sig-\nni\ufb01cantly impact model performance in real-world appli-\ncations. This section explores how these biases manifest in\nimage-text matching tasks, while further analyses of text-to-\nimage generation impacts are presented in Appendix 7.11.\nOur analysis in this section serves two primary purposes.\nFirst, it provides concrete evidence of how these theoretical\nbiases can translate into practical limitations. Second, it of-\nfers insights into potential areas for improvement in vision-\nlanguage models, particularly in handling complex, multi-\n9314\n\n=== Page 8 ===\nFigure 5. An example of the correct and incorrect caption structures in the \ufb01rst and second scenarios.\nobject scenarios. Through a series of carefully designed ex-\nperiments, we illustrate how the biases in both text and im-\nage encoders can lead to unexpected or suboptimal results\nin tasks that are crucial for many downstream applications.\n5.1. Image-Text Matching\nBuilding upon our \ufb01ndings of biases in CLIP\u2019s image and\ntext encoders, we now demonstrate how these biases tangi-\nbly affect the model\u2019s performance in image-caption match-\ning tasks. We designed two experimental scenarios, con-\nducted on both the ComCO and COCO datasets, to evaluate\nthese biases. The results of these experiments are summa-\nrized in Table 6. To better illustrate the differences between\nthese two scenarios, an example of the caption structures is\nshown in Figure 5. In each scenario, we created incorrect\ncaptions by switching one object in the caption with an ob-\nject that is not present in the image. Additionally, GPT-4O\n[1] was used to rewrite the captions in the COCO dataset.\nFirst Scenario\nIn the \ufb01rst scenario, biases assist the\nmodel in distinguishing between the correct and incorrect\ncaptions. In the correct captions, the largest object in the\nimage is placed at the beginning, aligning with the model\u2019s\nbias towards prioritizing \ufb01rst-mentioned objects and larger\nobjects. For the incorrect captions, the non-existent object is\ndeliberately placed at the beginning, which helps the model\nrecognize the difference between the correct and incorrect\ncaptions more effectively. This positioning emphasizes the\ndiscrepancy early on, allowing the model to better detect the\nmismatch between the caption and the image. The perfor-\nmance of different models in this scenario can be seen in\nTable 6 under the \u201dFirst Scenario\u201d column.\nSecond Scenario\nIn the second scenario, biases lead the\nmodel to make errors. The correct captions place the largest\nobject at the end of the sentence, disrupting the model\u2019s\nbias towards objects mentioned earlier and its preference\nfor larger objects. In the incorrect captions, the non-existent\nobject is placed at the end, making it more dif\ufb01cult for the\nmodel to differentiate between correct and incorrect cap-\ntions as its attention is drawn away from the critical discrep-\nancies. The performance of different models in this scenario\nis shown in Table 6 under the \u201dSecond Scenario\u201d column.\nTable 6. Performance Comparison on Image-Text Matching for\nComCO and COCO Datasets\nDataset\nModel\nFirst Scenario\nSecond Scenario\nComCO\nCLIP Datacomp [6]\n99.99\n67.50\nCLIP Roberta\n99.98\n64.75\nSIGLIP [22]\n99.49\n72.36\nCLIP openAI\n99.59\n52.23\nNegCLIP\n96.82\n46.94\nSugarCrepe\n98.55\n60.43\nCOCO\nCLIP Datacomp [6]\n71.2\n54.2\nCLIP Roberta\n72.2\n54.1\nSIGLIP [22]\n64.8\n39.5\nCLIP openAI\n63.5\n26.4\nNegCLIP\n72\n28.7\nSugarCrepe\n80.0\n40.9\nBy comparing these two scenarios, we demonstrate that\nbiases in CLIP can either help or hinder the model\u2019s perfor-\nmance depending on how captions are structured. The ex-\nperimental results, particularly with the use of GPT-4O for\ncaption rephrasing in the COCO dataset, reveal how such\nbiases can in\ufb02uence the accuracy of image-text matching\ntasks. These biases must be addressed to improve CLIP\u2019s\nrobustness in real-world multi-object scenarios.\nFor further insights on how these biases affect text-to-\nimage generation, refer to our extended experiments in Ap-\npendix 7.11.\n6. Conclusion\nThis study uncovers biases in CLIP\u2019s encoders, with the\ntext encoder favoring \ufb01rst-mentioned objects and the im-\nage encoder emphasizing larger ones, which impacts per-\nformance in multi-object tasks. Using the ComCO dataset,\nwe highlighted these biases\u2019 effects on object representation\nand positioning, underscoring the need for balanced train-\ning. We attribute these biases to CLIP\u2019s contrastive frame-\nwork, where alignment issues propagate across modalities.\nAddressing these biases is essential for vision-language ad-\nvancements, as seen with models like Stable Diffusion.\n9315\n\n=== Page 9 ===\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 8\n[2] Dosovitskiy Alexey. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint arXiv:\n2010.11929, 2020. 5\n[3] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev.\nReproducible\nscaling laws for contrastive language-image learning.\nIn\n2023 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR). IEEE, 2023. 1\n[4] Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sas-\ntry, Evangelos Milios, Sageev Oore, and Hassan Saj-\njad.\nSugarcrepe++ dataset: Vision-language model sensi-\ntivity to semantic and lexical alterations.\narXiv preprint\narXiv:2406.11171, 2024. 1\n[5] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Or-\ngad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek\nRamanujan, Yonatan Bitton, Kalyani Marathe, Stephen\nMussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna,\nPang Wei Koh, Olga Saukh, Alexander Ratner, Shuran\nSong, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,\nSewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon,\nVaishaal Shankar, and Ludwig Schmidt.\nDatacomp: In\nsearch of the next generation of multimodal datasets, 2023.\n1\n[6] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-\nacomp:\nIn search of the next generation of multimodal\ndatasets. Advances in Neural Information Processing Sys-\ntems, 36, 2024. 8, 17\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen.\nSimcse:\nSimple contrastive learning of sentence embeddings. arXiv\npreprint arXiv:2104.08821, 2021. 5\n[8] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kem-\nbhavi, and Ranjay Krishna.\nSugarcrepe: Fixing hackable\nbenchmarks for vision-language compositionality. Advances\nin neural information processing systems, 36, 2024. 1, 7\n[9] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona, Deva\nRamanan, C. Lawrence Zitnick, and Piotr Doll\u00b4ar. Microsoft\ncoco: Common objects in context, 2015. 2\n[10] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi,\nIrena Gao, and Ranjay Krishna. Crepe: Can vision-language\nfoundation models reason compositionally? In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 10910\u201310921, 2023. 1\n[11] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\nSaenko, and Bo Wang. Moment matching for multi-source\ndomain adaptation. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 1406\u20131415,\n2019. 3\n[12] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 17\n[13] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision, 2021. 1\n[14] N Reimers.\nSentence-bert:\nSentence embeddings using\nsiamese bert-networks.\narXiv preprint arXiv:1908.10084,\n2019. 5\n[15] Dillon Reis, Jordan Kupec, Jacqueline Hong, and Ahmad\nDaoudi. Real-time \ufb02ying object detection with yolov8. arXiv\npreprint arXiv:2305.09972, 2023. 17\n[16] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, 2022. 17\n[17] Ugur Sahin, Hang Li, Qadeer Khan, Daniel Cremers, and\nVolker Tresp. Enhancing multimodal compositional reason-\ning of visual language models with generative negative min-\ning. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pages 5563\u20135573, 2024. 1\n[18] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-\ufb01ltered 400 million image-text pairs,\n2021. 1\n[19] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 6\n[20] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace Ross.\nWinoground: Probing vision and language models for visio-\nlinguistic compositionality. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5238\u20135248, 2022. 1\n[21] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,\nDan Jurafsky, and James Zou.\nWhen and why vision-\nlanguage models behave like bags-of-words, and what to\ndo about it?\nIn The Eleventh International Conference on\nLearning Representations, 2023. 1\n[22] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 11975\u201311986, 2023. 8, 17\n[23] Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan\nShen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin.\nVl-\nchecklist:\nEvaluating pre-trained vision-language models\n9316\n\n=== Page 10 ===\nwith objects, attributes and relations.\narXiv preprint\narXiv:2207.00221, 2022. 1\n9317\n",
      "searchable_content": "page 1 clip under the microscope a fine-grained analysis of multi-object representation reza abbasi ali nazari aminreza se\ufb01d mohammadali banayeeanzade mohammad hossein rohban mahdieh soleymani baghshah sharif university of technology tehran iran reza.abbasi ali.nazari02 aminreza.sefid a.banayeean rohban soleymani sharif.edu abstract contrastive language-image pre-training clip mod- els excel in zero-shot classi\ufb01cation yet face challenges in complex multi-object scenarios. this study offers a com- prehensive analysis of clip s limitations in these contexts using a specialized dataset comco designed to evaluate clip s encoders in diverse multi-object scenarios. our \ufb01ndings reveal signi\ufb01cant biases the text encoder prior- itizes \ufb01rst-mentioned objects and the image encoder fa- vors larger objects. through retrieval and classi\ufb01cation tasks we quantify these biases across multiple clip vari- ants and trace their origins to clip s training process sup- ported by analyses of the laion dataset and training pro- gression. our image-text matching experiments show sub- stantial performance drops when object size or token order changes underscoring clip s instability with rephrased but semantically similar captions. extending this to longer captions and text-to-image models like stable diffusion we demonstrate how prompt order in\ufb02uences object promi- nence in generated images. for more details and access to our dataset and analysis code visit our project repository https clip-oscope.github.io . 1. introduction the convergence of vision and language in arti\ufb01cial in- telligence has led to the development of vision-language models vlms that can interpret and generate multimodal content. among these openai s contrastive language- image pre-training clip model 13 has been particu- larly in\ufb02uential demonstrating remarkable capabilities in zero-shot image classi\ufb01cation and setting new standards for multimodal understanding 3 5 18 20 . the success of clip has catalyzed a wide array of applications from im- age retrieval and visual question answering to text-to-image generation signifying a paradigm shift in how models per- ceive and relate visual and linguistic information. visual language models like clip face signi\ufb01cant challenges in understanding and reasoning about complex scenes with multiple objects and intricate relationships. clip struggles to identify distinct objects and model their relationships accurately especially when captions contain the same objects but differ in their relationships. this re- sults in dif\ufb01culty distinguishing between similar captions with different object relationships. several benchmark datasets have been introduced to elucidate the limitations of existing models in capturing subtle relational nuances. no- tably winoground 20 vl-checklist 23 aro 21 and crepe 10 have been instrumental in evaluating models capacities to accurately match images with semantically ap- propriate captions. numerous studies have addressed compositionality chal- lenges in multi-object scenarios often through end-to-end methods like \ufb01ne-tuning with hard-negative samples 21 to improve model performance. however these approaches have faced criticism and subsequent re\ufb01nement as seen in methods like sugarcrepe 8 and 17 which generate negative captions with minor structural changes or llms to highlight semantic distinctions. while most focus on clip s ability to distinguish structurally similar yet concep- tually different captions few studies such as dumpala et al. 4 explore clip s performance on semantically equivalent but structurally distinct captions revealing a gap in under- standing clip s inconsistency with such prompts. while previous studies have advanced our understanding of clip s limitations our work uniquely focuses on clip s performance with semantically equivalent but structurally varied captions rather than simply distinguishing conceptu- ally different captions. this shift enables a deeper exam- ination of the model s grasp of language and visual con- tent where systematic errors reveal potential biases. un- like prior works that primarily propose benchmarks or end- to-end solutions we investigate the root causes of clip s behavior delving into the mechanisms of both image and text encoders to uncover why the model displays biases and lacks robustness to certain linguistic and visual varia- this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 9308 page 2 figure 1. overview of our key contributions. step 1 we create comco dataset for controlled multi-object experiments. step 2 we identify biases in clip s image encoder favoring larger objects and text encoder prioritizing \ufb01rst-mentioned objects . step 3 we investigate the origin of these biases \ufb01nding a connection to training data characteristics. step 4 we demonstrate the practical impacts of these biases on image-text matching task showing how they affect model performance in multi-object scenarios. tions. to support this analysis we introduce the comco dataset purpose-built for examining clip s performance under controlled multi-object scenarios. our study spans multiple versions of clip trained on diverse datasets and ar- chitectures ensuring the broad applicability of our \ufb01ndings. this comprehensive approach aims to deepen our under- standing of clip s limitations and pave the way for more adaptable vision-language models. beyond clip our in- sights have signi\ufb01cant implications for text-to-image t2i generative models and multimodal large language models mllms where decoding clip s encoding intricacies can inform advancements in arti\ufb01cial intelligence across do- mains. as shown in figure 1 our key contributions are as follows development of novel dataset we introduce comco a specialized dataset for creating controlled multi-object scenarios. unlike previous benchmarks comco al- lows control over object size and caption order enabling precise analysis of model performance across composi- tional challenges and enhancing understanding of vlms strengths and weaknesses. encoder analysis we conduct an in-depth examination of clip s image and text encoders in multi-object scenes revealing weaknesses in preserving information for object distinction and identifying where compositional informa- tion is lost. bias identi\ufb01cation our study reveals that clip s im- age encoder prefers larger objects while the text encoder favors \ufb01rst-mentioned and visually larger objects high- lighting biases in clip s handling of visual and linguistic information. investigation of bias origins we explore the origins of these biases showing that larger objects are often men- tioned earlier in clip s training captions and are favored in embeddings due to the abundance of their visual to- kens. we substantiate this with analyses of the laion dataset and clip s training progression. practical impact we show how these biases affect per- formance in multi-object tasks with signi\ufb01cant drops in image-text matching accuracy in comco and coco 9 . these biases also extend to text-to-image models in\ufb02u- encing object prominence based on prompt order. these \ufb01ndings reveal how biases in clip s text and im- age encoders signi\ufb01cantly reduce its performance in multi- object scenarios emphasizing the need to address these bi- ases to enhance vision-language models robustness. our work offers key insights into clip s behavior and lays groundwork for improving model performance in real- world applications. 2. methodology 2.1. dataset design to thoroughly evaluate the performance of clip models in multi-object scenarios under controlled conditions we constructed the comco complex coco objects dataset. utilizing blender software allowed us precise control over the number location and dimensions of objects in the im- ages see appendix 7.1 . the comco dataset comprises 72 objects derived from the coco dataset. we generated 9309 page 3 images containing 2 3 4 and 5 objects. each image is paired with a speci\ufb01c caption that accurately describes the objects present. this approach ensures high control over the dataset and minimizes confounding factors providing a robust platform for evaluating the clip models. we deliberately chose not to use text-to-image models for generating these datasets due to two main reasons. first these models often lack the capability to produce high- quality fully controlled multi-object images. second since clip is used in many of these models utilizing them could introduce unwanted biases into our evaluations. 2.2. experimental framework for encoder analy- sis the main goal of this study is to evaluate the performance of clip s text and image encoders separately in multi-object scenarios. we aim to analyze the impact and contribution of each object in the \ufb01nal output of the encoders. to achieve this we conducted experiments using our designed comco dataset with images and captions containing two to \ufb01ve ob- jects. to ensure the generalizability of our \ufb01ndings we also validated our results on the widely-used coco dataset. we designed two sets of experiments retrieval-based experi- ments and classi\ufb01cation-based experiments. given the con- sistency of the results in both types of experiments we have included the classi\ufb01cation results in the appendix 7.2 and 7.4 and explain the retrieval-based experiments bellow. 2.2.1. text-based object retrieval tor the text-based object retrieval task evaluates how well clip s text encoder can identify individual objects within multi-object captions. as illustrated in figure 2a this ex- periment involves several steps first we use clip s text encoder to create embeddings for both multi-object captions and single-object captions. we then measure the similar- ity between each multi-object caption embedding and all single-object caption embeddings. the single-object cap- tion with the highest similarity score is considered the re- trieved object. to assess performance we calculate re- trieval accuracy for each object position in the multi-object captions. this helps us identify any biases related to an object s position within a caption such as favoring objects mentioned \ufb01rst or last. 2.2.2. image-based object retrieval ior the image-based object retrieval task is similar to tor but focuses on clip s image encoder. as shown in fig- ure 2b this experiment involves several steps we begin by using clip s image encoder to generate embeddings for multi-object images and single-object images. we then compute similarity scores between each multi-object image embedding and all single-object image embeddings. the single-object image with the highest similarity score is con- sidered the retrieved object. to evaluate performance we calculate retrieval accuracy for different object size cate- gories e.g. large small within the multi-object images. this allows us to determine if the image encoder shows any preference for objects of a particular size. we also experimented with a variation of comco called simco where objects were replaced with simple geometric shapes from the clevr dataset. this was done to con\ufb01rm that bias persists even with non-natural geometric objects. further details are provided in appendix 7.1. 3. results and analysis our experiments revealed signi\ufb01cant biases in both the text and image encoders of the clip model. this section presents our \ufb01ndings organized by encoder type and focus- ing on retrieval tasks. 3.1. text encoder biases we observed a consistent bias in the text encoder towards the \ufb01rst object mentioned in descriptions. in the tor ex- periment the retrieval accuracy as shown in table 1 was highest for the \ufb01rst object indicating its dominant in\ufb02uence on the overall text representation. this suggests that the text encoder prioritizes the initial object leading to its more accurate retrieval compared to subsequent objects. the de- tailed results for the scenarios involving 2 3 and 5 objects can be found in the appendix 7.3 and experiments on longer caption templates are in appendix 7.6 and 7.7. 3.2. image encoder biases in multi-object images the image encoder exhibited a strong bias towards larger objects. the image-based ob- ject retrieval ior experiment detailed in table 2 shows that larger objects were more frequently and accurately re- trieved during single-object image searches. this \ufb01nding highlights the image encoder s bias towards larger objects which receive disproportionate emphasis in the \ufb01nal image representation. further detailed results speci\ufb01cally for sce- narios with 2 3 and 5 objects are provided in the appendix 7.5. 3.3. coco dataset experiments to validate the generalizability of our \ufb01ndings from the synthetic dataset we conducted similar experiments on the coco dataset which comprises real images with accom- panying captions. this real-world dataset allowed us to in- vestigate whether the previously observed biases persist in more naturalistic settings. due to the absence of single-object images for coco objects we approached the ior experiment in two ways. first we used single-object images from the domainnet dataset 11 as retrieval targets. second we introduced an alternative approach called image-to-text object retrieval i2tor . in i2tor we used the textual names of coco 9310 page 4 dvh pdjh b 7kuhh 2emhfwv pdjh 6lqjoh 2emhfw pdjh zklfk pdwfklqjb dvh pdjh 3 6fruh 3 6fruh 3 6fruh 2wkhub6lqjohb2emhfw pdjhv 3 6fruh 3 6fruh 3 6fruh e d 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3 6fruh 3 6fruh 3 6fruh 3 6fruh 3 6fruh 3 6fruh 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu dvh 7h w b 7kuhh 2emhfwv 7h w 6lqjoh 2emhfw 7h w zklfk pdwfklqjb dvh 7h w 2wkhub6lqjohb2emhfw 7h wv sl d dqgbdssoh dqgbghvn dssoh sl d ghvn kdw fdu d h 725 25 figure 2. experimental setup for text-based object retrieval tor and image-based object retrieval ior tasks. a tor the clip text encoder generates embeddings for multi-object and single-object texts. cosine similarity scores are calculated between the base text embedding and single-object text embeddings to identify the most similar object. b ior the clip image encoder generates embeddings for multi-object and single-object images. cosine similarity scores are calculated between the base image embedding and single-object image embeddings to identify the most similar object. table 1. performance on tor for comco datasets task model first obj second obj third obj fourth obj tor clip laion 63.96 21.59 10.68 3.76 clip datacomp 71.13 16.26 8.74 3.87 clip roberta 44.03 23.73 18.07 14.18 siglip 58.11 21.16 10.99 9.73 clip openai 50.31 20.74 14.45 6.79 negclip 51.63 28.92 14.86 4.59 sugarcrepe 44.29 30.32 18.73 6.66 table 2. performance on ior for comco datasets task model large object small obj 1 small obj 2 small obj 3 ior clip laion 85.45 6.36 5.45 2.73 clip datacomp 85.16 5.65 4.95 4.24 clip roberta 87.40 8.66 2.36 1.57 siglip 77.66 10.11 6.38 5.85 clip openai 65.22 17.39 8.70 8.70 negclip 61.67 15.00 13.33 10.00 sugarcrepe 60.0 18.38 16.85 4.7 objects instead of single-object images. these object names were embedded using clip s text encoder allowing us to perform a retrieval task consistent with the ior methodol- ogy while adapting to the constraints of the coco dataset. tables 3 and 4 present the results of our coco dataset experiments. in tor the \ufb01rst-mentioned object in coco captions was retrieved with higher accuracy which aligns with our earlier \ufb01ndings of bias in the text encoder. simi- larly in ior larger objects in coco images were retrieved more accurately consistent with the trends observed in our synthetic dataset experiments. the i2tor results further con\ufb01rmed this bias demonstrating that even when using textual object representations the bias towards larger ob- table 3. performance on tor for coco dataset task model first obj second obj third obj fourth obj tor clip openai 35.24 21.90 20.48 22.38 clip laion 67.89 13.76 8.26 10.09 clip datacomp 57.68 17.68 12.75 11.88 clip roberta 40.78 23.30 20.39 15.53 siglip 49.47 26.84 12.11 11.58 negclip 38.69 22.11 17.09 22.11 table 4. performance on ior for coco dataset task model large object small obj 1 small obj 2 small obj 3 ior clip openai 43.02 28.82 17.13 11.03 clip laion 39.44 28.45 17.70 14.41 clip datacomp 36.71 29.55 19.13 14.61 clip roberta 36.71 28.61 19.82 14.86 siglip 36.63 28.29 20.02 15.06 negclip 44.04 28.86 16.48 10.62 i2tor clip openai 51.49 24.87 13.68 9.97 clip laion 45.50 27.02 15.91 11.56 clip datacomp 46.64 26.82 14.53 12.01 clip roberta 44.69 26.98 16.04 12.29 siglip 47.09 27.07 15.10 10.74 negclip 49.04 27.07 14.08 9.81 jects persists. our experiments reveal two signi\ufb01cant biases in the clip model the text encoder shows a strong preference for the \ufb01rst mentioned object in textual descriptions while the image encoder exhibits greater sensitivity to larger objects in images. these biases can signi\ufb01cantly impact the overall system performance in various vision-language tasks par- ticularly in multi-object scenarios. 9311 page 5 4. origin of bias in clip models in this section we investigate the potential origins of the biases observed in clip models and provide evidence sup- porting our hypotheses. 4.1. bias in the image encoder the observed bias favoring larger objects within the image domain can be attributed to the architectural characteristics of vision transformers vit 2 utilized in clip s image encoder. our hypothesis is that larger objects which occupy a greater number of patches in the vit s patch-based image representation exert a more signi\ufb01cant in\ufb02uence on the \ufb01- nal class cls token representation. this bias is not exclu- sive to clip it appears to be a consistent feature across vit models as demonstrated by our experiments detailed in the appendix. to substantiate this hypothesis we designed an experi- ment to quantify the attention allocated by the cls token to each image patch. by calculating the cumulative atten- tion received by each object from the cls token we could assess the in\ufb02uence of object size on attention allocation. we applied this analysis to our three-object comco dataset and the results are illustrated in figure 3. the \ufb01ndings con- \ufb01rm our hypothesis larger objects indeed receive more at- tention from the cls token. 4.2. bias in the text encoder we explore the bias present in the text encoder from two perspectives the attention mechanism in the model struc- ture and the model s training method. 4.2.1. impact of attention mechanism text encoder models can be categorized based on their at- tention mechanisms uni-directional causal attention and bi-directional attention. in models with causal attention each token attends only to preceding tokens whereas in bi- directional models each token attends to all tokens in the sequence. when openai introduced the clip model its text en- coder employed causal attention meaning each token could only attend to tokens before it and itself. this differs from typical self-attention mechanisms where tokens at- tend to all other tokens. most clip models use causal self- attention with the exception of the variant using the xlm- roberta text encoder which also employs self-attention. however as shown in table 1 even this model exhibits the mentioned bias. this indicates that the bias does not origi- nate from the attention mechanism itself. 4.2.2. role of training method to determine whether the observed bias is speci\ufb01c to clip models we compared clip s text encoder with two other table 5. performance on toc and tor for comco datasets task model first obj second obj third obj fourth obj tor clip 56.28 22.71 13.17 7.48 sbert 29.02 19.80 17.50 33.57 simcse 7 27.59 19.07 17.76 34.83 models designed to embed sentences into a meaningful se- mantic space sentence-bert sbert 14 and simcse 7 . the primary distinction is that clip s embedding space is shared between images and text whereas sbert and simcse operate solely in the text domain. we conducted the tor experiment on our dataset using these models. as presented in table 5 the bias observed in clip differs from that in the other models. this suggests that clip s unique training method which aligns images and text in a shared embedding space through contrastive learning contributes to the bias. therefore to uncover the root cause of the bias we focus on the speci\ufb01cs of clip s training procedure. 4.3. hypothesized origin of text-side bias in clip we hypothesize that the text-side bias in clip which fa- vors objects mentioned earlier in text descriptions origi- nates from the image-side bias toward larger objects and is transferred to the text encoder during contrastive training. we present evidence supporting this hypothesis through two key claims and an analysis of the training progression. claim 1 larger objects have more in\ufb02uence on text embeddings. building upon the established image-side bias discussed earlier we posit that objects with larger physical sizes exert more in\ufb02uence on clip s text em- beddings due to the alignment enforced during contrastive training. to test this we categorized objects in the domain- net dataset into large medium and small groups based on their relative physical sizes in real-world with the full list of objects provided in the appendix 7.10 . speci\ufb01cally objects smaller than a school bag were categorized as small objects sized between a school bag and a medium-sized car were classi\ufb01ed as medium and objects larger than a car up to signi\ufb01cantly larger items were considered large. we then constructed two sets of sentences each containing four ob- jects one set with a large object mentioned \ufb01rst followed by three medium-sized objects and another with a small object mentioned \ufb01rst followed by three medium-sized objects. figure 4.a compares the tor accuracy for the \ufb01rst ob- ject in these two groups. the higher tor accuracy for sen- tences beginning with large objects supports our hypothe- sis that larger objects when mentioned \ufb01rst have a more signi\ufb01cant impact on the text embeddings due to the cross- modal alignment with their prominent representation in im- ages. 9312 page 6 a b figure 3. attention allocation from the cls token to objects of different sizes in the comco dataset. a qualitative results showing the cls token s attention to each object. b quantitative analysis of attention distribution across 8 000 images with each image containing one large and two small objects. the bar chart shows the average attention allocated to the large object versus the smaller ones demonstrating a bias towards larger objects. a b c figure 4. a top-1 object retrieval accuracy comparison for sentences where the \ufb01rst object is either large or small. the higher tor accuracy for sentences beginning with large objects supports the hypothesis that larger objects when mentioned \ufb01rst exert a stronger in\ufb02uence on text embeddings due to cross-modal alignment with their prominent visual representation in images. b distribution of the position of the largest object within image captions from the laion datasets. the results show a consistent bias where larger objects tend to be mentioned earlier in text descriptions. c progression of tor rates across different training stages indicating that text-side bias strengthens as the model is exposed to more data suggesting the cumulative effect of image-side bias being transferred to the text encoder through contrastive learning. claim 2 caption bias in training datasets. to inves- tigate potential biases in clip s training data we analyzed both the laion 19 and coco datasets. due to limited computational resources and the large size of the laion dataset which contains over 2 billion image-text pairs we randomly selected a subset of 200 000 samples for our anal- ysis. using the llama3 model we extracted objects from the image captions and employed the language segment- anything tool to generate object masks in the correspond- ing images calculating their areas based on these masks. a detailed description of our laion dataset analysis method- ology can be found in appendix 7.8. figure4.b shows the position of the largest object within each caption. the results indicate that in the majority of cases the largest object in an image is mentioned earlier in its caption. the same experiment was conducted on the coco dataset with detailed results and the distribution for two to \ufb01ve object scenarios provided in appendix 7.9. this demonstrates a consistent bias in the training data where larger objects are not only more visually prominent but are also described earlier in text annotations. analysis of bias development during training. to fur- ther validate our hypothesis we examined the progression of text-side bias during clip s training. we utilized model checkpoints from the laion dataset at \ufb01ve training stages corresponding to exposure to 2 4 6 8 and 10 billion sam- ples. we conducted tor experiments at each stage focus- 9313 page 7 ing on the retrieval accuracy for the \ufb01rst object mentioned in text descriptions. figure4.c depicts the evolution of the tor rate across different training stages for scenarios with varying numbers of objects from 3 to 8 . the consistent upward trend in the tor rate as the model is exposed to more training data suggests that the text-side bias strengthens over time likely due to the cumulative effect of the image-side bias being transferred to the text encoder through contrastive learning. incomplete text representation of clip here we want to theoretically highlight why the clip text encoder could learn an incomplete representation of the text. let z and w represent a latent representation of an image content de- scribed in the caption and such visual content not men- tioned in the text respectively. for example z represents the fact that an image contains a horse that is eating the grass. in this case w might represent other details in the image like the horse color where the horse is located etc. we assume a data generative process as follows i g z w t h z where i is the image and t is its corresponding caption. now we want to learn a joint embedding of the image and text through the clip. here we assume that f\u03b8 . and i\u03c9 . as learnable functions that map the image and text into the joint embedding space respectively. theorem 1 let elements of z be independent zero-mean and unit-variance. the contrastive loss for the ideal text en- coder i\u03c9 t z converges to that of a non-ideal incomplete one i.e. i\u03c9 t zs where zs is the \ufb01rst d k dimensions of z with k being a constant and d . proof the contrastive loss in making this learning hap- pen can be written as ez z w exp sim z z exp sim z z k exp sim z z k 1 with sim z z s f\u03b8 g z w i\u03c9 h z and z and z k 1 k b are b 1 i.i.d. samples of the content in the representation space and s is some normal- ized similarity metric e.g. cosine similarity and b 1 is the batch size. we assume that elements of z are independent unit-variance and zero mean. we further assume that the dimensionality of z denoted as d goes to in\ufb01nity. under such conditions and based on law of large numbers z p d when d is large. therefore for any two independent copies of z z k we have sim z z k z z k z z k p 0. it is evident that in the ideal case f\u03b8 g z w z and also i\u03c9 h z z so the contrastive loss would converge to e e b as the numerator is e and the second term in the denominator converges to exp 0 1 according to the mann-wald s theorem. however we show that other learning of this representa- tion could achieve the same amount of loss. for instance let zs be the \ufb01rst d k elements of z with k being a constant. we show that if f\u03b8 i zs and i\u03c9 t zs the same loss would be achieved in the limit of large d. to see this note that the numerator stays the same i.e. e while the second term in the denominator still converges to bexp 0 b. this means that even if the image and text encoder of the clip only partially recover the content embedding they reach an excellent loss. but such possible incomplete rep- resentations of z are combinatorially large making conver- gence of the clip to such local minima pretty likely. this makes the text encoding of clip be far from ideal. fur- thermore the text encoder would become biased depend- ing on which of such local minima it converges to. based on this explanation we would expect a text encoder that has learned a complete representation to exhibit such biases to a lesser degree. as mentioned earlier the subject of learning text representations in vlms that are discriminative of hard negatives e.g. negclip has been around for few years. we tested one of strongest such models 8 in our bench- mark to validate the hypothesis that an incomplete text rep- resentation is one of the causes of the bias in the vlms. we noticed that this model shows lower bias based on our benchmark see the sugarcrepe model in tables 1 and 2 . we have developed an initial approach to address the identi\ufb01ed bias in the clip model which is presented in appendix 7.12. while this method is speci\ufb01c to our cur- rent dataset it represents a promising step toward address- ing these challenges and can inspire further advancements. this work demonstrates our commitment to exploring prac- tical solutions while maintaining the primary focus of this study on the analysis of bias and its implications. 5. practical impacts of encoder biases the biases observed in clip s image and text encoders sig- ni\ufb01cantly impact model performance in real-world appli- cations. this section explores how these biases manifest in image-text matching tasks while further analyses of text-to- image generation impacts are presented in appendix 7.11. our analysis in this section serves two primary purposes. first it provides concrete evidence of how these theoretical biases can translate into practical limitations. second it of- fers insights into potential areas for improvement in vision- language models particularly in handling complex multi- 9314 page 8 figure 5. an example of the correct and incorrect caption structures in the \ufb01rst and second scenarios. object scenarios. through a series of carefully designed ex- periments we illustrate how the biases in both text and im- age encoders can lead to unexpected or suboptimal results in tasks that are crucial for many downstream applications. 5.1. image-text matching building upon our \ufb01ndings of biases in clip s image and text encoders we now demonstrate how these biases tangi- bly affect the model s performance in image-caption match- ing tasks. we designed two experimental scenarios con- ducted on both the comco and coco datasets to evaluate these biases. the results of these experiments are summa- rized in table 6. to better illustrate the differences between these two scenarios an example of the caption structures is shown in figure 5. in each scenario we created incorrect captions by switching one object in the caption with an ob- ject that is not present in the image. additionally gpt-4o 1 was used to rewrite the captions in the coco dataset. first scenario in the \ufb01rst scenario biases assist the model in distinguishing between the correct and incorrect captions. in the correct captions the largest object in the image is placed at the beginning aligning with the model s bias towards prioritizing \ufb01rst-mentioned objects and larger objects. for the incorrect captions the non-existent object is deliberately placed at the beginning which helps the model recognize the difference between the correct and incorrect captions more effectively. this positioning emphasizes the discrepancy early on allowing the model to better detect the mismatch between the caption and the image. the perfor- mance of different models in this scenario can be seen in table 6 under the first scenario column. second scenario in the second scenario biases lead the model to make errors. the correct captions place the largest object at the end of the sentence disrupting the model s bias towards objects mentioned earlier and its preference for larger objects. in the incorrect captions the non-existent object is placed at the end making it more dif\ufb01cult for the model to differentiate between correct and incorrect cap- tions as its attention is drawn away from the critical discrep- ancies. the performance of different models in this scenario is shown in table 6 under the second scenario column. table 6. performance comparison on image-text matching for comco and coco datasets dataset model first scenario second scenario comco clip datacomp 6 99.99 67.50 clip roberta 99.98 64.75 siglip 22 99.49 72.36 clip openai 99.59 52.23 negclip 96.82 46.94 sugarcrepe 98.55 60.43 coco clip datacomp 6 71.2 54.2 clip roberta 72.2 54.1 siglip 22 64.8 39.5 clip openai 63.5 26.4 negclip 72 28.7 sugarcrepe 80.0 40.9 by comparing these two scenarios we demonstrate that biases in clip can either help or hinder the model s perfor- mance depending on how captions are structured. the ex- perimental results particularly with the use of gpt-4o for caption rephrasing in the coco dataset reveal how such biases can in\ufb02uence the accuracy of image-text matching tasks. these biases must be addressed to improve clip s robustness in real-world multi-object scenarios. for further insights on how these biases affect text-to- image generation refer to our extended experiments in ap- pendix 7.11. 6. conclusion this study uncovers biases in clip s encoders with the text encoder favoring \ufb01rst-mentioned objects and the im- age encoder emphasizing larger ones which impacts per- formance in multi-object tasks. using the comco dataset we highlighted these biases effects on object representation and positioning underscoring the need for balanced train- ing. we attribute these biases to clip s contrastive frame- work where alignment issues propagate across modalities. addressing these biases is essential for vision-language ad- vancements as seen with models like stable diffusion. 9315 page 9 references 1 josh achiam steven adler sandhini agarwal lama ah- mad ilge akkaya florencia leoni aleman diogo almeida janko altenschmidt sam altman shyamal anadkat et al. gpt-4 technical report. arxiv preprint arxiv 2303.08774 2023. 8 2 dosovitskiy alexey. an image is worth 16x16 words trans- formers for image recognition at scale. arxiv preprint arxiv 2010.11929 2020. 5 3 mehdi cherti romain beaumont ross wightman mitchell wortsman gabriel ilharco cade gordon christoph schuh- mann ludwig schmidt and jenia jitsev. reproducible scaling laws for contrastive language-image learning. in 2023 ieee cvf conference on computer vision and pat- tern recognition cvpr . ieee 2023. 1 4 sri harsha dumpala aman jaiswal chandramouli sas- try evangelos milios sageev oore and hassan saj- jad. sugarcrepe dataset vision-language model sensi- tivity to semantic and lexical alterations. arxiv preprint arxiv 2406.11171 2024. 1 5 samir yitzhak gadre gabriel ilharco alex fang jonathan hayase georgios smyrnis thao nguyen ryan marten mitchell wortsman dhruba ghosh jieyu zhang eyal or- gad rahim entezari giannis daras sarah pratt vivek ramanujan yonatan bitton kalyani marathe stephen mussmann richard vencu mehdi cherti ranjay krishna pang wei koh olga saukh alexander ratner shuran song hannaneh hajishirzi ali farhadi romain beaumont sewoong oh alex dimakis jenia jitsev yair carmon vaishaal shankar and ludwig schmidt. datacomp in search of the next generation of multimodal datasets 2023. 1 6 samir yitzhak gadre gabriel ilharco alex fang jonathan hayase georgios smyrnis thao nguyen ryan marten mitchell wortsman dhruba ghosh jieyu zhang et al. dat- acomp in search of the next generation of multimodal datasets. advances in neural information processing sys- tems 36 2024. 8 17 7 tianyu gao xingcheng yao and danqi chen. simcse simple contrastive learning of sentence embeddings. arxiv preprint arxiv 2104.08821 2021. 5 8 cheng-yu hsieh jieyu zhang zixian ma aniruddha kem- bhavi and ranjay krishna. sugarcrepe fixing hackable benchmarks for vision-language compositionality. advances in neural information processing systems 36 2024. 1 7 9 tsung-yi lin michael maire serge belongie lubomir bourdev ross girshick james hays pietro perona deva ramanan c. lawrence zitnick and piotr doll ar. microsoft coco common objects in context 2015. 2 10 zixian ma jerry hong mustafa omer gul mona gandhi irena gao and ranjay krishna. crepe can vision-language foundation models reason compositionally in proceedings of the ieee cvf conference on computer vision and pat- tern recognition pages 10910 10921 2023. 1 11 xingchao peng qinxun bai xide xia zijun huang kate saenko and bo wang. moment matching for multi-source domain adaptation. in proceedings of the ieee cvf inter- national conference on computer vision pages 1406 1415 2019. 3 12 dustin podell zion english kyle lacey andreas blattmann tim dockhorn jonas m uller joe penna and robin rombach. sdxl improving latent diffusion mod- els for high-resolution image synthesis. arxiv preprint arxiv 2307.01952 2023. 17 13 alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sastry amanda askell pamela mishkin jack clark gretchen krueger and ilya sutskever. learning transferable visual models from natural language supervision 2021. 1 14 n reimers. sentence-bert sentence embeddings using siamese bert-networks. arxiv preprint arxiv 1908.10084 2019. 5 15 dillon reis jordan kupec jacqueline hong and ahmad daoudi. real-time \ufb02ying object detection with yolov8. arxiv preprint arxiv 2305.09972 2023. 17 16 robin rombach andreas blattmann dominik lorenz patrick esser and bj orn ommer. high-resolution image synthesis with latent diffusion models. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr pages 10684 10695 2022. 17 17 ugur sahin hang li qadeer khan daniel cremers and volker tresp. enhancing multimodal compositional reason- ing of visual language models with generative negative min- ing. in proceedings of the ieee cvf winter conference on applications of computer vision pages 5563 5573 2024. 1 18 christoph schuhmann richard vencu romain beaumont robert kaczmarczyk clayton mullis aarush katta theo coombes jenia jitsev and aran komatsuzaki. laion-400m open dataset of clip-\ufb01ltered 400 million image-text pairs 2021. 1 19 christoph schuhmann romain beaumont richard vencu cade gordon ross wightman mehdi cherti theo coombes aarush katta clayton mullis mitchell worts- man et al. laion-5b an open large-scale dataset for training next generation image-text models. advances in neural in- formation processing systems 35 25278 25294 2022. 6 20 tristan thrush ryan jiang max bartolo amanpreet singh adina williams douwe kiela and candace ross. winoground probing vision and language models for visio- linguistic compositionality. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 5238 5248 2022. 1 21 mert yuksekgonul federico bianchi pratyusha kalluri dan jurafsky and james zou. when and why vision- language models behave like bags-of-words and what to do about it in the eleventh international conference on learning representations 2023. 1 22 xiaohua zhai basil mustafa alexander kolesnikov and lucas beyer. sigmoid loss for language image pre-training. in proceedings of the ieee cvf international conference on computer vision pages 11975 11986 2023. 8 17 23 tiancheng zhao tianqi zhang mingwei zhu haozhan shen kyusong lee xiaopeng lu and jianwei yin. vl- checklist evaluating pre-trained vision-language models 9316 page 10 with objects attributes and relations. arxiv preprint arxiv 2207.00221 2022. 1 9317",
      "keywords": [
        "page",
        "clip",
        "under",
        "microscope",
        "fine-grained",
        "analysis",
        "multi-object",
        "representation",
        "reza",
        "abbasi",
        "ali",
        "nazari",
        "aminreza",
        "se\ufb01d",
        "mohammadali",
        "banayeeanzade",
        "mohammad",
        "hossein",
        "rohban",
        "mahdieh",
        "soleymani",
        "baghshah",
        "sharif",
        "university",
        "technology",
        "tehran",
        "iran",
        "reza.abbasi",
        "ali.nazari02",
        "aminreza.sefid",
        "a.banayeean",
        "sharif.edu",
        "abstract",
        "contrastive",
        "language-image",
        "pre-training",
        "mod-",
        "els",
        "excel",
        "zero-shot",
        "classi\ufb01cation",
        "yet",
        "face",
        "challenges",
        "complex",
        "scenarios.",
        "study",
        "offers",
        "com-",
        "prehensive",
        "limitations",
        "contexts",
        "using",
        "specialized",
        "dataset",
        "comco",
        "designed",
        "evaluate",
        "encoders",
        "diverse",
        "\ufb01ndings",
        "reveal",
        "signi\ufb01cant",
        "biases",
        "text",
        "encoder",
        "prior-",
        "itizes",
        "\ufb01rst-mentioned",
        "objects",
        "image",
        "fa-",
        "vors",
        "larger",
        "objects.",
        "through",
        "retrieval",
        "tasks",
        "quantify",
        "across",
        "multiple",
        "vari-",
        "ants",
        "trace",
        "origins",
        "training",
        "process",
        "sup-",
        "ported",
        "analyses",
        "laion",
        "pro-",
        "gression.",
        "image-text",
        "matching",
        "experiments",
        "show",
        "sub-",
        "stantial",
        "performance"
      ],
      "metadata": {
        "filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf",
        "original_filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 1583559,
        "upload_file_id": "upload_1752169054046_mxxkaozex",
        "upload_timestamp": "2025-07-10T17:37:34.076999",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T17:37:34.213716+00:00",
      "content_length": 40912,
      "content_type": ".pdf",
      "filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf"
    },
    "23": {
      "file_id": "23",
      "content": "\n=== Page 1 ===\nCLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object\nRepresentation\nReza Abbasi, Ali Nazari, Aminreza Se\ufb01d, Mohammadali Banayeeanzade,\nMohammad Hossein Rohban, Mahdieh Soleymani Baghshah\nSharif University of Technology, Tehran, Iran\n{reza.abbasi, ali.nazari02, aminreza.sefid, a.banayeean, rohban, soleymani}@sharif.edu\nAbstract\nContrastive Language-Image Pre-training (CLIP) mod-\nels excel in zero-shot classi\ufb01cation, yet face challenges in\ncomplex multi-object scenarios. This study offers a com-\nprehensive analysis of CLIP\u2019s limitations in these contexts\nusing a specialized dataset, ComCO, designed to evaluate\nCLIP\u2019s encoders in diverse multi-object scenarios.\nOur\n\ufb01ndings reveal signi\ufb01cant biases: the text encoder prior-\nitizes \ufb01rst-mentioned objects, and the image encoder fa-\nvors larger objects.\nThrough retrieval and classi\ufb01cation\ntasks, we quantify these biases across multiple CLIP vari-\nants and trace their origins to CLIP\u2019s training process, sup-\nported by analyses of the LAION dataset and training pro-\ngression. Our image-text matching experiments show sub-\nstantial performance drops when object size or token order\nchanges, underscoring CLIP\u2019s instability with rephrased\nbut semantically similar captions. Extending this to longer\ncaptions and text-to-image models like Stable Diffusion,\nwe demonstrate how prompt order in\ufb02uences object promi-\nnence in generated images. For more details and access to\nour dataset and analysis code, visit our project repository:\nhttps://clip-oscope.github.io/.\n1. Introduction\nThe convergence of vision and language in arti\ufb01cial in-\ntelligence has led to the development of Vision-Language\nModels (VLMs) that can interpret and generate multimodal\ncontent. Among these, OpenAI\u2019s Contrastive Language-\nImage Pre-training (CLIP) model [13] has been particu-\nlarly in\ufb02uential, demonstrating remarkable capabilities in\nzero-shot image classi\ufb01cation and setting new standards for\nmultimodal understanding [3, 5, 18, 20]. The success of\nCLIP has catalyzed a wide array of applications\u2014from im-\nage retrieval and visual question answering to text-to-image\ngeneration\u2014signifying a paradigm shift in how models per-\nceive and relate visual and linguistic information.\nVisual Language Models like CLIP face signi\ufb01cant\nchallenges in understanding and reasoning about complex\nscenes with multiple objects and intricate relationships.\nCLIP struggles to identify distinct objects and model their\nrelationships accurately, especially when captions contain\nthe same objects but differ in their relationships. This re-\nsults in dif\ufb01culty distinguishing between similar captions\nwith different object relationships.\nSeveral benchmark\ndatasets have been introduced to elucidate the limitations of\nexisting models in capturing subtle relational nuances. No-\ntably, Winoground [20], VL-CheckList [23], ARO [21], and\nCREPE [10] have been instrumental in evaluating models\u2019\ncapacities to accurately match images with semantically ap-\npropriate captions.\nNumerous studies have addressed compositionality chal-\nlenges in multi-object scenarios, often through end-to-end\nmethods like \ufb01ne-tuning with hard-negative samples [21] to\nimprove model performance. However, these approaches\nhave faced criticism and subsequent re\ufb01nement, as seen in\nmethods like SUGARCREPE [8] and [17], which generate\nnegative captions with minor structural changes or LLMs\nto highlight semantic distinctions.\nWhile most focus on\nCLIP\u2019s ability to distinguish structurally similar yet concep-\ntually different captions, few studies, such as Dumpala et al.\n[4], explore CLIP\u2019s performance on semantically equivalent\nbut structurally distinct captions, revealing a gap in under-\nstanding CLIP\u2019s inconsistency with such prompts.\nWhile previous studies have advanced our understanding\nof CLIP\u2019s limitations, our work uniquely focuses on CLIP\u2019s\nperformance with semantically equivalent but structurally\nvaried captions rather than simply distinguishing conceptu-\nally different captions. This shift enables a deeper exam-\nination of the model\u2019s grasp of language and visual con-\ntent, where systematic errors reveal potential biases. Un-\nlike prior works that primarily propose benchmarks or end-\nto-end solutions, we investigate the root causes of CLIP\u2019s\nbehavior, delving into the mechanisms of both image and\ntext encoders to uncover why the model displays biases\nand lacks robustness to certain linguistic and visual varia-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n9308\n\n=== Page 2 ===\nFigure 1. Overview of our key contributions. Step 1: We create ComCO dataset for controlled multi-object experiments. Step 2: We\nidentify biases in CLIP\u2019s image encoder (favoring larger objects) and text encoder (prioritizing \ufb01rst-mentioned objects). Step 3: We\ninvestigate the origin of these biases, \ufb01nding a connection to training data characteristics. Step 4: We demonstrate the practical impacts of\nthese biases on image-text matching task, showing how they affect model performance in multi-object scenarios.\ntions. To support this analysis, we introduce the ComCO\ndataset, purpose-built for examining CLIP\u2019s performance\nunder controlled multi-object scenarios. Our study spans\nmultiple versions of CLIP trained on diverse datasets and ar-\nchitectures, ensuring the broad applicability of our \ufb01ndings.\nThis comprehensive approach aims to deepen our under-\nstanding of CLIP\u2019s limitations and pave the way for more\nadaptable vision-language models. Beyond CLIP, our in-\nsights have signi\ufb01cant implications for text-to-image (T2I)\ngenerative models and multimodal large language models\n(MLLMs), where decoding CLIP\u2019s encoding intricacies can\ninform advancements in arti\ufb01cial intelligence across do-\nmains. As shown in Figure 1, our key contributions are as\nfollows:\n\u2022 Development of Novel Dataset: We introduce ComCO,\na specialized dataset for creating controlled multi-object\nscenarios.\nUnlike previous benchmarks, ComCO al-\nlows control over object size and caption order, enabling\nprecise analysis of model performance across composi-\ntional challenges and enhancing understanding of VLMs\u2019\nstrengths and weaknesses.\n\u2022 Encoder Analysis: We conduct an in-depth examination\nof CLIP\u2019s image and text encoders in multi-object scenes,\nrevealing weaknesses in preserving information for object\ndistinction and identifying where compositional informa-\ntion is lost.\n\u2022 Bias Identi\ufb01cation: Our study reveals that CLIP\u2019s im-\nage encoder prefers larger objects, while the text encoder\nfavors \ufb01rst-mentioned and visually larger objects, high-\nlighting biases in CLIP\u2019s handling of visual and linguistic\ninformation.\n\u2022 Investigation of Bias Origins: We explore the origins of\nthese biases, showing that larger objects are often men-\ntioned earlier in CLIP\u2019s training captions, and are favored\nin embeddings due to the abundance of their visual to-\nkens. We substantiate this with analyses of the LAION\ndataset and CLIP\u2019s training progression.\n\u2022 Practical Impact: We show how these biases affect per-\nformance in multi-object tasks, with signi\ufb01cant drops in\nimage-text matching accuracy in ComCO and COCO [9].\nThese biases also extend to text-to-image models, in\ufb02u-\nencing object prominence based on prompt order.\nThese \ufb01ndings reveal how biases in CLIP\u2019s text and im-\nage encoders signi\ufb01cantly reduce its performance in multi-\nobject scenarios, emphasizing the need to address these bi-\nases to enhance vision-language models\u2019 robustness. Our\nwork offers key insights into CLIP\u2019s behavior and lays\ngroundwork for improving model performance in real-\nworld applications.\n2. Methodology\n2.1. Dataset Design\nTo thoroughly evaluate the performance of CLIP models\nin multi-object scenarios under controlled conditions, we\nconstructed the ComCO (Complex COCO Objects) dataset.\nUtilizing Blender software allowed us precise control over\nthe number, location, and dimensions of objects in the im-\nages (see Appendix 7.1). The ComCO dataset comprises\n72 objects derived from the COCO dataset. We generated\n9309\n\n=== Page 3 ===\nimages containing 2, 3, 4, and 5 objects. Each image is\npaired with a speci\ufb01c caption that accurately describes the\nobjects present. This approach ensures high control over\nthe dataset and minimizes confounding factors, providing a\nrobust platform for evaluating the CLIP models.\nWe deliberately chose not to use text-to-image models\nfor generating these datasets due to two main reasons. First,\nthese models often lack the capability to produce high-\nquality, fully controlled multi-object images. Second, since\nCLIP is used in many of these models, utilizing them could\nintroduce unwanted biases into our evaluations.\n2.2. Experimental Framework for Encoder Analy-\nsis\nThe main goal of this study is to evaluate the performance of\nCLIP\u2019s text and image encoders separately in multi-object\nscenarios. We aim to analyze the impact and contribution of\neach object in the \ufb01nal output of the encoders. To achieve\nthis, we conducted experiments using our designed ComCO\ndataset, with images and captions containing two to \ufb01ve ob-\njects. To ensure the generalizability of our \ufb01ndings, we also\nvalidated our results on the widely-used COCO dataset. We\ndesigned two sets of experiments: retrieval-based experi-\nments and classi\ufb01cation-based experiments. Given the con-\nsistency of the results in both types of experiments, we have\nincluded the classi\ufb01cation results in the appendix 7.2 and\n7.4 and explain the retrieval-based experiments bellow.\n2.2.1. TEXT-BASED OBJECT RETRIEVAL (TOR)\nThe Text-based Object Retrieval task evaluates how well\nCLIP\u2019s text encoder can identify individual objects within\nmulti-object captions. As illustrated in Figure 2a, this ex-\nperiment involves several steps: First, we use CLIP\u2019s text\nencoder to create embeddings for both multi-object captions\nand single-object captions. We then measure the similar-\nity between each multi-object caption embedding and all\nsingle-object caption embeddings. The single-object cap-\ntion with the highest similarity score is considered the \u201dre-\ntrieved\u201d object. To assess performance, we calculate re-\ntrieval accuracy for each object position in the multi-object\ncaptions. This helps us identify any biases related to an\nobject\u2019s position within a caption, such as favoring objects\nmentioned \ufb01rst or last.\n2.2.2. IMAGE-BASED OBJECT RETRIEVAL (IOR)\nThe Image-based Object Retrieval task is similar to TOR\nbut focuses on CLIP\u2019s image encoder. As shown in Fig-\nure 2b, this experiment involves several steps: We begin\nby using CLIP\u2019s image encoder to generate embeddings\nfor multi-object images and single-object images. We then\ncompute similarity scores between each multi-object image\nembedding and all single-object image embeddings. The\nsingle-object image with the highest similarity score is con-\nsidered the \u201dretrieved\u201d object. To evaluate performance, we\ncalculate retrieval accuracy for different object size cate-\ngories (e.g., large, small) within the multi-object images.\nThis allows us to determine if the image encoder shows any\npreference for objects of a particular size.\nWe also experimented with a variation of ComCO, called\nSimCO, where objects were replaced with simple geometric\nshapes from the CLEVR dataset. This was done to con\ufb01rm\nthat bias persists even with non-natural, geometric objects.\nFurther details are provided in Appendix 7.1.\n3. Results and Analysis\nOur experiments revealed signi\ufb01cant biases in both the\ntext and image encoders of the CLIP model. This section\npresents our \ufb01ndings, organized by encoder type and focus-\ning on retrieval tasks.\n3.1. Text Encoder Biases\nWe observed a consistent bias in the text encoder towards\nthe \ufb01rst object mentioned in descriptions. In the TOR ex-\nperiment, the retrieval accuracy (as shown in Table 1) was\nhighest for the \ufb01rst object, indicating its dominant in\ufb02uence\non the overall text representation. This suggests that the\ntext encoder prioritizes the initial object, leading to its more\naccurate retrieval compared to subsequent objects. The de-\ntailed results for the scenarios involving 2, 3, and 5 objects\ncan be found in the appendix 7.3, and experiments on longer\ncaption templates are in Appendix 7.6 and 7.7.\n3.2. Image Encoder Biases\nIn multi-object images, the image encoder exhibited a\nstrong bias towards larger objects. The Image-based Ob-\nject Retrieval IOR experiment, detailed in Table 2, shows\nthat larger objects were more frequently and accurately re-\ntrieved during single-object image searches. This \ufb01nding\nhighlights the image encoder\u2019s bias towards larger objects,\nwhich receive disproportionate emphasis in the \ufb01nal image\nrepresentation. Further detailed results, speci\ufb01cally for sce-\nnarios with 2, 3, and 5 objects, are provided in the appendix\n7.5.\n3.3. COCO Dataset Experiments\nTo validate the generalizability of our \ufb01ndings from the\nsynthetic dataset, we conducted similar experiments on the\nCOCO dataset, which comprises real images with accom-\npanying captions. This real-world dataset allowed us to in-\nvestigate whether the previously observed biases persist in\nmore naturalistic settings.\nDue to the absence of single-object images for COCO\nobjects, we approached the IOR experiment in two ways.\nFirst, we used single-object images from the DomainNet\ndataset [11] as retrieval targets. Second, we introduced an\nalternative approach called Image-to-Text Object Retrieval\n(I2TOR). In I2TOR, we used the textual names of COCO\n9310\n\n=== Page 4 ===\n%DVH\u0003,PDJH\u001db\n7KUHH\u00032EMHFWV\u0003,PDJH\n6LQJOH\u00032EMHFW\u0003,PDJH\u0003ZKLFK\nPDWFKLQJb%DVH\u0003,PDJH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n2WKHUb6LQJOHb2EMHFW\n,PDJHV\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n\u0011\u0011\u0011\n&/,3\u00036FRUH\n\u0013\u0011\u001a\u0013\u0018\n\u0013\u0011\u001b\u0015\u0017\n\u0013\u0011\u0019\u0015\u0014\n\u0013\u0011\u0018\u0015\u001a\n\u0013\u0011\u0018\u001a\u001c\n\u0013\u0011\u0018\u0018\u0017\nE\f\nD\f\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n\u0011\u0011\u0011\n&/,3\u00036FRUH\n\u0013\u0011\u0016\u0017\u0019\n\u0013\u0011\u0019\u0017\u001a\n\u0013\u0011\u0017\u001b\u0014\n\u0013\u0011\u0015\u0014\u001c\n\u0013\u0011\u0015\u0017\u0015\n\u0013\u0011\u0015\u0016\u0019\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n%DVH\u00037H[W\u001db\n7KUHH\u00032EMHFWV\u00037H[W\n6LQJOH\u00032EMHFW\u00037H[W\u0003ZKLFK\nPDWFKLQJb%DVH\u00037H[W\n2WKHUb6LQJOHb2EMHFW\n7H[WV\nSL]]D\u0003DQGbDSSOH\nDQGbGHVN\nDSSOH\nSL]]D\nGHVN\nKDW\nFDU\nD[H\n\u0011\u0011\u0011\n\u0011\u0011\u0011\n725\n,25\nFigure 2. Experimental setup for Text-based Object Retrieval (TOR) and Image-based Object Retrieval (IOR) tasks. a) TOR: The CLIP\ntext encoder generates embeddings for multi-object and single-object texts. Cosine similarity scores are calculated between the base text\nembedding and single-object text embeddings to identify the most similar object. b) IOR: The CLIP image encoder generates embeddings\nfor multi-object and single-object images. Cosine similarity scores are calculated between the base image embedding and single-object\nimage embeddings to identify the most similar object.\nTable 1. Performance on TOR for ComCO datasets\nTask\nModel\nFirst Obj\nSecond Obj\nThird Obj\nFourth Obj\nTOR\nCLIP LAION\n63.96\n21.59\n10.68\n3.76\nCLIP Datacomp\n71.13\n16.26\n8.74\n3.87\nCLIP Roberta\n44.03\n23.73\n18.07\n14.18\nSIGLIP\n58.11\n21.16\n10.99\n9.73\nCLIP openAI\n50.31\n20.74\n14.45\n6.79\nNegCLIP\n51.63\n28.92\n14.86\n4.59\nSugarCrepe\n44.29\n30.32\n18.73\n6.66\nTable 2. Performance on IOR for ComCO datasets\nTask\nModel\nLarge Object\nSmall Obj 1\nSmall Obj 2\nSmall Obj 3\nIOR\nCLIP LAION\n85.45\n6.36\n5.45\n2.73\nCLIP Datacomp\n85.16\n5.65\n4.95\n4.24\nCLIP Roberta\n87.40\n8.66\n2.36\n1.57\nSIGLIP\n77.66\n10.11\n6.38\n5.85\nCLIP openAI\n65.22\n17.39\n8.70\n8.70\nNegCLIP\n61.67\n15.00\n13.33\n10.00\nSugarCrepe\n60.0\n18.38\n16.85\n4.7\nobjects instead of single-object images. These object names\nwere embedded using CLIP\u2019s text encoder, allowing us to\nperform a retrieval task consistent with the IOR methodol-\nogy while adapting to the constraints of the COCO dataset.\nTables 3 and 4 present the results of our COCO dataset\nexperiments. In TOR, the \ufb01rst-mentioned object in COCO\ncaptions was retrieved with higher accuracy, which aligns\nwith our earlier \ufb01ndings of bias in the text encoder. Simi-\nlarly, in IOR, larger objects in COCO images were retrieved\nmore accurately, consistent with the trends observed in our\nsynthetic dataset experiments. The I2TOR results further\ncon\ufb01rmed this bias, demonstrating that even when using\ntextual object representations, the bias towards larger ob-\nTable 3. Performance on TOR for coco dataset\nTask\nModel\nFirst Obj\nSecond Obj\nThird Obj\nFourth Obj\nTOR\nCLIP openAI\n35.24\n21.90\n20.48\n22.38\nCLIP LAION\n67.89\n13.76\n8.26\n10.09\nCLIP Datacomp\n57.68\n17.68\n12.75\n11.88\nCLIP Roberta\n40.78\n23.30\n20.39\n15.53\nSIGLIP\n49.47\n26.84\n12.11\n11.58\nNegCLIP\n38.69\n22.11\n17.09\n22.11\nTable 4. Performance on IOR for coco dataset\nTask\nModel\nLarge Object\nSmall Obj 1\nSmall Obj 2\nSmall Obj 3\nIOR\nCLIP openAI\n43.02\n28.82\n17.13\n11.03\nCLIP LAION\n39.44\n28.45\n17.70\n14.41\nCLIP Datacomp\n36.71\n29.55\n19.13\n14.61\nCLIP Roberta\n36.71\n28.61\n19.82\n14.86\nSIGLIP\n36.63\n28.29\n20.02\n15.06\nNegCLIP\n44.04\n28.86\n16.48\n10.62\nI2TOR\nCLIP openAI\n51.49\n24.87\n13.68\n9.97\nCLIP LAION\n45.50\n27.02\n15.91\n11.56\nCLIP Datacomp\n46.64\n26.82\n14.53\n12.01\nCLIP Roberta\n44.69\n26.98\n16.04\n12.29\nSIGLIP\n47.09\n27.07\n15.10\n10.74\nNegCLIP\n49.04\n27.07\n14.08\n9.81\njects persists.\nOur experiments reveal two signi\ufb01cant biases in the\nCLIP model: the text encoder shows a strong preference for\nthe \ufb01rst mentioned object in textual descriptions, while the\nimage encoder exhibits greater sensitivity to larger objects\nin images. These biases can signi\ufb01cantly impact the overall\nsystem performance in various vision-language tasks, par-\nticularly in multi-object scenarios.\n9311\n\n=== Page 5 ===\n4. Origin of Bias in CLIP Models\nIn this section, we investigate the potential origins of the\nbiases observed in CLIP models and provide evidence sup-\nporting our hypotheses.\n4.1. Bias in the Image Encoder\nThe observed bias favoring larger objects within the image\ndomain can be attributed to the architectural characteristics\nof Vision Transformers (ViT) [2] utilized in CLIP\u2019s image\nencoder. Our hypothesis is that larger objects, which occupy\na greater number of patches in the ViT\u2019s patch-based image\nrepresentation, exert a more signi\ufb01cant in\ufb02uence on the \ufb01-\nnal class (CLS) token representation. This bias is not exclu-\nsive to CLIP; it appears to be a consistent feature across ViT\nmodels, as demonstrated by our experiments detailed in the\nappendix.\nTo substantiate this hypothesis, we designed an experi-\nment to quantify the attention allocated by the CLS token\nto each image patch. By calculating the cumulative atten-\ntion received by each object from the CLS token, we could\nassess the in\ufb02uence of object size on attention allocation.\nWe applied this analysis to our three-object ComCO dataset,\nand the results are illustrated in Figure 3. The \ufb01ndings con-\n\ufb01rm our hypothesis: larger objects indeed receive more at-\ntention from the CLS token.\n4.2. Bias in the Text Encoder\nWe explore the bias present in the text encoder from two\nperspectives: the attention mechanism in the model struc-\nture and the model\u2019s training method.\n4.2.1. Impact of Attention Mechanism\nText encoder models can be categorized based on their at-\ntention mechanisms: uni-directional (causal) attention and\nbi-directional attention. In models with causal attention,\neach token attends only to preceding tokens, whereas in bi-\ndirectional models, each token attends to all tokens in the\nsequence.\nWhen OpenAI introduced the CLIP model, its text en-\ncoder employed causal attention, meaning each token could\nonly attend to tokens before it and itself.\nThis differs\nfrom typical self-attention mechanisms, where tokens at-\ntend to all other tokens. Most CLIP models use causal self-\nattention, with the exception of the variant using the XLM-\nRoberta text encoder, which also employs self-attention.\nHowever, as shown in Table 1, even this model exhibits the\nmentioned bias. This indicates that the bias does not origi-\nnate from the attention mechanism itself.\n4.2.2. Role of Training Method\nTo determine whether the observed bias is speci\ufb01c to CLIP\nmodels, we compared CLIP\u2019s text encoder with two other\nTable 5. Performance on TOC and TOR for ComCO datasets\nTask\nModel\nFirst Obj\nSecond Obj\nThird Obj\nFourth Obj\nTOR\nCLIP\n56.28\n22.71\n13.17\n7.48\nSBERT\n29.02\n19.80\n17.50\n33.57\nSimCSE [7]\n27.59\n19.07\n17.76\n34.83\nmodels designed to embed sentences into a meaningful se-\nmantic space: Sentence-BERT (SBERT) [14] and SimCSE\n[7]. The primary distinction is that CLIP\u2019s embedding space\nis shared between images and text, whereas SBERT and\nSimCSE operate solely in the text domain.\nWe conducted the TOR experiment on our dataset using\nthese models. As presented in Table 5, the bias observed in\nCLIP differs from that in the other models. This suggests\nthat CLIP\u2019s unique training method, which aligns images\nand text in a shared embedding space through contrastive\nlearning, contributes to the bias. Therefore, to uncover the\nroot cause of the bias, we focus on the speci\ufb01cs of CLIP\u2019s\ntraining procedure.\n4.3. Hypothesized Origin of Text-Side Bias in CLIP\nWe hypothesize that the text-side bias in CLIP, which fa-\nvors objects mentioned earlier in text descriptions, origi-\nnates from the image-side bias toward larger objects and is\ntransferred to the text encoder during contrastive training.\nWe present evidence supporting this hypothesis through two\nkey claims and an analysis of the training progression.\nClaim 1: Larger Objects Have More In\ufb02uence on Text\nEmbeddings.\nBuilding upon the established image-side\nbias discussed earlier, we posit that objects with larger\nphysical sizes exert more in\ufb02uence on CLIP\u2019s text em-\nbeddings due to the alignment enforced during contrastive\ntraining. To test this, we categorized objects in the Domain-\nNet dataset into large, medium, and small groups based on\ntheir relative physical sizes in real-world (with the full list of\nobjects provided in the appendix 7.10). Speci\ufb01cally, objects\nsmaller than a school bag were categorized as small, objects\nsized between a school bag and a medium-sized car were\nclassi\ufb01ed as medium, and objects larger than a car\u2014up to\nsigni\ufb01cantly larger items\u2014were considered large. We then\nconstructed two sets of sentences, each containing four ob-\njects: one set with a large object mentioned \ufb01rst followed by\nthree medium-sized objects, and another with a small object\nmentioned \ufb01rst followed by three medium-sized objects.\nFigure 4.a compares the TOR accuracy for the \ufb01rst ob-\nject in these two groups. The higher TOR accuracy for sen-\ntences beginning with large objects supports our hypothe-\nsis that larger objects, when mentioned \ufb01rst, have a more\nsigni\ufb01cant impact on the text embeddings due to the cross-\nmodal alignment with their prominent representation in im-\nages.\n9312\n\n=== Page 6 ===\na)\nb)\nFigure 3. Attention allocation from the CLS token to objects of different sizes in the ComCO dataset. a) Qualitative results showing the\nCLS token\u2019s attention to each object. b) Quantitative analysis of attention distribution across 8,000 images, with each image containing one\nlarge and two small objects. The bar chart shows the average attention allocated to the large object versus the smaller ones, demonstrating\na bias towards larger objects.\na)\nb)\nc)\nFigure 4. a) Top-1 Object Retrieval accuracy comparison for sentences where the \ufb01rst object is either large or small. The higher TOR\naccuracy for sentences beginning with large objects supports the hypothesis that larger objects, when mentioned \ufb01rst, exert a stronger\nin\ufb02uence on text embeddings due to cross-modal alignment with their prominent visual representation in images. b) Distribution of the\nposition of the largest object within image captions from the LAION datasets. The results show a consistent bias where larger objects\ntend to be mentioned earlier in text descriptions. c) Progression of TOR rates across different training stages, indicating that text-side bias\nstrengthens as the model is exposed to more data, suggesting the cumulative effect of image-side bias being transferred to the text encoder\nthrough contrastive learning.\nClaim 2: Caption Bias in Training Datasets.\nTo inves-\ntigate potential biases in CLIP\u2019s training data, we analyzed\nboth the LAION [19] and COCO datasets. Due to limited\ncomputational resources and the large size of the LAION\ndataset, which contains over 2 billion image-text pairs, we\nrandomly selected a subset of 200,000 samples for our anal-\nysis. Using the Llama3 model, we extracted objects from\nthe image captions and employed the Language Segment-\nAnything tool to generate object masks in the correspond-\ning images, calculating their areas based on these masks. A\ndetailed description of our LAION dataset analysis method-\nology can be found in Appendix 7.8.\nFigure4.b shows the position of the largest object within\neach caption. The results indicate that, in the majority of\ncases, the largest object in an image is mentioned earlier\nin its caption. The same experiment was conducted on the\nCOCO dataset, with detailed results and the distribution for\ntwo to \ufb01ve object scenarios provided in Appendix 7.9. This\ndemonstrates a consistent bias in the training data, where\nlarger objects are not only more visually prominent but are\nalso described earlier in text annotations.\nAnalysis of Bias Development During Training.\nTo fur-\nther validate our hypothesis, we examined the progression\nof text-side bias during CLIP\u2019s training. We utilized model\ncheckpoints from the LAION dataset at \ufb01ve training stages,\ncorresponding to exposure to 2, 4, 6, 8, and 10 billion sam-\nples. We conducted TOR experiments at each stage, focus-\n9313\n\n=== Page 7 ===\ning on the retrieval accuracy for the \ufb01rst object mentioned\nin text descriptions.\nFigure4.c depicts the evolution of the TOR rate across\ndifferent training stages for scenarios with varying numbers\nof objects (from 3 to 8). The consistent upward trend in\nthe TOR rate as the model is exposed to more training data\nsuggests that the text-side bias strengthens over time, likely\ndue to the cumulative effect of the image-side bias being\ntransferred to the text encoder through contrastive learning.\nIncomplete Text Representation of CLIP\nHere we want\nto theoretically highlight why the CLIP text encoder could\nlearn an incomplete representation of the text. Let z and\nw represent a latent representation of an image content de-\nscribed in the caption, and such visual content not men-\ntioned in the text, respectively. For example, z represents\nthe fact that an image contains \u201ca horse that is eating the\ngrass.\u201d In this case, w might represent other details in the\nimage, like the \u201chorse color,\u201d \u201cwhere the horse is located,\u201d\netc. We assume a data generative process as follows:\nI := g(z,w)\nT := h(z),\nwhere I is the image, and T is its corresponding caption.\nNow we want to learn a joint embedding of the image\nand text through the CLIP. Here, we assume that f\u03b8(.) and\ni\u03c9(.) as learnable functions that map the image and text into\nthe joint embedding space, respectively.\nTheorem 1 Let elements of z be independent, zero-mean,\nand unit-variance. The contrastive loss for the ideal text en-\ncoder, i\u03c9(T) = z converges to that of a non-ideal incomplete\none, i.e. i\u03c9\u2032(T) = zs, where zs is the \ufb01rst d \u2212k dimensions\nof z, with k being a constant, and d \u2192\u221e.\nProof: The contrastive loss in making this learning hap-\npen can be written as:\nEz,z\u2032,w\n\u0002\nexp(sim(z,z))\nexp(sim(z,z))+\u2211k exp(sim(z,z\u2032\nk))\n\u0003\n(1)\nwith\nsim(z,z\u2032) = S(f\u03b8(g(z,w),i\u03c9(h(z\u2032)))),\nand z and {z\u2032\nk|1 \u2264k \u2264b} are b + 1 i.i.d. samples of the\ncontent in the representation space, and S is some normal-\nized similarity metric, e.g. cosine similarity, and b+1 is the\nbatch size. We assume that elements of z are independent,\nunit-variance, and zero mean. We further assume that the\ndimensionality of z, denoted as d, goes to in\ufb01nity.\nUnder such conditions, and based on Law of Large\nNumbers, \u2225z\u2225\np\u2212\u2192\n\u221a\nd, when d is large.\nTherefore, for\nany two independent copies of z, z\u2032\nk, we have sim(z,z\u2032\nk) =\nz\u22a4z\u2032\nk/(\u2225z\u2225\u2225z\u2032\nk\u2225)\np\u2212\u21920.\nIt is evident that in the ideal case, f\u03b8(g(z,w)) = z and\nalso i\u03c9(h(z)) = z, so the contrastive loss would converge\nto e/(e + b), as the numerator is e, and the second term in\nthe denominator converges to exp(0) = 1, according to the\nMann-Wald\u2019s theorem.\nHowever, we show that other learning of this representa-\ntion could achieve the same amount of loss. For instance, let\nzs be the \ufb01rst d \u2212k elements of z, with k being a constant.\nWe show that if f\u03b8\u2032(I) = zs and i\u03c9\u2032(T) = zs, the same loss\nwould be achieved in the limit of large d. To see this, note\nthat the numerator stays the same, i.e. e, while the second\nterm in the denominator still converges to bexp(0) = b.\nThis means that even if the image and text encoder of\nthe CLIP only partially recover the content embedding, they\nreach an excellent loss. But such possible incomplete rep-\nresentations of z are combinatorially large, making conver-\ngence of the CLIP to such local minima pretty likely. This\nmakes the text encoding of CLIP be far from ideal. Fur-\nthermore, the text encoder would become biased, depend-\ning on which of such local minima it converges to. Based\non this explanation, we would expect a text encoder that has\nlearned a complete representation to exhibit such biases to a\nlesser degree. As mentioned earlier, the subject of learning\ntext representations in VLMs that are discriminative of hard\nnegatives (e.g. NegCLIP) has been around for few years.\nWe tested one of strongest such models, [8], in our bench-\nmark to validate the hypothesis that an incomplete text rep-\nresentation is one of the causes of the bias in the VLMs.\nWe noticed that this model shows lower bias based on our\nbenchmark (see the SugarCrepe model in tables 1 and 2).\nWe have developed an initial approach to address the\nidenti\ufb01ed bias in the CLIP model, which is presented in\nAppendix 7.12. While this method is speci\ufb01c to our cur-\nrent dataset, it represents a promising step toward address-\ning these challenges and can inspire further advancements.\nThis work demonstrates our commitment to exploring prac-\ntical solutions while maintaining the primary focus of this\nstudy on the analysis of bias and its implications.\n5. Practical Impacts of Encoder Biases\nThe biases observed in CLIP\u2019s image and text encoders sig-\nni\ufb01cantly impact model performance in real-world appli-\ncations. This section explores how these biases manifest in\nimage-text matching tasks, while further analyses of text-to-\nimage generation impacts are presented in Appendix 7.11.\nOur analysis in this section serves two primary purposes.\nFirst, it provides concrete evidence of how these theoretical\nbiases can translate into practical limitations. Second, it of-\nfers insights into potential areas for improvement in vision-\nlanguage models, particularly in handling complex, multi-\n9314\n\n=== Page 8 ===\nFigure 5. An example of the correct and incorrect caption structures in the \ufb01rst and second scenarios.\nobject scenarios. Through a series of carefully designed ex-\nperiments, we illustrate how the biases in both text and im-\nage encoders can lead to unexpected or suboptimal results\nin tasks that are crucial for many downstream applications.\n5.1. Image-Text Matching\nBuilding upon our \ufb01ndings of biases in CLIP\u2019s image and\ntext encoders, we now demonstrate how these biases tangi-\nbly affect the model\u2019s performance in image-caption match-\ning tasks. We designed two experimental scenarios, con-\nducted on both the ComCO and COCO datasets, to evaluate\nthese biases. The results of these experiments are summa-\nrized in Table 6. To better illustrate the differences between\nthese two scenarios, an example of the caption structures is\nshown in Figure 5. In each scenario, we created incorrect\ncaptions by switching one object in the caption with an ob-\nject that is not present in the image. Additionally, GPT-4O\n[1] was used to rewrite the captions in the COCO dataset.\nFirst Scenario\nIn the \ufb01rst scenario, biases assist the\nmodel in distinguishing between the correct and incorrect\ncaptions. In the correct captions, the largest object in the\nimage is placed at the beginning, aligning with the model\u2019s\nbias towards prioritizing \ufb01rst-mentioned objects and larger\nobjects. For the incorrect captions, the non-existent object is\ndeliberately placed at the beginning, which helps the model\nrecognize the difference between the correct and incorrect\ncaptions more effectively. This positioning emphasizes the\ndiscrepancy early on, allowing the model to better detect the\nmismatch between the caption and the image. The perfor-\nmance of different models in this scenario can be seen in\nTable 6 under the \u201dFirst Scenario\u201d column.\nSecond Scenario\nIn the second scenario, biases lead the\nmodel to make errors. The correct captions place the largest\nobject at the end of the sentence, disrupting the model\u2019s\nbias towards objects mentioned earlier and its preference\nfor larger objects. In the incorrect captions, the non-existent\nobject is placed at the end, making it more dif\ufb01cult for the\nmodel to differentiate between correct and incorrect cap-\ntions as its attention is drawn away from the critical discrep-\nancies. The performance of different models in this scenario\nis shown in Table 6 under the \u201dSecond Scenario\u201d column.\nTable 6. Performance Comparison on Image-Text Matching for\nComCO and COCO Datasets\nDataset\nModel\nFirst Scenario\nSecond Scenario\nComCO\nCLIP Datacomp [6]\n99.99\n67.50\nCLIP Roberta\n99.98\n64.75\nSIGLIP [22]\n99.49\n72.36\nCLIP openAI\n99.59\n52.23\nNegCLIP\n96.82\n46.94\nSugarCrepe\n98.55\n60.43\nCOCO\nCLIP Datacomp [6]\n71.2\n54.2\nCLIP Roberta\n72.2\n54.1\nSIGLIP [22]\n64.8\n39.5\nCLIP openAI\n63.5\n26.4\nNegCLIP\n72\n28.7\nSugarCrepe\n80.0\n40.9\nBy comparing these two scenarios, we demonstrate that\nbiases in CLIP can either help or hinder the model\u2019s perfor-\nmance depending on how captions are structured. The ex-\nperimental results, particularly with the use of GPT-4O for\ncaption rephrasing in the COCO dataset, reveal how such\nbiases can in\ufb02uence the accuracy of image-text matching\ntasks. These biases must be addressed to improve CLIP\u2019s\nrobustness in real-world multi-object scenarios.\nFor further insights on how these biases affect text-to-\nimage generation, refer to our extended experiments in Ap-\npendix 7.11.\n6. Conclusion\nThis study uncovers biases in CLIP\u2019s encoders, with the\ntext encoder favoring \ufb01rst-mentioned objects and the im-\nage encoder emphasizing larger ones, which impacts per-\nformance in multi-object tasks. Using the ComCO dataset,\nwe highlighted these biases\u2019 effects on object representation\nand positioning, underscoring the need for balanced train-\ning. We attribute these biases to CLIP\u2019s contrastive frame-\nwork, where alignment issues propagate across modalities.\nAddressing these biases is essential for vision-language ad-\nvancements, as seen with models like Stable Diffusion.\n9315\n\n=== Page 9 ===\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 8\n[2] Dosovitskiy Alexey. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint arXiv:\n2010.11929, 2020. 5\n[3] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev.\nReproducible\nscaling laws for contrastive language-image learning.\nIn\n2023 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR). IEEE, 2023. 1\n[4] Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sas-\ntry, Evangelos Milios, Sageev Oore, and Hassan Saj-\njad.\nSugarcrepe++ dataset: Vision-language model sensi-\ntivity to semantic and lexical alterations.\narXiv preprint\narXiv:2406.11171, 2024. 1\n[5] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Or-\ngad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek\nRamanujan, Yonatan Bitton, Kalyani Marathe, Stephen\nMussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna,\nPang Wei Koh, Olga Saukh, Alexander Ratner, Shuran\nSong, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,\nSewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon,\nVaishaal Shankar, and Ludwig Schmidt.\nDatacomp: In\nsearch of the next generation of multimodal datasets, 2023.\n1\n[6] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-\nacomp:\nIn search of the next generation of multimodal\ndatasets. Advances in Neural Information Processing Sys-\ntems, 36, 2024. 8, 17\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen.\nSimcse:\nSimple contrastive learning of sentence embeddings. arXiv\npreprint arXiv:2104.08821, 2021. 5\n[8] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kem-\nbhavi, and Ranjay Krishna.\nSugarcrepe: Fixing hackable\nbenchmarks for vision-language compositionality. Advances\nin neural information processing systems, 36, 2024. 1, 7\n[9] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona, Deva\nRamanan, C. Lawrence Zitnick, and Piotr Doll\u00b4ar. Microsoft\ncoco: Common objects in context, 2015. 2\n[10] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi,\nIrena Gao, and Ranjay Krishna. Crepe: Can vision-language\nfoundation models reason compositionally? In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 10910\u201310921, 2023. 1\n[11] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\nSaenko, and Bo Wang. Moment matching for multi-source\ndomain adaptation. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 1406\u20131415,\n2019. 3\n[12] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 17\n[13] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision, 2021. 1\n[14] N Reimers.\nSentence-bert:\nSentence embeddings using\nsiamese bert-networks.\narXiv preprint arXiv:1908.10084,\n2019. 5\n[15] Dillon Reis, Jordan Kupec, Jacqueline Hong, and Ahmad\nDaoudi. Real-time \ufb02ying object detection with yolov8. arXiv\npreprint arXiv:2305.09972, 2023. 17\n[16] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, 2022. 17\n[17] Ugur Sahin, Hang Li, Qadeer Khan, Daniel Cremers, and\nVolker Tresp. Enhancing multimodal compositional reason-\ning of visual language models with generative negative min-\ning. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pages 5563\u20135573, 2024. 1\n[18] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-\ufb01ltered 400 million image-text pairs,\n2021. 1\n[19] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 6\n[20] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace Ross.\nWinoground: Probing vision and language models for visio-\nlinguistic compositionality. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5238\u20135248, 2022. 1\n[21] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,\nDan Jurafsky, and James Zou.\nWhen and why vision-\nlanguage models behave like bags-of-words, and what to\ndo about it?\nIn The Eleventh International Conference on\nLearning Representations, 2023. 1\n[22] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 11975\u201311986, 2023. 8, 17\n[23] Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan\nShen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin.\nVl-\nchecklist:\nEvaluating pre-trained vision-language models\n9316\n\n=== Page 10 ===\nwith objects, attributes and relations.\narXiv preprint\narXiv:2207.00221, 2022. 1\n9317\n",
      "searchable_content": "page 1 clip under the microscope a fine-grained analysis of multi-object representation reza abbasi ali nazari aminreza se\ufb01d mohammadali banayeeanzade mohammad hossein rohban mahdieh soleymani baghshah sharif university of technology tehran iran reza.abbasi ali.nazari02 aminreza.sefid a.banayeean rohban soleymani sharif.edu abstract contrastive language-image pre-training clip mod- els excel in zero-shot classi\ufb01cation yet face challenges in complex multi-object scenarios. this study offers a com- prehensive analysis of clip s limitations in these contexts using a specialized dataset comco designed to evaluate clip s encoders in diverse multi-object scenarios. our \ufb01ndings reveal signi\ufb01cant biases the text encoder prior- itizes \ufb01rst-mentioned objects and the image encoder fa- vors larger objects. through retrieval and classi\ufb01cation tasks we quantify these biases across multiple clip vari- ants and trace their origins to clip s training process sup- ported by analyses of the laion dataset and training pro- gression. our image-text matching experiments show sub- stantial performance drops when object size or token order changes underscoring clip s instability with rephrased but semantically similar captions. extending this to longer captions and text-to-image models like stable diffusion we demonstrate how prompt order in\ufb02uences object promi- nence in generated images. for more details and access to our dataset and analysis code visit our project repository https clip-oscope.github.io . 1. introduction the convergence of vision and language in arti\ufb01cial in- telligence has led to the development of vision-language models vlms that can interpret and generate multimodal content. among these openai s contrastive language- image pre-training clip model 13 has been particu- larly in\ufb02uential demonstrating remarkable capabilities in zero-shot image classi\ufb01cation and setting new standards for multimodal understanding 3 5 18 20 . the success of clip has catalyzed a wide array of applications from im- age retrieval and visual question answering to text-to-image generation signifying a paradigm shift in how models per- ceive and relate visual and linguistic information. visual language models like clip face signi\ufb01cant challenges in understanding and reasoning about complex scenes with multiple objects and intricate relationships. clip struggles to identify distinct objects and model their relationships accurately especially when captions contain the same objects but differ in their relationships. this re- sults in dif\ufb01culty distinguishing between similar captions with different object relationships. several benchmark datasets have been introduced to elucidate the limitations of existing models in capturing subtle relational nuances. no- tably winoground 20 vl-checklist 23 aro 21 and crepe 10 have been instrumental in evaluating models capacities to accurately match images with semantically ap- propriate captions. numerous studies have addressed compositionality chal- lenges in multi-object scenarios often through end-to-end methods like \ufb01ne-tuning with hard-negative samples 21 to improve model performance. however these approaches have faced criticism and subsequent re\ufb01nement as seen in methods like sugarcrepe 8 and 17 which generate negative captions with minor structural changes or llms to highlight semantic distinctions. while most focus on clip s ability to distinguish structurally similar yet concep- tually different captions few studies such as dumpala et al. 4 explore clip s performance on semantically equivalent but structurally distinct captions revealing a gap in under- standing clip s inconsistency with such prompts. while previous studies have advanced our understanding of clip s limitations our work uniquely focuses on clip s performance with semantically equivalent but structurally varied captions rather than simply distinguishing conceptu- ally different captions. this shift enables a deeper exam- ination of the model s grasp of language and visual con- tent where systematic errors reveal potential biases. un- like prior works that primarily propose benchmarks or end- to-end solutions we investigate the root causes of clip s behavior delving into the mechanisms of both image and text encoders to uncover why the model displays biases and lacks robustness to certain linguistic and visual varia- this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 9308 page 2 figure 1. overview of our key contributions. step 1 we create comco dataset for controlled multi-object experiments. step 2 we identify biases in clip s image encoder favoring larger objects and text encoder prioritizing \ufb01rst-mentioned objects . step 3 we investigate the origin of these biases \ufb01nding a connection to training data characteristics. step 4 we demonstrate the practical impacts of these biases on image-text matching task showing how they affect model performance in multi-object scenarios. tions. to support this analysis we introduce the comco dataset purpose-built for examining clip s performance under controlled multi-object scenarios. our study spans multiple versions of clip trained on diverse datasets and ar- chitectures ensuring the broad applicability of our \ufb01ndings. this comprehensive approach aims to deepen our under- standing of clip s limitations and pave the way for more adaptable vision-language models. beyond clip our in- sights have signi\ufb01cant implications for text-to-image t2i generative models and multimodal large language models mllms where decoding clip s encoding intricacies can inform advancements in arti\ufb01cial intelligence across do- mains. as shown in figure 1 our key contributions are as follows development of novel dataset we introduce comco a specialized dataset for creating controlled multi-object scenarios. unlike previous benchmarks comco al- lows control over object size and caption order enabling precise analysis of model performance across composi- tional challenges and enhancing understanding of vlms strengths and weaknesses. encoder analysis we conduct an in-depth examination of clip s image and text encoders in multi-object scenes revealing weaknesses in preserving information for object distinction and identifying where compositional informa- tion is lost. bias identi\ufb01cation our study reveals that clip s im- age encoder prefers larger objects while the text encoder favors \ufb01rst-mentioned and visually larger objects high- lighting biases in clip s handling of visual and linguistic information. investigation of bias origins we explore the origins of these biases showing that larger objects are often men- tioned earlier in clip s training captions and are favored in embeddings due to the abundance of their visual to- kens. we substantiate this with analyses of the laion dataset and clip s training progression. practical impact we show how these biases affect per- formance in multi-object tasks with signi\ufb01cant drops in image-text matching accuracy in comco and coco 9 . these biases also extend to text-to-image models in\ufb02u- encing object prominence based on prompt order. these \ufb01ndings reveal how biases in clip s text and im- age encoders signi\ufb01cantly reduce its performance in multi- object scenarios emphasizing the need to address these bi- ases to enhance vision-language models robustness. our work offers key insights into clip s behavior and lays groundwork for improving model performance in real- world applications. 2. methodology 2.1. dataset design to thoroughly evaluate the performance of clip models in multi-object scenarios under controlled conditions we constructed the comco complex coco objects dataset. utilizing blender software allowed us precise control over the number location and dimensions of objects in the im- ages see appendix 7.1 . the comco dataset comprises 72 objects derived from the coco dataset. we generated 9309 page 3 images containing 2 3 4 and 5 objects. each image is paired with a speci\ufb01c caption that accurately describes the objects present. this approach ensures high control over the dataset and minimizes confounding factors providing a robust platform for evaluating the clip models. we deliberately chose not to use text-to-image models for generating these datasets due to two main reasons. first these models often lack the capability to produce high- quality fully controlled multi-object images. second since clip is used in many of these models utilizing them could introduce unwanted biases into our evaluations. 2.2. experimental framework for encoder analy- sis the main goal of this study is to evaluate the performance of clip s text and image encoders separately in multi-object scenarios. we aim to analyze the impact and contribution of each object in the \ufb01nal output of the encoders. to achieve this we conducted experiments using our designed comco dataset with images and captions containing two to \ufb01ve ob- jects. to ensure the generalizability of our \ufb01ndings we also validated our results on the widely-used coco dataset. we designed two sets of experiments retrieval-based experi- ments and classi\ufb01cation-based experiments. given the con- sistency of the results in both types of experiments we have included the classi\ufb01cation results in the appendix 7.2 and 7.4 and explain the retrieval-based experiments bellow. 2.2.1. text-based object retrieval tor the text-based object retrieval task evaluates how well clip s text encoder can identify individual objects within multi-object captions. as illustrated in figure 2a this ex- periment involves several steps first we use clip s text encoder to create embeddings for both multi-object captions and single-object captions. we then measure the similar- ity between each multi-object caption embedding and all single-object caption embeddings. the single-object cap- tion with the highest similarity score is considered the re- trieved object. to assess performance we calculate re- trieval accuracy for each object position in the multi-object captions. this helps us identify any biases related to an object s position within a caption such as favoring objects mentioned \ufb01rst or last. 2.2.2. image-based object retrieval ior the image-based object retrieval task is similar to tor but focuses on clip s image encoder. as shown in fig- ure 2b this experiment involves several steps we begin by using clip s image encoder to generate embeddings for multi-object images and single-object images. we then compute similarity scores between each multi-object image embedding and all single-object image embeddings. the single-object image with the highest similarity score is con- sidered the retrieved object. to evaluate performance we calculate retrieval accuracy for different object size cate- gories e.g. large small within the multi-object images. this allows us to determine if the image encoder shows any preference for objects of a particular size. we also experimented with a variation of comco called simco where objects were replaced with simple geometric shapes from the clevr dataset. this was done to con\ufb01rm that bias persists even with non-natural geometric objects. further details are provided in appendix 7.1. 3. results and analysis our experiments revealed signi\ufb01cant biases in both the text and image encoders of the clip model. this section presents our \ufb01ndings organized by encoder type and focus- ing on retrieval tasks. 3.1. text encoder biases we observed a consistent bias in the text encoder towards the \ufb01rst object mentioned in descriptions. in the tor ex- periment the retrieval accuracy as shown in table 1 was highest for the \ufb01rst object indicating its dominant in\ufb02uence on the overall text representation. this suggests that the text encoder prioritizes the initial object leading to its more accurate retrieval compared to subsequent objects. the de- tailed results for the scenarios involving 2 3 and 5 objects can be found in the appendix 7.3 and experiments on longer caption templates are in appendix 7.6 and 7.7. 3.2. image encoder biases in multi-object images the image encoder exhibited a strong bias towards larger objects. the image-based ob- ject retrieval ior experiment detailed in table 2 shows that larger objects were more frequently and accurately re- trieved during single-object image searches. this \ufb01nding highlights the image encoder s bias towards larger objects which receive disproportionate emphasis in the \ufb01nal image representation. further detailed results speci\ufb01cally for sce- narios with 2 3 and 5 objects are provided in the appendix 7.5. 3.3. coco dataset experiments to validate the generalizability of our \ufb01ndings from the synthetic dataset we conducted similar experiments on the coco dataset which comprises real images with accom- panying captions. this real-world dataset allowed us to in- vestigate whether the previously observed biases persist in more naturalistic settings. due to the absence of single-object images for coco objects we approached the ior experiment in two ways. first we used single-object images from the domainnet dataset 11 as retrieval targets. second we introduced an alternative approach called image-to-text object retrieval i2tor . in i2tor we used the textual names of coco 9310 page 4 dvh pdjh b 7kuhh 2emhfwv pdjh 6lqjoh 2emhfw pdjh zklfk pdwfklqjb dvh pdjh 3 6fruh 3 6fruh 3 6fruh 2wkhub6lqjohb2emhfw pdjhv 3 6fruh 3 6fruh 3 6fruh e d 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3 6fruh 3 6fruh 3 6fruh 3 6fruh 3 6fruh 3 6fruh 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu dvh 7h w b 7kuhh 2emhfwv 7h w 6lqjoh 2emhfw 7h w zklfk pdwfklqjb dvh 7h w 2wkhub6lqjohb2emhfw 7h wv sl d dqgbdssoh dqgbghvn dssoh sl d ghvn kdw fdu d h 725 25 figure 2. experimental setup for text-based object retrieval tor and image-based object retrieval ior tasks. a tor the clip text encoder generates embeddings for multi-object and single-object texts. cosine similarity scores are calculated between the base text embedding and single-object text embeddings to identify the most similar object. b ior the clip image encoder generates embeddings for multi-object and single-object images. cosine similarity scores are calculated between the base image embedding and single-object image embeddings to identify the most similar object. table 1. performance on tor for comco datasets task model first obj second obj third obj fourth obj tor clip laion 63.96 21.59 10.68 3.76 clip datacomp 71.13 16.26 8.74 3.87 clip roberta 44.03 23.73 18.07 14.18 siglip 58.11 21.16 10.99 9.73 clip openai 50.31 20.74 14.45 6.79 negclip 51.63 28.92 14.86 4.59 sugarcrepe 44.29 30.32 18.73 6.66 table 2. performance on ior for comco datasets task model large object small obj 1 small obj 2 small obj 3 ior clip laion 85.45 6.36 5.45 2.73 clip datacomp 85.16 5.65 4.95 4.24 clip roberta 87.40 8.66 2.36 1.57 siglip 77.66 10.11 6.38 5.85 clip openai 65.22 17.39 8.70 8.70 negclip 61.67 15.00 13.33 10.00 sugarcrepe 60.0 18.38 16.85 4.7 objects instead of single-object images. these object names were embedded using clip s text encoder allowing us to perform a retrieval task consistent with the ior methodol- ogy while adapting to the constraints of the coco dataset. tables 3 and 4 present the results of our coco dataset experiments. in tor the \ufb01rst-mentioned object in coco captions was retrieved with higher accuracy which aligns with our earlier \ufb01ndings of bias in the text encoder. simi- larly in ior larger objects in coco images were retrieved more accurately consistent with the trends observed in our synthetic dataset experiments. the i2tor results further con\ufb01rmed this bias demonstrating that even when using textual object representations the bias towards larger ob- table 3. performance on tor for coco dataset task model first obj second obj third obj fourth obj tor clip openai 35.24 21.90 20.48 22.38 clip laion 67.89 13.76 8.26 10.09 clip datacomp 57.68 17.68 12.75 11.88 clip roberta 40.78 23.30 20.39 15.53 siglip 49.47 26.84 12.11 11.58 negclip 38.69 22.11 17.09 22.11 table 4. performance on ior for coco dataset task model large object small obj 1 small obj 2 small obj 3 ior clip openai 43.02 28.82 17.13 11.03 clip laion 39.44 28.45 17.70 14.41 clip datacomp 36.71 29.55 19.13 14.61 clip roberta 36.71 28.61 19.82 14.86 siglip 36.63 28.29 20.02 15.06 negclip 44.04 28.86 16.48 10.62 i2tor clip openai 51.49 24.87 13.68 9.97 clip laion 45.50 27.02 15.91 11.56 clip datacomp 46.64 26.82 14.53 12.01 clip roberta 44.69 26.98 16.04 12.29 siglip 47.09 27.07 15.10 10.74 negclip 49.04 27.07 14.08 9.81 jects persists. our experiments reveal two signi\ufb01cant biases in the clip model the text encoder shows a strong preference for the \ufb01rst mentioned object in textual descriptions while the image encoder exhibits greater sensitivity to larger objects in images. these biases can signi\ufb01cantly impact the overall system performance in various vision-language tasks par- ticularly in multi-object scenarios. 9311 page 5 4. origin of bias in clip models in this section we investigate the potential origins of the biases observed in clip models and provide evidence sup- porting our hypotheses. 4.1. bias in the image encoder the observed bias favoring larger objects within the image domain can be attributed to the architectural characteristics of vision transformers vit 2 utilized in clip s image encoder. our hypothesis is that larger objects which occupy a greater number of patches in the vit s patch-based image representation exert a more signi\ufb01cant in\ufb02uence on the \ufb01- nal class cls token representation. this bias is not exclu- sive to clip it appears to be a consistent feature across vit models as demonstrated by our experiments detailed in the appendix. to substantiate this hypothesis we designed an experi- ment to quantify the attention allocated by the cls token to each image patch. by calculating the cumulative atten- tion received by each object from the cls token we could assess the in\ufb02uence of object size on attention allocation. we applied this analysis to our three-object comco dataset and the results are illustrated in figure 3. the \ufb01ndings con- \ufb01rm our hypothesis larger objects indeed receive more at- tention from the cls token. 4.2. bias in the text encoder we explore the bias present in the text encoder from two perspectives the attention mechanism in the model struc- ture and the model s training method. 4.2.1. impact of attention mechanism text encoder models can be categorized based on their at- tention mechanisms uni-directional causal attention and bi-directional attention. in models with causal attention each token attends only to preceding tokens whereas in bi- directional models each token attends to all tokens in the sequence. when openai introduced the clip model its text en- coder employed causal attention meaning each token could only attend to tokens before it and itself. this differs from typical self-attention mechanisms where tokens at- tend to all other tokens. most clip models use causal self- attention with the exception of the variant using the xlm- roberta text encoder which also employs self-attention. however as shown in table 1 even this model exhibits the mentioned bias. this indicates that the bias does not origi- nate from the attention mechanism itself. 4.2.2. role of training method to determine whether the observed bias is speci\ufb01c to clip models we compared clip s text encoder with two other table 5. performance on toc and tor for comco datasets task model first obj second obj third obj fourth obj tor clip 56.28 22.71 13.17 7.48 sbert 29.02 19.80 17.50 33.57 simcse 7 27.59 19.07 17.76 34.83 models designed to embed sentences into a meaningful se- mantic space sentence-bert sbert 14 and simcse 7 . the primary distinction is that clip s embedding space is shared between images and text whereas sbert and simcse operate solely in the text domain. we conducted the tor experiment on our dataset using these models. as presented in table 5 the bias observed in clip differs from that in the other models. this suggests that clip s unique training method which aligns images and text in a shared embedding space through contrastive learning contributes to the bias. therefore to uncover the root cause of the bias we focus on the speci\ufb01cs of clip s training procedure. 4.3. hypothesized origin of text-side bias in clip we hypothesize that the text-side bias in clip which fa- vors objects mentioned earlier in text descriptions origi- nates from the image-side bias toward larger objects and is transferred to the text encoder during contrastive training. we present evidence supporting this hypothesis through two key claims and an analysis of the training progression. claim 1 larger objects have more in\ufb02uence on text embeddings. building upon the established image-side bias discussed earlier we posit that objects with larger physical sizes exert more in\ufb02uence on clip s text em- beddings due to the alignment enforced during contrastive training. to test this we categorized objects in the domain- net dataset into large medium and small groups based on their relative physical sizes in real-world with the full list of objects provided in the appendix 7.10 . speci\ufb01cally objects smaller than a school bag were categorized as small objects sized between a school bag and a medium-sized car were classi\ufb01ed as medium and objects larger than a car up to signi\ufb01cantly larger items were considered large. we then constructed two sets of sentences each containing four ob- jects one set with a large object mentioned \ufb01rst followed by three medium-sized objects and another with a small object mentioned \ufb01rst followed by three medium-sized objects. figure 4.a compares the tor accuracy for the \ufb01rst ob- ject in these two groups. the higher tor accuracy for sen- tences beginning with large objects supports our hypothe- sis that larger objects when mentioned \ufb01rst have a more signi\ufb01cant impact on the text embeddings due to the cross- modal alignment with their prominent representation in im- ages. 9312 page 6 a b figure 3. attention allocation from the cls token to objects of different sizes in the comco dataset. a qualitative results showing the cls token s attention to each object. b quantitative analysis of attention distribution across 8 000 images with each image containing one large and two small objects. the bar chart shows the average attention allocated to the large object versus the smaller ones demonstrating a bias towards larger objects. a b c figure 4. a top-1 object retrieval accuracy comparison for sentences where the \ufb01rst object is either large or small. the higher tor accuracy for sentences beginning with large objects supports the hypothesis that larger objects when mentioned \ufb01rst exert a stronger in\ufb02uence on text embeddings due to cross-modal alignment with their prominent visual representation in images. b distribution of the position of the largest object within image captions from the laion datasets. the results show a consistent bias where larger objects tend to be mentioned earlier in text descriptions. c progression of tor rates across different training stages indicating that text-side bias strengthens as the model is exposed to more data suggesting the cumulative effect of image-side bias being transferred to the text encoder through contrastive learning. claim 2 caption bias in training datasets. to inves- tigate potential biases in clip s training data we analyzed both the laion 19 and coco datasets. due to limited computational resources and the large size of the laion dataset which contains over 2 billion image-text pairs we randomly selected a subset of 200 000 samples for our anal- ysis. using the llama3 model we extracted objects from the image captions and employed the language segment- anything tool to generate object masks in the correspond- ing images calculating their areas based on these masks. a detailed description of our laion dataset analysis method- ology can be found in appendix 7.8. figure4.b shows the position of the largest object within each caption. the results indicate that in the majority of cases the largest object in an image is mentioned earlier in its caption. the same experiment was conducted on the coco dataset with detailed results and the distribution for two to \ufb01ve object scenarios provided in appendix 7.9. this demonstrates a consistent bias in the training data where larger objects are not only more visually prominent but are also described earlier in text annotations. analysis of bias development during training. to fur- ther validate our hypothesis we examined the progression of text-side bias during clip s training. we utilized model checkpoints from the laion dataset at \ufb01ve training stages corresponding to exposure to 2 4 6 8 and 10 billion sam- ples. we conducted tor experiments at each stage focus- 9313 page 7 ing on the retrieval accuracy for the \ufb01rst object mentioned in text descriptions. figure4.c depicts the evolution of the tor rate across different training stages for scenarios with varying numbers of objects from 3 to 8 . the consistent upward trend in the tor rate as the model is exposed to more training data suggests that the text-side bias strengthens over time likely due to the cumulative effect of the image-side bias being transferred to the text encoder through contrastive learning. incomplete text representation of clip here we want to theoretically highlight why the clip text encoder could learn an incomplete representation of the text. let z and w represent a latent representation of an image content de- scribed in the caption and such visual content not men- tioned in the text respectively. for example z represents the fact that an image contains a horse that is eating the grass. in this case w might represent other details in the image like the horse color where the horse is located etc. we assume a data generative process as follows i g z w t h z where i is the image and t is its corresponding caption. now we want to learn a joint embedding of the image and text through the clip. here we assume that f\u03b8 . and i\u03c9 . as learnable functions that map the image and text into the joint embedding space respectively. theorem 1 let elements of z be independent zero-mean and unit-variance. the contrastive loss for the ideal text en- coder i\u03c9 t z converges to that of a non-ideal incomplete one i.e. i\u03c9 t zs where zs is the \ufb01rst d k dimensions of z with k being a constant and d . proof the contrastive loss in making this learning hap- pen can be written as ez z w exp sim z z exp sim z z k exp sim z z k 1 with sim z z s f\u03b8 g z w i\u03c9 h z and z and z k 1 k b are b 1 i.i.d. samples of the content in the representation space and s is some normal- ized similarity metric e.g. cosine similarity and b 1 is the batch size. we assume that elements of z are independent unit-variance and zero mean. we further assume that the dimensionality of z denoted as d goes to in\ufb01nity. under such conditions and based on law of large numbers z p d when d is large. therefore for any two independent copies of z z k we have sim z z k z z k z z k p 0. it is evident that in the ideal case f\u03b8 g z w z and also i\u03c9 h z z so the contrastive loss would converge to e e b as the numerator is e and the second term in the denominator converges to exp 0 1 according to the mann-wald s theorem. however we show that other learning of this representa- tion could achieve the same amount of loss. for instance let zs be the \ufb01rst d k elements of z with k being a constant. we show that if f\u03b8 i zs and i\u03c9 t zs the same loss would be achieved in the limit of large d. to see this note that the numerator stays the same i.e. e while the second term in the denominator still converges to bexp 0 b. this means that even if the image and text encoder of the clip only partially recover the content embedding they reach an excellent loss. but such possible incomplete rep- resentations of z are combinatorially large making conver- gence of the clip to such local minima pretty likely. this makes the text encoding of clip be far from ideal. fur- thermore the text encoder would become biased depend- ing on which of such local minima it converges to. based on this explanation we would expect a text encoder that has learned a complete representation to exhibit such biases to a lesser degree. as mentioned earlier the subject of learning text representations in vlms that are discriminative of hard negatives e.g. negclip has been around for few years. we tested one of strongest such models 8 in our bench- mark to validate the hypothesis that an incomplete text rep- resentation is one of the causes of the bias in the vlms. we noticed that this model shows lower bias based on our benchmark see the sugarcrepe model in tables 1 and 2 . we have developed an initial approach to address the identi\ufb01ed bias in the clip model which is presented in appendix 7.12. while this method is speci\ufb01c to our cur- rent dataset it represents a promising step toward address- ing these challenges and can inspire further advancements. this work demonstrates our commitment to exploring prac- tical solutions while maintaining the primary focus of this study on the analysis of bias and its implications. 5. practical impacts of encoder biases the biases observed in clip s image and text encoders sig- ni\ufb01cantly impact model performance in real-world appli- cations. this section explores how these biases manifest in image-text matching tasks while further analyses of text-to- image generation impacts are presented in appendix 7.11. our analysis in this section serves two primary purposes. first it provides concrete evidence of how these theoretical biases can translate into practical limitations. second it of- fers insights into potential areas for improvement in vision- language models particularly in handling complex multi- 9314 page 8 figure 5. an example of the correct and incorrect caption structures in the \ufb01rst and second scenarios. object scenarios. through a series of carefully designed ex- periments we illustrate how the biases in both text and im- age encoders can lead to unexpected or suboptimal results in tasks that are crucial for many downstream applications. 5.1. image-text matching building upon our \ufb01ndings of biases in clip s image and text encoders we now demonstrate how these biases tangi- bly affect the model s performance in image-caption match- ing tasks. we designed two experimental scenarios con- ducted on both the comco and coco datasets to evaluate these biases. the results of these experiments are summa- rized in table 6. to better illustrate the differences between these two scenarios an example of the caption structures is shown in figure 5. in each scenario we created incorrect captions by switching one object in the caption with an ob- ject that is not present in the image. additionally gpt-4o 1 was used to rewrite the captions in the coco dataset. first scenario in the \ufb01rst scenario biases assist the model in distinguishing between the correct and incorrect captions. in the correct captions the largest object in the image is placed at the beginning aligning with the model s bias towards prioritizing \ufb01rst-mentioned objects and larger objects. for the incorrect captions the non-existent object is deliberately placed at the beginning which helps the model recognize the difference between the correct and incorrect captions more effectively. this positioning emphasizes the discrepancy early on allowing the model to better detect the mismatch between the caption and the image. the perfor- mance of different models in this scenario can be seen in table 6 under the first scenario column. second scenario in the second scenario biases lead the model to make errors. the correct captions place the largest object at the end of the sentence disrupting the model s bias towards objects mentioned earlier and its preference for larger objects. in the incorrect captions the non-existent object is placed at the end making it more dif\ufb01cult for the model to differentiate between correct and incorrect cap- tions as its attention is drawn away from the critical discrep- ancies. the performance of different models in this scenario is shown in table 6 under the second scenario column. table 6. performance comparison on image-text matching for comco and coco datasets dataset model first scenario second scenario comco clip datacomp 6 99.99 67.50 clip roberta 99.98 64.75 siglip 22 99.49 72.36 clip openai 99.59 52.23 negclip 96.82 46.94 sugarcrepe 98.55 60.43 coco clip datacomp 6 71.2 54.2 clip roberta 72.2 54.1 siglip 22 64.8 39.5 clip openai 63.5 26.4 negclip 72 28.7 sugarcrepe 80.0 40.9 by comparing these two scenarios we demonstrate that biases in clip can either help or hinder the model s perfor- mance depending on how captions are structured. the ex- perimental results particularly with the use of gpt-4o for caption rephrasing in the coco dataset reveal how such biases can in\ufb02uence the accuracy of image-text matching tasks. these biases must be addressed to improve clip s robustness in real-world multi-object scenarios. for further insights on how these biases affect text-to- image generation refer to our extended experiments in ap- pendix 7.11. 6. conclusion this study uncovers biases in clip s encoders with the text encoder favoring \ufb01rst-mentioned objects and the im- age encoder emphasizing larger ones which impacts per- formance in multi-object tasks. using the comco dataset we highlighted these biases effects on object representation and positioning underscoring the need for balanced train- ing. we attribute these biases to clip s contrastive frame- work where alignment issues propagate across modalities. addressing these biases is essential for vision-language ad- vancements as seen with models like stable diffusion. 9315 page 9 references 1 josh achiam steven adler sandhini agarwal lama ah- mad ilge akkaya florencia leoni aleman diogo almeida janko altenschmidt sam altman shyamal anadkat et al. gpt-4 technical report. arxiv preprint arxiv 2303.08774 2023. 8 2 dosovitskiy alexey. an image is worth 16x16 words trans- formers for image recognition at scale. arxiv preprint arxiv 2010.11929 2020. 5 3 mehdi cherti romain beaumont ross wightman mitchell wortsman gabriel ilharco cade gordon christoph schuh- mann ludwig schmidt and jenia jitsev. reproducible scaling laws for contrastive language-image learning. in 2023 ieee cvf conference on computer vision and pat- tern recognition cvpr . ieee 2023. 1 4 sri harsha dumpala aman jaiswal chandramouli sas- try evangelos milios sageev oore and hassan saj- jad. sugarcrepe dataset vision-language model sensi- tivity to semantic and lexical alterations. arxiv preprint arxiv 2406.11171 2024. 1 5 samir yitzhak gadre gabriel ilharco alex fang jonathan hayase georgios smyrnis thao nguyen ryan marten mitchell wortsman dhruba ghosh jieyu zhang eyal or- gad rahim entezari giannis daras sarah pratt vivek ramanujan yonatan bitton kalyani marathe stephen mussmann richard vencu mehdi cherti ranjay krishna pang wei koh olga saukh alexander ratner shuran song hannaneh hajishirzi ali farhadi romain beaumont sewoong oh alex dimakis jenia jitsev yair carmon vaishaal shankar and ludwig schmidt. datacomp in search of the next generation of multimodal datasets 2023. 1 6 samir yitzhak gadre gabriel ilharco alex fang jonathan hayase georgios smyrnis thao nguyen ryan marten mitchell wortsman dhruba ghosh jieyu zhang et al. dat- acomp in search of the next generation of multimodal datasets. advances in neural information processing sys- tems 36 2024. 8 17 7 tianyu gao xingcheng yao and danqi chen. simcse simple contrastive learning of sentence embeddings. arxiv preprint arxiv 2104.08821 2021. 5 8 cheng-yu hsieh jieyu zhang zixian ma aniruddha kem- bhavi and ranjay krishna. sugarcrepe fixing hackable benchmarks for vision-language compositionality. advances in neural information processing systems 36 2024. 1 7 9 tsung-yi lin michael maire serge belongie lubomir bourdev ross girshick james hays pietro perona deva ramanan c. lawrence zitnick and piotr doll ar. microsoft coco common objects in context 2015. 2 10 zixian ma jerry hong mustafa omer gul mona gandhi irena gao and ranjay krishna. crepe can vision-language foundation models reason compositionally in proceedings of the ieee cvf conference on computer vision and pat- tern recognition pages 10910 10921 2023. 1 11 xingchao peng qinxun bai xide xia zijun huang kate saenko and bo wang. moment matching for multi-source domain adaptation. in proceedings of the ieee cvf inter- national conference on computer vision pages 1406 1415 2019. 3 12 dustin podell zion english kyle lacey andreas blattmann tim dockhorn jonas m uller joe penna and robin rombach. sdxl improving latent diffusion mod- els for high-resolution image synthesis. arxiv preprint arxiv 2307.01952 2023. 17 13 alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sastry amanda askell pamela mishkin jack clark gretchen krueger and ilya sutskever. learning transferable visual models from natural language supervision 2021. 1 14 n reimers. sentence-bert sentence embeddings using siamese bert-networks. arxiv preprint arxiv 1908.10084 2019. 5 15 dillon reis jordan kupec jacqueline hong and ahmad daoudi. real-time \ufb02ying object detection with yolov8. arxiv preprint arxiv 2305.09972 2023. 17 16 robin rombach andreas blattmann dominik lorenz patrick esser and bj orn ommer. high-resolution image synthesis with latent diffusion models. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr pages 10684 10695 2022. 17 17 ugur sahin hang li qadeer khan daniel cremers and volker tresp. enhancing multimodal compositional reason- ing of visual language models with generative negative min- ing. in proceedings of the ieee cvf winter conference on applications of computer vision pages 5563 5573 2024. 1 18 christoph schuhmann richard vencu romain beaumont robert kaczmarczyk clayton mullis aarush katta theo coombes jenia jitsev and aran komatsuzaki. laion-400m open dataset of clip-\ufb01ltered 400 million image-text pairs 2021. 1 19 christoph schuhmann romain beaumont richard vencu cade gordon ross wightman mehdi cherti theo coombes aarush katta clayton mullis mitchell worts- man et al. laion-5b an open large-scale dataset for training next generation image-text models. advances in neural in- formation processing systems 35 25278 25294 2022. 6 20 tristan thrush ryan jiang max bartolo amanpreet singh adina williams douwe kiela and candace ross. winoground probing vision and language models for visio- linguistic compositionality. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 5238 5248 2022. 1 21 mert yuksekgonul federico bianchi pratyusha kalluri dan jurafsky and james zou. when and why vision- language models behave like bags-of-words and what to do about it in the eleventh international conference on learning representations 2023. 1 22 xiaohua zhai basil mustafa alexander kolesnikov and lucas beyer. sigmoid loss for language image pre-training. in proceedings of the ieee cvf international conference on computer vision pages 11975 11986 2023. 8 17 23 tiancheng zhao tianqi zhang mingwei zhu haozhan shen kyusong lee xiaopeng lu and jianwei yin. vl- checklist evaluating pre-trained vision-language models 9316 page 10 with objects attributes and relations. arxiv preprint arxiv 2207.00221 2022. 1 9317",
      "keywords": [
        "page",
        "clip",
        "under",
        "microscope",
        "fine-grained",
        "analysis",
        "multi-object",
        "representation",
        "reza",
        "abbasi",
        "ali",
        "nazari",
        "aminreza",
        "se\ufb01d",
        "mohammadali",
        "banayeeanzade",
        "mohammad",
        "hossein",
        "rohban",
        "mahdieh",
        "soleymani",
        "baghshah",
        "sharif",
        "university",
        "technology",
        "tehran",
        "iran",
        "reza.abbasi",
        "ali.nazari02",
        "aminreza.sefid",
        "a.banayeean",
        "sharif.edu",
        "abstract",
        "contrastive",
        "language-image",
        "pre-training",
        "mod-",
        "els",
        "excel",
        "zero-shot",
        "classi\ufb01cation",
        "yet",
        "face",
        "challenges",
        "complex",
        "scenarios.",
        "study",
        "offers",
        "com-",
        "prehensive",
        "limitations",
        "contexts",
        "using",
        "specialized",
        "dataset",
        "comco",
        "designed",
        "evaluate",
        "encoders",
        "diverse",
        "\ufb01ndings",
        "reveal",
        "signi\ufb01cant",
        "biases",
        "text",
        "encoder",
        "prior-",
        "itizes",
        "\ufb01rst-mentioned",
        "objects",
        "image",
        "fa-",
        "vors",
        "larger",
        "objects.",
        "through",
        "retrieval",
        "tasks",
        "quantify",
        "across",
        "multiple",
        "vari-",
        "ants",
        "trace",
        "origins",
        "training",
        "process",
        "sup-",
        "ported",
        "analyses",
        "laion",
        "pro-",
        "gression.",
        "image-text",
        "matching",
        "experiments",
        "show",
        "sub-",
        "stantial",
        "performance"
      ],
      "metadata": {
        "filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf",
        "original_filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 1583559,
        "upload_file_id": "upload_1752169139828_ag4sjzeg9",
        "upload_timestamp": "2025-07-10T17:38:59.887630",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T17:38:59.999945+00:00",
      "content_length": 40912,
      "content_type": ".pdf",
      "filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf"
    },
    "24": {
      "file_id": "24",
      "content": "\n=== Page 1 ===\nTIDE: Training Locally Interpretable Domain Generalization Models\nEnables Test-time Correction\nAishwarya Agarwal1,2*\nSrikrishna Karanam2\u2020\nVineet Gandhi1\u2021\n1CVIT, Kohli Centre for Intelligent Systems, IIIT Hyderabad, India\n2Adobe Research, Bengaluru, India\nAbstract\nWe consider the problem of single-source domain gener-\nalization. Existing methods typically rely on extensive aug-\nmentations to synthetically cover diverse domains during\ntraining. However, they struggle with semantic shifts (e.g.,\nbackground and viewpoint changes), as they often learn\nglobal features instead of local concepts that tend to be do-\nmain invariant. To address this gap, we propose an ap-\nproach that compels models to leverage such local concepts\nduring prediction. Given no suitable dataset with per-class\nconcepts and localization maps exists, we first develop a\nnovel pipeline to generate annotations by exploiting the rich\nfeatures of diffusion and large-language models. Our next\ninnovation is TIDE, a novel training scheme with a con-\ncept saliency alignment loss that ensures model focus on the\nright per-concept regions and a local concept contrastive\nloss that promotes learning domain-invariant concept rep-\nresentations. This not only gives a robust model but also can\nbe visually interpreted using the predicted concept saliency\nmaps. Given these maps at test time, our final contribution\nis a new correction algorithm that uses the corresponding\nlocal concept representations to iteratively refine the pre-\ndiction until it aligns with prototypical concept representa-\ntions that we store at the end of model training. We evaluate\nour approach extensively on four standard DG benchmark\ndatasets and substantially outperform the current state-of-\nthe-art (12% improvement on average) while also demon-\nstrating that our predictions can be visually interpreted.\n1. Introduction\nEnhancing deep neural networks to generalize to out-of-\ndistribution samples remains a core challenge in machine\nlearning and computer vision research, as real-world test\ndata often diverges significantly from the training distribu-\n*aishwarya.agarwal@research.iiit.ac.in, aishagar@adobe.com\n\u2020skaranam@adobe.com\n\u2021vgandhi@iiit.ac.in\nTraining Domain\nTest Domain\nTraining Domain\nTest Domain\nABA\nTIDE\nbird\nperson\nbeak\nfeathers\neyes\nlips\n(Caltech101)\n(VOC2007)\n(Photo)\n(Sketch)\nFigure 1. Samples from VLCS (left) and PACS dataset (right)\nacross domain shifts, corresponding to bird and person class. First\nrow displays GradCAM maps [43] for ABA\u2019s class predictions.\nWe observe that model attention of ABA [5] falters across domain\nshifts. Second and third row display the concept specific Grad-\nCAM maps from TIDE. We posit that accurate concept learning\nand localization facilitates DG.\ntion [31]. The challenges compound when obtaining la-\nbeled samples from the target domain is expensive or un-\nfeasible, hindering application of semi-supervised learning\nor domain adaptation [25, 32, 49]. The problem of Do-\nmain Generalization (DG) [30, 44, 54, 61, 63] represents\na promising avenue for developing techniques that capture\ndomain-invariant patterns and improve performance on out-\nof-distribution samples. In this paper, we focus on Single\nSource Domain Generalization (SSDG), where a model is\ntrained on data from a single domain and aims to generalize\nwell to unseen domains [40, 51]. It represents the most strict\nform of DG, as the model must extract domain-invariant\nfeatures from a single, often limited perspective, without\nexposure to the variation present across multiple domains.\nMost previous work on SSDG [5, 7, 8, 19, 20, 22, 59, 63]\nrelies extensively on data augmentations to support the\nlearning of domain-generalized features. The premise is\nthat constructing an extensive repertoire of augmentations\nsynthesizes instances encompassing a wide spectrum of\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n30210\n\n=== Page 2 ===\nhuman-recognizable domains. However, accounting for all\nconceivable real-world augmentations presents an immense\nchallenge. These models have shown reasonable success in\naddressing variations in style and texture [45, 64]; however,\ntheir performance remains modest when faced with more\nsemantic domain shifts, such as changes in background and\nviewpoint e.g., in the VLCS dataset [41, 48].\nConsider the results of the state-of-the-art ABA [5]\nmodel on the two examples in Figure 1. In the first ex-\nample (left), domain shifts manifest as variations in back-\nground and viewpoint. In the second example (right), the\ntraining data comprises photos of human faces, whereas the\ntest data consists of skeletal sketches. In both cases, the do-\nmain shifts extend beyond style or texture variations, with\nABA failing to correctly classify the samples. Class-level\nsaliency maps [43] reveal that this misclassification stems\nfrom inadequate focus on critical local concepts under do-\nmain shifts, such as beak and feathers for birds, or\neyes and lips for persons. Such local concepts are inte-\ngral to class definition and remain invariant across domains,\nand a robust DG model must adeptly capture these stable\nfeatures. We posit that prior efforts learn global features per\nclass; and if the model fails to learn the correct feature set in\nthe training domain, its generalization performance is com-\npromised as noted from Figure 1 above. The inconsistency\nin concept localization further exacerbates the interpretabil-\nity and explainability of these models.\nWe adopt an alternative approach, wherein, rather than\nattempting to encompass all potential augmentations, we\ncompel the model to leverage essential class-specific con-\ncepts for classification. Our key idea is to force the model\nto attend to these concepts during training. The primary\nhurdle in this path is lack of annotated data specifying rele-\nvant class-specific concepts along with their corresponding\nlocalizations. To this end, our first contribution is a novel\npipeline that harnesses large language models (LLMs) and\ndiffusion models (DMs) to identify key concepts and gen-\nerate concept-level saliency maps in a scalable, automated\nfashion. We demonstrate that DMs, via cross-attention lay-\ners, can generate concept-level saliency maps for generated\nimages with notable granularity. Given these maps for a sin-\ngle synthesized image within a class, we leverage the rich\nfeature space of DMs to transfer them to images across di-\nverse domains in DG benchmarks [47].\nWe subsequently introduce TIDE, our second contribu-\ntion which employs cross-entropy losses over class and con-\ncept labels along with a novel pair of loss functions: a con-\ncept saliency alignment loss that ensures the model attends\nto the correct regions for each local concept, and a local\nconcept contrastive loss that promotes learning domain in-\nvariant features for these regions. As shown in Figure 1,\nTIDE consistently attends to local concepts such as beak\nand eyes in the training domain as well as across substan-\ntial domain shifts. This precise localization significantly\nbolsters SSDG performance while enabling the generation\nof visual explanations for model decisions.\nMarking a major leap in model interpretability and per-\nformance gains, our third contribution is to demonstrate\nhow our model\u2019s concept-level localization can be effec-\ntively leveraged for performance verification and test-time\ncorrection. To this end, we introduce the notion of local\nconcept signatures: prototypical features derived by pool-\ning concept-level features across training samples, guided\nby corresponding saliency maps. If the concept features\nassociated with the predicted class label do not align with\ntheir signatures, it signals the use of incorrect features for\nprediction. Consequently, we employ iterative refinement\nthrough concept saliency masking until concept predictions\nalign with their corresponding signatures. More formally,\nour paper makes following contributions:\n\u2022 We propose a novel synthetic annotation pipeline to au-\ntomatically identify key per-class local concepts and their\nlocalization maps, and transfer them to real images found\nin DG benchmarks.\n\u2022 We propose a novel approach, TIDE, to train locally in-\nterpretable models with a novel concept saliency align-\nment loss that ensures accurate concept localization and a\nnovel local concept contrastive loss that ensures domain-\ninvariant features.\n\u2022 We show TIDE enables correcting model prediction at\ntest time using the predicted concept saliency maps as part\nof a novel iterative attention refinement strategy.\n\u2022 We report extensive results on four standard DG bench-\nmarks, outperforming the current state-of-the-art by a sig-\nnificant 12% on average.\n2. Related Works\nMulti Source domain generalization (MSDG) methods as-\nsume access to multiple source domains and domain-\nspecific knowledge during training and have proposed ways\nto learn multi-source domain-invariant features [10, 15, 34,\n35, 53], utilize domain labels to capture domain shifts [11,\n35, 56], and design target-specific augmentation strate-\ngies [14, 16, 23]. However, these assumptions are not prac-\ntical for real-world applications [44, 57] and we instead fo-\ncus on the more challenging SSDG setting, where only one\nsource domain and no prior target knowledge is available.\nMost SSDG approaches have used augmentations to im-\nprove DG [5, 7, 8, 19, 20, 22, 59, 63]. While [59] also uses\ndiffusion models (DMs) [42] for data augmentation, our ap-\nproach differs by leveraging the rich feature space of DMs\nprimarily for offline saliency map annotation instead of aug-\nmentation. More recent work in SSDG [4, 28, 29] has also\nturned to approaches based on domain-specific prompting\nand causal inference instead of data augmentations. How-\never, these methods learn global features, limiting invari-\n30211\n\n=== Page 3 ===\nLocalized Contrastive Training\nTest-time correction\nTraining Phase\nAnchor\nPositive\nNegative\nFC Layers\nperson\nClass Prediction\nActivations\nGradients\nFC Layers\neyes\nlips\n\ud835\udc58!\n\ud835\udc58\"\n\ud835\udc66 \"\n\ud835\udc66 #\n\ud835\udc50\nConcept Prediction and Alignment \nActivations\nGradients\n\ud835\udc53$#\nGlobal Avg Pool\nguitar\n\ud835\udc50\nstrings\nknobs\n\ud835\udc58!\n\ud835\udc58\"\nSuppress irrelevant features\nDetect  mis-classification\n\ud835\udc39$\nCNN\n\ud835\udc53$!\n#\n\ud835\udc53$\"\n#\n\ud835\udc46$\n##\n\ud835\udc3a$\n##\n(\ud835\udc65 %) \n(\ud835\udc65 & )\n(\ud835\udc65)  \nCNN\n\ud835\udc39$\"\n\ud835\udc39$!\nCNN\nFeature \nMaps\n\ud835\udc66 \", (\n\ud835\udc66 #, (\n\ud835\udc46$\n#$, (\n\ud835\udc46$\n#%, ( \n\ud835\udc46$\n\", (\n\ud835\udc36\n\ud835\udc4a\n\ud835\udc3b\n\ud835\udc40 \", (\nTest Image (\ud835\udc65)  \n\ud835\udc36\n1\n\u2112 \"\n\u2112 #\n\u2112 $%&\n\u2112 '$$\nFigure 2. The TIDE pipeline: Left\u2014Training on a single domain with cross-entropy losses for class (Lc) and concept labels (Lk), alongside\nConcept Saliency Alignment (LCSA) and Local Concept Contrastive losses (LLCC). Right\u2014Test-time correction strategy applied in TIDE.\nance across unseen domains. Finally, these methods also do\nnot provide any signals to interpret model predictions.\nOn the other hand, while methods like Concept Bottle-\nneck Models (CBMs) [24, 33] also learn local concepts,\nthey are restricted to a predefined set of concepts and more\ncrucially, cannot ground them to image regions that these\nconcepts represent. Recent methods [37, 58] have scaled\nCBMs to a large number of classes and also proposed to\nways to learn multi-scale representations [52], but they are\nalso unable to connect these concepts to image pixels. Our\nSSDG method addresses these challenges by producing lo-\ncal concept saliency maps, enhancing both DG performance\nand model prediction interpretability.\n3. Methodology\nThe proposed framework is depicted in Figure 2. Training\ncomprises three components: class prediction, concept pre-\ndiction with saliency alignment, and localized contrastive\ntraining. During inference, class and concept predictions\nare integrated with a test-time correction mechanism.\nIn the concept alignment phase, we employ concept-\nlevel saliency maps as ground truth to enforce focus on in-\nterpretable features, using a saliency alignment loss. Do-\nmain invariance is further promoted by localized contrastive\ntraining, where we train the model to cluster similar con-\ncepts (e.g., eyes across augmentations) while separating\nunrelated concepts (e.g., strings). Finally, a test-time\ncorrection mechanism iteratively refines attention by sup-\npressing irrelevant regions, leveraging concept-signatures to\nredirect focus and improve classification accuracy. We pro-\nceed with a concise review of the key notations, followed\nby an in-depth exposition of our approach and the pipeline\nfor generating ground truth concept-level annotations.\nWe assume the source data is observed from a single\ndomain, denoted as D = {xi, yc\ni , yk\ni }N\ni=1, where xi repre-\nsents the i-th image, yc\ni the class label, yk\ni the concept label,\nand N the total sample count in the source domain. The\nshared CNN backbone is used to obtain the backbone fea-\ntures Fx \u2208RW \u00d7H\u00d7C. The automatically generated ground\ntruth concept-level saliency maps are denoted by Gk\nx. The\nGradCAM [43] saliency maps corresponding to class and\nconcepts labels are denoted by Sc\nx and Sk\nx respectively.\n3.1. Generating Concept-level Annotations\nDuring training, we aim our model to identify and spatially\nattend to stable, discriminative regions. However, existing\nDG datasets lack fine-grained, concept-level annotations for\nsuch regions. To address this, we propose a novel pipeline\nthat uses LLMs and DMs to automate scalable concept-level\nsaliency map generation.\nOur primary insight is that DMs can be harnessed to gen-\nerate high-quality, concept-level saliency maps for synthe-\nsized images. Extracting cross-attention maps [1, 3] from\na DM, given the prompt with concepts and correspond-\ning synthesized image, yields highly granular concept-level\nsaliency maps. Figure 3 demonstrates how DMs emphasize\nspecific regions for distinct concepts, capturing fine-grained\nattention to features such as a cat\u2019s whiskers or a snow-\nman\u2019s hands. With this technique serving as an efficient\ntool for yielding concept maps for synthesized images, the\nensuing questions are: (i) how to automate the identification\nof pertinent concepts for each class across datasets, and (ii)\nhow to transfer these concept maps to real-world images to\ngenerate ground-truth annotations.\nFirst, to identify the key concepts associated with each\nclass, we use GPT-3.5 [2], which we prompt to generate a\n30212\n\n=== Page 4 ===\nGenerated Image\nExtracted Cross Attention Maps\nPrompt: A photo of a cat with its whiskers, ears, eyes\nPrompt: A photo of a snowman with its nose, eyes, hands\nFigure 3. The first column displays the image generated from the\ngiven prompt, while the subsequent three columns show the cross-\nattention maps corresponding to each concept in the prompt.\nlist of distinctive, stable features (prompt in the supplement)\nfor each class. For instance, GPT-3.5 outputs concepts such\nas whiskers, ears, and eyes for a cat. We generate\na prompt leveraging these concepts, which is then used to\nsynthesize an exemplar image for each class. We further de-\nrive the corresponding concept-level attention maps as out-\nlined in the preceding paragraph.\nGiven a single synthesized exemplar image for each class\nand the concept-level saliency maps, we turn to the task\nof transferring these saliency annotations to real-world im-\nages.\nDM\u2019s feature space is particularly well-suited for\nthis, as it captures detailed, semantically-rich representa-\ntions that allow us to match synthetic concept regions to real\nimages across domains. Leveraging this, we use the Diffu-\nsion Feature Transfer (DIFT) method [47], which computes\npixel-level correspondences by comparing cosine similar-\nities between DM features.\nThrough this approach, we\nestablish a region-to-region correspondence between syn-\nthetic saliency maps (e.g., mouth of a dog) and similar\nregions in real-world images. This enables us to generate\ncomprehensive, consistent concept-level annotations across\ndomains as shown in Figure 4.\nWe obtain these concept-level annotations for widely-\nused benchmarks, including VLCS [13], PACS [27], Do-\nmainNet [38], and Office-Home [50]. For an image x, the\nbinarized concept-level maps (Gk\nx) are henceforth referred\nto as ground-truth saliency maps (GT-maps) in this paper.\nHaving established a process for generating saliency maps,\nthe next challenge is identifying the subset of concepts that\nthe model actually relies on for making predictions.\n3.1.1\nConcepts that matter\nWe restrict our method to essential concepts, filtering out\nthose that do not contribute meaningfully to classification.\nSynthesized \nExemplar\nPACS \u2013 photo\nPACS \u2013 sketch\nPACS \u2013 painting\nmouth\nPACS \u2013 cartoon\near\nPrompt: A photo of a dog with its ear, mouth\nFigure 4. The first row presents the prompt, corresponding synthe-\nsized image and attention maps for the concepts ear and mouth.\nBelow, we demonstrate that using diffusion features correspon-\ndences these concept saliency maps from a single exemplar can be\nautomatically transferred on dog images across domains.\nTo do this, we train a ResNet-18 [18] classifier on the source\ndomain [26], compute GradCAM [43] saliency maps for\neach class label, and use them to identify regions the model\nfocuses on when making predictions.\nWe compute the overlap between the GradCAM maps\nand GT-maps for each known concept in the dataset. Given\nimage x, its saliency map Sc\nx for class c and the GT-map Gk\nx\nfor concept k, we define the overlap Ok\nc (x) as:\n  \nO _c^ k\n(\nx) = \\fra\nc {\\ sum  _\n{i,j } \\\nm\nin (S^c_x (i\n,j), G^k_x(i,j))}{\\sum _{i,j} G^k_x(i,j)}. \n(1)\nwhere (i, j) are matrix indices. This measures how much\nof the model\u2019s attention for a given class aligns with the\nregions corresponding to concept k. We compute this over-\nlap for all concepts and images in the training set, and for\neach class c, we define the set of important concepts Kc as\nthose that consistently exhibit high overlap with the saliency\nmaps:\n  \\\nm\na\nthcal\n \n{K\n}\n_c =\n \\\nl eft  \\\n{\n k \\ \\middle |\\ \\frac {1}{N_c} \\sum _{x \\in \\mathcal {D}_c} O_c^k(x) > \\tau \\right \\}, \n(2)\nwhere Nc is the number of training examples in class c,\nDc is the set of images in class c, and \u03c4 is a threshold that\ndetermines the importance of the concept. This procedure\nfunctions as a concept discovery module, identifying which\n30213\n\n=== Page 5 ===\nlocal concepts are critical for predicting each class.\n3.2. TIDE\nAs shown in Figure 2, TIDE utilizes cross-entropy losses\nLc and Lk for class and concept labels, respectively, inte-\ngrated with novel concept saliency alignment and local con-\ncept contrastive losses. We detail these loss terms below.\n3.2.1\nConcept saliency alignment loss\nFor each image x, we predict important concepts Kc and\nenforce alignment between the saliency maps Sk\nx for pre-\ndicted concepts and the GT-maps Gk\nx. This is encouraged\nby our proposed concept saliency alignment (CSA) loss:\n  \\m a\nt\nhcal\n \n{L}_\n{\\t\ne x t \n{CS\nA}} = \\frac {1}{|\\mathcal {K}_c|} \\sum _{k \\in \\mathcal {K}_c} \\| S_x^k - G_x^k \\|_2^2. \n(3)\nAligning the model\u2019s attention with GT-maps enables\nclass-specific reasoning, thereby elucidating the rationale\nbehind predictions by linking them to relevant local fea-\ntures.\n3.2.2\nLocal concept contrastive loss\nWhile the CSA loss facilitates explicit localization of con-\ncepts, it is equally essential for these concept-level features\nto exhibit invariance across domain shifts. To achieve do-\nmain invariance, we propose a local concept contrastive\n(LCC) loss, employing a triplet strategy to cluster similar\nconcepts (e.g., eyes) across domains while distinguishing\nunrelated ones (e.g., feathers and ears).\nLet x be an anchor image containing concept k, a posi-\ntive image x+ (an augmentation of x that retains concept k),\nand a negative image x\u2212(containing a different concept k\u2032).\nFor each image, we compute a concept-specific feature vec-\ntor f k\nx \u2208RC, emphasizing the concept\u2019s relevant regions\nusing the GT-map Gk\nx. Each element f k\nx(l) is computed as:\n  \nf_x^ k\n(\nl) \n= \n\\ s um _{ i, j} G_x^k \\cdot F_x(i,j,l). \\label {eqn:fx_compute} \n(4)\nwhere \u00b7 is an element-wise multiplication operation, Gk\nx\nis a matrix representing the GT-map, Fx represents convo-\nlutional feature maps from the last convolution layer, and\n(i, j, l) denotes the lth channel\u2019s (i, j) element. These vec-\ntors focus on the concepts of interest (concept k for x and\nx+, k\u2032 for x\u2212), in contrast to the global features used in\nprior works. The LCC loss is then defined as:\n  \\m a thcal {L} _\n{\\ t e\nxt { LCC }\n} =  \\\nmax  (0, d(f_x^k, f_{x^+}^k) - d(f_x^k, f_{x^-}^{k'}) + \\alpha ), \n(5)\nwhere d(.) is the euclidean distance and \u03b1 is the margin.\nAlgorithm 1 Iterative Test-time Correction\nInput: Test image x, initial class prediction c(0), signatures pk for each concept k\nParameter: Threshold \u03b4, max iterations T\nOutput: Corrected class prediction, cfinal\n1: Sc(0),0\nx\n\u2190GradCAM(x, c(0)) {Initialize saliency map for iteration t = 0}\n2: x0\nmasked \u2190x\n3: for each k \u2208Kc(0) do\n4:\nf k,0\nx\n\u2190P\ni,j Sk\nx \u00b7 Fx(i, j, :)\n5:\nd(f k,0\nx\n, pk) \u21901 \u2212\nfk,0\nx\n\u00b7pk\n\u2225fk,0\nx\n\u2225\u2225pk\u2225\n6:\nif d(f k,0\nx\n, pk) > \u03b4 then\n7:\nCorrection Phase:\n8:\nfor t \u21901 to T do\n9:\nM c(t\u22121),t \u2190Binarize(Sc(t\u22121),t\u22121\nx\n)\n10:\nxt\nmasked \u2190xt\u22121\nmasked \u00b7 M c(t\u22121),t\n11:\nc(t) \u2190PredictClass(xt\nmasked)\n12:\nSc(t),t\nxmasked \u2190GradCAM(xt\nmasked, c(t))\n13:\nfor each k \u2208Kc(t) do\n14:\nf k,t\nxmasked \u2190P\ni,j Sk,t\nxmasked \u00b7 Fx(i, j, :)\n15:\nd(f k,t\nxmasked, pk) \u21901 \u2212\nfk,t\nxmasked \u00b7pk\n\u2225fk,t\nxmasked \u2225\u2225pk\u2225\n16:\nif d(f k,t\nxmasked, pk) \u2264\u03b4 then\n17:\nReturn cfinal \u2190c(t)\n18:\nend if\n19:\nend for\n20:\nend for\n21:\nend if\n22:\nReturn cfinal \u2190c(0)\n23: end for\n3.3. Test-time Correction\nWe establish that our localized, interpretable approach facil-\nitates correction of misclassifications through concept-level\nfeature verification. In this section, we first introduce con-\ncept signatures and detail the proposed correction strategy.\n3.3.1\nLocal concept signatures\nFor each concept k, we define a concept-signature pk \u2208\nRC, as its representative vector. We derive pk by averaging\nthe concept-specific feature vectors f k\nx across all training\nsamples x \u2208D containing concept k (denoted as Dk).\n  p\n^\nk = \n\\\nfrac\n {\n1}{|\\mathcal {D}^k|} \\sum _{x \\in \\mathcal {D}^k} f^k_x. \n(6)\nThese vectors act as reference points, helping the model\nrecognize when its attention is aligned with the right con-\ncepts during prediction, even when encountering new, un-\nseen domains.\n3.3.2\nDetecting and Correcting misclassifications\nConsider the example of test-time correction in Figure 2,\nwhere the model misclassifies a person as a guitar. The pre-\ndicted class guitar, involves concepts like strings and\n30214\n\n=== Page 6 ===\nknobs, but these are absent in the image. As a result, the\nmodel erroneously focuses on irrelevant features (e.g., the\nperson\u2019s legs or the background), as reflected in the Grad-\nCAM saliency maps.\nTo address misclassifications, we employ a two-step ap-\nproach: possible mistake detection followed by correction.\nThe process is outlined in Algorithm 1. First, the model ex-\ntracts concept-level saliency maps (Sk\nx), for all the concepts\ncorresponding to the predicted class, which are then used to\ncompute concept-level features f k\nx (step 4). In Step 5 and 6,\nwe compare the features to the stored concept signatures; a\ndeviation exceeding \u03b4 signals a misclassification.\nOnce a misalignment is detected, the model enters an it-\nerative refinement phase. The features prominent for the\ncurrent class level predictions are masked through the cor-\nresponding saliency map Sc\nx (step 9 and 10). The masking\nprocess is cumulative, in effect utilizing all masks from the\nfirst to the tth iteration. The masked features are then used\nfor subsequent predictions (step 11). This process contin-\nues until the concepts corresponding to the predicted class\naligns with concept-signatures or the maximum iteration\ncount is reached (steps 8\u201316 in the algorithm). If alignment\nis achieved, the predicted class in that iteration is confirmed\nas the final output.\n4. Experiments\nDatasets: We conduct experiments on four widely used\ndatasets - PACS [27], VLCS [13], OfficeHome [50], and\nDomainNet [38], within the DomainBed [17] evaluation\nbenchmarks.\nPACS contains 9,991 images across 7\ncategories in four domains:\n\u2018sketch\u2019, \u2018photo\u2019, \u2018clipart\u2019,\nand \u2018painting\u2019. VLCS consists of 10,729 images over 5\ncategories and 4 domains. The domains are from VOC2007\n(V), LabelMe (L), Caltech101 (C), and SUN09 (S), with\ndomain shifts primarily driven by background and view-\npoint variations. OfficeHome, with 15,500 images across\n65 categories, emphasizes indoor classes across \u2018product\u2019,\n\u2018real\u2019, \u2018clipart\u2019, and \u2018art\u2019 domains.\nFor DomainNet, we\nfollow prior work [39, 46] and use a subset of the 40\nmost common classes across \u2018sketch\u2019, \u2018real\u2019, \u2018clipart\u2019, and\n\u2018painting\u2019.\nExperimental Setup: We adhere to the SSDG paradigm,\ntraining on one source domain and testing across three\ntarget domains, i.e., the model is trained independently\nfour times, with the averaged test accuracies across target\ndomains reported in each case.\nTraining utilizes solely\nsource-domain GT-maps,\nexcluding any target-domain\nprior knowledge, data or annotations. Minimal augmen-\ntations (quantization, blurring, and canny edge), were\nused to introduce slight perturbations to create the triplets.\nWe use SDv2.1 [42] for generating exemplar images and\ncomputing cross-attention maps [1, 3].\nTo ensure a fair\ncomparison, we adopt a ResNet-18 backbone throughout\nall the experiments. We use the Adam optimizer with an\ninitial learning rate of 1 \u00d7 10\u22124 and a warm-up schedule\nover the first 1000 steps, after which the rate remains\nconstant. Margin value \u03b1 is set to 1.0. The batch size is\nset to 32. We empirically set \u03b4 = 0.1 and cap test-time\ncorrection at T = 10 iterations.\nCompared Approaches: We compare our method with\nERM [17] baseline and existing approaches that utilize\naugmentation-based techniques (NJPP [59], AugMix [19],\nMixStyle [63], CutMix [60], RandAugment [7]), self-\nsupervised and domain adaptation methods (RSC [22],\npAdaIn [36], L2D [55], RSC+ASR [12]), uncertainty mod-\neling (DSU [29], DSU-MAD [41]), attention and meta-\nlearning methods (ACVC [8], P-RC [6], Meta-Casual [4]),\nand prompt-based learning (PromptD [28]). The number\nof methods evaluated differs by dataset. The discrepancy\nstems from PACS being the dominant benchmark in prior\nSSDG works, resulting in a larger number of methods eval-\nuated on it. For VLCS, OfficeHome, and DomainNet, we\nrely on reported results from respective papers, or compute\nthem ourselves (where code was available) to ensure a fair\ncomparison.\n4.1. Main Results\nTable 1 provide a comparison of average classification ac-\ncuracy for our approach against existing methods across the\nPACS, VLCS, OfficeHome, and DomainNet datasets. Each\ncolumn in these tables represents a source domain used for\ntraining, with the numerical values indicating the average\naccuracy on the three target domains.\nThe last column\npresents the average of the four columns.\nTIDE decisively outperforms existing approaches across\nall datasets. It achieves average accuracy gains of 8.33%,\n13.37%, 16.16%, and 8.84% over the second-best ap-\nproach on PACS, VLCS, OfficeHome, and DomainNet, re-\nspectively.\nTable 1b details the performance on VLCS,\nwhere domain shifts primarily stem from background and\nviewpoint variations, with scenes spanning urban to ru-\nral and favoring non-standard viewpoints. Data augmenta-\ntion based methods, which focus on style variation, yield\nlimited gains on VLCS relative to PACS. Nonetheless,\nTIDE secures substantial improvements, notably achiev-\ning a 25.34% gain over DSU-MAD when Caltech101 (C)\nserves as the source domain. Another noteworthy obser-\nvation is that TIDE maintains strong performance across\nvarying class counts, from VLCS\u2019s 5 to OfficeHome\u2019s 65,\nunderscoring the scalability of the proposed approach.\n4.2. Ablations\nComponents of TIDE: Ablation experiments on the PACS\ndataset are conducted to assess the contribution of each loss\n30215\n\n=== Page 7 ===\nMethod Venue\nArt\nCartoon\nSketch\nPhoto\nAverage\nERM\n65.38\n64.20\n34.15\n33.65\n49.35\nAugmix [19] ICLR\u201921\n66.54\n70.16\n52.48\n38.30\n57.12\nRSC [22] ECCV\u201920\n73.40\n75.90\n56.20\n41.60\n61.03\nMixstyle [63] ICLR\u201921\n67.60\n70.38\n34.57\n37.44\n52.00\npAdaIn [36] CVPR\u201921\n64.96\n65.24\n32.04\n33.66\n49.72\nRSC+ASR [12] CVPR\u201921\n76.70\n79.30\n61.60\n54.60\n68.30\nL2D [55] ICCV\u201921\n76.91\n77.88\n53.66\n52.29\n65.93\nDSU [29] ICLR\u201922\n71.54\n74.51\n47.75\n42.10\n58.73\nACVC [8] CVPR\u201922\n73.68\n77.39\n55.30\n48.05\n63.10\nDSU-MAD [41] CVPR\u201923\n72.41\n74.47\n49.60\n44.15\n60.66\nP-RC [6] CVPR\u201923\n76.98\n78.54\n62.89\n57.11\n68.88\nMeta-Casual [4] CVPR\u201923\n77.13\n80.14\n62.55\n59.60\n69.86\nABA [5] ICCV\u201923\n75.69\n77.36\n54.12\n59.04\n66.30\nPromptD [28] CVPR\u201924\n78.77\n82.69\n62.94\n60.09\n71.87\nTIDE\n86.24\n86.37\n73.11\n74.36\n80.02\n(a) PACS\nMethod\nV\nL\nC\nS\nAverage\nERM\n76.72\n58.86\n44.95\n57.71\n59.06\nAugmix [19] ICLR\u201919\n75.25\n59.52\n45.90\n57.43\n59.03\npAdaIn [36] CVPR\u201921\n76.03\n65.21\n43.17\n57.94\n60.34\nMixstyle [63] ICLR\u201921\n75.73\n61.29\n44.66\n56.57\n59.06\nACVC [8] CVPR\u201922\n76.15\n61.23\n47.43\n60.18\n61.75\nDSU [29] ICLR\u201922\n76.93\n69.20\n46.54\n58.36\n62.11\nDSU-MAD [41] CVPR\u201923\n76.99\n70.85\n44.78\n62.23\n63.71\nTIDE\n82.62\n86.13\n70.12\n72.44\n77.08\n(b) VLCS\nMethod\nArt\nClipart\nProduct\nReal\nAverage\nERM\n57.43\n50.83\n48.9\n58.68\n53.96\nMixUp [21] ICLR\u201918\n50.41\n43.19\n41.24\n51.89\n46.93\nCutMix [60] ICCV\u201919\n49.17\n46.15\n41.2\n53.64\n47.04\nAugmix [19] ICLR\u201919\n56.86\n54.12\n52.02\n60.12\n56.03\nRandAugment [7] CVPRW\u201920\n58.07\n55.32\n52.02\n60.82\n56.56\nCutOut [9] arXiv:1708\n54.36\n50.79\n47.68\n58.24\n52.77\nRSC [22] ECCV\u201920\n53.51\n48.98\n47.16\n58.3\n52.73\nMEADA [62] NIPS\u201920\n57.0\n53.2\n48.81\n59.21\n54.80\nPixMix [20] CVPR\u201922\n53.77\n52.68\n48.91\n58.68\n53.51\nL2D [55] ICCV\u201921\n52.79\n48.97\n47.75\n58.31\n51.71\nACVC [8] CVPR\u201922\n54.3\n51.32\n47.69\n56.25\n52.89\nNJPP [59] ICML\u201924\n60.72\n54.95\n52.47\n61.26\n57.85\nTIDE\n72.32\n75.13\n75.22\n73.37\n74.01\n(c) OfficeHome\nMethod\nClipart\nPainting\nReal\nSketch\nAverage\nERM\n68.73\n66.12\n68.51\n69.44\n68.2\nMixUp [21] ICLR\u201918\n70.31\n64.34\n69.21\n68.82\n68.17\nCutMix [60] ICCV\u201919\n71.52\n63.84\n67.13\n69.41\n67.98\nAugmix [19] ICLR\u201919\n72.37\n62.91\n69.84\n71.22\n69.09\nRandAugment [7] CVPRW\u201920\n69.71\n65.51\n68.36\n66.93\n67.63\nCutOut [9] arXiv:1708\n70.86\n64.48\n69.92\n71.55\n69.20\nRSC [22] ECCV\u201920\n68.25\n67.91\n70.76\n66.18\n68.28\nPixMix [20] CVPR\u201922\n72.12\n63.51\n71.34\n65.46\n68.12\nNJPP [59] ICML\u201924\n76.14\n69.24\n76.61\n71.21\n73.3\nTIDE\n82.42\n80.37\n84.15\n81.61\n82.14\n(d) DomainNet\nTable 1. SSDG classification accuracies (%) on PACS, VLCS, Of-\nficeHome and DomainNet datasets, with ResNet-18 as backbone.\nEach column title indicates the source domain, and the numerical\nvalues represent the average performance in the target domains.\nterm on the overall performance. In Table 2, we present\nclassification accuracy by incrementally adding compo-\nnents to the training pipeline, demonstrating the contribu-\nMethod\nArt\nCartoon\nSketch\nPhoto\nAverage\nLc\n65.38\n64.20\n34.15\n33.65\n49.35\nLc + Lk\n65.47\n64.12\n35.44\n33.81\n49.71\nLc + Lk + LCSA\n66.11\n64.23\n35.14\n34.27\n49.93\nLc + Lk + LCSA + LLCC\n80.28\n82.91\n66.12\n65.37\n73.67\n+ test-time correction\n86.24\n86.37\n73.11\n74.36\n80.02\nTable 2. Ablation study (%) on PACS.\nphoto\nsketch\nphoto\ncartoon\nphoto\npainting\neyes\nhorns\nmouth\nwindows\n\ud835\udc73\ud835\udc73\ud835\udc6a\ud835\udc6a\nwithout\n\ud835\udc73\ud835\udc73\ud835\udc6a\ud835\udc6a\nwith\nFigure 5. t-SNE visualizations to demonstrate impact of LLCC.\nEach column represents a test domain (Sketch, Cartoon, Painting),\nwith the top row showing t-SNE plots without LLCC applied and\nthe bottom one with it. Please zoom in for optimal viewing.\ntion of each element to the model\u2019s performance. We ob-\nserve that the introduction of the concept classification loss\nLk and concept saliency alignment loss LCSA does not sig-\nnificantly affect the classifier\u2019s accuracy on the test domain.\nHowever, these components are integral to our approach,\nenabling test-time correction and, in turn, enhancing both\nperformance and model interpretability. We observe that\nthe introduction of the LLCC loss leads to a substantial in-\ncrease in test domain accuracy, clearly demonstrating its\neffectiveness in fostering domain invariance. Finally, we\ncompare results before and after test-time correction, high-\nlighting that even prior to correction, our method achieves\nstate-of-the-art performance across all source domains on\nPACS, underscoring the strength of our approach.\nError analysis of test-time correction: For this analy-\nsis, we train TIDE on the photo domain of PACS and test\nit on the sketch domain. The model gives an initial accu-\nracy of 74.79%, which improves to 82.29% post test-time\ncorrection. In 72.2% of test samples, TIDE does not invoke\ntest-time correction, with class predictions correct in 93.8%\nof these cases, demonstrating substantial reliability. The\nsupplement provides examples of cases where the model\nmisclassifications are not picked-up in the signature match-\ning step of test-time correction. In the remaining 27.8% of\ncases where correction is initiated, 52.5% successfully con-\nverge to the correct classification, significantly contributing\n30216\n\n=== Page 8 ===\nLabelMe (bird)\nSUN09 (chair)\nSketch (dog)\nCartoon (guitar)\nSketch (cat)\nPainting (mushroom)\nClipart (kettle)\nPACS\nVLCS\nOfficeHome\nDomainNet\nProduct (candles)\nstrings\nknobs\nsnout\nears\nbeak\nfeathers\nlegs\nseat\nopening\nhandle\nwick\nwick\nstem\ncap\nwhiskers\neyes\nFigure 6. Illustrative examples of concept level GradCAM maps corresponding to TIDE\u2019s predictions, across the four studied datasets. The\nconcept names are displayed beneath the maps, with the target domain and predicted class indicated on the left. More results in supplement.\nInitial Prediction: dog\nPost Correction: person\nears\nsnout\neyes\nlips\nInitial Prediction: elephant\nPost Correction: house\ntusks\ntrunk\nroof\nwindows\nFigure 7. The top row shows initial class predictions and Grad-\nCAM maps for the concepts, while the bottom row presents the\nresults after test-time correction.\nto the final post-correction accuracy of 82.29%.\nt-SNE plots: To demonstrate the impact of LLCC, we\npresent t-SNE visualizations in Figure 5, showing the dis-\ntribution of concept-specific vectors (as computed in Equa-\ntion 4). We individually plot the source domain (photo)\nalong with sketch, cartoon, and painting domains as tar-\ngets. Each case includes two plots: one without LLCC (top\nrow) and one with it (bottom row). We observe that with\nLLCC, concept samples (e.g., mouth) align closely across\ndomains, while distinct separation occurs between different\nconcepts (e.g., mouth and horns). Without LLCC, cluster\nseparation and alignment across domains are weaker, high-\nlighting its role in improving intra-concept compactness and\ninter-concept separability.\n4.3. Qualitative Results\nConcept Localization:\nWe train the SSDG model on\nphoto domain for PACS, OfficeHome, and DomainNet and\non Caltech101 for VLCS. The predicted class, concepts\nand corresponding saliency maps are shown in Figure 6.\nFor each target class (e.g., mushroom, kettle, guitar), the\nmodel reliably highlights key concept-specific regions (e.g.,\nstem, handle, strings) essential for classification.\nThe model effectively isolates key features across diverse\ncontexts, such as the legs and seat of a chair in com-\nplex zoomed-out scenes, the beak and feathers of cam-\nouflaged birds, the eyes and whiskers in deformed\nsketches of cats, and varying feature sizes, with the wick\noccupying a small area and the mushroom cap spanning\na larger one.\nTest-time Correction: The Figure 7 visually illustrates the\nefficacy of TIDE\u2019s test time correction abilities, by compar-\ning the initial and corrected results. The model initially mis-\nclassifies the images as dog and elephant, resulting in poorly\naligned concept-level saliency maps for corresponding con-\ncepts i.e. ears, snout, tusks and trunk. TIDE detects\nand rectifies such errors during inference, leading to accu-\nrate classification accompanied by precise concept localiza-\ntion. This qualitative evaluation reinforces the robustness of\nour approach in refining predictions and generating reliable,\nclass-specific concept maps.\n5. Conclusion\nIn this work, we considered the problem of single-source\ndomain generalization and observed that the current state-\nof-the-art methods fail in cases of semantic domain shifts\nprimarily due to the global nature of their learned features.\nTo alleviate this issue, we proposed TIDE, a new approach\nthat not only learns local concept representations but also\nproduces localization maps for these concepts. With these\nmaps, we showed we could visually interpret model deci-\nsions while also enabling correction of these decisions at\ntest time using our iterative attention refinement strategy.\nExtensive experimentation on standard benchmarks demon-\nstrated substantial and consistent performance gains over\nthe current state-of-the-art. Future work will rigorously ex-\nplore methods to generate confidence scores grounded in\nTIDE\u2019s concept verification strategy and would explore the\napplication of TIDE on other (e.g. fine-grained) classifica-\ntion datasets.\n30217\n\n=== Page 9 ===\nReferences\n[1] Aishwarya Agarwal,\nSrikrishna Karanam,\nKJ Joseph,\nApoorv Saxena, Koustava Goswami, and Balaji Vasan Srini-\nvasan. A-star: Test-time attention segregation and retention\nfor text-to-image synthesis. In International Conference on\nComputer Vision (ICCV), 2023. 3, 6\n[2] Tom B Brown. Language models are few-shot learners. Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2020. 3\n[3] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models. ACM\nTransactions on Graphics (TOG), 42(4):1\u201310, 2023. 3, 6\n[4] Jin Chen, Zhi Gao, Xinxiao Wu, and Jiebo Luo. Meta-causal\nlearning for single domain generalization. In Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 2,\n6, 7\n[5] Sheng Cheng, Tejas Gokhale, and Yezhou Yang. Adversar-\nial bayesian augmentation for single-source domain gener-\nalization. In International Conference on Computer Vision\n(ICCV), 2023. 1, 2, 7\n[6] Seokeon Choi, Debasmit Das, Sungha Choi, Seunghan Yang,\nHyunsin Park, and Sungrack Yun. Progressive random con-\nvolutions for single domain generalization. In Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 6,\n7\n[7] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In Conference on Computer\nVision and Pattern Recognition Workshops (CVPRW), 2020.\n1, 2, 6, 7\n[8] Ilke Cugu, Massimiliano Mancini, Yanbei Chen, and Zeynep\nAkata.\nAttention consistency on visual corruptions for\nsingle-source domain generalization. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2022. 1, 2, 6,\n7\n[9] Terrance Devries and Graham W. Taylor. Improved regular-\nization of convolutional neural networks with cutout. ArXiv,\nabs/1708.04552, 2017. 7\n[10] Yingjun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong\nZhen, Cees GM Snoek, and Ling Shao. Learning to learn\nwith variational information bottleneck for domain gener-\nalization.\nIn European Conference on Computer Vision\n(ECCV), 2020. 2\n[11] Antonio D\u2019Innocente and Barbara Caputo. Domain gener-\nalization with domain-specific aggregation modules. In The\nGerman Conference on Pattern Recognition, 2019. 2\n[12] Xinjie Fan, Qifei Wang, Junjie Ke, Feng Yang, Boqing Gong,\nand Mingyuan Zhou. Adversarially adaptive normalization\nfor single domain generalization. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2021. 6, 7\n[13] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased met-\nric learning: On the utilization of multiple datasets and web\nimages for softening bias. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2013. 4, 6\n[14] Robert Geirhos,\nPatricia Rubisch,\nClaudio Michaelis,\nMatthias Bethge, Felix A Wichmann, and Wieland Brendel.\nImagenet-trained cnns are biased towards texture; increas-\ning shape bias improves accuracy and robustness. In Inter-\nnational Conference on Learning Representations (ICLR),\n2019. 2\n[15] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang,\nand David Balduzzi. Domain generalization for object recog-\nnition with multi-task autoencoders. In International Confer-\nence on Computer Vision (ICCV), 2015. 2\n[16] Tejas Gokhale, Rushil Anirudh, Bhavya Kailkhura, Ja-\nyaraman J Thiagarajan, Chitta Baral, and Yezhou Yang.\nAttribute-guided adversarial training for robustness to nat-\nural perturbations. In Association for the Advancement of\nArtificial Intelligence (AAAI), 2021. 2\n[17] Ishaan Gulrajani and David Lopez-Paz. In search of lost do-\nmain generalization. International Conference on Learning\nRepresentations (ICLR), 2020. 6\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Conference\non Computer Vision and Pattern Recognition (CVPR), 2016.\n4\n[19] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph,\nJustin Gilmer, and Balaji Lakshminarayanan. Augmix: A\nsimple data processing method to improve robustness and\nuncertainty. International Conference on Learning Repre-\nsentations (ICLR), 2020. 1, 2, 6, 7\n[20] Dan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang,\nBo Li, Dawn Song, and Jacob Steinhardt. Pixmix: Dream-\nlike pictures comprehensively improve safety measures. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2022. 1, 2, 7\n[21] Yann\nN.\nDauphin\nDavid\nLopez-Paz\nHongyi\nZhang,\nMoustapha Cisse. mixup: Beyond empirical risk minimiza-\ntion. International Conference on Learning Representations\n(ICLR), 2018. 7\n[22] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang.\nSelf-challenging improves cross-domain generalization. In\nEuropean Conference on Computer Vision (ECCV), 2020. 1,\n2, 6, 7\n[23] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial transformer networks. Advances in Neural Informa-\ntion Processing Systems (NeurIPS), 2015. 2\n[24] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen\nMussmann, Emma Pierson, Been Kim, and Percy Liang.\nConcept bottleneck models. In International Conference on\nMachine Learning (ICML), 2020. 3\n[25] Wouter M Kouw and Marco Loog. A review of domain adap-\ntation without target labels. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (PAMI), 43(3):766\u2013785,\n2019. 1\n[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In Advances in Neural Information Processing Sys-\ntems (NeurIPS), 2012. 4\n[27] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M\nHospedales.\nDeeper, broader and artier domain general-\nization.\nIn International Conference on Computer Vision\n(ICCV), 2017. 4, 6\n30218\n\n=== Page 10 ===\n[28] Deng Li, Aming Wu, Yaowei Wang, and Yahong Han.\nPrompt-driven dynamic object-centric learning for single do-\nmain generalization. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2024. 2, 6, 7\n[29] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying\nShan, and Ling-Yu Duan.\nUncertainty modeling for out-\nof-distribution generalization. International Conference on\nLearning Representations (ICLR), 2022. 2, 6, 7\n[30] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang\nLiu, Kun Zhang, and Dacheng Tao.\nDeep domain gener-\nalization via conditional invariant adversarial networks. In\nEuropean Conference on Computer Vision (ECCV), 2018. 1\n[31] Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Ren-\nzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution\ngeneralization: A survey. ArXiv, abs/2108.13624, 2021. 1\n[32] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor-\ndan. Learning transferable features with deep adaptation net-\nworks. In International Conference on Machine Learning\n(ICML), 2015. 1\n[33] Andrei Margeloiu, Matthew Ashman, Umang Bhatt, Yanzhi\nChen, Mateja Jamnik, and Adrian Weller. Do concept bot-\ntleneck models learn as intended? International Conference\non Learning Representations (ICLR), 2021. 3\n[34] Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gi-\nanfranco Doretto.\nUnified deep supervised domain adap-\ntation and generalization.\nIn International Conference on\nComputer Vision (ICCV), 2017. 2\n[35] Krikamol\nMuandet,\nDavid\nBalduzzi,\nand\nBernhard\nSch\u00a8olkopf.\nDomain generalization via invariant fea-\nture representation. In International Conference on Machine\nLearning (ICML), 2013. 2\n[36] Oren Nuriel, Sagie Benaim, and Lior Wolf. Permuted adain:\nReducing the bias towards global statistics in image clas-\nsification. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021. 6, 7\n[37] Tuomas Oikarinen, Subhro Das, Lam M Nguyen, and Tsui-\nWei Weng. Label-free concept bottleneck models. Inter-\nnational Conference on Learning Representations (ICLR),\n2023. 3\n[38] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\nSaenko, and Bo Wang. Moment matching for multi-source\ndomain adaptation.\nIn International Conference on Com-\nputer Vision (ICCV), 2019. 4, 6\n[39] Daiqing Qi, Handong Zhao, Aidong Zhang, and Sheng Li.\nGeneralizing to unseen domains via text-guided augmenta-\ntion. In European Conference on Computer Vision (ECCV),\n2024. 6\n[40] Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn\nsingle domain generalization. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2020. 1\n[41] Sanqing Qu, Yingwei Pan, Guang Chen, Ting Yao, Changjun\nJiang, and Tao Mei. Modality-agnostic debiasing for single\ndomain generalization. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 2, 6, 7\n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2022. 2, 6\n[43] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam:\nVisual explanations from deep networks via\ngradient-based localization. In International Conference on\nComputer Vision (ICCV), 2017. 1, 2, 3, 4\n[44] Sarath Sivaprasad, Akshay Goindani, Mario Fritz, and Vi-\nneet Gandhi.\nClass-wise domain generalization: A novel\nframework for evaluating distributional shift.\nIn NeurIPS\nWorkshop on Distribution Shifts: Connecting Methods and\nApplications, 2022. 1, 2\n[45] Nathan Somavarapu, Chih-Yao Ma, and Zsolt Kira. Frus-\ntratingly simple domain generalization via image stylization.\nArXiv, abs/2006.11207, 2020. 2\n[46] Shuhan Tan, Xingchao Peng, and Kate Saenko.\nClass-\nimbalanced domain adaptation: An empirical odyssey. In\nEuropean Conference on Computer Vision Workshops (EC-\nCVW), 2020. 6\n[47] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan.\nEmergent correspondence\nfrom image diffusion. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2023. 2, 4\n[48] Antonio Torralba and Alexei A Efros.\nUnbiased look at\ndataset bias. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2011. 2\n[49] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.\nAdversarial discriminative domain adaptation.\nIn Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2017. 1\n[50] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty,\nand Sethuraman Panchanathan. Deep hashing network for\nunsupervised domain adaptation.\nIn Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2017. 4, 6\n[51] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C\nDuchi, Vittorio Murino, and Silvio Savarese. Generalizing\nto unseen domains via adversarial data augmentation. Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2018. 1\n[52] Bor-Shiun Wang, Chien-Yi Wang, and Wei-Chen Chiu.\nMcpnet: An interpretable classifier via multi-level concept\nprototypes. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2024. 3\n[53] Haohan Wang, Zexue He, Zachary C Lipton, and Eric P\nXing. Learning robust representations by projecting super-\nficial statistics out. International Conference on Learning\nRepresentations (ICLR), 2019. 2\n[54] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang,\nTao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and S Yu\nPhilip. Generalizing to unseen domains: A survey on do-\nmain generalization. IEEE transactions on knowledge and\ndata engineering, 35(8):8052\u20138072, 2022. 1\n[55] Zijian Wang, Yadan Luo, Ruihong Qiu, Zi Huang, and\nMahsa Baktashmotlagh. Learning to diversify for single do-\nmain generalization. In International Conference on Com-\nputer Vision (ICCV), 2021. 6, 7\n[56] Zehao Xiao, Jiayi Shen, Xiantong Zhen, Ling Shao, and\nCees Snoek. A bit more bayesian: Domain-invariant learning\nwith uncertainty. In International Conference on Machine\nLearning (ICML), 2021. 2\n30219\n\n=== Page 11 ===\n[57] Qinwei Xu, Ruipeng Zhang, Yi-Yan Wu, Ya Zhang, Ning\nLiu, and Yanfeng Wang.\nSimde: A simple domain ex-\npansion approach for single-source domain generalization.\nIn Conference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 2\n[58] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel\nJin, Chris Callison-Burch, and Mark Yatskar. Language in\na bottle: Language model guided concept bottlenecks for in-\nterpretable image classification. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2023. 3\n[59] Jianhao Yuan, Francesco Pinto, Adam Davies, and Philip\nTorr. Not just pretty pictures: Toward interventional data\naugmentation using text-to-image generators. International\nConference on Machine Learning (ICML), 2024. 1, 2, 6, 7\n[60] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localizable\nfeatures. In International Conference on Computer Vision\n(ICCV), 2019. 6, 7\n[61] Hanlin Zhang, Yi-Fan Zhang, Weiyang Liu, Adrian Weller,\nBernhard Sch\u00a8olkopf, and Eric P Xing. Towards principled\ndisentanglement for domain generalization. In Conference\non Computer Vision and Pattern Recognition (CVPR), 2022.\n1\n[62] Long Zhao, Ting Liu, Xi Peng, and Dimitris Metaxas.\nMaximum-entropy adversarial data augmentation for im-\nproved generalization and robustness. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2020. 7\n[63] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do-\nmain generalization with mixstyle. International Conference\non Learning Representations (ICLR), 2021. 1, 2, 6, 7\n[64] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xi-\nang.\nMixstyle neural networks for domain generalization\nand adaptation. International Journal of Computer Vision\n(IJCV), 132(3):822\u2013836, 2024. 2\n30220\n",
      "searchable_content": "page 1 tide training locally interpretable domain generalization models enables test-time correction aishwarya agarwal1 2 srikrishna karanam2 vineet gandhi1 1cvit kohli centre for intelligent systems iiit hyderabad india 2adobe research bengaluru india abstract we consider the problem of single-source domain gener- alization. existing methods typically rely on extensive aug- mentations to synthetically cover diverse domains during training. however they struggle with semantic shifts e.g. background and viewpoint changes as they often learn global features instead of local concepts that tend to be do- main invariant. to address this gap we propose an ap- proach that compels models to leverage such local concepts during prediction. given no suitable dataset with per-class concepts and localization maps exists we first develop a novel pipeline to generate annotations by exploiting the rich features of diffusion and large-language models. our next innovation is tide a novel training scheme with a con- cept saliency alignment loss that ensures model focus on the right per-concept regions and a local concept contrastive loss that promotes learning domain-invariant concept rep- resentations. this not only gives a robust model but also can be visually interpreted using the predicted concept saliency maps. given these maps at test time our final contribution is a new correction algorithm that uses the corresponding local concept representations to iteratively refine the pre- diction until it aligns with prototypical concept representa- tions that we store at the end of model training. we evaluate our approach extensively on four standard dg benchmark datasets and substantially outperform the current state-of- the-art 12 improvement on average while also demon- strating that our predictions can be visually interpreted. 1. introduction enhancing deep neural networks to generalize to out-of- distribution samples remains a core challenge in machine learning and computer vision research as real-world test data often diverges significantly from the training distribu- aishwarya.agarwal research.iiit.ac.in aishagar adobe.com skaranam adobe.com vgandhi iiit.ac.in training domain test domain training domain test domain aba tide bird person beak feathers eyes lips caltech101 voc2007 photo sketch figure 1. samples from vlcs left and pacs dataset right across domain shifts corresponding to bird and person class. first row displays gradcam maps 43 for aba s class predictions. we observe that model attention of aba 5 falters across domain shifts. second and third row display the concept specific grad- cam maps from tide. we posit that accurate concept learning and localization facilitates dg. tion 31 . the challenges compound when obtaining la- beled samples from the target domain is expensive or un- feasible hindering application of semi-supervised learning or domain adaptation 25 32 49 . the problem of do- main generalization dg 30 44 54 61 63 represents a promising avenue for developing techniques that capture domain-invariant patterns and improve performance on out- of-distribution samples. in this paper we focus on single source domain generalization ssdg where a model is trained on data from a single domain and aims to generalize well to unseen domains 40 51 . it represents the most strict form of dg as the model must extract domain-invariant features from a single often limited perspective without exposure to the variation present across multiple domains. most previous work on ssdg 5 7 8 19 20 22 59 63 relies extensively on data augmentations to support the learning of domain-generalized features. the premise is that constructing an extensive repertoire of augmentations synthesizes instances encompassing a wide spectrum of this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 30210 page 2 human-recognizable domains. however accounting for all conceivable real-world augmentations presents an immense challenge. these models have shown reasonable success in addressing variations in style and texture 45 64 however their performance remains modest when faced with more semantic domain shifts such as changes in background and viewpoint e.g. in the vlcs dataset 41 48 . consider the results of the state-of-the-art aba 5 model on the two examples in figure 1. in the first ex- ample left domain shifts manifest as variations in back- ground and viewpoint. in the second example right the training data comprises photos of human faces whereas the test data consists of skeletal sketches. in both cases the do- main shifts extend beyond style or texture variations with aba failing to correctly classify the samples. class-level saliency maps 43 reveal that this misclassification stems from inadequate focus on critical local concepts under do- main shifts such as beak and feathers for birds or eyes and lips for persons. such local concepts are inte- gral to class definition and remain invariant across domains and a robust dg model must adeptly capture these stable features. we posit that prior efforts learn global features per class and if the model fails to learn the correct feature set in the training domain its generalization performance is com- promised as noted from figure 1 above. the inconsistency in concept localization further exacerbates the interpretabil- ity and explainability of these models. we adopt an alternative approach wherein rather than attempting to encompass all potential augmentations we compel the model to leverage essential class-specific con- cepts for classification. our key idea is to force the model to attend to these concepts during training. the primary hurdle in this path is lack of annotated data specifying rele- vant class-specific concepts along with their corresponding localizations. to this end our first contribution is a novel pipeline that harnesses large language models llms and diffusion models dms to identify key concepts and gen- erate concept-level saliency maps in a scalable automated fashion. we demonstrate that dms via cross-attention lay- ers can generate concept-level saliency maps for generated images with notable granularity. given these maps for a sin- gle synthesized image within a class we leverage the rich feature space of dms to transfer them to images across di- verse domains in dg benchmarks 47 . we subsequently introduce tide our second contribu- tion which employs cross-entropy losses over class and con- cept labels along with a novel pair of loss functions a con- cept saliency alignment loss that ensures the model attends to the correct regions for each local concept and a local concept contrastive loss that promotes learning domain in- variant features for these regions. as shown in figure 1 tide consistently attends to local concepts such as beak and eyes in the training domain as well as across substan- tial domain shifts. this precise localization significantly bolsters ssdg performance while enabling the generation of visual explanations for model decisions. marking a major leap in model interpretability and per- formance gains our third contribution is to demonstrate how our model s concept-level localization can be effec- tively leveraged for performance verification and test-time correction. to this end we introduce the notion of local concept signatures prototypical features derived by pool- ing concept-level features across training samples guided by corresponding saliency maps. if the concept features associated with the predicted class label do not align with their signatures it signals the use of incorrect features for prediction. consequently we employ iterative refinement through concept saliency masking until concept predictions align with their corresponding signatures. more formally our paper makes following contributions we propose a novel synthetic annotation pipeline to au- tomatically identify key per-class local concepts and their localization maps and transfer them to real images found in dg benchmarks. we propose a novel approach tide to train locally in- terpretable models with a novel concept saliency align- ment loss that ensures accurate concept localization and a novel local concept contrastive loss that ensures domain- invariant features. we show tide enables correcting model prediction at test time using the predicted concept saliency maps as part of a novel iterative attention refinement strategy. we report extensive results on four standard dg bench- marks outperforming the current state-of-the-art by a sig- nificant 12 on average. 2. related works multi source domain generalization msdg methods as- sume access to multiple source domains and domain- specific knowledge during training and have proposed ways to learn multi-source domain-invariant features 10 15 34 35 53 utilize domain labels to capture domain shifts 11 35 56 and design target-specific augmentation strate- gies 14 16 23 . however these assumptions are not prac- tical for real-world applications 44 57 and we instead fo- cus on the more challenging ssdg setting where only one source domain and no prior target knowledge is available. most ssdg approaches have used augmentations to im- prove dg 5 7 8 19 20 22 59 63 . while 59 also uses diffusion models dms 42 for data augmentation our ap- proach differs by leveraging the rich feature space of dms primarily for offline saliency map annotation instead of aug- mentation. more recent work in ssdg 4 28 29 has also turned to approaches based on domain-specific prompting and causal inference instead of data augmentations. how- ever these methods learn global features limiting invari- 30211 page 3 localized contrastive training test-time correction training phase anchor positive negative fc layers person class prediction activations gradients fc layers eyes lips \ud835\udc58 \ud835\udc58 \ud835\udc66 \ud835\udc66 \ud835\udc50 concept prediction and alignment activations gradients \ud835\udc53 global avg pool guitar \ud835\udc50 strings knobs \ud835\udc58 \ud835\udc58 suppress irrelevant features detect mis-classification \ud835\udc39 cnn \ud835\udc53 \ud835\udc53 \ud835\udc46 \ud835\udc3a \ud835\udc65 \ud835\udc65 \ud835\udc65 cnn \ud835\udc39 \ud835\udc39 cnn feature maps \ud835\udc66 \ud835\udc66 \ud835\udc46 \ud835\udc46 \ud835\udc46 \ud835\udc36 \ud835\udc4a \ud835\udc3b \ud835\udc40 test image \ud835\udc65 \ud835\udc36 1 \u2112 \u2112 \u2112 \u2112 figure 2. the tide pipeline left training on a single domain with cross-entropy losses for class lc and concept labels lk alongside concept saliency alignment lcsa and local concept contrastive losses llcc . right test-time correction strategy applied in tide. ance across unseen domains. finally these methods also do not provide any signals to interpret model predictions. on the other hand while methods like concept bottle- neck models cbms 24 33 also learn local concepts they are restricted to a predefined set of concepts and more crucially cannot ground them to image regions that these concepts represent. recent methods 37 58 have scaled cbms to a large number of classes and also proposed to ways to learn multi-scale representations 52 but they are also unable to connect these concepts to image pixels. our ssdg method addresses these challenges by producing lo- cal concept saliency maps enhancing both dg performance and model prediction interpretability. 3. methodology the proposed framework is depicted in figure 2. training comprises three components class prediction concept pre- diction with saliency alignment and localized contrastive training. during inference class and concept predictions are integrated with a test-time correction mechanism. in the concept alignment phase we employ concept- level saliency maps as ground truth to enforce focus on in- terpretable features using a saliency alignment loss. do- main invariance is further promoted by localized contrastive training where we train the model to cluster similar con- cepts e.g. eyes across augmentations while separating unrelated concepts e.g. strings . finally a test-time correction mechanism iteratively refines attention by sup- pressing irrelevant regions leveraging concept-signatures to redirect focus and improve classification accuracy. we pro- ceed with a concise review of the key notations followed by an in-depth exposition of our approach and the pipeline for generating ground truth concept-level annotations. we assume the source data is observed from a single domain denoted as d xi yc i yk i n i 1 where xi repre- sents the i-th image yc i the class label yk i the concept label and n the total sample count in the source domain. the shared cnn backbone is used to obtain the backbone fea- tures fx rw h c. the automatically generated ground truth concept-level saliency maps are denoted by gk x. the gradcam 43 saliency maps corresponding to class and concepts labels are denoted by sc x and sk x respectively. 3.1. generating concept-level annotations during training we aim our model to identify and spatially attend to stable discriminative regions. however existing dg datasets lack fine-grained concept-level annotations for such regions. to address this we propose a novel pipeline that uses llms and dms to automate scalable concept-level saliency map generation. our primary insight is that dms can be harnessed to gen- erate high-quality concept-level saliency maps for synthe- sized images. extracting cross-attention maps 1 3 from a dm given the prompt with concepts and correspond- ing synthesized image yields highly granular concept-level saliency maps. figure 3 demonstrates how dms emphasize specific regions for distinct concepts capturing fine-grained attention to features such as a cat s whiskers or a snow- man s hands. with this technique serving as an efficient tool for yielding concept maps for synthesized images the ensuing questions are i how to automate the identification of pertinent concepts for each class across datasets and ii how to transfer these concept maps to real-world images to generate ground-truth annotations. first to identify the key concepts associated with each class we use gpt-3.5 2 which we prompt to generate a 30212 page 4 generated image extracted cross attention maps prompt a photo of a cat with its whiskers ears eyes prompt a photo of a snowman with its nose eyes hands figure 3. the first column displays the image generated from the given prompt while the subsequent three columns show the cross- attention maps corresponding to each concept in the prompt. list of distinctive stable features prompt in the supplement for each class. for instance gpt-3.5 outputs concepts such as whiskers ears and eyes for a cat. we generate a prompt leveraging these concepts which is then used to synthesize an exemplar image for each class. we further de- rive the corresponding concept-level attention maps as out- lined in the preceding paragraph. given a single synthesized exemplar image for each class and the concept-level saliency maps we turn to the task of transferring these saliency annotations to real-world im- ages. dm s feature space is particularly well-suited for this as it captures detailed semantically-rich representa- tions that allow us to match synthetic concept regions to real images across domains. leveraging this we use the diffu- sion feature transfer dift method 47 which computes pixel-level correspondences by comparing cosine similar- ities between dm features. through this approach we establish a region-to-region correspondence between syn- thetic saliency maps e.g. mouth of a dog and similar regions in real-world images. this enables us to generate comprehensive consistent concept-level annotations across domains as shown in figure 4. we obtain these concept-level annotations for widely- used benchmarks including vlcs 13 pacs 27 do- mainnet 38 and office-home 50 . for an image x the binarized concept-level maps gk x are henceforth referred to as ground-truth saliency maps gt-maps in this paper. having established a process for generating saliency maps the next challenge is identifying the subset of concepts that the model actually relies on for making predictions. 3.1.1 concepts that matter we restrict our method to essential concepts filtering out those that do not contribute meaningfully to classification. synthesized exemplar pacs photo pacs sketch pacs painting mouth pacs cartoon ear prompt a photo of a dog with its ear mouth figure 4. the first row presents the prompt corresponding synthe- sized image and attention maps for the concepts ear and mouth. below we demonstrate that using diffusion features correspon- dences these concept saliency maps from a single exemplar can be automatically transferred on dog images across domains. to do this we train a resnet-18 18 classifier on the source domain 26 compute gradcam 43 saliency maps for each class label and use them to identify regions the model focuses on when making predictions. we compute the overlap between the gradcam maps and gt-maps for each known concept in the dataset. given image x its saliency map sc x for class c and the gt-map gk x for concept k we define the overlap ok c x as o _c k x fra c sum _ i j m in s c_x i j g k_x i j sum _ i j g k_x i j . 1 where i j are matrix indices. this measures how much of the model s attention for a given class aligns with the regions corresponding to concept k. we compute this over- lap for all concepts and images in the training set and for each class c we define the set of important concepts kc as those that consistently exhibit high overlap with the saliency maps m a thcal k _c l eft k middle frac 1 n_c sum _ x in mathcal d _c o_c k x tau right 2 where nc is the number of training examples in class c dc is the set of images in class c and \u03c4 is a threshold that determines the importance of the concept. this procedure functions as a concept discovery module identifying which 30213 page 5 local concepts are critical for predicting each class. 3.2. tide as shown in figure 2 tide utilizes cross-entropy losses lc and lk for class and concept labels respectively inte- grated with novel concept saliency alignment and local con- cept contrastive losses. we detail these loss terms below. 3.2.1 concept saliency alignment loss for each image x we predict important concepts kc and enforce alignment between the saliency maps sk x for pre- dicted concepts and the gt-maps gk x. this is encouraged by our proposed concept saliency alignment csa loss m a t hcal l _ t e x t cs a frac 1 mathcal k _c sum _ k in mathcal k _c s_x k - g_x k _2 2. 3 aligning the model s attention with gt-maps enables class-specific reasoning thereby elucidating the rationale behind predictions by linking them to relevant local fea- tures. 3.2.2 local concept contrastive loss while the csa loss facilitates explicit localization of con- cepts it is equally essential for these concept-level features to exhibit invariance across domain shifts. to achieve do- main invariance we propose a local concept contrastive lcc loss employing a triplet strategy to cluster similar concepts e.g. eyes across domains while distinguishing unrelated ones e.g. feathers and ears . let x be an anchor image containing concept k a posi- tive image x an augmentation of x that retains concept k and a negative image x containing a different concept k . for each image we compute a concept-specific feature vec- tor f k x rc emphasizing the concept s relevant regions using the gt-map gk x. each element f k x l is computed as f_x k l s um _ i j g_x k cdot f_x i j l . label eqn fx_compute 4 where is an element-wise multiplication operation gk x is a matrix representing the gt-map fx represents convo- lutional feature maps from the last convolution layer and i j l denotes the lth channel s i j element. these vec- tors focus on the concepts of interest concept k for x and x k for x in contrast to the global features used in prior works. the lcc loss is then defined as m a thcal l _ t e xt lcc max 0 d f_x k f_ x k - d f_x k f_ x - k alpha 5 where d . is the euclidean distance and \u03b1 is the margin. algorithm 1 iterative test-time correction input test image x initial class prediction c 0 signatures pk for each concept k parameter threshold \u03b4 max iterations t output corrected class prediction cfinal 1 sc 0 0 x gradcam x c 0 initialize saliency map for iteration t 0 2 x0 masked x 3 for each k kc 0 do 4 f k 0 x p i j sk x fx i j 5 d f k 0 x pk 1 fk 0 x pk fk 0 x pk 6 if d f k 0 x pk \u03b4 then 7 correction phase 8 for t 1 to t do 9 m c t 1 t binarize sc t 1 t 1 x 10 xt masked xt 1 masked m c t 1 t 11 c t predictclass xt masked 12 sc t t xmasked gradcam xt masked c t 13 for each k kc t do 14 f k t xmasked p i j sk t xmasked fx i j 15 d f k t xmasked pk 1 fk t xmasked pk fk t xmasked pk 16 if d f k t xmasked pk \u03b4 then 17 return cfinal c t 18 end if 19 end for 20 end for 21 end if 22 return cfinal c 0 23 end for 3.3. test-time correction we establish that our localized interpretable approach facil- itates correction of misclassifications through concept-level feature verification. in this section we first introduce con- cept signatures and detail the proposed correction strategy. 3.3.1 local concept signatures for each concept k we define a concept-signature pk rc as its representative vector. we derive pk by averaging the concept-specific feature vectors f k x across all training samples x d containing concept k denoted as dk . p k frac 1 mathcal d k sum _ x in mathcal d k f k_x. 6 these vectors act as reference points helping the model recognize when its attention is aligned with the right con- cepts during prediction even when encountering new un- seen domains. 3.3.2 detecting and correcting misclassifications consider the example of test-time correction in figure 2 where the model misclassifies a person as a guitar. the pre- dicted class guitar involves concepts like strings and 30214 page 6 knobs but these are absent in the image. as a result the model erroneously focuses on irrelevant features e.g. the person s legs or the background as reflected in the grad- cam saliency maps. to address misclassifications we employ a two-step ap- proach possible mistake detection followed by correction. the process is outlined in algorithm 1. first the model ex- tracts concept-level saliency maps sk x for all the concepts corresponding to the predicted class which are then used to compute concept-level features f k x step 4 . in step 5 and 6 we compare the features to the stored concept signatures a deviation exceeding \u03b4 signals a misclassification. once a misalignment is detected the model enters an it- erative refinement phase. the features prominent for the current class level predictions are masked through the cor- responding saliency map sc x step 9 and 10 . the masking process is cumulative in effect utilizing all masks from the first to the tth iteration. the masked features are then used for subsequent predictions step 11 . this process contin- ues until the concepts corresponding to the predicted class aligns with concept-signatures or the maximum iteration count is reached steps 8 16 in the algorithm . if alignment is achieved the predicted class in that iteration is confirmed as the final output. 4. experiments datasets we conduct experiments on four widely used datasets - pacs 27 vlcs 13 officehome 50 and domainnet 38 within the domainbed 17 evaluation benchmarks. pacs contains 9 991 images across 7 categories in four domains sketch photo clipart and painting . vlcs consists of 10 729 images over 5 categories and 4 domains. the domains are from voc2007 v labelme l caltech101 c and sun09 s with domain shifts primarily driven by background and view- point variations. officehome with 15 500 images across 65 categories emphasizes indoor classes across product real clipart and art domains. for domainnet we follow prior work 39 46 and use a subset of the 40 most common classes across sketch real clipart and painting . experimental setup we adhere to the ssdg paradigm training on one source domain and testing across three target domains i.e. the model is trained independently four times with the averaged test accuracies across target domains reported in each case. training utilizes solely source-domain gt-maps excluding any target-domain prior knowledge data or annotations. minimal augmen- tations quantization blurring and canny edge were used to introduce slight perturbations to create the triplets. we use sdv2.1 42 for generating exemplar images and computing cross-attention maps 1 3 . to ensure a fair comparison we adopt a resnet-18 backbone throughout all the experiments. we use the adam optimizer with an initial learning rate of 1 10 4 and a warm-up schedule over the first 1000 steps after which the rate remains constant. margin value \u03b1 is set to 1.0. the batch size is set to 32. we empirically set \u03b4 0.1 and cap test-time correction at t 10 iterations. compared approaches we compare our method with erm 17 baseline and existing approaches that utilize augmentation-based techniques njpp 59 augmix 19 mixstyle 63 cutmix 60 randaugment 7 self- supervised and domain adaptation methods rsc 22 padain 36 l2d 55 rsc asr 12 uncertainty mod- eling dsu 29 dsu-mad 41 attention and meta- learning methods acvc 8 p-rc 6 meta-casual 4 and prompt-based learning promptd 28 . the number of methods evaluated differs by dataset. the discrepancy stems from pacs being the dominant benchmark in prior ssdg works resulting in a larger number of methods eval- uated on it. for vlcs officehome and domainnet we rely on reported results from respective papers or compute them ourselves where code was available to ensure a fair comparison. 4.1. main results table 1 provide a comparison of average classification ac- curacy for our approach against existing methods across the pacs vlcs officehome and domainnet datasets. each column in these tables represents a source domain used for training with the numerical values indicating the average accuracy on the three target domains. the last column presents the average of the four columns. tide decisively outperforms existing approaches across all datasets. it achieves average accuracy gains of 8.33 13.37 16.16 and 8.84 over the second-best ap- proach on pacs vlcs officehome and domainnet re- spectively. table 1b details the performance on vlcs where domain shifts primarily stem from background and viewpoint variations with scenes spanning urban to ru- ral and favoring non-standard viewpoints. data augmenta- tion based methods which focus on style variation yield limited gains on vlcs relative to pacs. nonetheless tide secures substantial improvements notably achiev- ing a 25.34 gain over dsu-mad when caltech101 c serves as the source domain. another noteworthy obser- vation is that tide maintains strong performance across varying class counts from vlcs s 5 to officehome s 65 underscoring the scalability of the proposed approach. 4.2. ablations components of tide ablation experiments on the pacs dataset are conducted to assess the contribution of each loss 30215 page 7 method venue art cartoon sketch photo average erm 65.38 64.20 34.15 33.65 49.35 augmix 19 iclr 21 66.54 70.16 52.48 38.30 57.12 rsc 22 eccv 20 73.40 75.90 56.20 41.60 61.03 mixstyle 63 iclr 21 67.60 70.38 34.57 37.44 52.00 padain 36 cvpr 21 64.96 65.24 32.04 33.66 49.72 rsc asr 12 cvpr 21 76.70 79.30 61.60 54.60 68.30 l2d 55 iccv 21 76.91 77.88 53.66 52.29 65.93 dsu 29 iclr 22 71.54 74.51 47.75 42.10 58.73 acvc 8 cvpr 22 73.68 77.39 55.30 48.05 63.10 dsu-mad 41 cvpr 23 72.41 74.47 49.60 44.15 60.66 p-rc 6 cvpr 23 76.98 78.54 62.89 57.11 68.88 meta-casual 4 cvpr 23 77.13 80.14 62.55 59.60 69.86 aba 5 iccv 23 75.69 77.36 54.12 59.04 66.30 promptd 28 cvpr 24 78.77 82.69 62.94 60.09 71.87 tide 86.24 86.37 73.11 74.36 80.02 a pacs method v l c s average erm 76.72 58.86 44.95 57.71 59.06 augmix 19 iclr 19 75.25 59.52 45.90 57.43 59.03 padain 36 cvpr 21 76.03 65.21 43.17 57.94 60.34 mixstyle 63 iclr 21 75.73 61.29 44.66 56.57 59.06 acvc 8 cvpr 22 76.15 61.23 47.43 60.18 61.75 dsu 29 iclr 22 76.93 69.20 46.54 58.36 62.11 dsu-mad 41 cvpr 23 76.99 70.85 44.78 62.23 63.71 tide 82.62 86.13 70.12 72.44 77.08 b vlcs method art clipart product real average erm 57.43 50.83 48.9 58.68 53.96 mixup 21 iclr 18 50.41 43.19 41.24 51.89 46.93 cutmix 60 iccv 19 49.17 46.15 41.2 53.64 47.04 augmix 19 iclr 19 56.86 54.12 52.02 60.12 56.03 randaugment 7 cvprw 20 58.07 55.32 52.02 60.82 56.56 cutout 9 arxiv 1708 54.36 50.79 47.68 58.24 52.77 rsc 22 eccv 20 53.51 48.98 47.16 58.3 52.73 meada 62 nips 20 57.0 53.2 48.81 59.21 54.80 pixmix 20 cvpr 22 53.77 52.68 48.91 58.68 53.51 l2d 55 iccv 21 52.79 48.97 47.75 58.31 51.71 acvc 8 cvpr 22 54.3 51.32 47.69 56.25 52.89 njpp 59 icml 24 60.72 54.95 52.47 61.26 57.85 tide 72.32 75.13 75.22 73.37 74.01 c officehome method clipart painting real sketch average erm 68.73 66.12 68.51 69.44 68.2 mixup 21 iclr 18 70.31 64.34 69.21 68.82 68.17 cutmix 60 iccv 19 71.52 63.84 67.13 69.41 67.98 augmix 19 iclr 19 72.37 62.91 69.84 71.22 69.09 randaugment 7 cvprw 20 69.71 65.51 68.36 66.93 67.63 cutout 9 arxiv 1708 70.86 64.48 69.92 71.55 69.20 rsc 22 eccv 20 68.25 67.91 70.76 66.18 68.28 pixmix 20 cvpr 22 72.12 63.51 71.34 65.46 68.12 njpp 59 icml 24 76.14 69.24 76.61 71.21 73.3 tide 82.42 80.37 84.15 81.61 82.14 d domainnet table 1. ssdg classification accuracies on pacs vlcs of- ficehome and domainnet datasets with resnet-18 as backbone. each column title indicates the source domain and the numerical values represent the average performance in the target domains. term on the overall performance. in table 2 we present classification accuracy by incrementally adding compo- nents to the training pipeline demonstrating the contribu- method art cartoon sketch photo average lc 65.38 64.20 34.15 33.65 49.35 lc lk 65.47 64.12 35.44 33.81 49.71 lc lk lcsa 66.11 64.23 35.14 34.27 49.93 lc lk lcsa llcc 80.28 82.91 66.12 65.37 73.67 test-time correction 86.24 86.37 73.11 74.36 80.02 table 2. ablation study on pacs. photo sketch photo cartoon photo painting eyes horns mouth windows \ud835\udc73\ud835\udc73\ud835\udc6a\ud835\udc6a without \ud835\udc73\ud835\udc73\ud835\udc6a\ud835\udc6a with figure 5. t-sne visualizations to demonstrate impact of llcc. each column represents a test domain sketch cartoon painting with the top row showing t-sne plots without llcc applied and the bottom one with it. please zoom in for optimal viewing. tion of each element to the model s performance. we ob- serve that the introduction of the concept classification loss lk and concept saliency alignment loss lcsa does not sig- nificantly affect the classifier s accuracy on the test domain. however these components are integral to our approach enabling test-time correction and in turn enhancing both performance and model interpretability. we observe that the introduction of the llcc loss leads to a substantial in- crease in test domain accuracy clearly demonstrating its effectiveness in fostering domain invariance. finally we compare results before and after test-time correction high- lighting that even prior to correction our method achieves state-of-the-art performance across all source domains on pacs underscoring the strength of our approach. error analysis of test-time correction for this analy- sis we train tide on the photo domain of pacs and test it on the sketch domain. the model gives an initial accu- racy of 74.79 which improves to 82.29 post test-time correction. in 72.2 of test samples tide does not invoke test-time correction with class predictions correct in 93.8 of these cases demonstrating substantial reliability. the supplement provides examples of cases where the model misclassifications are not picked-up in the signature match- ing step of test-time correction. in the remaining 27.8 of cases where correction is initiated 52.5 successfully con- verge to the correct classification significantly contributing 30216 page 8 labelme bird sun09 chair sketch dog cartoon guitar sketch cat painting mushroom clipart kettle pacs vlcs officehome domainnet product candles strings knobs snout ears beak feathers legs seat opening handle wick wick stem cap whiskers eyes figure 6. illustrative examples of concept level gradcam maps corresponding to tide s predictions across the four studied datasets. the concept names are displayed beneath the maps with the target domain and predicted class indicated on the left. more results in supplement. initial prediction dog post correction person ears snout eyes lips initial prediction elephant post correction house tusks trunk roof windows figure 7. the top row shows initial class predictions and grad- cam maps for the concepts while the bottom row presents the results after test-time correction. to the final post-correction accuracy of 82.29 . t-sne plots to demonstrate the impact of llcc we present t-sne visualizations in figure 5 showing the dis- tribution of concept-specific vectors as computed in equa- tion 4 . we individually plot the source domain photo along with sketch cartoon and painting domains as tar- gets. each case includes two plots one without llcc top row and one with it bottom row . we observe that with llcc concept samples e.g. mouth align closely across domains while distinct separation occurs between different concepts e.g. mouth and horns . without llcc cluster separation and alignment across domains are weaker high- lighting its role in improving intra-concept compactness and inter-concept separability. 4.3. qualitative results concept localization we train the ssdg model on photo domain for pacs officehome and domainnet and on caltech101 for vlcs. the predicted class concepts and corresponding saliency maps are shown in figure 6. for each target class e.g. mushroom kettle guitar the model reliably highlights key concept-specific regions e.g. stem handle strings essential for classification. the model effectively isolates key features across diverse contexts such as the legs and seat of a chair in com- plex zoomed-out scenes the beak and feathers of cam- ouflaged birds the eyes and whiskers in deformed sketches of cats and varying feature sizes with the wick occupying a small area and the mushroom cap spanning a larger one. test-time correction the figure 7 visually illustrates the efficacy of tide s test time correction abilities by compar- ing the initial and corrected results. the model initially mis- classifies the images as dog and elephant resulting in poorly aligned concept-level saliency maps for corresponding con- cepts i.e. ears snout tusks and trunk. tide detects and rectifies such errors during inference leading to accu- rate classification accompanied by precise concept localiza- tion. this qualitative evaluation reinforces the robustness of our approach in refining predictions and generating reliable class-specific concept maps. 5. conclusion in this work we considered the problem of single-source domain generalization and observed that the current state- of-the-art methods fail in cases of semantic domain shifts primarily due to the global nature of their learned features. to alleviate this issue we proposed tide a new approach that not only learns local concept representations but also produces localization maps for these concepts. with these maps we showed we could visually interpret model deci- sions while also enabling correction of these decisions at test time using our iterative attention refinement strategy. extensive experimentation on standard benchmarks demon- strated substantial and consistent performance gains over the current state-of-the-art. future work will rigorously ex- plore methods to generate confidence scores grounded in tide s concept verification strategy and would explore the application of tide on other e.g. fine-grained classifica- tion datasets. 30217 page 9 references 1 aishwarya agarwal srikrishna karanam kj joseph apoorv saxena koustava goswami and balaji vasan srini- vasan. a-star test-time attention segregation and retention for text-to-image synthesis. in international conference on computer vision iccv 2023. 3 6 2 tom b brown. language models are few-shot learners. ad- vances in neural information processing systems neurips 2020. 3 3 hila chefer yuval alaluf yael vinker lior wolf and daniel cohen-or. attend-and-excite attention-based se- mantic guidance for text-to-image diffusion models. acm transactions on graphics tog 42 4 1 10 2023. 3 6 4 jin chen zhi gao xinxiao wu and jiebo luo. meta-causal learning for single domain generalization. in conference on computer vision and pattern recognition cvpr 2023. 2 6 7 5 sheng cheng tejas gokhale and yezhou yang. adversar- ial bayesian augmentation for single-source domain gener- alization. in international conference on computer vision iccv 2023. 1 2 7 6 seokeon choi debasmit das sungha choi seunghan yang hyunsin park and sungrack yun. progressive random con- volutions for single domain generalization. in conference on computer vision and pattern recognition cvpr 2023. 6 7 7 ekin d cubuk barret zoph jonathon shlens and quoc v le. randaugment practical automated data augmentation with a reduced search space. in conference on computer vision and pattern recognition workshops cvprw 2020. 1 2 6 7 8 ilke cugu massimiliano mancini yanbei chen and zeynep akata. attention consistency on visual corruptions for single-source domain generalization. in conference on com- puter vision and pattern recognition cvpr 2022. 1 2 6 7 9 terrance devries and graham w. taylor. improved regular- ization of convolutional neural networks with cutout. arxiv abs 1708.04552 2017. 7 10 yingjun du jun xu huan xiong qiang qiu xiantong zhen cees gm snoek and ling shao. learning to learn with variational information bottleneck for domain gener- alization. in european conference on computer vision eccv 2020. 2 11 antonio d innocente and barbara caputo. domain gener- alization with domain-specific aggregation modules. in the german conference on pattern recognition 2019. 2 12 xinjie fan qifei wang junjie ke feng yang boqing gong and mingyuan zhou. adversarially adaptive normalization for single domain generalization. in conference on com- puter vision and pattern recognition cvpr 2021. 6 7 13 chen fang ye xu and daniel n rockmore. unbiased met- ric learning on the utilization of multiple datasets and web images for softening bias. in conference on computer vision and pattern recognition cvpr 2013. 4 6 14 robert geirhos patricia rubisch claudio michaelis matthias bethge felix a wichmann and wieland brendel. imagenet-trained cnns are biased towards texture increas- ing shape bias improves accuracy and robustness. in inter- national conference on learning representations iclr 2019. 2 15 muhammad ghifary w bastiaan kleijn mengjie zhang and david balduzzi. domain generalization for object recog- nition with multi-task autoencoders. in international confer- ence on computer vision iccv 2015. 2 16 tejas gokhale rushil anirudh bhavya kailkhura ja- yaraman j thiagarajan chitta baral and yezhou yang. attribute-guided adversarial training for robustness to nat- ural perturbations. in association for the advancement of artificial intelligence aaai 2021. 2 17 ishaan gulrajani and david lopez-paz. in search of lost do- main generalization. international conference on learning representations iclr 2020. 6 18 kaiming he xiangyu zhang shaoqing ren and jian sun. deep residual learning for image recognition. in conference on computer vision and pattern recognition cvpr 2016. 4 19 dan hendrycks norman mu ekin d cubuk barret zoph justin gilmer and balaji lakshminarayanan. augmix a simple data processing method to improve robustness and uncertainty. international conference on learning repre- sentations iclr 2020. 1 2 6 7 20 dan hendrycks andy zou mantas mazeika leonard tang bo li dawn song and jacob steinhardt. pixmix dream- like pictures comprehensively improve safety measures. in conference on computer vision and pattern recognition cvpr 2022. 1 2 7 21 yann n. dauphin david lopez-paz hongyi zhang moustapha cisse. mixup beyond empirical risk minimiza- tion. international conference on learning representations iclr 2018. 7 22 zeyi huang haohan wang eric p xing and dong huang. self-challenging improves cross-domain generalization. in european conference on computer vision eccv 2020. 1 2 6 7 23 max jaderberg karen simonyan andrew zisserman et al. spatial transformer networks. advances in neural informa- tion processing systems neurips 2015. 2 24 pang wei koh thao nguyen yew siang tang stephen mussmann emma pierson been kim and percy liang. concept bottleneck models. in international conference on machine learning icml 2020. 3 25 wouter m kouw and marco loog. a review of domain adap- tation without target labels. ieee transactions on pattern analysis and machine intelligence pami 43 3 766 785 2019. 1 26 alex krizhevsky ilya sutskever and geoffrey e hinton. imagenet classification with deep convolutional neural net- works. in advances in neural information processing sys- tems neurips 2012. 4 27 da li yongxin yang yi-zhe song and timothy m hospedales. deeper broader and artier domain general- ization. in international conference on computer vision iccv 2017. 4 6 30218 page 10 28 deng li aming wu yaowei wang and yahong han. prompt-driven dynamic object-centric learning for single do- main generalization. in conference on computer vision and pattern recognition cvpr 2024. 2 6 7 29 xiaotong li yongxing dai yixiao ge jun liu ying shan and ling-yu duan. uncertainty modeling for out- of-distribution generalization. international conference on learning representations iclr 2022. 2 6 7 30 ya li xinmei tian mingming gong yajing liu tongliang liu kun zhang and dacheng tao. deep domain gener- alization via conditional invariant adversarial networks. in european conference on computer vision eccv 2018. 1 31 jiashuo liu zheyan shen yue he xingxuan zhang ren- zhe xu han yu and peng cui. towards out-of-distribution generalization a survey. arxiv abs 2108.13624 2021. 1 32 mingsheng long yue cao jianmin wang and michael jor- dan. learning transferable features with deep adaptation net- works. in international conference on machine learning icml 2015. 1 33 andrei margeloiu matthew ashman umang bhatt yanzhi chen mateja jamnik and adrian weller. do concept bot- tleneck models learn as intended international conference on learning representations iclr 2021. 3 34 saeid motiian marco piccirilli donald a adjeroh and gi- anfranco doretto. unified deep supervised domain adap- tation and generalization. in international conference on computer vision iccv 2017. 2 35 krikamol muandet david balduzzi and bernhard sch olkopf. domain generalization via invariant fea- ture representation. in international conference on machine learning icml 2013. 2 36 oren nuriel sagie benaim and lior wolf. permuted adain reducing the bias towards global statistics in image clas- sification. in conference on computer vision and pattern recognition cvpr 2021. 6 7 37 tuomas oikarinen subhro das lam m nguyen and tsui- wei weng. label-free concept bottleneck models. inter- national conference on learning representations iclr 2023. 3 38 xingchao peng qinxun bai xide xia zijun huang kate saenko and bo wang. moment matching for multi-source domain adaptation. in international conference on com- puter vision iccv 2019. 4 6 39 daiqing qi handong zhao aidong zhang and sheng li. generalizing to unseen domains via text-guided augmenta- tion. in european conference on computer vision eccv 2024. 6 40 fengchun qiao long zhao and xi peng. learning to learn single domain generalization. in conference on computer vision and pattern recognition cvpr 2020. 1 41 sanqing qu yingwei pan guang chen ting yao changjun jiang and tao mei. modality-agnostic debiasing for single domain generalization. in conference on computer vision and pattern recognition cvpr 2023. 2 6 7 42 robin rombach andreas blattmann dominik lorenz patrick esser and bj orn ommer. high-resolution image syn- thesis with latent diffusion models. in conference on com- puter vision and pattern recognition cvpr 2022. 2 6 43 ramprasaath r selvaraju michael cogswell abhishek das ramakrishna vedantam devi parikh and dhruv batra. grad-cam visual explanations from deep networks via gradient-based localization. in international conference on computer vision iccv 2017. 1 2 3 4 44 sarath sivaprasad akshay goindani mario fritz and vi- neet gandhi. class-wise domain generalization a novel framework for evaluating distributional shift. in neurips workshop on distribution shifts connecting methods and applications 2022. 1 2 45 nathan somavarapu chih-yao ma and zsolt kira. frus- tratingly simple domain generalization via image stylization. arxiv abs 2006.11207 2020. 2 46 shuhan tan xingchao peng and kate saenko. class- imbalanced domain adaptation an empirical odyssey. in european conference on computer vision workshops ec- cvw 2020. 6 47 luming tang menglin jia qianqian wang cheng perng phoo and bharath hariharan. emergent correspondence from image diffusion. in advances in neural information processing systems neurips 2023. 2 4 48 antonio torralba and alexei a efros. unbiased look at dataset bias. in conference on computer vision and pattern recognition cvpr 2011. 2 49 eric tzeng judy hoffman kate saenko and trevor darrell. adversarial discriminative domain adaptation. in confer- ence on computer vision and pattern recognition cvpr 2017. 1 50 hemanth venkateswara jose eusebio shayok chakraborty and sethuraman panchanathan. deep hashing network for unsupervised domain adaptation. in conference on com- puter vision and pattern recognition cvpr 2017. 4 6 51 riccardo volpi hongseok namkoong ozan sener john c duchi vittorio murino and silvio savarese. generalizing to unseen domains via adversarial data augmentation. ad- vances in neural information processing systems neurips 2018. 1 52 bor-shiun wang chien-yi wang and wei-chen chiu. mcpnet an interpretable classifier via multi-level concept prototypes. in conference on computer vision and pattern recognition cvpr 2024. 3 53 haohan wang zexue he zachary c lipton and eric p xing. learning robust representations by projecting super- ficial statistics out. international conference on learning representations iclr 2019. 2 54 jindong wang cuiling lan chang liu yidong ouyang tao qin wang lu yiqiang chen wenjun zeng and s yu philip. generalizing to unseen domains a survey on do- main generalization. ieee transactions on knowledge and data engineering 35 8 8052 8072 2022. 1 55 zijian wang yadan luo ruihong qiu zi huang and mahsa baktashmotlagh. learning to diversify for single do- main generalization. in international conference on com- puter vision iccv 2021. 6 7 56 zehao xiao jiayi shen xiantong zhen ling shao and cees snoek. a bit more bayesian domain-invariant learning with uncertainty. in international conference on machine learning icml 2021. 2 30219 page 11 57 qinwei xu ruipeng zhang yi-yan wu ya zhang ning liu and yanfeng wang. simde a simple domain ex- pansion approach for single-source domain generalization. in conference on computer vision and pattern recognition cvpr 2023. 2 58 yue yang artemis panagopoulou shenghao zhou daniel jin chris callison-burch and mark yatskar. language in a bottle language model guided concept bottlenecks for in- terpretable image classification. in conference on computer vision and pattern recognition cvpr 2023. 3 59 jianhao yuan francesco pinto adam davies and philip torr. not just pretty pictures toward interventional data augmentation using text-to-image generators. international conference on machine learning icml 2024. 1 2 6 7 60 sangdoo yun dongyoon han seong joon oh sanghyuk chun junsuk choe and youngjoon yoo. cutmix regu- larization strategy to train strong classifiers with localizable features. in international conference on computer vision iccv 2019. 6 7 61 hanlin zhang yi-fan zhang weiyang liu adrian weller bernhard sch olkopf and eric p xing. towards principled disentanglement for domain generalization. in conference on computer vision and pattern recognition cvpr 2022. 1 62 long zhao ting liu xi peng and dimitris metaxas. maximum-entropy adversarial data augmentation for im- proved generalization and robustness. in advances in neural information processing systems neurips 2020. 7 63 kaiyang zhou yongxin yang yu qiao and tao xiang. do- main generalization with mixstyle. international conference on learning representations iclr 2021. 1 2 6 7 64 kaiyang zhou yongxin yang yu qiao and tao xi- ang. mixstyle neural networks for domain generalization and adaptation. international journal of computer vision ijcv 132 3 822 836 2024. 2 30220",
      "keywords": [
        "page",
        "tide",
        "training",
        "locally",
        "interpretable",
        "domain",
        "generalization",
        "models",
        "enables",
        "test-time",
        "correction",
        "aishwarya",
        "agarwal1",
        "srikrishna",
        "karanam2",
        "vineet",
        "gandhi1",
        "1cvit",
        "kohli",
        "centre",
        "intelligent",
        "systems",
        "iiit",
        "hyderabad",
        "india",
        "2adobe",
        "research",
        "bengaluru",
        "abstract",
        "consider",
        "problem",
        "single-source",
        "gener-",
        "alization.",
        "existing",
        "methods",
        "typically",
        "rely",
        "extensive",
        "aug-",
        "mentations",
        "synthetically",
        "cover",
        "diverse",
        "domains",
        "during",
        "training.",
        "however",
        "struggle",
        "semantic",
        "shifts",
        "e.g.",
        "background",
        "viewpoint",
        "changes",
        "often",
        "learn",
        "global",
        "features",
        "instead",
        "local",
        "concepts",
        "tend",
        "do-",
        "main",
        "invariant.",
        "address",
        "gap",
        "propose",
        "ap-",
        "proach",
        "compels",
        "leverage",
        "such",
        "prediction.",
        "given",
        "suitable",
        "dataset",
        "per-class",
        "localization",
        "maps",
        "exists",
        "first",
        "develop",
        "novel",
        "pipeline",
        "generate",
        "annotations",
        "exploiting",
        "rich",
        "diffusion",
        "large-language",
        "models.",
        "next",
        "innovation",
        "scheme",
        "con-",
        "cept",
        "saliency",
        "alignment"
      ],
      "metadata": {
        "filename": "Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction_CVPR_2025_paper.pdf",
        "original_filename": "Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 5969923,
        "upload_file_id": "upload_1752169363721_1755yited",
        "upload_timestamp": "2025-07-10T17:42:43.783431",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-10T17:42:44.054145+00:00",
      "content_length": 50504,
      "content_type": ".pdf",
      "filename": "Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction_CVPR_2025_paper.pdf"
    }
  },
  "file_index": {
    "5": {
      "filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:25:52.919373+00:00",
      "content_length": 40912,
      "processing_stage": "immediate"
    },
    "3": {
      "filename": "New Account - Toronto Hydro.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:04:47.211730+00:00",
      "content_length": 1597,
      "processing_stage": "immediate"
    },
    "14": {
      "filename": "Abdessaied_V2Dial_Unification_of_Video_and_Visual_Dialog_via_Multimodal_Experts_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:36:55.145728+00:00",
      "content_length": 52361,
      "processing_stage": "immediate"
    },
    "15": {
      "filename": "Abulnaga_MultiMorph_On-demand_Atlas_Construction_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:39:38.752232+00:00",
      "content_length": 59513,
      "processing_stage": "immediate"
    },
    "1": {
      "filename": "2503.06433v1.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T14:58:55.550159+00:00",
      "content_length": 73234,
      "processing_stage": "immediate"
    },
    "2": {
      "filename": "2504.09307v1.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:00:47.045657+00:00",
      "content_length": 55816,
      "processing_stage": "immediate"
    },
    "4": {
      "filename": "Official EVUS Enrollment Website - Enrollment Status.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:02:29.841967+00:00",
      "content_length": 2951,
      "processing_stage": "immediate"
    },
    "6": {
      "filename": "Agro_MAD_Memory-Augmented_Detection_of_3D_Objects_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:05:38.709353+00:00",
      "content_length": 61240,
      "processing_stage": "immediate"
    },
    "7": {
      "filename": "Aghzal_Evaluating_Vision-Language_Models_as_Evaluators_in_Path_Planning_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T17:15:58.557793+00:00",
      "content_length": 63431,
      "processing_stage": "immediate"
    },
    "8": {
      "filename": "Ahmed_Efficient_Event-Based_Object_Detection_A_Hybrid_Neural_Network_with_Spatial_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:17:57.774948+00:00",
      "content_length": 48705,
      "processing_stage": "immediate"
    },
    "9": {
      "filename": "Ahn_Nearly_Zero-Cost_Protection_Against_Mimicry_by_Personalized_Diffusion_Models_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:18:42.614142+00:00",
      "content_length": 44718,
      "processing_stage": "immediate"
    },
    "10": {
      "filename": "Aira_Gaussian_Splatting_for_Efficient_Satellite_Image_Photogrammetry_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:24:11.391035+00:00",
      "content_length": 47136,
      "processing_stage": "immediate"
    },
    "11": {
      "filename": "Al-Emadi_Benchmarking_Object_Detectors_under_Real-World_Distribution_Shifts_in_Satellite_Imagery_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:19:37.029253+00:00",
      "content_length": 52246,
      "processing_stage": "immediate"
    },
    "12": {
      "filename": "Akkerman_InterDyn_Controllable_Interactive_Dynamics_with_Video_Diffusion_Models_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:24:49.149233+00:00",
      "content_length": 59523,
      "processing_stage": "immediate"
    },
    "13": {
      "filename": "Abdelsamad_Multi-Scale_Neighborhood_Occupancy_Masked_Autoencoder_for_Self-Supervised_Learning_in_LiDAR_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:26:28.074431+00:00",
      "content_length": 48832,
      "processing_stage": "immediate"
    },
    "16": {
      "filename": "Afane_ATP_Adaptive_Threshold_Pruning_for_Efficient_Data_Encoding_in_Quantum_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:41:23.259187+00:00",
      "content_length": 43900,
      "processing_stage": "immediate"
    },
    "17": {
      "filename": "Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T15:43:38.406120+00:00",
      "content_length": 50504,
      "processing_stage": "immediate"
    },
    "18": {
      "filename": "Aghzal_Evaluating_Vision-Language_Models_as_Evaluators_in_Path_Planning_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T17:24:09.483271+00:00",
      "content_length": 63431,
      "processing_stage": "immediate"
    },
    "19": {
      "filename": "Agro_MAD_Memory-Augmented_Detection_of_3D_Objects_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T17:24:46.824042+00:00",
      "content_length": 61240,
      "processing_stage": "immediate"
    },
    "20": {
      "filename": "Ahmed_DeepCompress-ViT_Rethinking_Model_Compression_to_Enhance_Efficiency_of_Vision_Transformers_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T17:34:20.610985+00:00",
      "content_length": 49383,
      "processing_stage": "immediate"
    },
    "21": {
      "filename": "Ahmed_Efficient_Event-Based_Object_Detection_A_Hybrid_Neural_Network_with_Spatial_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T17:36:03.957590+00:00",
      "content_length": 48705,
      "processing_stage": "immediate"
    },
    "22": {
      "filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T17:37:34.213716+00:00",
      "content_length": 40912,
      "processing_stage": "immediate"
    },
    "23": {
      "filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T17:38:59.999945+00:00",
      "content_length": 40912,
      "processing_stage": "immediate"
    },
    "24": {
      "filename": "Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-10T17:42:44.054145+00:00",
      "content_length": 50504,
      "processing_stage": "immediate"
    }
  },
  "last_updated": "2025-07-10T17:42:44.054174+00:00"
}