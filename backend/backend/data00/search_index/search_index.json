{
  "search_index": {
    "5": {
      "file_id": "5",
      "content": "\n=== Page 1 ===\nCLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object\nRepresentation\nReza Abbasi, Ali Nazari, Aminreza Se\ufb01d, Mohammadali Banayeeanzade,\nMohammad Hossein Rohban, Mahdieh Soleymani Baghshah\nSharif University of Technology, Tehran, Iran\n{reza.abbasi, ali.nazari02, aminreza.sefid, a.banayeean, rohban, soleymani}@sharif.edu\nAbstract\nContrastive Language-Image Pre-training (CLIP) mod-\nels excel in zero-shot classi\ufb01cation, yet face challenges in\ncomplex multi-object scenarios. This study offers a com-\nprehensive analysis of CLIP\u2019s limitations in these contexts\nusing a specialized dataset, ComCO, designed to evaluate\nCLIP\u2019s encoders in diverse multi-object scenarios.\nOur\n\ufb01ndings reveal signi\ufb01cant biases: the text encoder prior-\nitizes \ufb01rst-mentioned objects, and the image encoder fa-\nvors larger objects.\nThrough retrieval and classi\ufb01cation\ntasks, we quantify these biases across multiple CLIP vari-\nants and trace their origins to CLIP\u2019s training process, sup-\nported by analyses of the LAION dataset and training pro-\ngression. Our image-text matching experiments show sub-\nstantial performance drops when object size or token order\nchanges, underscoring CLIP\u2019s instability with rephrased\nbut semantically similar captions. Extending this to longer\ncaptions and text-to-image models like Stable Diffusion,\nwe demonstrate how prompt order in\ufb02uences object promi-\nnence in generated images. For more details and access to\nour dataset and analysis code, visit our project repository:\nhttps://clip-oscope.github.io/.\n1. Introduction\nThe convergence of vision and language in arti\ufb01cial in-\ntelligence has led to the development of Vision-Language\nModels (VLMs) that can interpret and generate multimodal\ncontent. Among these, OpenAI\u2019s Contrastive Language-\nImage Pre-training (CLIP) model [13] has been particu-\nlarly in\ufb02uential, demonstrating remarkable capabilities in\nzero-shot image classi\ufb01cation and setting new standards for\nmultimodal understanding [3, 5, 18, 20]. The success of\nCLIP has catalyzed a wide array of applications\u2014from im-\nage retrieval and visual question answering to text-to-image\ngeneration\u2014signifying a paradigm shift in how models per-\nceive and relate visual and linguistic information.\nVisual Language Models like CLIP face signi\ufb01cant\nchallenges in understanding and reasoning about complex\nscenes with multiple objects and intricate relationships.\nCLIP struggles to identify distinct objects and model their\nrelationships accurately, especially when captions contain\nthe same objects but differ in their relationships. This re-\nsults in dif\ufb01culty distinguishing between similar captions\nwith different object relationships.\nSeveral benchmark\ndatasets have been introduced to elucidate the limitations of\nexisting models in capturing subtle relational nuances. No-\ntably, Winoground [20], VL-CheckList [23], ARO [21], and\nCREPE [10] have been instrumental in evaluating models\u2019\ncapacities to accurately match images with semantically ap-\npropriate captions.\nNumerous studies have addressed compositionality chal-\nlenges in multi-object scenarios, often through end-to-end\nmethods like \ufb01ne-tuning with hard-negative samples [21] to\nimprove model performance. However, these approaches\nhave faced criticism and subsequent re\ufb01nement, as seen in\nmethods like SUGARCREPE [8] and [17], which generate\nnegative captions with minor structural changes or LLMs\nto highlight semantic distinctions.\nWhile most focus on\nCLIP\u2019s ability to distinguish structurally similar yet concep-\ntually different captions, few studies, such as Dumpala et al.\n[4], explore CLIP\u2019s performance on semantically equivalent\nbut structurally distinct captions, revealing a gap in under-\nstanding CLIP\u2019s inconsistency with such prompts.\nWhile previous studies have advanced our understanding\nof CLIP\u2019s limitations, our work uniquely focuses on CLIP\u2019s\nperformance with semantically equivalent but structurally\nvaried captions rather than simply distinguishing conceptu-\nally different captions. This shift enables a deeper exam-\nination of the model\u2019s grasp of language and visual con-\ntent, where systematic errors reveal potential biases. Un-\nlike prior works that primarily propose benchmarks or end-\nto-end solutions, we investigate the root causes of CLIP\u2019s\nbehavior, delving into the mechanisms of both image and\ntext encoders to uncover why the model displays biases\nand lacks robustness to certain linguistic and visual varia-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n9308\n\n=== Page 2 ===\nFigure 1. Overview of our key contributions. Step 1: We create ComCO dataset for controlled multi-object experiments. Step 2: We\nidentify biases in CLIP\u2019s image encoder (favoring larger objects) and text encoder (prioritizing \ufb01rst-mentioned objects). Step 3: We\ninvestigate the origin of these biases, \ufb01nding a connection to training data characteristics. Step 4: We demonstrate the practical impacts of\nthese biases on image-text matching task, showing how they affect model performance in multi-object scenarios.\ntions. To support this analysis, we introduce the ComCO\ndataset, purpose-built for examining CLIP\u2019s performance\nunder controlled multi-object scenarios. Our study spans\nmultiple versions of CLIP trained on diverse datasets and ar-\nchitectures, ensuring the broad applicability of our \ufb01ndings.\nThis comprehensive approach aims to deepen our under-\nstanding of CLIP\u2019s limitations and pave the way for more\nadaptable vision-language models. Beyond CLIP, our in-\nsights have signi\ufb01cant implications for text-to-image (T2I)\ngenerative models and multimodal large language models\n(MLLMs), where decoding CLIP\u2019s encoding intricacies can\ninform advancements in arti\ufb01cial intelligence across do-\nmains. As shown in Figure 1, our key contributions are as\nfollows:\n\u2022 Development of Novel Dataset: We introduce ComCO,\na specialized dataset for creating controlled multi-object\nscenarios.\nUnlike previous benchmarks, ComCO al-\nlows control over object size and caption order, enabling\nprecise analysis of model performance across composi-\ntional challenges and enhancing understanding of VLMs\u2019\nstrengths and weaknesses.\n\u2022 Encoder Analysis: We conduct an in-depth examination\nof CLIP\u2019s image and text encoders in multi-object scenes,\nrevealing weaknesses in preserving information for object\ndistinction and identifying where compositional informa-\ntion is lost.\n\u2022 Bias Identi\ufb01cation: Our study reveals that CLIP\u2019s im-\nage encoder prefers larger objects, while the text encoder\nfavors \ufb01rst-mentioned and visually larger objects, high-\nlighting biases in CLIP\u2019s handling of visual and linguistic\ninformation.\n\u2022 Investigation of Bias Origins: We explore the origins of\nthese biases, showing that larger objects are often men-\ntioned earlier in CLIP\u2019s training captions, and are favored\nin embeddings due to the abundance of their visual to-\nkens. We substantiate this with analyses of the LAION\ndataset and CLIP\u2019s training progression.\n\u2022 Practical Impact: We show how these biases affect per-\nformance in multi-object tasks, with signi\ufb01cant drops in\nimage-text matching accuracy in ComCO and COCO [9].\nThese biases also extend to text-to-image models, in\ufb02u-\nencing object prominence based on prompt order.\nThese \ufb01ndings reveal how biases in CLIP\u2019s text and im-\nage encoders signi\ufb01cantly reduce its performance in multi-\nobject scenarios, emphasizing the need to address these bi-\nases to enhance vision-language models\u2019 robustness. Our\nwork offers key insights into CLIP\u2019s behavior and lays\ngroundwork for improving model performance in real-\nworld applications.\n2. Methodology\n2.1. Dataset Design\nTo thoroughly evaluate the performance of CLIP models\nin multi-object scenarios under controlled conditions, we\nconstructed the ComCO (Complex COCO Objects) dataset.\nUtilizing Blender software allowed us precise control over\nthe number, location, and dimensions of objects in the im-\nages (see Appendix 7.1). The ComCO dataset comprises\n72 objects derived from the COCO dataset. We generated\n9309\n\n=== Page 3 ===\nimages containing 2, 3, 4, and 5 objects. Each image is\npaired with a speci\ufb01c caption that accurately describes the\nobjects present. This approach ensures high control over\nthe dataset and minimizes confounding factors, providing a\nrobust platform for evaluating the CLIP models.\nWe deliberately chose not to use text-to-image models\nfor generating these datasets due to two main reasons. First,\nthese models often lack the capability to produce high-\nquality, fully controlled multi-object images. Second, since\nCLIP is used in many of these models, utilizing them could\nintroduce unwanted biases into our evaluations.\n2.2. Experimental Framework for Encoder Analy-\nsis\nThe main goal of this study is to evaluate the performance of\nCLIP\u2019s text and image encoders separately in multi-object\nscenarios. We aim to analyze the impact and contribution of\neach object in the \ufb01nal output of the encoders. To achieve\nthis, we conducted experiments using our designed ComCO\ndataset, with images and captions containing two to \ufb01ve ob-\njects. To ensure the generalizability of our \ufb01ndings, we also\nvalidated our results on the widely-used COCO dataset. We\ndesigned two sets of experiments: retrieval-based experi-\nments and classi\ufb01cation-based experiments. Given the con-\nsistency of the results in both types of experiments, we have\nincluded the classi\ufb01cation results in the appendix 7.2 and\n7.4 and explain the retrieval-based experiments bellow.\n2.2.1. TEXT-BASED OBJECT RETRIEVAL (TOR)\nThe Text-based Object Retrieval task evaluates how well\nCLIP\u2019s text encoder can identify individual objects within\nmulti-object captions. As illustrated in Figure 2a, this ex-\nperiment involves several steps: First, we use CLIP\u2019s text\nencoder to create embeddings for both multi-object captions\nand single-object captions. We then measure the similar-\nity between each multi-object caption embedding and all\nsingle-object caption embeddings. The single-object cap-\ntion with the highest similarity score is considered the \u201dre-\ntrieved\u201d object. To assess performance, we calculate re-\ntrieval accuracy for each object position in the multi-object\ncaptions. This helps us identify any biases related to an\nobject\u2019s position within a caption, such as favoring objects\nmentioned \ufb01rst or last.\n2.2.2. IMAGE-BASED OBJECT RETRIEVAL (IOR)\nThe Image-based Object Retrieval task is similar to TOR\nbut focuses on CLIP\u2019s image encoder. As shown in Fig-\nure 2b, this experiment involves several steps: We begin\nby using CLIP\u2019s image encoder to generate embeddings\nfor multi-object images and single-object images. We then\ncompute similarity scores between each multi-object image\nembedding and all single-object image embeddings. The\nsingle-object image with the highest similarity score is con-\nsidered the \u201dretrieved\u201d object. To evaluate performance, we\ncalculate retrieval accuracy for different object size cate-\ngories (e.g., large, small) within the multi-object images.\nThis allows us to determine if the image encoder shows any\npreference for objects of a particular size.\nWe also experimented with a variation of ComCO, called\nSimCO, where objects were replaced with simple geometric\nshapes from the CLEVR dataset. This was done to con\ufb01rm\nthat bias persists even with non-natural, geometric objects.\nFurther details are provided in Appendix 7.1.\n3. Results and Analysis\nOur experiments revealed signi\ufb01cant biases in both the\ntext and image encoders of the CLIP model. This section\npresents our \ufb01ndings, organized by encoder type and focus-\ning on retrieval tasks.\n3.1. Text Encoder Biases\nWe observed a consistent bias in the text encoder towards\nthe \ufb01rst object mentioned in descriptions. In the TOR ex-\nperiment, the retrieval accuracy (as shown in Table 1) was\nhighest for the \ufb01rst object, indicating its dominant in\ufb02uence\non the overall text representation. This suggests that the\ntext encoder prioritizes the initial object, leading to its more\naccurate retrieval compared to subsequent objects. The de-\ntailed results for the scenarios involving 2, 3, and 5 objects\ncan be found in the appendix 7.3, and experiments on longer\ncaption templates are in Appendix 7.6 and 7.7.\n3.2. Image Encoder Biases\nIn multi-object images, the image encoder exhibited a\nstrong bias towards larger objects. The Image-based Ob-\nject Retrieval IOR experiment, detailed in Table 2, shows\nthat larger objects were more frequently and accurately re-\ntrieved during single-object image searches. This \ufb01nding\nhighlights the image encoder\u2019s bias towards larger objects,\nwhich receive disproportionate emphasis in the \ufb01nal image\nrepresentation. Further detailed results, speci\ufb01cally for sce-\nnarios with 2, 3, and 5 objects, are provided in the appendix\n7.5.\n3.3. COCO Dataset Experiments\nTo validate the generalizability of our \ufb01ndings from the\nsynthetic dataset, we conducted similar experiments on the\nCOCO dataset, which comprises real images with accom-\npanying captions. This real-world dataset allowed us to in-\nvestigate whether the previously observed biases persist in\nmore naturalistic settings.\nDue to the absence of single-object images for COCO\nobjects, we approached the IOR experiment in two ways.\nFirst, we used single-object images from the DomainNet\ndataset [11] as retrieval targets. Second, we introduced an\nalternative approach called Image-to-Text Object Retrieval\n(I2TOR). In I2TOR, we used the textual names of COCO\n9310\n\n=== Page 4 ===\n%DVH\u0003,PDJH\u001db\n7KUHH\u00032EMHFWV\u0003,PDJH\n6LQJOH\u00032EMHFW\u0003,PDJH\u0003ZKLFK\nPDWFKLQJb%DVH\u0003,PDJH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n2WKHUb6LQJOHb2EMHFW\n,PDJHV\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n\u0011\u0011\u0011\n&/,3\u00036FRUH\n\u0013\u0011\u001a\u0013\u0018\n\u0013\u0011\u001b\u0015\u0017\n\u0013\u0011\u0019\u0015\u0014\n\u0013\u0011\u0018\u0015\u001a\n\u0013\u0011\u0018\u001a\u001c\n\u0013\u0011\u0018\u0018\u0017\nE\f\nD\f\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3b\n,PDJH\n(QFRGHU\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n&/,3\u00036FRUH\n\u0011\u0011\u0011\n&/,3\u00036FRUH\n\u0013\u0011\u0016\u0017\u0019\n\u0013\u0011\u0019\u0017\u001a\n\u0013\u0011\u0017\u001b\u0014\n\u0013\u0011\u0015\u0014\u001c\n\u0013\u0011\u0015\u0017\u0015\n\u0013\u0011\u0015\u0016\u0019\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n&/,3b\n7H[W\n(QFRGHU\n%DVH\u00037H[W\u001db\n7KUHH\u00032EMHFWV\u00037H[W\n6LQJOH\u00032EMHFW\u00037H[W\u0003ZKLFK\nPDWFKLQJb%DVH\u00037H[W\n2WKHUb6LQJOHb2EMHFW\n7H[WV\nSL]]D\u0003DQGbDSSOH\nDQGbGHVN\nDSSOH\nSL]]D\nGHVN\nKDW\nFDU\nD[H\n\u0011\u0011\u0011\n\u0011\u0011\u0011\n725\n,25\nFigure 2. Experimental setup for Text-based Object Retrieval (TOR) and Image-based Object Retrieval (IOR) tasks. a) TOR: The CLIP\ntext encoder generates embeddings for multi-object and single-object texts. Cosine similarity scores are calculated between the base text\nembedding and single-object text embeddings to identify the most similar object. b) IOR: The CLIP image encoder generates embeddings\nfor multi-object and single-object images. Cosine similarity scores are calculated between the base image embedding and single-object\nimage embeddings to identify the most similar object.\nTable 1. Performance on TOR for ComCO datasets\nTask\nModel\nFirst Obj\nSecond Obj\nThird Obj\nFourth Obj\nTOR\nCLIP LAION\n63.96\n21.59\n10.68\n3.76\nCLIP Datacomp\n71.13\n16.26\n8.74\n3.87\nCLIP Roberta\n44.03\n23.73\n18.07\n14.18\nSIGLIP\n58.11\n21.16\n10.99\n9.73\nCLIP openAI\n50.31\n20.74\n14.45\n6.79\nNegCLIP\n51.63\n28.92\n14.86\n4.59\nSugarCrepe\n44.29\n30.32\n18.73\n6.66\nTable 2. Performance on IOR for ComCO datasets\nTask\nModel\nLarge Object\nSmall Obj 1\nSmall Obj 2\nSmall Obj 3\nIOR\nCLIP LAION\n85.45\n6.36\n5.45\n2.73\nCLIP Datacomp\n85.16\n5.65\n4.95\n4.24\nCLIP Roberta\n87.40\n8.66\n2.36\n1.57\nSIGLIP\n77.66\n10.11\n6.38\n5.85\nCLIP openAI\n65.22\n17.39\n8.70\n8.70\nNegCLIP\n61.67\n15.00\n13.33\n10.00\nSugarCrepe\n60.0\n18.38\n16.85\n4.7\nobjects instead of single-object images. These object names\nwere embedded using CLIP\u2019s text encoder, allowing us to\nperform a retrieval task consistent with the IOR methodol-\nogy while adapting to the constraints of the COCO dataset.\nTables 3 and 4 present the results of our COCO dataset\nexperiments. In TOR, the \ufb01rst-mentioned object in COCO\ncaptions was retrieved with higher accuracy, which aligns\nwith our earlier \ufb01ndings of bias in the text encoder. Simi-\nlarly, in IOR, larger objects in COCO images were retrieved\nmore accurately, consistent with the trends observed in our\nsynthetic dataset experiments. The I2TOR results further\ncon\ufb01rmed this bias, demonstrating that even when using\ntextual object representations, the bias towards larger ob-\nTable 3. Performance on TOR for coco dataset\nTask\nModel\nFirst Obj\nSecond Obj\nThird Obj\nFourth Obj\nTOR\nCLIP openAI\n35.24\n21.90\n20.48\n22.38\nCLIP LAION\n67.89\n13.76\n8.26\n10.09\nCLIP Datacomp\n57.68\n17.68\n12.75\n11.88\nCLIP Roberta\n40.78\n23.30\n20.39\n15.53\nSIGLIP\n49.47\n26.84\n12.11\n11.58\nNegCLIP\n38.69\n22.11\n17.09\n22.11\nTable 4. Performance on IOR for coco dataset\nTask\nModel\nLarge Object\nSmall Obj 1\nSmall Obj 2\nSmall Obj 3\nIOR\nCLIP openAI\n43.02\n28.82\n17.13\n11.03\nCLIP LAION\n39.44\n28.45\n17.70\n14.41\nCLIP Datacomp\n36.71\n29.55\n19.13\n14.61\nCLIP Roberta\n36.71\n28.61\n19.82\n14.86\nSIGLIP\n36.63\n28.29\n20.02\n15.06\nNegCLIP\n44.04\n28.86\n16.48\n10.62\nI2TOR\nCLIP openAI\n51.49\n24.87\n13.68\n9.97\nCLIP LAION\n45.50\n27.02\n15.91\n11.56\nCLIP Datacomp\n46.64\n26.82\n14.53\n12.01\nCLIP Roberta\n44.69\n26.98\n16.04\n12.29\nSIGLIP\n47.09\n27.07\n15.10\n10.74\nNegCLIP\n49.04\n27.07\n14.08\n9.81\njects persists.\nOur experiments reveal two signi\ufb01cant biases in the\nCLIP model: the text encoder shows a strong preference for\nthe \ufb01rst mentioned object in textual descriptions, while the\nimage encoder exhibits greater sensitivity to larger objects\nin images. These biases can signi\ufb01cantly impact the overall\nsystem performance in various vision-language tasks, par-\nticularly in multi-object scenarios.\n9311\n\n=== Page 5 ===\n4. Origin of Bias in CLIP Models\nIn this section, we investigate the potential origins of the\nbiases observed in CLIP models and provide evidence sup-\nporting our hypotheses.\n4.1. Bias in the Image Encoder\nThe observed bias favoring larger objects within the image\ndomain can be attributed to the architectural characteristics\nof Vision Transformers (ViT) [2] utilized in CLIP\u2019s image\nencoder. Our hypothesis is that larger objects, which occupy\na greater number of patches in the ViT\u2019s patch-based image\nrepresentation, exert a more signi\ufb01cant in\ufb02uence on the \ufb01-\nnal class (CLS) token representation. This bias is not exclu-\nsive to CLIP; it appears to be a consistent feature across ViT\nmodels, as demonstrated by our experiments detailed in the\nappendix.\nTo substantiate this hypothesis, we designed an experi-\nment to quantify the attention allocated by the CLS token\nto each image patch. By calculating the cumulative atten-\ntion received by each object from the CLS token, we could\nassess the in\ufb02uence of object size on attention allocation.\nWe applied this analysis to our three-object ComCO dataset,\nand the results are illustrated in Figure 3. The \ufb01ndings con-\n\ufb01rm our hypothesis: larger objects indeed receive more at-\ntention from the CLS token.\n4.2. Bias in the Text Encoder\nWe explore the bias present in the text encoder from two\nperspectives: the attention mechanism in the model struc-\nture and the model\u2019s training method.\n4.2.1. Impact of Attention Mechanism\nText encoder models can be categorized based on their at-\ntention mechanisms: uni-directional (causal) attention and\nbi-directional attention. In models with causal attention,\neach token attends only to preceding tokens, whereas in bi-\ndirectional models, each token attends to all tokens in the\nsequence.\nWhen OpenAI introduced the CLIP model, its text en-\ncoder employed causal attention, meaning each token could\nonly attend to tokens before it and itself.\nThis differs\nfrom typical self-attention mechanisms, where tokens at-\ntend to all other tokens. Most CLIP models use causal self-\nattention, with the exception of the variant using the XLM-\nRoberta text encoder, which also employs self-attention.\nHowever, as shown in Table 1, even this model exhibits the\nmentioned bias. This indicates that the bias does not origi-\nnate from the attention mechanism itself.\n4.2.2. Role of Training Method\nTo determine whether the observed bias is speci\ufb01c to CLIP\nmodels, we compared CLIP\u2019s text encoder with two other\nTable 5. Performance on TOC and TOR for ComCO datasets\nTask\nModel\nFirst Obj\nSecond Obj\nThird Obj\nFourth Obj\nTOR\nCLIP\n56.28\n22.71\n13.17\n7.48\nSBERT\n29.02\n19.80\n17.50\n33.57\nSimCSE [7]\n27.59\n19.07\n17.76\n34.83\nmodels designed to embed sentences into a meaningful se-\nmantic space: Sentence-BERT (SBERT) [14] and SimCSE\n[7]. The primary distinction is that CLIP\u2019s embedding space\nis shared between images and text, whereas SBERT and\nSimCSE operate solely in the text domain.\nWe conducted the TOR experiment on our dataset using\nthese models. As presented in Table 5, the bias observed in\nCLIP differs from that in the other models. This suggests\nthat CLIP\u2019s unique training method, which aligns images\nand text in a shared embedding space through contrastive\nlearning, contributes to the bias. Therefore, to uncover the\nroot cause of the bias, we focus on the speci\ufb01cs of CLIP\u2019s\ntraining procedure.\n4.3. Hypothesized Origin of Text-Side Bias in CLIP\nWe hypothesize that the text-side bias in CLIP, which fa-\nvors objects mentioned earlier in text descriptions, origi-\nnates from the image-side bias toward larger objects and is\ntransferred to the text encoder during contrastive training.\nWe present evidence supporting this hypothesis through two\nkey claims and an analysis of the training progression.\nClaim 1: Larger Objects Have More In\ufb02uence on Text\nEmbeddings.\nBuilding upon the established image-side\nbias discussed earlier, we posit that objects with larger\nphysical sizes exert more in\ufb02uence on CLIP\u2019s text em-\nbeddings due to the alignment enforced during contrastive\ntraining. To test this, we categorized objects in the Domain-\nNet dataset into large, medium, and small groups based on\ntheir relative physical sizes in real-world (with the full list of\nobjects provided in the appendix 7.10). Speci\ufb01cally, objects\nsmaller than a school bag were categorized as small, objects\nsized between a school bag and a medium-sized car were\nclassi\ufb01ed as medium, and objects larger than a car\u2014up to\nsigni\ufb01cantly larger items\u2014were considered large. We then\nconstructed two sets of sentences, each containing four ob-\njects: one set with a large object mentioned \ufb01rst followed by\nthree medium-sized objects, and another with a small object\nmentioned \ufb01rst followed by three medium-sized objects.\nFigure 4.a compares the TOR accuracy for the \ufb01rst ob-\nject in these two groups. The higher TOR accuracy for sen-\ntences beginning with large objects supports our hypothe-\nsis that larger objects, when mentioned \ufb01rst, have a more\nsigni\ufb01cant impact on the text embeddings due to the cross-\nmodal alignment with their prominent representation in im-\nages.\n9312\n\n=== Page 6 ===\na)\nb)\nFigure 3. Attention allocation from the CLS token to objects of different sizes in the ComCO dataset. a) Qualitative results showing the\nCLS token\u2019s attention to each object. b) Quantitative analysis of attention distribution across 8,000 images, with each image containing one\nlarge and two small objects. The bar chart shows the average attention allocated to the large object versus the smaller ones, demonstrating\na bias towards larger objects.\na)\nb)\nc)\nFigure 4. a) Top-1 Object Retrieval accuracy comparison for sentences where the \ufb01rst object is either large or small. The higher TOR\naccuracy for sentences beginning with large objects supports the hypothesis that larger objects, when mentioned \ufb01rst, exert a stronger\nin\ufb02uence on text embeddings due to cross-modal alignment with their prominent visual representation in images. b) Distribution of the\nposition of the largest object within image captions from the LAION datasets. The results show a consistent bias where larger objects\ntend to be mentioned earlier in text descriptions. c) Progression of TOR rates across different training stages, indicating that text-side bias\nstrengthens as the model is exposed to more data, suggesting the cumulative effect of image-side bias being transferred to the text encoder\nthrough contrastive learning.\nClaim 2: Caption Bias in Training Datasets.\nTo inves-\ntigate potential biases in CLIP\u2019s training data, we analyzed\nboth the LAION [19] and COCO datasets. Due to limited\ncomputational resources and the large size of the LAION\ndataset, which contains over 2 billion image-text pairs, we\nrandomly selected a subset of 200,000 samples for our anal-\nysis. Using the Llama3 model, we extracted objects from\nthe image captions and employed the Language Segment-\nAnything tool to generate object masks in the correspond-\ning images, calculating their areas based on these masks. A\ndetailed description of our LAION dataset analysis method-\nology can be found in Appendix 7.8.\nFigure4.b shows the position of the largest object within\neach caption. The results indicate that, in the majority of\ncases, the largest object in an image is mentioned earlier\nin its caption. The same experiment was conducted on the\nCOCO dataset, with detailed results and the distribution for\ntwo to \ufb01ve object scenarios provided in Appendix 7.9. This\ndemonstrates a consistent bias in the training data, where\nlarger objects are not only more visually prominent but are\nalso described earlier in text annotations.\nAnalysis of Bias Development During Training.\nTo fur-\nther validate our hypothesis, we examined the progression\nof text-side bias during CLIP\u2019s training. We utilized model\ncheckpoints from the LAION dataset at \ufb01ve training stages,\ncorresponding to exposure to 2, 4, 6, 8, and 10 billion sam-\nples. We conducted TOR experiments at each stage, focus-\n9313\n\n=== Page 7 ===\ning on the retrieval accuracy for the \ufb01rst object mentioned\nin text descriptions.\nFigure4.c depicts the evolution of the TOR rate across\ndifferent training stages for scenarios with varying numbers\nof objects (from 3 to 8). The consistent upward trend in\nthe TOR rate as the model is exposed to more training data\nsuggests that the text-side bias strengthens over time, likely\ndue to the cumulative effect of the image-side bias being\ntransferred to the text encoder through contrastive learning.\nIncomplete Text Representation of CLIP\nHere we want\nto theoretically highlight why the CLIP text encoder could\nlearn an incomplete representation of the text. Let z and\nw represent a latent representation of an image content de-\nscribed in the caption, and such visual content not men-\ntioned in the text, respectively. For example, z represents\nthe fact that an image contains \u201ca horse that is eating the\ngrass.\u201d In this case, w might represent other details in the\nimage, like the \u201chorse color,\u201d \u201cwhere the horse is located,\u201d\netc. We assume a data generative process as follows:\nI := g(z,w)\nT := h(z),\nwhere I is the image, and T is its corresponding caption.\nNow we want to learn a joint embedding of the image\nand text through the CLIP. Here, we assume that f\u03b8(.) and\ni\u03c9(.) as learnable functions that map the image and text into\nthe joint embedding space, respectively.\nTheorem 1 Let elements of z be independent, zero-mean,\nand unit-variance. The contrastive loss for the ideal text en-\ncoder, i\u03c9(T) = z converges to that of a non-ideal incomplete\none, i.e. i\u03c9\u2032(T) = zs, where zs is the \ufb01rst d \u2212k dimensions\nof z, with k being a constant, and d \u2192\u221e.\nProof: The contrastive loss in making this learning hap-\npen can be written as:\nEz,z\u2032,w\n\u0002\nexp(sim(z,z))\nexp(sim(z,z))+\u2211k exp(sim(z,z\u2032\nk))\n\u0003\n(1)\nwith\nsim(z,z\u2032) = S(f\u03b8(g(z,w),i\u03c9(h(z\u2032)))),\nand z and {z\u2032\nk|1 \u2264k \u2264b} are b + 1 i.i.d. samples of the\ncontent in the representation space, and S is some normal-\nized similarity metric, e.g. cosine similarity, and b+1 is the\nbatch size. We assume that elements of z are independent,\nunit-variance, and zero mean. We further assume that the\ndimensionality of z, denoted as d, goes to in\ufb01nity.\nUnder such conditions, and based on Law of Large\nNumbers, \u2225z\u2225\np\u2212\u2192\n\u221a\nd, when d is large.\nTherefore, for\nany two independent copies of z, z\u2032\nk, we have sim(z,z\u2032\nk) =\nz\u22a4z\u2032\nk/(\u2225z\u2225\u2225z\u2032\nk\u2225)\np\u2212\u21920.\nIt is evident that in the ideal case, f\u03b8(g(z,w)) = z and\nalso i\u03c9(h(z)) = z, so the contrastive loss would converge\nto e/(e + b), as the numerator is e, and the second term in\nthe denominator converges to exp(0) = 1, according to the\nMann-Wald\u2019s theorem.\nHowever, we show that other learning of this representa-\ntion could achieve the same amount of loss. For instance, let\nzs be the \ufb01rst d \u2212k elements of z, with k being a constant.\nWe show that if f\u03b8\u2032(I) = zs and i\u03c9\u2032(T) = zs, the same loss\nwould be achieved in the limit of large d. To see this, note\nthat the numerator stays the same, i.e. e, while the second\nterm in the denominator still converges to bexp(0) = b.\nThis means that even if the image and text encoder of\nthe CLIP only partially recover the content embedding, they\nreach an excellent loss. But such possible incomplete rep-\nresentations of z are combinatorially large, making conver-\ngence of the CLIP to such local minima pretty likely. This\nmakes the text encoding of CLIP be far from ideal. Fur-\nthermore, the text encoder would become biased, depend-\ning on which of such local minima it converges to. Based\non this explanation, we would expect a text encoder that has\nlearned a complete representation to exhibit such biases to a\nlesser degree. As mentioned earlier, the subject of learning\ntext representations in VLMs that are discriminative of hard\nnegatives (e.g. NegCLIP) has been around for few years.\nWe tested one of strongest such models, [8], in our bench-\nmark to validate the hypothesis that an incomplete text rep-\nresentation is one of the causes of the bias in the VLMs.\nWe noticed that this model shows lower bias based on our\nbenchmark (see the SugarCrepe model in tables 1 and 2).\nWe have developed an initial approach to address the\nidenti\ufb01ed bias in the CLIP model, which is presented in\nAppendix 7.12. While this method is speci\ufb01c to our cur-\nrent dataset, it represents a promising step toward address-\ning these challenges and can inspire further advancements.\nThis work demonstrates our commitment to exploring prac-\ntical solutions while maintaining the primary focus of this\nstudy on the analysis of bias and its implications.\n5. Practical Impacts of Encoder Biases\nThe biases observed in CLIP\u2019s image and text encoders sig-\nni\ufb01cantly impact model performance in real-world appli-\ncations. This section explores how these biases manifest in\nimage-text matching tasks, while further analyses of text-to-\nimage generation impacts are presented in Appendix 7.11.\nOur analysis in this section serves two primary purposes.\nFirst, it provides concrete evidence of how these theoretical\nbiases can translate into practical limitations. Second, it of-\nfers insights into potential areas for improvement in vision-\nlanguage models, particularly in handling complex, multi-\n9314\n\n=== Page 8 ===\nFigure 5. An example of the correct and incorrect caption structures in the \ufb01rst and second scenarios.\nobject scenarios. Through a series of carefully designed ex-\nperiments, we illustrate how the biases in both text and im-\nage encoders can lead to unexpected or suboptimal results\nin tasks that are crucial for many downstream applications.\n5.1. Image-Text Matching\nBuilding upon our \ufb01ndings of biases in CLIP\u2019s image and\ntext encoders, we now demonstrate how these biases tangi-\nbly affect the model\u2019s performance in image-caption match-\ning tasks. We designed two experimental scenarios, con-\nducted on both the ComCO and COCO datasets, to evaluate\nthese biases. The results of these experiments are summa-\nrized in Table 6. To better illustrate the differences between\nthese two scenarios, an example of the caption structures is\nshown in Figure 5. In each scenario, we created incorrect\ncaptions by switching one object in the caption with an ob-\nject that is not present in the image. Additionally, GPT-4O\n[1] was used to rewrite the captions in the COCO dataset.\nFirst Scenario\nIn the \ufb01rst scenario, biases assist the\nmodel in distinguishing between the correct and incorrect\ncaptions. In the correct captions, the largest object in the\nimage is placed at the beginning, aligning with the model\u2019s\nbias towards prioritizing \ufb01rst-mentioned objects and larger\nobjects. For the incorrect captions, the non-existent object is\ndeliberately placed at the beginning, which helps the model\nrecognize the difference between the correct and incorrect\ncaptions more effectively. This positioning emphasizes the\ndiscrepancy early on, allowing the model to better detect the\nmismatch between the caption and the image. The perfor-\nmance of different models in this scenario can be seen in\nTable 6 under the \u201dFirst Scenario\u201d column.\nSecond Scenario\nIn the second scenario, biases lead the\nmodel to make errors. The correct captions place the largest\nobject at the end of the sentence, disrupting the model\u2019s\nbias towards objects mentioned earlier and its preference\nfor larger objects. In the incorrect captions, the non-existent\nobject is placed at the end, making it more dif\ufb01cult for the\nmodel to differentiate between correct and incorrect cap-\ntions as its attention is drawn away from the critical discrep-\nancies. The performance of different models in this scenario\nis shown in Table 6 under the \u201dSecond Scenario\u201d column.\nTable 6. Performance Comparison on Image-Text Matching for\nComCO and COCO Datasets\nDataset\nModel\nFirst Scenario\nSecond Scenario\nComCO\nCLIP Datacomp [6]\n99.99\n67.50\nCLIP Roberta\n99.98\n64.75\nSIGLIP [22]\n99.49\n72.36\nCLIP openAI\n99.59\n52.23\nNegCLIP\n96.82\n46.94\nSugarCrepe\n98.55\n60.43\nCOCO\nCLIP Datacomp [6]\n71.2\n54.2\nCLIP Roberta\n72.2\n54.1\nSIGLIP [22]\n64.8\n39.5\nCLIP openAI\n63.5\n26.4\nNegCLIP\n72\n28.7\nSugarCrepe\n80.0\n40.9\nBy comparing these two scenarios, we demonstrate that\nbiases in CLIP can either help or hinder the model\u2019s perfor-\nmance depending on how captions are structured. The ex-\nperimental results, particularly with the use of GPT-4O for\ncaption rephrasing in the COCO dataset, reveal how such\nbiases can in\ufb02uence the accuracy of image-text matching\ntasks. These biases must be addressed to improve CLIP\u2019s\nrobustness in real-world multi-object scenarios.\nFor further insights on how these biases affect text-to-\nimage generation, refer to our extended experiments in Ap-\npendix 7.11.\n6. Conclusion\nThis study uncovers biases in CLIP\u2019s encoders, with the\ntext encoder favoring \ufb01rst-mentioned objects and the im-\nage encoder emphasizing larger ones, which impacts per-\nformance in multi-object tasks. Using the ComCO dataset,\nwe highlighted these biases\u2019 effects on object representation\nand positioning, underscoring the need for balanced train-\ning. We attribute these biases to CLIP\u2019s contrastive frame-\nwork, where alignment issues propagate across modalities.\nAddressing these biases is essential for vision-language ad-\nvancements, as seen with models like Stable Diffusion.\n9315\n\n=== Page 9 ===\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 8\n[2] Dosovitskiy Alexey. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint arXiv:\n2010.11929, 2020. 5\n[3] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev.\nReproducible\nscaling laws for contrastive language-image learning.\nIn\n2023 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR). IEEE, 2023. 1\n[4] Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sas-\ntry, Evangelos Milios, Sageev Oore, and Hassan Saj-\njad.\nSugarcrepe++ dataset: Vision-language model sensi-\ntivity to semantic and lexical alterations.\narXiv preprint\narXiv:2406.11171, 2024. 1\n[5] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Or-\ngad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek\nRamanujan, Yonatan Bitton, Kalyani Marathe, Stephen\nMussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna,\nPang Wei Koh, Olga Saukh, Alexander Ratner, Shuran\nSong, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,\nSewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon,\nVaishaal Shankar, and Ludwig Schmidt.\nDatacomp: In\nsearch of the next generation of multimodal datasets, 2023.\n1\n[6] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-\nacomp:\nIn search of the next generation of multimodal\ndatasets. Advances in Neural Information Processing Sys-\ntems, 36, 2024. 8, 17\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen.\nSimcse:\nSimple contrastive learning of sentence embeddings. arXiv\npreprint arXiv:2104.08821, 2021. 5\n[8] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kem-\nbhavi, and Ranjay Krishna.\nSugarcrepe: Fixing hackable\nbenchmarks for vision-language compositionality. Advances\nin neural information processing systems, 36, 2024. 1, 7\n[9] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona, Deva\nRamanan, C. Lawrence Zitnick, and Piotr Doll\u00b4ar. Microsoft\ncoco: Common objects in context, 2015. 2\n[10] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi,\nIrena Gao, and Ranjay Krishna. Crepe: Can vision-language\nfoundation models reason compositionally? In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 10910\u201310921, 2023. 1\n[11] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\nSaenko, and Bo Wang. Moment matching for multi-source\ndomain adaptation. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 1406\u20131415,\n2019. 3\n[12] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 17\n[13] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision, 2021. 1\n[14] N Reimers.\nSentence-bert:\nSentence embeddings using\nsiamese bert-networks.\narXiv preprint arXiv:1908.10084,\n2019. 5\n[15] Dillon Reis, Jordan Kupec, Jacqueline Hong, and Ahmad\nDaoudi. Real-time \ufb02ying object detection with yolov8. arXiv\npreprint arXiv:2305.09972, 2023. 17\n[16] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, 2022. 17\n[17] Ugur Sahin, Hang Li, Qadeer Khan, Daniel Cremers, and\nVolker Tresp. Enhancing multimodal compositional reason-\ning of visual language models with generative negative min-\ning. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pages 5563\u20135573, 2024. 1\n[18] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-\ufb01ltered 400 million image-text pairs,\n2021. 1\n[19] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 6\n[20] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace Ross.\nWinoground: Probing vision and language models for visio-\nlinguistic compositionality. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5238\u20135248, 2022. 1\n[21] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,\nDan Jurafsky, and James Zou.\nWhen and why vision-\nlanguage models behave like bags-of-words, and what to\ndo about it?\nIn The Eleventh International Conference on\nLearning Representations, 2023. 1\n[22] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 11975\u201311986, 2023. 8, 17\n[23] Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan\nShen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin.\nVl-\nchecklist:\nEvaluating pre-trained vision-language models\n9316\n\n=== Page 10 ===\nwith objects, attributes and relations.\narXiv preprint\narXiv:2207.00221, 2022. 1\n9317\n",
      "searchable_content": "page 1 clip under the microscope a fine-grained analysis of multi-object representation reza abbasi ali nazari aminreza se\ufb01d mohammadali banayeeanzade mohammad hossein rohban mahdieh soleymani baghshah sharif university of technology tehran iran reza.abbasi ali.nazari02 aminreza.sefid a.banayeean rohban soleymani sharif.edu abstract contrastive language-image pre-training clip mod- els excel in zero-shot classi\ufb01cation yet face challenges in complex multi-object scenarios. this study offers a com- prehensive analysis of clip s limitations in these contexts using a specialized dataset comco designed to evaluate clip s encoders in diverse multi-object scenarios. our \ufb01ndings reveal signi\ufb01cant biases the text encoder prior- itizes \ufb01rst-mentioned objects and the image encoder fa- vors larger objects. through retrieval and classi\ufb01cation tasks we quantify these biases across multiple clip vari- ants and trace their origins to clip s training process sup- ported by analyses of the laion dataset and training pro- gression. our image-text matching experiments show sub- stantial performance drops when object size or token order changes underscoring clip s instability with rephrased but semantically similar captions. extending this to longer captions and text-to-image models like stable diffusion we demonstrate how prompt order in\ufb02uences object promi- nence in generated images. for more details and access to our dataset and analysis code visit our project repository https clip-oscope.github.io . 1. introduction the convergence of vision and language in arti\ufb01cial in- telligence has led to the development of vision-language models vlms that can interpret and generate multimodal content. among these openai s contrastive language- image pre-training clip model 13 has been particu- larly in\ufb02uential demonstrating remarkable capabilities in zero-shot image classi\ufb01cation and setting new standards for multimodal understanding 3 5 18 20 . the success of clip has catalyzed a wide array of applications from im- age retrieval and visual question answering to text-to-image generation signifying a paradigm shift in how models per- ceive and relate visual and linguistic information. visual language models like clip face signi\ufb01cant challenges in understanding and reasoning about complex scenes with multiple objects and intricate relationships. clip struggles to identify distinct objects and model their relationships accurately especially when captions contain the same objects but differ in their relationships. this re- sults in dif\ufb01culty distinguishing between similar captions with different object relationships. several benchmark datasets have been introduced to elucidate the limitations of existing models in capturing subtle relational nuances. no- tably winoground 20 vl-checklist 23 aro 21 and crepe 10 have been instrumental in evaluating models capacities to accurately match images with semantically ap- propriate captions. numerous studies have addressed compositionality chal- lenges in multi-object scenarios often through end-to-end methods like \ufb01ne-tuning with hard-negative samples 21 to improve model performance. however these approaches have faced criticism and subsequent re\ufb01nement as seen in methods like sugarcrepe 8 and 17 which generate negative captions with minor structural changes or llms to highlight semantic distinctions. while most focus on clip s ability to distinguish structurally similar yet concep- tually different captions few studies such as dumpala et al. 4 explore clip s performance on semantically equivalent but structurally distinct captions revealing a gap in under- standing clip s inconsistency with such prompts. while previous studies have advanced our understanding of clip s limitations our work uniquely focuses on clip s performance with semantically equivalent but structurally varied captions rather than simply distinguishing conceptu- ally different captions. this shift enables a deeper exam- ination of the model s grasp of language and visual con- tent where systematic errors reveal potential biases. un- like prior works that primarily propose benchmarks or end- to-end solutions we investigate the root causes of clip s behavior delving into the mechanisms of both image and text encoders to uncover why the model displays biases and lacks robustness to certain linguistic and visual varia- this cvpr paper is the open access version provided by the computer vision foundation. except for this watermark it is identical to the accepted version the final published version of the proceedings is available on ieee xplore. 9308 page 2 figure 1. overview of our key contributions. step 1 we create comco dataset for controlled multi-object experiments. step 2 we identify biases in clip s image encoder favoring larger objects and text encoder prioritizing \ufb01rst-mentioned objects . step 3 we investigate the origin of these biases \ufb01nding a connection to training data characteristics. step 4 we demonstrate the practical impacts of these biases on image-text matching task showing how they affect model performance in multi-object scenarios. tions. to support this analysis we introduce the comco dataset purpose-built for examining clip s performance under controlled multi-object scenarios. our study spans multiple versions of clip trained on diverse datasets and ar- chitectures ensuring the broad applicability of our \ufb01ndings. this comprehensive approach aims to deepen our under- standing of clip s limitations and pave the way for more adaptable vision-language models. beyond clip our in- sights have signi\ufb01cant implications for text-to-image t2i generative models and multimodal large language models mllms where decoding clip s encoding intricacies can inform advancements in arti\ufb01cial intelligence across do- mains. as shown in figure 1 our key contributions are as follows development of novel dataset we introduce comco a specialized dataset for creating controlled multi-object scenarios. unlike previous benchmarks comco al- lows control over object size and caption order enabling precise analysis of model performance across composi- tional challenges and enhancing understanding of vlms strengths and weaknesses. encoder analysis we conduct an in-depth examination of clip s image and text encoders in multi-object scenes revealing weaknesses in preserving information for object distinction and identifying where compositional informa- tion is lost. bias identi\ufb01cation our study reveals that clip s im- age encoder prefers larger objects while the text encoder favors \ufb01rst-mentioned and visually larger objects high- lighting biases in clip s handling of visual and linguistic information. investigation of bias origins we explore the origins of these biases showing that larger objects are often men- tioned earlier in clip s training captions and are favored in embeddings due to the abundance of their visual to- kens. we substantiate this with analyses of the laion dataset and clip s training progression. practical impact we show how these biases affect per- formance in multi-object tasks with signi\ufb01cant drops in image-text matching accuracy in comco and coco 9 . these biases also extend to text-to-image models in\ufb02u- encing object prominence based on prompt order. these \ufb01ndings reveal how biases in clip s text and im- age encoders signi\ufb01cantly reduce its performance in multi- object scenarios emphasizing the need to address these bi- ases to enhance vision-language models robustness. our work offers key insights into clip s behavior and lays groundwork for improving model performance in real- world applications. 2. methodology 2.1. dataset design to thoroughly evaluate the performance of clip models in multi-object scenarios under controlled conditions we constructed the comco complex coco objects dataset. utilizing blender software allowed us precise control over the number location and dimensions of objects in the im- ages see appendix 7.1 . the comco dataset comprises 72 objects derived from the coco dataset. we generated 9309 page 3 images containing 2 3 4 and 5 objects. each image is paired with a speci\ufb01c caption that accurately describes the objects present. this approach ensures high control over the dataset and minimizes confounding factors providing a robust platform for evaluating the clip models. we deliberately chose not to use text-to-image models for generating these datasets due to two main reasons. first these models often lack the capability to produce high- quality fully controlled multi-object images. second since clip is used in many of these models utilizing them could introduce unwanted biases into our evaluations. 2.2. experimental framework for encoder analy- sis the main goal of this study is to evaluate the performance of clip s text and image encoders separately in multi-object scenarios. we aim to analyze the impact and contribution of each object in the \ufb01nal output of the encoders. to achieve this we conducted experiments using our designed comco dataset with images and captions containing two to \ufb01ve ob- jects. to ensure the generalizability of our \ufb01ndings we also validated our results on the widely-used coco dataset. we designed two sets of experiments retrieval-based experi- ments and classi\ufb01cation-based experiments. given the con- sistency of the results in both types of experiments we have included the classi\ufb01cation results in the appendix 7.2 and 7.4 and explain the retrieval-based experiments bellow. 2.2.1. text-based object retrieval tor the text-based object retrieval task evaluates how well clip s text encoder can identify individual objects within multi-object captions. as illustrated in figure 2a this ex- periment involves several steps first we use clip s text encoder to create embeddings for both multi-object captions and single-object captions. we then measure the similar- ity between each multi-object caption embedding and all single-object caption embeddings. the single-object cap- tion with the highest similarity score is considered the re- trieved object. to assess performance we calculate re- trieval accuracy for each object position in the multi-object captions. this helps us identify any biases related to an object s position within a caption such as favoring objects mentioned \ufb01rst or last. 2.2.2. image-based object retrieval ior the image-based object retrieval task is similar to tor but focuses on clip s image encoder. as shown in fig- ure 2b this experiment involves several steps we begin by using clip s image encoder to generate embeddings for multi-object images and single-object images. we then compute similarity scores between each multi-object image embedding and all single-object image embeddings. the single-object image with the highest similarity score is con- sidered the retrieved object. to evaluate performance we calculate retrieval accuracy for different object size cate- gories e.g. large small within the multi-object images. this allows us to determine if the image encoder shows any preference for objects of a particular size. we also experimented with a variation of comco called simco where objects were replaced with simple geometric shapes from the clevr dataset. this was done to con\ufb01rm that bias persists even with non-natural geometric objects. further details are provided in appendix 7.1. 3. results and analysis our experiments revealed signi\ufb01cant biases in both the text and image encoders of the clip model. this section presents our \ufb01ndings organized by encoder type and focus- ing on retrieval tasks. 3.1. text encoder biases we observed a consistent bias in the text encoder towards the \ufb01rst object mentioned in descriptions. in the tor ex- periment the retrieval accuracy as shown in table 1 was highest for the \ufb01rst object indicating its dominant in\ufb02uence on the overall text representation. this suggests that the text encoder prioritizes the initial object leading to its more accurate retrieval compared to subsequent objects. the de- tailed results for the scenarios involving 2 3 and 5 objects can be found in the appendix 7.3 and experiments on longer caption templates are in appendix 7.6 and 7.7. 3.2. image encoder biases in multi-object images the image encoder exhibited a strong bias towards larger objects. the image-based ob- ject retrieval ior experiment detailed in table 2 shows that larger objects were more frequently and accurately re- trieved during single-object image searches. this \ufb01nding highlights the image encoder s bias towards larger objects which receive disproportionate emphasis in the \ufb01nal image representation. further detailed results speci\ufb01cally for sce- narios with 2 3 and 5 objects are provided in the appendix 7.5. 3.3. coco dataset experiments to validate the generalizability of our \ufb01ndings from the synthetic dataset we conducted similar experiments on the coco dataset which comprises real images with accom- panying captions. this real-world dataset allowed us to in- vestigate whether the previously observed biases persist in more naturalistic settings. due to the absence of single-object images for coco objects we approached the ior experiment in two ways. first we used single-object images from the domainnet dataset 11 as retrieval targets. second we introduced an alternative approach called image-to-text object retrieval i2tor . in i2tor we used the textual names of coco 9310 page 4 dvh pdjh b 7kuhh 2emhfwv pdjh 6lqjoh 2emhfw pdjh zklfk pdwfklqjb dvh pdjh 3 6fruh 3 6fruh 3 6fruh 2wkhub6lqjohb2emhfw pdjhv 3 6fruh 3 6fruh 3 6fruh e d 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3b pdjh qfrghu 3 6fruh 3 6fruh 3 6fruh 3 6fruh 3 6fruh 3 6fruh 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu 3b 7h w qfrghu dvh 7h w b 7kuhh 2emhfwv 7h w 6lqjoh 2emhfw 7h w zklfk pdwfklqjb dvh 7h w 2wkhub6lqjohb2emhfw 7h wv sl d dqgbdssoh dqgbghvn dssoh sl d ghvn kdw fdu d h 725 25 figure 2. experimental setup for text-based object retrieval tor and image-based object retrieval ior tasks. a tor the clip text encoder generates embeddings for multi-object and single-object texts. cosine similarity scores are calculated between the base text embedding and single-object text embeddings to identify the most similar object. b ior the clip image encoder generates embeddings for multi-object and single-object images. cosine similarity scores are calculated between the base image embedding and single-object image embeddings to identify the most similar object. table 1. performance on tor for comco datasets task model first obj second obj third obj fourth obj tor clip laion 63.96 21.59 10.68 3.76 clip datacomp 71.13 16.26 8.74 3.87 clip roberta 44.03 23.73 18.07 14.18 siglip 58.11 21.16 10.99 9.73 clip openai 50.31 20.74 14.45 6.79 negclip 51.63 28.92 14.86 4.59 sugarcrepe 44.29 30.32 18.73 6.66 table 2. performance on ior for comco datasets task model large object small obj 1 small obj 2 small obj 3 ior clip laion 85.45 6.36 5.45 2.73 clip datacomp 85.16 5.65 4.95 4.24 clip roberta 87.40 8.66 2.36 1.57 siglip 77.66 10.11 6.38 5.85 clip openai 65.22 17.39 8.70 8.70 negclip 61.67 15.00 13.33 10.00 sugarcrepe 60.0 18.38 16.85 4.7 objects instead of single-object images. these object names were embedded using clip s text encoder allowing us to perform a retrieval task consistent with the ior methodol- ogy while adapting to the constraints of the coco dataset. tables 3 and 4 present the results of our coco dataset experiments. in tor the \ufb01rst-mentioned object in coco captions was retrieved with higher accuracy which aligns with our earlier \ufb01ndings of bias in the text encoder. simi- larly in ior larger objects in coco images were retrieved more accurately consistent with the trends observed in our synthetic dataset experiments. the i2tor results further con\ufb01rmed this bias demonstrating that even when using textual object representations the bias towards larger ob- table 3. performance on tor for coco dataset task model first obj second obj third obj fourth obj tor clip openai 35.24 21.90 20.48 22.38 clip laion 67.89 13.76 8.26 10.09 clip datacomp 57.68 17.68 12.75 11.88 clip roberta 40.78 23.30 20.39 15.53 siglip 49.47 26.84 12.11 11.58 negclip 38.69 22.11 17.09 22.11 table 4. performance on ior for coco dataset task model large object small obj 1 small obj 2 small obj 3 ior clip openai 43.02 28.82 17.13 11.03 clip laion 39.44 28.45 17.70 14.41 clip datacomp 36.71 29.55 19.13 14.61 clip roberta 36.71 28.61 19.82 14.86 siglip 36.63 28.29 20.02 15.06 negclip 44.04 28.86 16.48 10.62 i2tor clip openai 51.49 24.87 13.68 9.97 clip laion 45.50 27.02 15.91 11.56 clip datacomp 46.64 26.82 14.53 12.01 clip roberta 44.69 26.98 16.04 12.29 siglip 47.09 27.07 15.10 10.74 negclip 49.04 27.07 14.08 9.81 jects persists. our experiments reveal two signi\ufb01cant biases in the clip model the text encoder shows a strong preference for the \ufb01rst mentioned object in textual descriptions while the image encoder exhibits greater sensitivity to larger objects in images. these biases can signi\ufb01cantly impact the overall system performance in various vision-language tasks par- ticularly in multi-object scenarios. 9311 page 5 4. origin of bias in clip models in this section we investigate the potential origins of the biases observed in clip models and provide evidence sup- porting our hypotheses. 4.1. bias in the image encoder the observed bias favoring larger objects within the image domain can be attributed to the architectural characteristics of vision transformers vit 2 utilized in clip s image encoder. our hypothesis is that larger objects which occupy a greater number of patches in the vit s patch-based image representation exert a more signi\ufb01cant in\ufb02uence on the \ufb01- nal class cls token representation. this bias is not exclu- sive to clip it appears to be a consistent feature across vit models as demonstrated by our experiments detailed in the appendix. to substantiate this hypothesis we designed an experi- ment to quantify the attention allocated by the cls token to each image patch. by calculating the cumulative atten- tion received by each object from the cls token we could assess the in\ufb02uence of object size on attention allocation. we applied this analysis to our three-object comco dataset and the results are illustrated in figure 3. the \ufb01ndings con- \ufb01rm our hypothesis larger objects indeed receive more at- tention from the cls token. 4.2. bias in the text encoder we explore the bias present in the text encoder from two perspectives the attention mechanism in the model struc- ture and the model s training method. 4.2.1. impact of attention mechanism text encoder models can be categorized based on their at- tention mechanisms uni-directional causal attention and bi-directional attention. in models with causal attention each token attends only to preceding tokens whereas in bi- directional models each token attends to all tokens in the sequence. when openai introduced the clip model its text en- coder employed causal attention meaning each token could only attend to tokens before it and itself. this differs from typical self-attention mechanisms where tokens at- tend to all other tokens. most clip models use causal self- attention with the exception of the variant using the xlm- roberta text encoder which also employs self-attention. however as shown in table 1 even this model exhibits the mentioned bias. this indicates that the bias does not origi- nate from the attention mechanism itself. 4.2.2. role of training method to determine whether the observed bias is speci\ufb01c to clip models we compared clip s text encoder with two other table 5. performance on toc and tor for comco datasets task model first obj second obj third obj fourth obj tor clip 56.28 22.71 13.17 7.48 sbert 29.02 19.80 17.50 33.57 simcse 7 27.59 19.07 17.76 34.83 models designed to embed sentences into a meaningful se- mantic space sentence-bert sbert 14 and simcse 7 . the primary distinction is that clip s embedding space is shared between images and text whereas sbert and simcse operate solely in the text domain. we conducted the tor experiment on our dataset using these models. as presented in table 5 the bias observed in clip differs from that in the other models. this suggests that clip s unique training method which aligns images and text in a shared embedding space through contrastive learning contributes to the bias. therefore to uncover the root cause of the bias we focus on the speci\ufb01cs of clip s training procedure. 4.3. hypothesized origin of text-side bias in clip we hypothesize that the text-side bias in clip which fa- vors objects mentioned earlier in text descriptions origi- nates from the image-side bias toward larger objects and is transferred to the text encoder during contrastive training. we present evidence supporting this hypothesis through two key claims and an analysis of the training progression. claim 1 larger objects have more in\ufb02uence on text embeddings. building upon the established image-side bias discussed earlier we posit that objects with larger physical sizes exert more in\ufb02uence on clip s text em- beddings due to the alignment enforced during contrastive training. to test this we categorized objects in the domain- net dataset into large medium and small groups based on their relative physical sizes in real-world with the full list of objects provided in the appendix 7.10 . speci\ufb01cally objects smaller than a school bag were categorized as small objects sized between a school bag and a medium-sized car were classi\ufb01ed as medium and objects larger than a car up to signi\ufb01cantly larger items were considered large. we then constructed two sets of sentences each containing four ob- jects one set with a large object mentioned \ufb01rst followed by three medium-sized objects and another with a small object mentioned \ufb01rst followed by three medium-sized objects. figure 4.a compares the tor accuracy for the \ufb01rst ob- ject in these two groups. the higher tor accuracy for sen- tences beginning with large objects supports our hypothe- sis that larger objects when mentioned \ufb01rst have a more signi\ufb01cant impact on the text embeddings due to the cross- modal alignment with their prominent representation in im- ages. 9312 page 6 a b figure 3. attention allocation from the cls token to objects of different sizes in the comco dataset. a qualitative results showing the cls token s attention to each object. b quantitative analysis of attention distribution across 8 000 images with each image containing one large and two small objects. the bar chart shows the average attention allocated to the large object versus the smaller ones demonstrating a bias towards larger objects. a b c figure 4. a top-1 object retrieval accuracy comparison for sentences where the \ufb01rst object is either large or small. the higher tor accuracy for sentences beginning with large objects supports the hypothesis that larger objects when mentioned \ufb01rst exert a stronger in\ufb02uence on text embeddings due to cross-modal alignment with their prominent visual representation in images. b distribution of the position of the largest object within image captions from the laion datasets. the results show a consistent bias where larger objects tend to be mentioned earlier in text descriptions. c progression of tor rates across different training stages indicating that text-side bias strengthens as the model is exposed to more data suggesting the cumulative effect of image-side bias being transferred to the text encoder through contrastive learning. claim 2 caption bias in training datasets. to inves- tigate potential biases in clip s training data we analyzed both the laion 19 and coco datasets. due to limited computational resources and the large size of the laion dataset which contains over 2 billion image-text pairs we randomly selected a subset of 200 000 samples for our anal- ysis. using the llama3 model we extracted objects from the image captions and employed the language segment- anything tool to generate object masks in the correspond- ing images calculating their areas based on these masks. a detailed description of our laion dataset analysis method- ology can be found in appendix 7.8. figure4.b shows the position of the largest object within each caption. the results indicate that in the majority of cases the largest object in an image is mentioned earlier in its caption. the same experiment was conducted on the coco dataset with detailed results and the distribution for two to \ufb01ve object scenarios provided in appendix 7.9. this demonstrates a consistent bias in the training data where larger objects are not only more visually prominent but are also described earlier in text annotations. analysis of bias development during training. to fur- ther validate our hypothesis we examined the progression of text-side bias during clip s training. we utilized model checkpoints from the laion dataset at \ufb01ve training stages corresponding to exposure to 2 4 6 8 and 10 billion sam- ples. we conducted tor experiments at each stage focus- 9313 page 7 ing on the retrieval accuracy for the \ufb01rst object mentioned in text descriptions. figure4.c depicts the evolution of the tor rate across different training stages for scenarios with varying numbers of objects from 3 to 8 . the consistent upward trend in the tor rate as the model is exposed to more training data suggests that the text-side bias strengthens over time likely due to the cumulative effect of the image-side bias being transferred to the text encoder through contrastive learning. incomplete text representation of clip here we want to theoretically highlight why the clip text encoder could learn an incomplete representation of the text. let z and w represent a latent representation of an image content de- scribed in the caption and such visual content not men- tioned in the text respectively. for example z represents the fact that an image contains a horse that is eating the grass. in this case w might represent other details in the image like the horse color where the horse is located etc. we assume a data generative process as follows i g z w t h z where i is the image and t is its corresponding caption. now we want to learn a joint embedding of the image and text through the clip. here we assume that f\u03b8 . and i\u03c9 . as learnable functions that map the image and text into the joint embedding space respectively. theorem 1 let elements of z be independent zero-mean and unit-variance. the contrastive loss for the ideal text en- coder i\u03c9 t z converges to that of a non-ideal incomplete one i.e. i\u03c9 t zs where zs is the \ufb01rst d k dimensions of z with k being a constant and d . proof the contrastive loss in making this learning hap- pen can be written as ez z w exp sim z z exp sim z z k exp sim z z k 1 with sim z z s f\u03b8 g z w i\u03c9 h z and z and z k 1 k b are b 1 i.i.d. samples of the content in the representation space and s is some normal- ized similarity metric e.g. cosine similarity and b 1 is the batch size. we assume that elements of z are independent unit-variance and zero mean. we further assume that the dimensionality of z denoted as d goes to in\ufb01nity. under such conditions and based on law of large numbers z p d when d is large. therefore for any two independent copies of z z k we have sim z z k z z k z z k p 0. it is evident that in the ideal case f\u03b8 g z w z and also i\u03c9 h z z so the contrastive loss would converge to e e b as the numerator is e and the second term in the denominator converges to exp 0 1 according to the mann-wald s theorem. however we show that other learning of this representa- tion could achieve the same amount of loss. for instance let zs be the \ufb01rst d k elements of z with k being a constant. we show that if f\u03b8 i zs and i\u03c9 t zs the same loss would be achieved in the limit of large d. to see this note that the numerator stays the same i.e. e while the second term in the denominator still converges to bexp 0 b. this means that even if the image and text encoder of the clip only partially recover the content embedding they reach an excellent loss. but such possible incomplete rep- resentations of z are combinatorially large making conver- gence of the clip to such local minima pretty likely. this makes the text encoding of clip be far from ideal. fur- thermore the text encoder would become biased depend- ing on which of such local minima it converges to. based on this explanation we would expect a text encoder that has learned a complete representation to exhibit such biases to a lesser degree. as mentioned earlier the subject of learning text representations in vlms that are discriminative of hard negatives e.g. negclip has been around for few years. we tested one of strongest such models 8 in our bench- mark to validate the hypothesis that an incomplete text rep- resentation is one of the causes of the bias in the vlms. we noticed that this model shows lower bias based on our benchmark see the sugarcrepe model in tables 1 and 2 . we have developed an initial approach to address the identi\ufb01ed bias in the clip model which is presented in appendix 7.12. while this method is speci\ufb01c to our cur- rent dataset it represents a promising step toward address- ing these challenges and can inspire further advancements. this work demonstrates our commitment to exploring prac- tical solutions while maintaining the primary focus of this study on the analysis of bias and its implications. 5. practical impacts of encoder biases the biases observed in clip s image and text encoders sig- ni\ufb01cantly impact model performance in real-world appli- cations. this section explores how these biases manifest in image-text matching tasks while further analyses of text-to- image generation impacts are presented in appendix 7.11. our analysis in this section serves two primary purposes. first it provides concrete evidence of how these theoretical biases can translate into practical limitations. second it of- fers insights into potential areas for improvement in vision- language models particularly in handling complex multi- 9314 page 8 figure 5. an example of the correct and incorrect caption structures in the \ufb01rst and second scenarios. object scenarios. through a series of carefully designed ex- periments we illustrate how the biases in both text and im- age encoders can lead to unexpected or suboptimal results in tasks that are crucial for many downstream applications. 5.1. image-text matching building upon our \ufb01ndings of biases in clip s image and text encoders we now demonstrate how these biases tangi- bly affect the model s performance in image-caption match- ing tasks. we designed two experimental scenarios con- ducted on both the comco and coco datasets to evaluate these biases. the results of these experiments are summa- rized in table 6. to better illustrate the differences between these two scenarios an example of the caption structures is shown in figure 5. in each scenario we created incorrect captions by switching one object in the caption with an ob- ject that is not present in the image. additionally gpt-4o 1 was used to rewrite the captions in the coco dataset. first scenario in the \ufb01rst scenario biases assist the model in distinguishing between the correct and incorrect captions. in the correct captions the largest object in the image is placed at the beginning aligning with the model s bias towards prioritizing \ufb01rst-mentioned objects and larger objects. for the incorrect captions the non-existent object is deliberately placed at the beginning which helps the model recognize the difference between the correct and incorrect captions more effectively. this positioning emphasizes the discrepancy early on allowing the model to better detect the mismatch between the caption and the image. the perfor- mance of different models in this scenario can be seen in table 6 under the first scenario column. second scenario in the second scenario biases lead the model to make errors. the correct captions place the largest object at the end of the sentence disrupting the model s bias towards objects mentioned earlier and its preference for larger objects. in the incorrect captions the non-existent object is placed at the end making it more dif\ufb01cult for the model to differentiate between correct and incorrect cap- tions as its attention is drawn away from the critical discrep- ancies. the performance of different models in this scenario is shown in table 6 under the second scenario column. table 6. performance comparison on image-text matching for comco and coco datasets dataset model first scenario second scenario comco clip datacomp 6 99.99 67.50 clip roberta 99.98 64.75 siglip 22 99.49 72.36 clip openai 99.59 52.23 negclip 96.82 46.94 sugarcrepe 98.55 60.43 coco clip datacomp 6 71.2 54.2 clip roberta 72.2 54.1 siglip 22 64.8 39.5 clip openai 63.5 26.4 negclip 72 28.7 sugarcrepe 80.0 40.9 by comparing these two scenarios we demonstrate that biases in clip can either help or hinder the model s perfor- mance depending on how captions are structured. the ex- perimental results particularly with the use of gpt-4o for caption rephrasing in the coco dataset reveal how such biases can in\ufb02uence the accuracy of image-text matching tasks. these biases must be addressed to improve clip s robustness in real-world multi-object scenarios. for further insights on how these biases affect text-to- image generation refer to our extended experiments in ap- pendix 7.11. 6. conclusion this study uncovers biases in clip s encoders with the text encoder favoring \ufb01rst-mentioned objects and the im- age encoder emphasizing larger ones which impacts per- formance in multi-object tasks. using the comco dataset we highlighted these biases effects on object representation and positioning underscoring the need for balanced train- ing. we attribute these biases to clip s contrastive frame- work where alignment issues propagate across modalities. addressing these biases is essential for vision-language ad- vancements as seen with models like stable diffusion. 9315 page 9 references 1 josh achiam steven adler sandhini agarwal lama ah- mad ilge akkaya florencia leoni aleman diogo almeida janko altenschmidt sam altman shyamal anadkat et al. gpt-4 technical report. arxiv preprint arxiv 2303.08774 2023. 8 2 dosovitskiy alexey. an image is worth 16x16 words trans- formers for image recognition at scale. arxiv preprint arxiv 2010.11929 2020. 5 3 mehdi cherti romain beaumont ross wightman mitchell wortsman gabriel ilharco cade gordon christoph schuh- mann ludwig schmidt and jenia jitsev. reproducible scaling laws for contrastive language-image learning. in 2023 ieee cvf conference on computer vision and pat- tern recognition cvpr . ieee 2023. 1 4 sri harsha dumpala aman jaiswal chandramouli sas- try evangelos milios sageev oore and hassan saj- jad. sugarcrepe dataset vision-language model sensi- tivity to semantic and lexical alterations. arxiv preprint arxiv 2406.11171 2024. 1 5 samir yitzhak gadre gabriel ilharco alex fang jonathan hayase georgios smyrnis thao nguyen ryan marten mitchell wortsman dhruba ghosh jieyu zhang eyal or- gad rahim entezari giannis daras sarah pratt vivek ramanujan yonatan bitton kalyani marathe stephen mussmann richard vencu mehdi cherti ranjay krishna pang wei koh olga saukh alexander ratner shuran song hannaneh hajishirzi ali farhadi romain beaumont sewoong oh alex dimakis jenia jitsev yair carmon vaishaal shankar and ludwig schmidt. datacomp in search of the next generation of multimodal datasets 2023. 1 6 samir yitzhak gadre gabriel ilharco alex fang jonathan hayase georgios smyrnis thao nguyen ryan marten mitchell wortsman dhruba ghosh jieyu zhang et al. dat- acomp in search of the next generation of multimodal datasets. advances in neural information processing sys- tems 36 2024. 8 17 7 tianyu gao xingcheng yao and danqi chen. simcse simple contrastive learning of sentence embeddings. arxiv preprint arxiv 2104.08821 2021. 5 8 cheng-yu hsieh jieyu zhang zixian ma aniruddha kem- bhavi and ranjay krishna. sugarcrepe fixing hackable benchmarks for vision-language compositionality. advances in neural information processing systems 36 2024. 1 7 9 tsung-yi lin michael maire serge belongie lubomir bourdev ross girshick james hays pietro perona deva ramanan c. lawrence zitnick and piotr doll ar. microsoft coco common objects in context 2015. 2 10 zixian ma jerry hong mustafa omer gul mona gandhi irena gao and ranjay krishna. crepe can vision-language foundation models reason compositionally in proceedings of the ieee cvf conference on computer vision and pat- tern recognition pages 10910 10921 2023. 1 11 xingchao peng qinxun bai xide xia zijun huang kate saenko and bo wang. moment matching for multi-source domain adaptation. in proceedings of the ieee cvf inter- national conference on computer vision pages 1406 1415 2019. 3 12 dustin podell zion english kyle lacey andreas blattmann tim dockhorn jonas m uller joe penna and robin rombach. sdxl improving latent diffusion mod- els for high-resolution image synthesis. arxiv preprint arxiv 2307.01952 2023. 17 13 alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sastry amanda askell pamela mishkin jack clark gretchen krueger and ilya sutskever. learning transferable visual models from natural language supervision 2021. 1 14 n reimers. sentence-bert sentence embeddings using siamese bert-networks. arxiv preprint arxiv 1908.10084 2019. 5 15 dillon reis jordan kupec jacqueline hong and ahmad daoudi. real-time \ufb02ying object detection with yolov8. arxiv preprint arxiv 2305.09972 2023. 17 16 robin rombach andreas blattmann dominik lorenz patrick esser and bj orn ommer. high-resolution image synthesis with latent diffusion models. in proceedings of the ieee cvf conference on computer vision and pattern recognition cvpr pages 10684 10695 2022. 17 17 ugur sahin hang li qadeer khan daniel cremers and volker tresp. enhancing multimodal compositional reason- ing of visual language models with generative negative min- ing. in proceedings of the ieee cvf winter conference on applications of computer vision pages 5563 5573 2024. 1 18 christoph schuhmann richard vencu romain beaumont robert kaczmarczyk clayton mullis aarush katta theo coombes jenia jitsev and aran komatsuzaki. laion-400m open dataset of clip-\ufb01ltered 400 million image-text pairs 2021. 1 19 christoph schuhmann romain beaumont richard vencu cade gordon ross wightman mehdi cherti theo coombes aarush katta clayton mullis mitchell worts- man et al. laion-5b an open large-scale dataset for training next generation image-text models. advances in neural in- formation processing systems 35 25278 25294 2022. 6 20 tristan thrush ryan jiang max bartolo amanpreet singh adina williams douwe kiela and candace ross. winoground probing vision and language models for visio- linguistic compositionality. in proceedings of the ieee cvf conference on computer vision and pattern recognition pages 5238 5248 2022. 1 21 mert yuksekgonul federico bianchi pratyusha kalluri dan jurafsky and james zou. when and why vision- language models behave like bags-of-words and what to do about it in the eleventh international conference on learning representations 2023. 1 22 xiaohua zhai basil mustafa alexander kolesnikov and lucas beyer. sigmoid loss for language image pre-training. in proceedings of the ieee cvf international conference on computer vision pages 11975 11986 2023. 8 17 23 tiancheng zhao tianqi zhang mingwei zhu haozhan shen kyusong lee xiaopeng lu and jianwei yin. vl- checklist evaluating pre-trained vision-language models 9316 page 10 with objects attributes and relations. arxiv preprint arxiv 2207.00221 2022. 1 9317",
      "keywords": [
        "page",
        "clip",
        "under",
        "microscope",
        "fine-grained",
        "analysis",
        "multi-object",
        "representation",
        "reza",
        "abbasi",
        "ali",
        "nazari",
        "aminreza",
        "se\ufb01d",
        "mohammadali",
        "banayeeanzade",
        "mohammad",
        "hossein",
        "rohban",
        "mahdieh",
        "soleymani",
        "baghshah",
        "sharif",
        "university",
        "technology",
        "tehran",
        "iran",
        "reza.abbasi",
        "ali.nazari02",
        "aminreza.sefid",
        "a.banayeean",
        "sharif.edu",
        "abstract",
        "contrastive",
        "language-image",
        "pre-training",
        "mod-",
        "els",
        "excel",
        "zero-shot",
        "classi\ufb01cation",
        "yet",
        "face",
        "challenges",
        "complex",
        "scenarios.",
        "study",
        "offers",
        "com-",
        "prehensive",
        "limitations",
        "contexts",
        "using",
        "specialized",
        "dataset",
        "comco",
        "designed",
        "evaluate",
        "encoders",
        "diverse",
        "\ufb01ndings",
        "reveal",
        "signi\ufb01cant",
        "biases",
        "text",
        "encoder",
        "prior-",
        "itizes",
        "\ufb01rst-mentioned",
        "objects",
        "image",
        "fa-",
        "vors",
        "larger",
        "objects.",
        "through",
        "retrieval",
        "tasks",
        "quantify",
        "across",
        "multiple",
        "vari-",
        "ants",
        "trace",
        "origins",
        "training",
        "process",
        "sup-",
        "ported",
        "analyses",
        "laion",
        "pro-",
        "gression.",
        "image-text",
        "matching",
        "experiments",
        "show",
        "sub-",
        "stantial",
        "performance"
      ],
      "metadata": {
        "filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf",
        "original_filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 1583559,
        "upload_file_id": "upload_1751635583781_h98buqrlt",
        "upload_timestamp": "2025-07-04T13:26:23.834925",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-04T13:26:23.938343+00:00",
      "content_length": 40912,
      "content_type": ".pdf",
      "filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf"
    },
    "3": {
      "file_id": "3",
      "content": "\n=== Page 1 ===\nKEXUAN ZHANG\nACCOUNT SUMMARY | ACCOUNTS\nFOR HOME\n CUSTOMER SERVICE\n|\nSet up a new account\nOpen up a new Toronto Hydro account.\nYour reference number is: C2RVR2\nDate/time submitted: 04/29/2025 07:42 PM\nThank you for submitting your request to set up a new account. Your request will be processed within two business days and you'll receive an\nemail confirmation once complete. If additional information is required, we'll be in touch.\nInformation you provided to us\nNew service address\n72 ESTHER SHINER BLVD SUITE 1210, Toronto, Ontario, M2K 0C4\nPrimary account holder\nFull name: Zhang, Kexuan\nPossession date: 5/1/2025\nPrimary phone number: (437) 361-2016\nEmail address: zhangkexuan0514@outlook.com\nReceive paperless statement: Yes\nDate of birth: 5/14/2002\nCanadian Driver's Licence Number: Z3187-43400-20514\nPricing plan: Time-of-Use (TOU)\nMailing address\n72 ESTHER SHINER BLVD SUITE 1210, Toronto, Ontario, M2K 0C4\nLandlord/property manager information\nFull name: Merve Bekaroglu\nPhone number: (416) 568-6988\nPrint this page\nEasy ways to help you manage your account\nWant an easy way to manage your Toronto Hydro account? Learn more about our My TorontoHydro\u2122 online self-service portal today!\nLooking for ways to better manage your bill? See our conservation programs and find ways to save!\nLearn about our payment options .\nStarting on May 1, summer Time-of-Use (TOU) hours and Tier thresholds will be in effect. Customers on the Ultra-Low Overnight price plan will not be affected,\nand electricity rates will remain unchanged.\n\u00d7\nHome\nBusiness\nContractors & Developers\nLog Out\n",
      "searchable_content": "page 1 kexuan zhang account summary accounts for home customer service set up a new account open up a new toronto hydro account. your reference number is c2rvr2 date time submitted 04 29 2025 07 42 pm thank you for submitting your request to set up a new account. your request will be processed within two business days and you ll receive an email confirmation once complete. if additional information is required we ll be in touch. information you provided to us new service address 72 esther shiner blvd suite 1210 toronto ontario m2k 0c4 primary account holder full name zhang kexuan possession date 5 1 2025 primary phone number 437 361-2016 email address zhangkexuan0514 outlook.com receive paperless statement yes date of birth 5 14 2002 canadian driver s licence number z3187-43400-20514 pricing plan time-of-use tou mailing address 72 esther shiner blvd suite 1210 toronto ontario m2k 0c4 landlord property manager information full name merve bekaroglu phone number 416 568-6988 print this page easy ways to help you manage your account want an easy way to manage your toronto hydro account learn more about our my torontohydro online self-service portal today looking for ways to better manage your bill see our conservation programs and find ways to save learn about our payment options . starting on may 1 summer time-of-use tou hours and tier thresholds will be in effect. customers on the ultra-low overnight price plan will not be affected and electricity rates will remain unchanged. home business contractors developers log out",
      "keywords": [
        "page",
        "kexuan",
        "zhang",
        "account",
        "summary",
        "accounts",
        "home",
        "customer",
        "service",
        "set",
        "new",
        "open",
        "toronto",
        "hydro",
        "account.",
        "reference",
        "number",
        "c2rvr2",
        "date",
        "time",
        "submitted",
        "2025",
        "thank",
        "submitting",
        "request",
        "processed",
        "within",
        "two",
        "business",
        "days",
        "receive",
        "email",
        "confirmation",
        "once",
        "complete.",
        "additional",
        "information",
        "required",
        "touch.",
        "provided",
        "address",
        "esther",
        "shiner",
        "blvd",
        "suite",
        "1210",
        "ontario",
        "m2k",
        "0c4",
        "primary",
        "holder",
        "full",
        "name",
        "possession",
        "phone",
        "437",
        "361-2016",
        "zhangkexuan0514",
        "outlook.com",
        "paperless",
        "statement",
        "yes",
        "birth",
        "2002",
        "canadian",
        "driver",
        "licence",
        "z3187-43400-20514",
        "pricing",
        "plan",
        "time-of-use",
        "tou",
        "mailing",
        "landlord",
        "property",
        "manager",
        "merve",
        "bekaroglu",
        "416",
        "568-6988",
        "print",
        "easy",
        "ways",
        "help",
        "manage",
        "want",
        "way",
        "learn",
        "more",
        "about",
        "torontohydro",
        "online",
        "self-service",
        "portal",
        "today",
        "looking",
        "better",
        "bill",
        "see",
        "conservation"
      ],
      "metadata": {
        "filename": "New Account - Toronto Hydro.pdf",
        "original_filename": "New Account - Toronto Hydro.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 106280,
        "upload_file_id": "upload_1751635964788_8qxpnmtxw",
        "upload_timestamp": "2025-07-04T13:32:44.822318",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-04T13:32:44.928821+00:00",
      "content_length": 1597,
      "content_type": ".pdf",
      "filename": "New Account - Toronto Hydro.pdf"
    },
    "14": {
      "file_id": "14",
      "content": "\n=== Page 1 ===\nKEXUAN ZHANG\nACCOUNT SUMMARY | ACCOUNTS\nFOR HOME\n CUSTOMER SERVICE\n|\nSet up a new account\nOpen up a new Toronto Hydro account.\nYour reference number is: C2RVR2\nDate/time submitted: 04/29/2025 07:42 PM\nThank you for submitting your request to set up a new account. Your request will be processed within two business days and you'll receive an\nemail confirmation once complete. If additional information is required, we'll be in touch.\nInformation you provided to us\nNew service address\n72 ESTHER SHINER BLVD SUITE 1210, Toronto, Ontario, M2K 0C4\nPrimary account holder\nFull name: Zhang, Kexuan\nPossession date: 5/1/2025\nPrimary phone number: (437) 361-2016\nEmail address: zhangkexuan0514@outlook.com\nReceive paperless statement: Yes\nDate of birth: 5/14/2002\nCanadian Driver's Licence Number: Z3187-43400-20514\nPricing plan: Time-of-Use (TOU)\nMailing address\n72 ESTHER SHINER BLVD SUITE 1210, Toronto, Ontario, M2K 0C4\nLandlord/property manager information\nFull name: Merve Bekaroglu\nPhone number: (416) 568-6988\nPrint this page\nEasy ways to help you manage your account\nWant an easy way to manage your Toronto Hydro account? Learn more about our My TorontoHydro\u2122 online self-service portal today!\nLooking for ways to better manage your bill? See our conservation programs and find ways to save!\nLearn about our payment options .\nStarting on May 1, summer Time-of-Use (TOU) hours and Tier thresholds will be in effect. Customers on the Ultra-Low Overnight price plan will not be affected,\nand electricity rates will remain unchanged.\n\u00d7\nHome\nBusiness\nContractors & Developers\nLog Out\n",
      "searchable_content": "page 1 kexuan zhang account summary accounts for home customer service set up a new account open up a new toronto hydro account. your reference number is c2rvr2 date time submitted 04 29 2025 07 42 pm thank you for submitting your request to set up a new account. your request will be processed within two business days and you ll receive an email confirmation once complete. if additional information is required we ll be in touch. information you provided to us new service address 72 esther shiner blvd suite 1210 toronto ontario m2k 0c4 primary account holder full name zhang kexuan possession date 5 1 2025 primary phone number 437 361-2016 email address zhangkexuan0514 outlook.com receive paperless statement yes date of birth 5 14 2002 canadian driver s licence number z3187-43400-20514 pricing plan time-of-use tou mailing address 72 esther shiner blvd suite 1210 toronto ontario m2k 0c4 landlord property manager information full name merve bekaroglu phone number 416 568-6988 print this page easy ways to help you manage your account want an easy way to manage your toronto hydro account learn more about our my torontohydro online self-service portal today looking for ways to better manage your bill see our conservation programs and find ways to save learn about our payment options . starting on may 1 summer time-of-use tou hours and tier thresholds will be in effect. customers on the ultra-low overnight price plan will not be affected and electricity rates will remain unchanged. home business contractors developers log out",
      "keywords": [
        "page",
        "kexuan",
        "zhang",
        "account",
        "summary",
        "accounts",
        "home",
        "customer",
        "service",
        "set",
        "new",
        "open",
        "toronto",
        "hydro",
        "account.",
        "reference",
        "number",
        "c2rvr2",
        "date",
        "time",
        "submitted",
        "2025",
        "thank",
        "submitting",
        "request",
        "processed",
        "within",
        "two",
        "business",
        "days",
        "receive",
        "email",
        "confirmation",
        "once",
        "complete.",
        "additional",
        "information",
        "required",
        "touch.",
        "provided",
        "address",
        "esther",
        "shiner",
        "blvd",
        "suite",
        "1210",
        "ontario",
        "m2k",
        "0c4",
        "primary",
        "holder",
        "full",
        "name",
        "possession",
        "phone",
        "437",
        "361-2016",
        "zhangkexuan0514",
        "outlook.com",
        "paperless",
        "statement",
        "yes",
        "birth",
        "2002",
        "canadian",
        "driver",
        "licence",
        "z3187-43400-20514",
        "pricing",
        "plan",
        "time-of-use",
        "tou",
        "mailing",
        "landlord",
        "property",
        "manager",
        "merve",
        "bekaroglu",
        "416",
        "568-6988",
        "print",
        "easy",
        "ways",
        "help",
        "manage",
        "want",
        "way",
        "learn",
        "more",
        "about",
        "torontohydro",
        "online",
        "self-service",
        "portal",
        "today",
        "looking",
        "better",
        "bill",
        "see",
        "conservation"
      ],
      "metadata": {
        "filename": "New Account - Toronto Hydro.pdf",
        "original_filename": "New Account - Toronto Hydro.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 106280,
        "upload_file_id": "upload_1751660314470_jg87aamfh",
        "upload_timestamp": "2025-07-04T20:18:34.537821",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-04T20:18:35.702004+00:00",
      "content_length": 1597,
      "content_type": ".pdf",
      "filename": "New Account - Toronto Hydro.pdf"
    },
    "15": {
      "file_id": "15",
      "content": "\n=== Page 1 ===\nSEESAW: HIGH-THROUGHPUT LLM INFERENCE VIA MODEL RE-SHARDING\nQidong Su123 Wei Zhao34 Xin Li3 Muralidhar Andoorveedu3 Chenhao Jiang12 Zhanda Zhu123 Kevin Song12\nChristina Giannoula123 Gennady Pekhimenko123\nABSTRACT\nTo improve the efficiency of distributed large language model (LLM) inference, various parallelization strategies,\nsuch as tensor and pipeline parallelism, have been proposed. However, the distinct computational characteristics\ninherent in the two stages of LLM inference\u2014prefilling and decoding\u2014render a single static parallelization\nstrategy insufficient for the effective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind Seesaw is dynamic model re-\nsharding, a technique that facilitates the dynamic reconfiguration of parallelization strategies across stages, thereby\nmaximizing throughput at both phases. To mitigate re-sharding overhead and optimize computational efficiency,\nwe employ tiered KV cache buffering and transition-minimizing scheduling. These approaches work synergistically\nto reduce the overhead caused by frequent stage transitions while ensuring maximum batching efficiency. Our\nevaluation demonstrates that Seesaw achieves a throughput increase of up to 1.78\u00d7 (1.36\u00d7 on average) compared\nto vLLM, the most widely used state-of-the-art LLM inference engine.\n1\nINTRODUCTION\nLarge language models (LLMs), such as the LLaMA (Tou-\nvron et al., 2023a) and GPT (Achiam et al., 2023) families,\nhave demonstrated exceptional performance across a wide\nrange of tasks. Beyond their prevalent use in interactive\napplications like chatbots (OpenAI, 2024), LLMs are also\ngaining high interest in throughput-oriented offline inference\nworkloads such as information extraction (Narayan et al.,\n2022), database querying (Liu et al., 2024), and knowledge\ngraph processing (Edge et al., 2024). Unlike interactive\napplications where low latency is crucial, these offline in-\nference tasks prioritize high throughput over response time.\nThese offline inference workloads are widely adopted in in-\ndustry (Kamsetty et al., 2023; Yu et al., 2024; Dell Technolo-\ngies, 2024; Chan et al., 2024), leading MLPerf to develop\nbenchmarks specifically for them (MLCommons, 2024). In\nthis work, we focus on improving inference efficiency for\noffline, throughput-oriented LLM inference workloads.\nAs LLMs often exceed the memory capacity of individual\nGPUs, parallelization is essential for their deployment (Ben-\nNun & Hoefler, 2019; Shoeybi et al., 2019). Several paral-\nlelization strategies, including tensor parallelism (Shoeybi\net al., 2019) and pipeline parallelism (Narayanan et al., 2019;\nHuang et al., 2019), have been proposed, each presenting\ndistinct trade-offs in memory efficiency, inter-device com-\nmunication, and computational efficiency. Tensor paral-\nlelism distributes model weights across devices but suffers\n1 University of Toronto\n2 Vector Institute\n3 CentML\n4 Stan-\nford University\nTP1PP8\nTP2PP4\nTP4PP2\nTP8PP1\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized Time\ncommunication\ncompute\nweight transfer\n(a) Prefill\nTP1PP8\nTP2PP4\nTP4PP2\nTP8PP1\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized Time\ncommunication\ncompute\nweight transfer\n(b) Decode\nFigure 1. Breakdown of execution time for the prefill and decode\nstages for LLaMA2-13B inference on 8 L4 GPUs (The global\nbatch size is 16. Pipeline parallelism further divides the data into\nmicro-batches of size 16/PP to fully utilize pipelining).\nfrom high communication costs due to frequent all-reduce\noperations at each layer (Pope et al., 2023; Chang et al.,\n2024). The communication cost becomes particularly severe\nin systems connected via PCIe (Dell Technologies, 2023) or\nwith partial high-speed connections (NVIDIA Corporation,\n2020). In contrast, pipeline parallelism partitions the model\ninto sequential stages, reducing inter-device communica-\ntion by passing only activations between them. However,\nto enable pipelining, each data batch needs to be divided\ninto micro-batches, leading to extra execution overheads,\nsince every micro-batch repeatedly loads weights into the\ncompute units (see Section 3.1 for details).\nWhile numerous studies have proposed methods to optimize\nparallelization strategies for LLMs (Miao et al., 2023; Kwon\net al., 2023; Li et al., 2023; Pope et al., 2023), prior works\ntypically rely on a single, static configuration throughout\narXiv:2503.06433v1  [cs.DC]  9 Mar 2025\n\n=== Page 2 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nthe entire generation process. However, our findings indi-\ncate that this one-size-fits-all approach is often inefficient\nfor throughput-oriented LLM inference because it fails to\nleverage the distinct patterns between the two stages in LLM\ngeneration: the prefill stage, where the input sequence is pro-\ncessed at once to produce the initial token, and the decode\nstage, where subsequent tokens are generated sequentially\nbased on prior tokens. These two stages exhibit fundamen-\ntally different computational characteristics (Yuan et al.,\n2024). During the prefill stage, multiple tokens from the\ninput prompt are processed simultaneously, making com-\nputation and communication the dominant contributors to\nruntime. In contrast, the decode stage processes one token at\na time for each sequence, increasing the relative time spent\non weight transfer. This difference indicates that the optimal\nparallelization strategy for each stage may also vary.\nTo illustrate the performance limitations of applying a uni-\nform parallelization strategy for both prefill and decode,\nwe measure the execution time of each stage under various\ncombinations of tensor and pipeline parallelism, as shown\nin Figure 1. In the prefill stage, as the degree of tensor par-\nallelism increases, the communication overhead increases\nsignificantly due to additional GPUs participating in all-\nreduce operations. As a result, tensor parallelism performs\nsignificantly worse than pipeline parallelism. In contrast,\nduring the decode stage, pipeline parallelism is slower than\ntensor parallelism, largely due to increased weight trans-\nferring overhead caused by micro-batching required for\npipelining (see Section 3.1 for more details). Therefore,\nwe need stage-specific parallelization strategies to provide\nbetter LLM inference throughput.\nAn existing approach is disaggregated prefill-decode (Zhong\net al., 2024; Qin et al., 2024), which assigns prefill and de-\ncode computation to different GPU instances. The prefill\ninstances and decode instances form a two-stage pipeline\nto serve inference requests. Therefore, the overall through-\nput of disaggregated prefill-decode is constrained by the\nslower of the two stages, and balancing throughput between\nthese two stages is essential. The key drawback of disag-\ngregated prefill-decode is that it can cause large amounts of\npipeline bubbles under resource-constrained environments.\nFor example, when deploying a 70B model on 8\u00d740GB\nGPUs, even the most balanced configuration results in a\n6\u00d7 difference in throughput between the prefill and decode\nstages. In this setup, the decode stage operates at one-sixth\nthe throughput of the prefill stage, resulting in a significant\nbottleneck at the prefill stage that slows down the entire\nsystem (see Section 3.2 for details).\nTo address these challenges, we present Seesaw, a high-\nthroughput LLM inference engine that dynamically recon-\nfigures parallelization strategies between the prefill and de-\ncode stages. The key idea behind Seesaw is model re-\ntime\nGPU max #seqs\ntime\nGPU max #seqs\ntime\nCPU max #seqs\n#seqs\n#seqs\n#seqs\nunder-utilize\ntoo frequent transitions\n(a) Prefill-prioritizing\n(b) Decode-prioritizing\n(c) Tiered KV cache buffering\u00a0\n\u00a0\u00a0\u00a0 + transition-minimizing scheduling\np\nd\np\nd\np\nd\np\nd\np\nd\np\nd\np\nd\np\nd\nGPU max #seqs\np\nd\np\nd\np\nFigure 2. Different scheduling policies considering transition over-\nhead. Decoding throughput is positively correlated with the num-\nber of sequences in GPU memory (the maximal batch size), which\nis highlighted as light green area.\nsharding, a novel technique that dynamically re-partitions\nmodel weights and KV cache 1 between prefill and decode\nstages. By tailoring parallelization strategies to the dis-\ntinct computational demands of each stage, Seesaw reduces\ncommunication overhead during the prefill stage, while en-\nhancing memory efficiency in the decode stage, resulting in\na substantial increase in overall throughput.\nHowever, the overhead associated with model re-sharding\ncan be high due to frequent transitions between prefill and\ndecode. To maximize throughput, existing systems typi-\ncally adopt prefill-prioritized scheduling (Yu et al., 2022;\nKwon et al., 2023), which interleaves prefill and decode\nstages across batches to achieve continuous batching. Yet,\nas illustrated in Figure 2(a), integrating this approach with\nmodel re-sharding can result in significant overhead due\nto frequent transitions between prefill and decode. On the\nother hand, decode-prioritized scheduling (NVIDIA, 2024a)\ncompletes all decode steps for a batch before proceeding to\nthe next, resulting in lower re-sharding overhead. However,\nas depicted in Figure 2(b), this method suffers from low\nresource utilization due to smaller batch sizes.\nTo overcome this constraint and achieve both minimal\nre-sharding overhead and large batch size, we propose\ntwo synergetic techniques: tiered KV cache buffering\nand transition-minimizing scheduling. Tiered KV cache\nbuffering leverages CPU memory as auxiliary storage for\nthe KV cache, enabling Seesaw to store the KV cache for\na large number of prefill requests. Transition-minimizing\nscheduling reduces re-sharding overhead by minimizing the\nnumber of transitions to the decode stage. Seesaw transi-\ntions from prefill to decode only after the CPU KV cache is\nfull. During decoding, the large number of KV cache in the\nCPU buffer enables Seesaw to perform decode with large\nbatch sizes, and thus enabling high throughput. As depicted\n1 The tensors cached for each sequence\u2019s decoding steps.\n\n=== Page 3 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nin Figure 2(c), this approach maintains the maximal batch\nsize during the decode stage, while significantly reducing\nthe frequency of stage transitions, thereby minimizing re-\nsharding overhead. Additionally, to mitigate the overhead\nof KV cache transfers between CPU and GPU, Seesaw em-\nploys asynchronous pipelining to overlap data transfers with\ncomputation.\nIn summary, we make the following contributions.\n\u2022 We identify and quantitatively analyze the different pref-\nerences for parallelisms in the prefill and decode stages of\nthroughput-oriented LLM inference tasks. Our analysis\ncomprehensively accounts for data movement, computa-\ntion, and communication costs.\n\u2022 We propose dynamic model re-sharding, a novel technique\nthat dynamically reconfigures the parallelization strategies\nfor prefill and decode stages. We address the challenge of\ntransition overhead in model re-sharding with continuous\nbatching by introducing tiered KV cache buffering and\ntransition-minimizing scheduling. Based on these tech-\nniques, we implement Seesaw, a high-throughput offline\ninference system that optimizes parallelization strategies\nfor each LLM inference stage.\n\u2022 We conduct a comprehensive evaluation of Seesaw across\na variety of workloads and hardware configurations. Our\nresults show Seesaw achieves an average speedup of\n1.36\u00d7 and a throughput improvement of up to 1.78\u00d7\ncompared to the state-of-the-art LLM inference engines.\n2\nBACKGROUND\n2.1\nLLM Inference\nTransformer Architecture.\nModern large language mod-\nels are based on the transformer architecture (Vaswani et al.,\n2017), which typically consists of multiple identical decoder\nlayers (OpenAI, 2024). Each layer includes several linear\nlayers and an attention layer. The weights of the linear\nlayers account for the majority of the model\u2019s parameters.\nAuto-regressive Generation.\nLLM inference follows an\nauto-regressive paradigm (Bengio et al., 2000), which takes\nan input prompt and generates a sequence of output tokens.\nThis process is divided into two stages: prefilling, which\nprocesses the input tokens, and decoding, which generates a\ntoken per step. These stages exhibit distinct computational\nproperties (Zhong et al., 2024; Yuan et al., 2024). Prefilling\nprocesses the prompt that are typically hundreds to thou-\nsands of tokens long. The computation and communication\ncosts, both of which scale with the number of tokens, domi-\nnate the runtime during this stage. Since the cost of loading\nweights is amortized over a larger set of tokens, the overall\nperformance is primarily bound by compute and/or commu-\nnication. In contrast, Decoding processes only the newly\ngenerated tokens in each auto-regressive step and has com-\nparatively smaller computation in each step. Therefore the\ncost for loading the weight data from off-chip memory to\ncomputation units has a relatively higher percentage. In each\ngeneration step, the intermediate tensors K and V in each\nattention operator can be cached for reuse in the future gen-\neration, which is called Key-value cache (KV cache) (Pope\net al., 2023). While being able to accelerate computation,\nit occupies a substantial amount of GPU memory, which is\nproportional to the total number of tokens.\n2.2\nLLM Inference Optimization\nParallelism.\nAs the size of LLMs grows, the memory\ncapacity on a single GPU becomes insufficient. Conse-\nquently, various techniques are developed to partition mod-\nels onto multiple GPUs (Zheng et al., 2022). These paral-\nlelization strategies can be classified as (1) inter-operator,\nwhich places different operators or layers across multiple\nGPUs, overlapping them with pipelining (known as Pipeline\nparallelism, PP) (Huang et al., 2019; Narayanan et al., 2019;\nLi et al., 2023), and (2) intra-operator, which partitions\ndifferent dimensions of tensors involved in computation,\nincluding data parallelism (Srivatsa et al., 2024), tensor\nparallelism (Shoeybi et al., 2019), etc. Data parallelism du-\nplicates models on different devices and dispatches requests\namong them. Tensor parallelism shards model weights and\neach device performs a portion of the computation, then\naggregates these partial results to produce the final output.\nBatching.\nBatching more tokens in a single forward pass\nincreases inference efficiency by, for example, amortizing\nthe time required to load model weights (Sheng et al., 2023;\nFang et al., 2021). However, its effectiveness differs be-\ntween the prefilling and decoding stages (Yuan et al., 2024;\nHe & Zhai, 2024; Agrawal et al., 2023). In decoding, where\nweight-loading overhead occupies a larger portion of the\nruntime, batching significantly boosts throughput by effec-\ntively amortizing this overhead. Conversely, in the prefilling\nstage, batching has a less pronounced impact since the token\ncount in input prompts is generally sufficient to keep the\nprocess compute-bound. Overall, larger batch sizes yield\nhigher throughput, though the maximum batch size is lim-\nited by available GPU memory, as it requires additional\nspace for activations and the KV cache.\nContinuous Batching and Scheduling.\nContinuous\nbatching is an essential optimization for throughput-oriented\nLLM inference (Yu et al., 2022; Kwon et al., 2023). By\nbatching multiple sequences at the token level, it allows the\nsystem to onboard new sequences and clear the KV cache of\ncompleted sequences at any generation step. This approach\nenables prefill-prioritizing scheduling, which removes se-\nquences as they finish, frees up their KV cache, and eagerly\n\n=== Page 4 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nTensor Parallel\nPipeline Parallel\nL1\nL1\nGPU1\nGPU2\nGPU1\nGPU2\nGPU1\nGPU2\nGPU1\nGPU2\nL1\n1/2\nL1\n2/2\nL1\nL1\nPrefill\nDecode\nS1\nS2\nS1\nS2\nS2\nS3\nL1\nS1\nL2\nS1\nL2\nS2\n0.5x load weight\n0.5x compute\n+ allreduce\nper-sequence time\ntime\n1x load weight\n0.5x compute\nS2\nS1\nS1 S2\nS1\nS1 S2\nS1\nS2\nS2\nL2\nS1\nL2\nS2\nL1\nS1\ntime\n0.5x load weight\n0.5x compute\n+ allreduce\nper-sequence time\n1x load weight\n0.5x compute\nlayer\ntoken\nsequence\nbatch\nallreduce\nPP is better because of\nlower allreduce overhead\nTP is better because of more\neffective batching\nProportion of allreduce is\nlarger in prefilling\nProportion of loading weights is\nhigher in decoding\nTP shards weights, so load\nweights is parallelized\nL2\n1/2\nL2\n2/2\nS2\nS1\nS2\nS1\nL1\n1/2\nL1\n2/2\nL2\n1/2\nL2\n2/2\nPP has smaller\nbatch sizes\nFigure 3. Different effects of tensor and pipeline parallelisms on prefilling and decoding. Tensor parallelism incurs all-reduce overhead,\nwhich has a higher percentage in prefilling, therefore pipeline parallelism is better for prefilling. Conversely, pipeline parallelism splits\nbatches into smaller micro-batches, which leads to more forward passes and repetitive loading weights, which is insufficient in decoding.\nschedules the prefilling of new sequences whenever GPU\nmemory becomes available. This strategy maximizes the\nnumber of concurrent sequences being processed, resulting\nin higher throughput. Another alternative is to use decode-\nprioritizing scheduling, which minimizes the frequency of\ntransitions. Instead of scheduling to prefilling eagerly, this\napproach waits until all sequences in a batch have finished\ndecoding before initiating the next round of prefilling. How-\never, this scheduling policy results in suboptimal decoding\nthroughput (Agrawal et al., 2024).\n3\nMOTIVATION AND ANALYSIS\nIn this section, we provide an in-depth analysis of two key\nobservations we identify from Figure 1 in Section 1: (1)\nTensor parallelism often exhibits significantly worse per-\nformance than pipeline parallelism during the prefill stage\ndue to its substantial communication overhead; (2) Pipeline\nparallelism tends to fall short in the decode stage owing to\nthe considerable weight loading overhead it incurs. We then\nargue that a dynamic parallelization strategy is essential to\nattain optimal performance across both stages.\nGiven the importance of batching in throughput-oriented\ntasks, it can be useful to consider how different paralleliza-\ntion strategies impact the maximum batch size, rather than\nassuming batch size as a tunable parameter, as is often done\nin online-serving contexts such as DistServe (Zhong et al.,\n2024) and Sarathi-serve (Agrawal et al., 2024).\n3.1\nParallelism Analysis\nObservation 1: Tensor parallelism incurs substantial\ncommunication overhead during the prefill stage.\nIn\nTensor parallelism, each device performs a part of computa-\ntion and aggregate the partial result. The activations at each\nlayer are synchronized across all GPUs using all-reduce\noperations. The overhead associated with this operation can\nbe quantified as:\n#tokens \u00d7 activation size\nall-reduce bandwidth\n,\nwhere all-reduce bandwidth refers to the rate of data transfer\nduring all-reduce operations, calculated as the size of the\ntensor being all-reduced divided by the all-reduce runtime.\nAs the degree of tensor parallelism increases, the proportion\nof execution time of all-reduce operations grows substan-\ntially. This growth is attributed to two main factors. First,\nwhile model weights are partitioned, activations in tensor\nparallelism remain fully replicated across GPUs, leading to\na constant activation size regardless of the degree of tensor\nparallelism. Second, all-reduce bandwidth decreases as the\nnumber of GPUs grows, due to more complex communi-\ncation schemes. Therefore, increasing the degree of tensor\nparallelism not only fails to reduce the traffic of all-reduce\noperations but further limits the communication bandwidth,\nresulting in escalated communication overhead. This is-\nsue is particularly pronounced in the prefill stage, where a\nlarge number of tokens are processed simultaneously, mak-\ning communication overhead the primary bottleneck. Thus,\ntensor parallelism tends to perform worse than pipeline par-\nallelism due to its large communication overhead.\nObservation 2: Pipeline parallelism suffers from signifi-\ncant weight transferring overhead in the decode stage.\nPipeline parallelism distributes model layers sequentially\nacross devices, with each device responsible for processing\na set of consecutive layers before passing the output to the\nnext device. Due to the auto-regressive nature of LLM infer-\nence, a sequence cannot enter the pipeline until its preceding\ntoken is generated. As a result, at any given time step, a\nsequence can appear in only one stage of the pipeline, mak-\ning the batches processed by each device mutually exclusive.\nHowever, the total number of sequences that the pipeline\ncan handle at a time, referred to as the global batch size,\n\n=== Page 5 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nis constrained by the size of KV cache. Given the mutual\nexclusion of batches at each device, pipeline parallelism can\nprocess only approximately 1/PP of the global batch per\nforward pass. We denote this reduced batch size in pipeline\nparallelism as the micro-batch size.\nDividing batches into micro-batches increases the number\nof LLM forward passes required to process the same amount\nof requests. Specifically, a pipeline parallelism degree of\nPP necessitates PP times more forward passes for a given\nglobal batch. This repeated execution degrades inference\nperformance, as model weight matrices must be loaded from\nglobal memory repeatedly. This inefficiency is especially\nsignificant in the decode stage, where weight-loading over-\nhead accounts for a substantial portion of total execution\ntime. As a result, pipeline parallelism generally underper-\nforms relative to tensor parallelism in the decode stage due\nto the amplified weight loading overhead.\nDiscussion on Data Parallelism.\nUnlike tensor and\npipeline parallelism, which distribute the model across de-\nvices, data parallelism distributes the data while duplicating\nthe model. While data parallelism has minimal commu-\nnication overhead, it has two key disadvantages: (1) the\nvolume of weight transferring is higher by the number of\nduplicates compared to tensor parallelism; and (2) it occu-\npies more GPU memory, reducing the available space for\nthe KV cache and thus limiting the maximum batch size re-\nsulting in lower throughput. Data parallelism can be applied\northogonally alongside both tensor and pipeline parallelism.\nWe do not dynamically adjust data parallelism, which will\nbe explained in Section 4.1.\nConclusion: No one-size-fits-all\nWhen comparing these\nthree parallelism strategies for high-throughput LLM infer-\nence, a key observation is that prefilling and decoding stages\nbenefit from different parallelism approaches. This differ-\nence arises from the distinct characteristics of each stage, as\nillustrated in Figure 3. Tensor parallelism is preferred for\ndecoding due to its ability to efficiently accelerate weight\nmatrix loading. However, it incurs significant communica-\ntion overhead, as it requires all-reduce operations at each\nlayer. In contrast, pipeline and data parallelism have much\nlower communication overhead, making them preferable for\nprefilling. However, their decoding throughput is limited by\ninefficient batching and additional weight-loading overhead.\nTo quantitatively analyze the trade-offs across different par-\nallelisms, we model the average runtime per sequence (the\ninverse of throughput) as follows. Derivations and further\ndetails are provided in the Appendix A.\nT \u221dT linear\ndm\nTP\n+ T attn\ndm + Tcomp\nDP \u00b7 TP \u00b7 PP + Tcomm(TP)\nPP \u00b7 DP\nHere T linear\ndm\nrepresents data movement for linear layers (pri-\nGPU0\nGPU1\nGPU2\nGPU3\nGPU4\nGPU5\nGPU6\nGPU7\nprefill worker\ndecode worker\nprefill\nthroughput\ndecode\nthroughput\n0.0\n0.5\n1.0\n1.5\n2.0\nThroughput (reqs/sec)\nDecode (8 GPUs)\nDecode (4 GPUs)\nPrefill (4 GPUs)\nThroughput Mismatch\nFigure 4. An example of spatially disaggregating prefilling and\ndecoding has a restricted search space. Deploying a 70B model on\neight 40GiB GPUs allows only one disaggregation strategy: four\nGPUs for prefilling and four for decoding. However, this causes\nsevere throughput mismatch between the two stages.\nmarily model weights), T attn\ndm represents data movement for\nattention layers (primarily KV cache) , Tcomp represents\ncomputation time, Tcomm represents communication time.\nNote that Tcomm is a monotonically increasing function\nwith respect to TP, as all-reduce operations require more\ntime as TP increases.\nTensor parallelism can effectively accelerate loading model\nweights, which is T linear\ndm , while pipeline and data parallelism\ncannot. On the other hand, pipeline and data parallelism\neffectively reduce the overhead of communication, while\ntensor parallelism contrarily increases the communication\noverhead. In prefilling, T linear\ndm\nis negligible, and Tcomm be-\ncomes larger, so pipeline and data parallelisms are more\npreferred, while in decoding, T linear\ndm\noccupies a larger pro-\nportion so tensor parallelism is more advantageous.\n3.2\nWhy not Disaggregate Prefilling and Decoding?\nSpatially disaggregating prefilling and decoding with sepa-\nrate hardware resources, as done in online serving systems\nsuch as DistServe (Zhong et al., 2024) and MoonCake (Qin\net al., 2024), is one approach to separately select paralleliza-\ntion strategies for prefilling and decoding. Sequences are\nfirst processed by the devices dedicated for prefilling before\nbeing transferred to decoding devices.\nHowever, there are two obstacles when applying prefill-\ndecode disaggregation to purely throughput-oriented sce-\nnarios.\nFirst, since the overall throughput is bound by\nthe slower stage, the throughput of prefilling and decod-\ning needs to be matched by adjusting the devices allocated\nfor each stage. However, it can be impractical in resource-\nconstrained scenarios. As shown in Figure 4, to deploy\na 70B model (which takes 140GiB memory for model\nweights) on eight 40GiB GPUs, there is only one disag-\ngregation strategy, that is four GPUs for prefilling and four\n\n=== Page 6 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nL1\nKV1\nL2\nKV2\nL1\n1/2\nKV1\n1/2\nL2\n1/2\nKV2\n1/2\nL1\n2/2\nKV1\n2/2\nL2\n2/2\nKV2\n2/2\nmodel\nresharding\nGPU 1\nGPU 2\nprefill\n\u00a0(pipeline parallelism)\ndecode\u00a0\n(tensor parallelism)\nFigure 5. Model weights and KV cache need to be re-sharded when\nswitching between different parallelism.\nfor decoding2. However, it causes severe throughput mis-\nmatch where prefilling has more than 6\u00d7 higher throughput\nthan decoding. Second, disaggregation duplicates the model\nweights similarly to data parallelism, bringing similar draw-\nbacks, such as limited KV cache space and increased weight\ntransfer. As a result, decoding throughput with four GPUs\nis only 15% of that with eight GPUs.\nIn conclusion, although disaggregation allows for select-\ning different parallelization strategies for each stage, the\nthroughput mismatch between stages and limited resources\nallocated to each can lead to suboptimal performance. This\ncalls for a method that offers flexibility in parallelization\nwhile maximizing hardware resource utilization.\n4\nSEESAW: KEY IDEAS\n4.1\nDynamic Model Re-sharding\nObserving that prefilling and decoding have distinct pref-\nerences for parallelism, we propose a technique called dy-\nnamic model re-sharding. This technique enables the se-\nlection of different parallelism strategies for each stage and\nautomatically transitions between them. This approach ex-\npands the configuration space, allowing for separate opti-\nmization of the two stages, potentially improving overall\nthroughput compared to using a single configuration. In the\nfollowing paragraphs, we denote the parallelization strategy\nused in prefilling as cp and that in decoding as cd.\nTo support transitions between different parallelization con-\nfigurations, the cluster must rearrange the data stored on\neach device to align with the new parallelism which involves\nboth model weights and KV cache, as illustrated in Figure 5.\nIn Seesaw, model weights are re-sharded by reloading the re-\nquired shards from CPU memory, and KV cache re-sharding\nis performed through CPU shared memory.\nThe inter-device movement of tensors incurs overhead. To\nmitigate this re-sharding cost, we design an asynchronous\npipeline to overlap data transfer with computation, as de-\ntailed in Section 5.2.\nDiscussion on data parallelism.\nUnlike switching be-\ntween tensor and pipeline parallelism, adjusting the degree\n2 At least four GPUs (160 GiB memory) are needed to fit the\nmodel weights.\nasynchronous\u00a0\nswap in\nCPU\nGPUs\nprefill\nCPU\nGPUs\nCPU memory is\u00a0\nempty\nCPU\nGPUs\ndecoding\nCPU\nGPUs\nCPU memory is\nfilled\nCPU\nGPUs\nprefill\n(warm up)\nFigure 6. Tiered KV cache buffering and transition-minimizing\nscheduling, and the change of KV cache occupancy.\nrequest\ncp\ncd\nGPU 1 (worker 1)\nGPU 2 (worker 2)\nCPU\nwrite back kv\nafter prefill\nload kv before decode\ncp\ncp\n(prefill)\ncd\ncd\n(decode)\nCPU KV cache is empty\nCPU KV cache is full\nscheduler\nFigure 7. KV cache re-sharding is completed during swapping,\nleveraging CPU shared memory.\nof data parallelism alters the proportion of GPU memory al-\nlocated to model weights versus KV cache. This adjustment\nincreases system complexity or necessitates additional data\nmovement between the CPU and GPU. Therefore, we only\ndynamically adjust tensor and pipeline parallelism.\n4.2\nTiered KV Cache Buffering and\nTransition-minimizing Scheduling\nChallenge: Transition Overhead.\nIn practice, dynamic\nmodel resharding encounters an obstacle of transition\noverhead, which is amplified by the widely-used contin-\nuous batching and prefill-prioritizing scheduling. Prefill-\nprioritizing scheduling eagerly schedules new prefilling\ntasks, causing frequent transitions between the two stages.\nAs a result, directly applying model re-sharding with this in-\nterleaved prefill-decode scheduling policy would introduce\nsignificant re-sharding overhead. On the other hand, decode-\nprioritizing scheduling minimizes the frequency of transi-\ntions but results in suboptimal decoding throughput. Other\ncompromise solutions involve setting a threshold-based ap-\nproach for managing the prefill-decode transition (Cheng\net al., 2024). However, they still involve a trade-off be-\ntween reducing transition overhead and maximizing decod-\ning throughput.\nTo address this problem, we propose 1) tiered KV cache\nbuffering, which leverages CPU memory offloading 2)\ntransition-minimizing scheduling policy. These two syn-\nergistic techniques prevent frequent stage transitions and\nmaintain a high decoding throughput.\nTiered KV cache buffering uses CPU memory as auxiliary\nstorage for the KV cache, enabling the pre-computation\nof a large batch of prefilling consecutively. During the\n\n=== Page 7 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nprefill stage, the generated KV cache is offloaded to CPU\nKV cache storage, freeing it from the limitations of GPU\nmemory space. During decoding, continuous batching runs\nas normal, except that new sequences are on-boarded by\nswapping in its KV cache from the CPU memory.\nTransition-minimizing scheduling controls the transition to\nonly happen when the CPU KV storage is either full or\nempty. During prefill, once the CPU KV cache storage\nis fully utilized, re-sharding is triggered, and the cluster\ntransitions to decoding. During decoding, GPUs continue\nprocessing requests and loading KV cache from CPU mem-\nory, keeping GPU KV cache fully utilized for high decod-\ning throughput. When the entire CPU KV cache has been\ntransferred to GPU memory, the cluster switches back to\nprefilling. The whole process is illustrated in Figure 6.\nKV cache re-sharding occurs throughout this process. As\nillustrated in Figure 7, in a multi-GPU setup, the CPU KV\ncache storage is shared among all GPUs. During swap-out,\neach GPU pushes its shard (based on cp) of the generated\nKV cache to the shared CPU storage, where these shards\ncollectively form the complete KV cache. During swap-\nin, each GPU retrieves its required KV shard (based on\ncd) from the shared storage. We implement the shared KV\ncache using shared memory of the operating system.\n5\nSYSTEM DESIGN AND IMPLEMENTATION\n5.1\nScheduler-worker Architecture\nIn order to support dynamically switching parallelization\nconfigurations for prefilling and decoding, we build Seesaw,\na new LLM inference engine designed for high-throughput\nLLM inference. The overall architecture of Seesaw fol-\nlows a single-scheduler, multi-worker design. The sched-\nuler manages all generation requests, organizes them into\nbatches, and sends instructions to the workers. To fully\nutilize pipelining, each decoding step processes 1/PP of the\nsequences in GPU KV storage. Once a batch is formed, it is\nsent to workers through shared queues. Each worker is re-\nsponsible for controlling a single GPU and maintains a task\nqueue to receive and execute instructions sequentially. This\narchitecture facilitates the implementation of asynchronous\nfeatures, such as pipeline parallelism and the asynchronous\npipeline for tiered KV cache buffering.\n5.2\nAsynchronous Pipeline\nWhile re-sharding and tiered KV cache buffering offer sub-\nstantial benefits, they also introduce new overhead related\nto moving model weights and KV cache. The overhead of\nreloading model weights remains constant relative to batch\nsize, allowing it to be amortized with larger batches. In con-\ntrast, swapping the KV cache incurs overhead proportional\nto batch size, making it harder to amortize. Fortunately,\nswap out\nqkv_proj\nattn + ffn\n\u00a0\u00a0 swap in\nmain thread\nprefetcher thread\nscheduler\n(prefill)\nscheduler\n(decode)\ndecode\nCPU KV\nGPU KV\nnon-blocking copy\nCPU KV\u00a0 is empty\nCPU KV\u00a0 is full\nFigure 8. Async pipeline of Seesaw: Swap-in overlaps with prefill\ncomputation, while swap-out occurs in a separate asynchronous\nprefetcher thread.\nthese overheads can be mitigated through computation-\ncommunication overlap. We implement an asynchronous\npipeline to overlap KV cache transfer with ongoing compu-\ntation, as illustrated in Figure 8.\nOverlap swap-out with computation.\nThe KV cache\ngenerated during the prefilling stage is not used until decod-\ning begins, allowing the KV cache swap-out to overlap with\nother computations during prefilling. Although CPU-GPU\ndata transfer is relatively slow due to PCIe bandwidth limi-\ntations, it can still be overlapped with computation, given\nthe high FLOPS involved in prefilling.\nIn practice, CPU-GPU data transfer can only overlap with\ncomputation when using pinned memory, but shared mem-\nory cannot be pinned (AlbanD, 2023). To address this, we\nsplit the transfer into two stages: GPU to pinned memory\n(overlapped with computation) and then pinned to shared\nmemory, which is a host-side operation that also runs con-\ncurrently with GPU kernels.\nAsynchronous swap-in.\nWe implement swap-in using a\nbackground thread called the prefetcher on each worker, op-\nerating in a fully asynchronous paradigm. The prefetcher is\ncontrolled directly by the scheduler and runs independently\nof the main thread, whether the main thread is handling pre-\nfilling or decoding. In each iteration, the scheduler creates\nnew prefetching tasks when there are free slots in the GPU\nKV store. Once the prefetcher completes moving the KV\ncache for certain sequences, it notifies the scheduler via a\nshared queue, allowing those sequences to be scheduled for\ndecoding tasks later. As long as the output length is not too\nshort, the swap-in can also be well overlapped.\nBandwidth-aware KV cache layout.\nThe data layout of\nthe KV cache significantly impacts the bandwidth efficiency\nof data movement. There are two common layouts for stor-\ning KV cache: (seq len, num heads, head dim) (NHD) and\n(num heads, seq len, head dim) (HND). NHD is less opti-\nmal for memory access because tensor parallelism shards\nthe KV cache along the H dimension (number of heads),\nwhich is the second-to-last dimension, leading to more non-\ncontiguous memory access. Therefore, we use the HND\n\n=== Page 8 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nlayout for storing the KV cache in CPU memory.\n6\nEVALUATION\nIn this section, we evaluate the performance of Seesaw under\na variety of hardware configurations and workloads.\n6.1\nExperiment Settings\nHardware.\nWe use three types of GPUs: NVIDIA A10,\nL4, and A100. The A10 and L4 are deployed on AWS EC2\ninstances (g5.48xlarge and g6.48xlarge (Amazon\nWeb Services, 2024)), and the A100 is used on GCP (Google\nCloud, 2024). GPU specifications are listed in Table 1. The\nPCIe connection for each GPU is PCIe 4.0 8\u00d7, providing 16\nGiB/s bandwidth (PCI-SIG, 2017), while NVLink (NVIDIA\nCorporation, 2024) offers a bandwidth of 600 GiB/s. Addi-\ntionally, we allocate 80 GiB of CPU memory per GPU.\nModel.\nWe use three different LLMs with different\nsizes: (1) a 15B variety of LLaMA3 (Elinas, 2024); (2)\nCodeLLaMA-34B (Roziere et al., 2023); (3) LLaMA2-\n70B (Touvron et al., 2023b). They all use Grouped Query\nAttention (GQA) (Ainslie et al., 2023). For brevity, we refer\nto them as 15B, 34B, and 70B, respectively, in the following\nsections. We use float16 as the data type.\nWorkload.\nWe use two different datasets in our eval-\nuation,\nnamely\nsharegpt\n(ShareGPT,\n2023)\nand\narxiv-summarization (Cohan et al., 2018). They\ncorrespond\nto\ntwo\ndifferent\ndistributions\nof\nwork-\nload.\nsharegpt is a dataset of chatting history, so\nits input and output have comparable lengths, while\narxiv-summarization dataset is a summarization\ndataset where inputs are much longer than outputs. The\ncharacteristics of these two datasets are shown in Figure 9.\nWe sample 2000 requests from the sharegpt dataset and\n500 requests from arxiv-summarization and also use\nconstant-length workloads in Section 6.5. Since Seesaw\nis purely throughput-oriented, we measure the end-to-end\nthroughput as the metrics.\nBaselines.\nWe use vLLM 0.5.4 (Kwon et al., 2023) as\nthe baseline. It is the most widely used open-source LLM\nserving engine with wide support for different parallelisms.\nWe also directly use the vLLM\u2019s model implementation for\na straightforward comparison. SGLang (Zheng et al., 2023)\nand DeepSpeed-FastGen (Holmes et al., 2024) do not sup-\nport pipeline parallelism. TensorRT-LLM (NVIDIA, 2024b)\nis not included in the comparison because it uses a simi-\nlar scheduling policy as vLLM, and vLLM demonstrates\ncomparable performance (vLLM Team, 2024) in throughput-\noriented tasks. The techniques proposed in Seesaw can also\nbe applied to modifying TensorRT-LLM.\nTable 1. GPU hardware specification\nGPU Model\nMemory Size\nMemory\nBandwidth\nFLOPS\nNVLink\nA10\n24 GiB\n600 GiB/s\n125T\n\u2717\nL4\n24 GiB\n300 GiB/s\n121T\n\u2717\nA100\n40 GiB\n1,555 GiB/s\n312T\n\u2713\n0\n2000\n4000\n#tokens\n0\n2\n4\nDensity\n1e\n3\ninput tokens\noutput tokens\n(a) arxiv-summarization\n0\n2000\n4000\n#tokens\n0.0\n0.5\n1.0\nDensity\n1e\n2\ninput tokens\noutput tokens\n(b) ShareGPT\nFigure 9. Input and output length distributions of the datasets\nWe enable chunked prefill and tune the chunk size for vLLM\nto get the optimal throughput, following the practice of\nSarathi-serve (Agrawal et al., 2024). Otherwise, suboptimal\nchunk sizes would cause severe throughput degradation.\n6.2\nEnd-to-end Throughput on PCIe Systems\nFirst, we measure the end-to-end throughput of Seesaw. We\nsweep over all available single parallelism configurations\nfor vLLM and show the result of the best configuration. We\nuse four GPUs for the 15B model, and eight GPUs for the\n34B and 70B models. The result is shown in Figure 10, with\nthe used parallelism labeled above each bar.\nOn A10, compared with the highest single parallelism base-\nline, Seesaw achieves a geometrically average speedup of\n1.45\u00d7, with up to 1.78\u00d7 speedup. On L4, Seesaw achieves\na geometrically average speedup of 1.29\u00d7, with up to 1.52\u00d7\nspeedup.\nThe overall average speedup is 1.36\u00d7.\nThe\nspeedup is more significant on A10, because A10 has better\nsingle GPU performance than L4, while they have similar\nPCIe inter-connection bandwidth, causing a higher percent-\nage of communication overhead.\n6.3\nSpeedup Breakdown: An Example\nFigure 12 illustrates how Seesaw merges the advantages\nof different parallelisms. Using CodeLLaMA34B on the\narxiv-summarization dataset with four A10 GPUs\nas an example, we measured the runtime of each stage. TP4\nis optimal for decoding but significantly slower for prefilling,\nwhile PP4 excels at prefilling but is slower during decoding.\nSeesaw uses a mixed parallelism strategy, applying PP4\nfor prefilling and TP4 for decoding, achieving performance\ncomparable to the best configuration for each stage.\nCompared to the optimal single parallelism configuration\n(TP2PP2) with chunked prefill, Seesaw is still faster because\n(1) chunked prefill does not piggy-back all decoding steps,\n\n=== Page 9 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\n15b\n34b\n70b\n15b\n34b\n70b\n0\n1\n2\nNormalized\nThroughput\n D2T2\n P4->T4\n D2T2P2\n D2P4->D2T4\n T2P4\n P8->T4P2\n D2P2  P4->T4  D2T2P2\n D2P4->D2T4\n T4P2\n P8->T4P2\n arxiv\n sharegpt\nvllm\nseesaw\n(a) End-to-end Throughput on A10\n15b\n34b\n70b\n15b\n34b\n70b\n0\n1\n2\nNormalized\nThroughput\n T2P2\n P4->T4\n D2T4\n D2P4->D2T4\n T4P2\n P8->T4P2\n P4\n P4->T4\n D2T4\n P8->T4P2\n T4P2 P8->T4P2\n arxiv\n sharegpt\nvllm\nseesaw\n(b) End-to-end Throughput on L4\nFigure 10. End-to-end throughput comparison on PCIe systems.\nThe used parallelization strategies are labelled above each bar.\nLabels such as \u201cP4 \u2192D4\u201d represent the parallelization strategies\nfor prefilling and decoding respectively in Seesaw.\narxiv\nsharegpt\n0.0\n0.5\n1.0\nThroughput\n(Normalized)\n0.61\n0.62\n0.89\n0.82\n1.00\n1.00\n1.00\n1.13\nvllm+pcie\nseesaw+pcie\nvllm+nvlink\nseesaw+nvlink\nFigure 11. Throughput comparison on A100.\nleaving some purely decoding steps, and (2) chunked prefill\nwith TP2PP2 is slower than prefilling with PP4.\n6.4\nEnd-to-end Throughput on A100\nSpeedup on A100 + NVLink\nThe NVLink interconnec-\ntion across A100 GPUs significantly reduces the all-reduce\noverhead and further scales tensor parallelism. Usually,\ntensor parallelism alone is enough to achieve optimal per-\nformance when there are no more than four GPUs. Never-\ntheless, there is still a noticeable percentage of all-reduce\noverhead in prefilling when tensor parallelism scales be-\nyond four GPUs. Seesaw can still provide speedup in this\ncase. As shown in Figure 11, Seesaw still achieves a 13%\nthroughput increase over vLLM for the sharegpt dataset\non LLaMA3-70B on eight A100s.\nSpeedup on A100 + PCIe\nBesides A100 SXM with\nNVLink inter-connection, there is also another version of\nA100 that is inter-connected with PCIe links, where Seesaw\ncan achieve noticeable speedup. As shown in Figure 11, See-\nsaw provides 46% speedup on arxiv-summarization\nand 30% speedup on sharegpt. Seesaw brings the per-\nformance of the A100 PCIe version much closer to the per-\nformance level of the NVLink version. vLLM gets roughly\n60% throughput on A100 PCIe compared with A100 SXM,\nwhile Seesaw boosts it up to 82% \u2013 89%.\ntp4\npp4\np4->t4\ntp2pp2\n+chunked prefill\n0\n500\n1000\n1500\nEnd-to-end time (s)\nprefill\nmix\ndecode\nother\nFigure 12. Speedup breakdown. \u201cmix\u201d represents batches contain-\ning both prefilling and decoding when chunked prefill is enabled.\nWe disable chunked prefill for TP4 and PP4 in order to show the\nreference prefilling and decoding time. TP2PP2 with chunked\nprefill is the optimal parallelism for vLLM.\n0.0\n0.1\n0.2\n0.3\nD:P\n0.5\n1.0\nthroughput\n(normalized)\ntp4pp2\ntp2pp4\npp8\npp8->tp4pp2\nFigure 13. Throughput of various parallelization strategies with\ndifferent ratios between output and input lengths (D : P), mea-\nsured on 70B model and eight A10 GPUs.\n6.5\nSensitivity Study\nRatio between Input and Output Length\nThe speedup\nof Seesaw depends on the ratio between the input and output\nlength, or P : D. Model re-sharding has the opportunity\nto provide speedup when prefilling and decoding have bal-\nanced time. To investigate to what extent model re-sharding\nwould be effective, we measure the throughput of various\nparallelization strategies on synthesized datasets with uni-\nform lengths and different P : D ratios. We fix the input\nlength as 3000 and vary the output length.\nAs shown in Figure 13, PP8 achieves the highest throughput\nduring prefilling, while TP4PP2 excels in decoding. When\nthe output length equals one (prefilling only), Seesaw and\nPP8 show similar throughput, and TP4PP2 performs worse\ndue to high communication overhead. As output length\nincreases, the inefficiency of PP in decoding outweighs its\nadvantage in prefilling, causing PP8\u2019s throughput to drop\nrapidly. There is a range where TP2PP4 becomes optimal\nbefore decoding dominates the runtime and TP4PP2 takes\nover as the fastest. Nonetheless, Seesaw achieves the highest\noverall throughput across all data points. In real scenarios\nwith variable input and output lengths, Seesaw is even more\nadvantageous due to its adaptive capabilities.\nInter-connection Bandwidth\nThe effectiveness of See-\nsaw also depends on the inter-connection bandwidth. We\ninvestigate this by measuring the runtime and tracing all-\nreduce operations of running arxiv-summarization\nand 34B model on eight A10s. We then mutate the all-\nreduce time to project the end-to-end throughput with dif-\n\n=== Page 10 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\n10\n1\n100\n101\nBandwidth scale (\u00d7 all_reduce through PCIe)\n0.0\n0.5\n1.0\nThroughput\n(Normalized)\nd2t1p4\nd2t2p2\nd2t4p1\nd1t1p8\nd1t2p4\nd1t4p2\nd1t8p1\nd2p4->d2t4\nFigure 14. Projected throughput of various parallelization strate-\ngies with different inter-connection bandwidth, measured and\ntraced on 34B model and eight A10 GPUs.\nferent inter-connection bandwidths. As shown in Figure 14,\nwhen the inter-connection bandwidth is slow (for example,\namong geographically distributed devices (Borzunov et al.,\n2022)), pipeline parallelism is optimal; when the bandwidth\nis very high, tensor parallelism is optimal. The throughput\nof Seesaw is superior to fixed parallelization strategies on a\nwide range from 0.1\u00d7 to 50\u00d7 of PCIe bandwidth.\n7\nRELATED WORK\n7.1\nHeterogenity between Prefilling and Decoding\nDue to the different computational characteristics between\nprefilling and decoding leading to under-utilization of hard-\nware resources, prior research has investigated two direc-\ntions to address this problem, namely disaggregating or\nmerging the two stages. Disaggregation places prefilling\nand decoding onto different devices to avoid their interfer-\nence while merging processes prefilling and decoding in\none batch.\nDisaggregate Prefill and Decoding\nDistServe (Zhong\net al., 2024) proposed placing prefilling and decoding on\ndifferent devices to prevent interference and leverage dif-\nferent characteristics of the two stages. Mooncake (Qin\net al., 2024) uses similar through a distributed KV cache\npool. P/D-Serve (Jin et al., 2024) uses the device-to-device\nnetwork to transfer the KV cache between prefill and decode\ndevices. Splitwise (Patel et al., 2024) proposes using dif-\nferent GPU models for the two stages. TetriInfer (Hu et al.,\n2024) further disaggregates different downstream tasks to\navoid interference. These works are designed for online\nserving while Seesaw focuses on offline inference. More-\nover, they are usually designed for large clusters.\nMerge Prefill and Decode\nChunked prefill, as proposed\nby SplitFuse (Holmes et al., 2024), Sarathi (Agrawal et al.,\n2023), and Sarathi-serve (Agrawal et al., 2024), splits long\nprompts in the prefilling stage into smaller chunks, combin-\ning them with decoding steps to strike a balance between\ndata movement and computation and reduce pipeline bub-\nbles in pipeline parallelism. However, determining the opti-\nmal chunk size is challenging. A chunk size that\u2019s too large\nresults in excessive decode-only steps, closely resembling\ntraditional prefill-decode scheduling. Conversely, a chunk\nsize that\u2019s too small reduces kernel efficiency.\n7.2\nParallel and Distributed LLM Inference\nAside from tensor parallelism, pipeline parallelism, and data\nparallelism discussed in Section 2.2, there are also other\ntypes of parallelisms, such as sequence parallelism (SP) (Li\net al., 2021; Liu et al., 2023; Lin et al., 2024; Brandon et al.,\n2023; Xue et al., 2024) and fully sharded data parallelism\n(FSDP) (Zhao et al., 2023; Rajbhandari et al., 2020). Se-\nquence parallelism is especially designed for long sequence\nlengths, and is orthogonal with our work. FSDP requires\nfrequently transferring weight matrices across GPUs, thus\nmainly used in training.\nHexGen (Jiang et al., 2023), LLM-PQ (Zhao et al., 2024),\nHelix (Mei et al., 2024) investigate parallelisms in hetero-\ngeneous clusters. Intra-device parallelism leverages over-\nlapping functions using different resources within each de-\nvice, including NanoFlow (Zhu et al., 2024) and Liger (Du\net al., 2024).\nPetals (Borzunov et al., 2022) explores\nLLM inference in geographically distributed setups, em-\nploying pipeline parallelism to minimize communication\ncosts. SpotServe (Miao et al., 2024) runs LLM inference on\npreemptible instances.\n7.3\nOffloading in LLM Inference\nOffloading is a widely used technique to run LLM applica-\ntions in resource-constrained scenarios (Ren et al., 2021).\nFlexGen (Sheng et al., 2023) swaps tensors across GPU\nmemory, CPU memory, and disks. Fiddler (Kamahori et al.,\n2024), HeteGen (Xuanlei et al., 2024), PowerInfer (Song\net al., 2023) and FastDecoder (He & Zhai, 2024) perform\npart of computation in CPU, which require CPUs with\nstrong compute capability or external CPU nodes connected\nwith high-bandwidth networking. Instinfer (Pan et al., 2024)\noffloads computation to Computational Storage Drives.\n8\nCONCLUSION\nThis paper proposes Seesaw, a high-throughput LLM infer-\nence engine, to address the inefficiencies of fixed paralleliza-\ntion by selecting different parallelization strategies for the\nprefilling and decoding stages and switching between them\nusing model re-sharding. It uses tiered KV cache buffering\nto minimize re-sharding overheads. Our experiments show\nthat Seesaw outperforms widely-used open-source inference\nengines, with a throughput increase of 1.06-1.78\u00d7 and an\naverage throughput improvement of 1.36\u00d7. These results\nhighlight Seesaw\u2019s effectiveness and adaptability.\n\n=== Page 11 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nREFERENCES\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\nAgrawal, A., Panwar, A., Mohan, J., Kwatra, N., Gulavani,\nB. S., and Ramjee, R. Sarathi: Efficient llm inference\nby piggybacking decodes with chunked prefills. arXiv\npreprint arXiv:2308.16369, 2023.\nAgrawal, A., Kedia, N., Panwar, A., Mohan, J., Kwatra, N.,\nGulavani, B. S., Tumanov, A., and Ramjee, R. Taming\nthroughput-latency tradeoff in llm inference with sarathi-\nserve. arXiv preprint arXiv:2403.02310, 2024.\nAinslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y.,\nLebr\u00b4on, F., and Sanghai, S. Gqa: Training generalized\nmulti-query transformer models from multi-head check-\npoints. arXiv preprint arXiv:2305.13245, 2023.\nAlbanD.\nWhy not multiprocess pin memory in\ndata loader?\nhttps://discuss.pytorch.org/t/why-not-\nmultiprocess-pin-memory-in-data-loader/197345/2, 2023.\nAccessed: 2024-10-14.\nAmazon Web Services.\nAmazon EC2 Instance Types,\n2024.\nURL https://aws.amazon.com/ec2/\ninstance-types/. Accessed: 2024-10-26.\nBen-Nun, T. and Hoefler, T. Demystifying parallel and dis-\ntributed deep learning: An in-depth concurrency analysis.\nACM Computing Surveys (CSUR), 52(4):1\u201343, 2019.\nBengio, Y., Ducharme, R., and Vincent, P. A neural proba-\nbilistic language model. Advances in neural information\nprocessing systems, 13, 2000.\nBorzunov, A., Baranchuk, D., Dettmers, T., Ryabinin, M.,\nBelkada, Y., Chumachenko, A., Samygin, P., and Raffel,\nC. Petals: Collaborative inference and fine-tuning of\nlarge models. arXiv preprint arXiv:2209.01188, 2022.\nBrandon, W., Nrusimha, A., Qian, K., Ankner, Z., Jin, T.,\nSong, Z., and Ragan-Kelley, J. Striped attention: Faster\nring attention for causal transformers. arXiv preprint\narXiv:2311.09431, 2023.\nChan, V., Zhang, H., and Wang, F.\nSnowflake llm\ninference:\nOptimizing gpu capacity for interactive\nworkloads.\nhttps://www.snowflake.com/engineering-\nblog/snowflake-llm-inference-interactive-workloads/,\nSeptember 2024. Accessed: 2024-10-30.\nChang, L., Bao, W., Hou, Q., Jiang, C., Zheng, N., Zhong,\nY., Zhang, X., Song, Z., Jiang, Z., Lin, H., et al. Flux: Fast\nsoftware-based communication overlap on gpus through\nkernel fusion. arXiv preprint arXiv:2406.06858, 2024.\nCheng, K., Hu, W., Wang, Z., Peng, H., Li, J., and Zhang,\nS. Slice-level scheduling for high throughput and load\nbalanced llm serving. arXiv preprint arXiv:2406.13511,\n2024.\nCohan, A., Dernoncourt, F., Kim, D. S., Bui, T., Kim, S.,\nChang, W., and Goharian, N. A discourse-aware attention\nmodel for abstractive summarization of long documents.\narXiv preprint arXiv:1804.05685, 2018.\nDell\nTechnologies.\nPoweredge\nserver\ngpu\nmatrix,\n2023.\nURL\nhttps://www.\ndelltechnologies.com/asset/en-ca/\nproducts/servers/briefs-summaries/\npoweredge-server-gpu-matrix.pdf.\nAc-\ncessed: 2024-10-24.\nDell Technologies.\nInferencing performance for gen-\nerative ai in the enterprise with amd accelerators.\nhttps://infohub.delltechnologies.com/en-au/l/generative-\nai-in-the-enterprise-with-amd-accelerators/inferencing-\nperformance/, 2024. Accessed: 2024-10-30.\nDu, J., Wei, J., Jiang, J., Cheng, S., Huang, D., Chen, Z.,\nand Lu, Y. Liger: Interleaving intra-and inter-operator\nparallelism for distributed large model inference. In Pro-\nceedings of the 29th ACM SIGPLAN Annual Symposium\non Principles and Practice of Parallel Programming, pp.\n42\u201354, 2024.\nEdge, D., Trinh, H., Cheng, N., Bradley, J., Chao, A., Mody,\nA., Truitt, S., and Larson, J. From local to global: A graph\nrag approach to query-focused summarization. arXiv\npreprint arXiv:2404.16130, 2024.\nElinas.\nLlama-3-15b\ninstruct-zeroed.\nhttps://huggingface.co/elinas/\nLlama-3-15B-Instruct-zeroed, 2024.\nFang, J., Yu, Y., Zhao, C., and Zhou, J. Turbotransformers:\nan efficient gpu serving system for transformer models.\nIn Proceedings of the 26th ACM SIGPLAN Symposium\non Principles and Practice of Parallel Programming, pp.\n389\u2013402, 2021.\nGoogle Cloud.\nGPU platforms:\nA100 GPUs, 2024.\nURL https://cloud.google.com/compute/\ndocs/gpus#a100-gpus. Accessed: 2024-10-26.\nHe, J. and Zhai, J.\nFastdecode: High-throughput gpu-\nefficient llm serving using heterogeneous pipelines. arXiv\npreprint arXiv:2403.11421, 2024.\nHolmes, C., Tanaka, M., Wyatt, M., Awan, A. A., Rasley, J.,\nRajbhandari, S., Aminabadi, R. Y., Qin, H., Bakhtiari, A.,\nKurilenko, L., et al. Deepspeed-fastgen: High-throughput\ntext generation for llms via mii and deepspeed-inference.\narXiv preprint arXiv:2401.08671, 2024.\n\n=== Page 12 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nHu, C., Huang, H., Xu, L., Chen, X., Xu, J., Chen, S., Feng,\nH., Wang, C., Wang, S., Bao, Y., et al. Inference without\ninterference: Disaggregate llm inference for mixed down-\nstream workloads.\narXiv preprint arXiv:2401.11181,\n2024.\nHuang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen,\nM., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe:\nEfficient training of giant neural networks using pipeline\nparallelism. Advances in neural information processing\nsystems, 32, 2019.\nJiang, Y., Yan, R., Yao, X., Zhou, Y., Chen, B., and Yuan, B.\nHexgen: Generative inference of large language model\nover heterogeneous environment. In Forty-first Interna-\ntional Conference on Machine Learning, 2023.\nJin, Y., Wang, T., Lin, H., Song, M., Li, P., Ma, Y., Shan, Y.,\nYuan, Z., Li, C., Sun, Y., et al. P/d-serve: Serving disag-\ngregated large language model at scale. arXiv preprint\narXiv:2408.08147, 2024.\nKamahori, K., Gu, Y., Zhu, K., and Kasikci, B. Fiddler:\nCpu-gpu orchestration for fast inference of mixture-of-\nexperts models. arXiv preprint arXiv:2402.07033, 2024.\nKamsetty, A., Chen, H., and Xie, L. How bytedance scales\noffline inference with multi-modal llms to 200tb data.\nhttps://www.anyscale.com/blog/how-bytedance-scales-\noffline-inference-with-multi-modal-llms-to-200TB-data,\nAugust 2023. Accessed: 2024-10-30.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention. In Proceedings of the 29th Sym-\nposium on Operating Systems Principles, pp. 611\u2013626,\n2023.\nLi, S., Xue, F., Baranwal, C., Li, Y., and You, Y. Sequence\nparallelism: Long sequence training from system perspec-\ntive. arXiv preprint arXiv:2105.13120, 2021.\nLi, Z., Zheng, L., Zhong, Y., Liu, V., Sheng, Y., Jin, X.,\nHuang, Y., Chen, Z., Zhang, H., Gonzalez, J. E., et al.\n{AlpaServe}: Statistical multiplexing with model paral-\nlelism for deep learning serving. In 17th USENIX Sympo-\nsium on Operating Systems Design and Implementation\n(OSDI 23), pp. 663\u2013679, 2023.\nLin, B., Peng, T., Zhang, C., Sun, M., Li, L., Zhao, H., Xiao,\nW., Xu, Q., Qiu, X., Li, S., et al. Infinite-llm: Efficient llm\nservice for long context with distattention and distributed\nkvcache. arXiv preprint arXiv:2401.02669, 2024.\nLiu, H., Zaharia, M., and Abbeel, P. Ring attention with\nblockwise transformers for near-infinite context. arXiv\npreprint arXiv:2310.01889, 2023.\nLiu, S., Biswal, A., Cheng, A., Mo, X., Cao, S., Gonzalez,\nJ. E., Stoica, I., and Zaharia, M. Optimizing llm queries in\nrelational workloads. arXiv preprint arXiv:2403.05821,\n2024.\nMei, Y., Zhuang, Y., Miao, X., Yang, J., Jia, Z., and Vinayak,\nR. Helix: Distributed serving of large language models\nvia max-flow on heterogeneous gpus. arXiv preprint\narXiv:2406.01566, 2024.\nMiao, X., Oliaro, G., Zhang, Z., Cheng, X., Jin, H., Chen,\nT., and Jia, Z. Towards efficient generative large language\nmodel serving: A survey from algorithms to systems.\narXiv preprint arXiv:2312.15234, 2023.\nMiao, X., Shi, C., Duan, J., Xi, X., Lin, D., Cui, B., and Jia,\nZ. Spotserve: Serving generative large language models\non preemptible instances. In Proceedings of the 29th ACM\nInternational Conference on Architectural Support for\nProgramming Languages and Operating Systems, Volume\n2, pp. 1112\u20131127, 2024.\nMLCommons. Mlperf inference: Datacenter benchmark\nsuite. https://mlcommons.org/benchmarks/\ninference-datacenter/, 2024. Accessed: 2024-\n10-30.\nNarayan, A., Chami, I., Orr, L., Arora, S., and R\u00b4e, C. Can\nfoundation models wrangle your data? arXiv preprint\narXiv:2205.09911, 2022.\nNarayanan, D., Harlap, A., Phanishayee, A., Seshadri, V.,\nDevanur, N. R., Ganger, G. R., Gibbons, P. B., and Za-\nharia, M. Pipedream: Generalized pipeline parallelism for\ndnn training. In Proceedings of the 27th ACM symposium\non operating systems principles, pp. 1\u201315, 2019.\nNVIDIA. Fastertransformer: Transformer related optimiza-\ntion, including bert, gpt.\nhttps://github.com/\nNVIDIA/FasterTransformer, 2024a.\nNVIDIA. Tensorrt-llm: Optimized inference for large lan-\nguage models.\nhttps://github.com/NVIDIA/\nTensorRT-LLM, 2024b.\nNVIDIA Corporation. Nvidia a100 pcie product brief, 2020.\nURL\nhttps://www.nvidia.com/content/\ndam/en-zz/Solutions/Data-Center/a100/\npdf/A100-PCIE-Prduct-Brief.pdf. Accessed:\n2024-10-24.\nNVIDIA Corporation. NVIDIA NVLink: High-Speed GPU\nInterconnect, 2024.\nURL https://www.nvidia.\ncom/en-us/data-center/nvlink/. Accessed:\n2024-10-26.\nOpenAI. Chatgpt (gpt-4), 2024. URL https://www.\nopenai.com/research/gpt-4. Accessed: 2024-\n08-02.\n\n=== Page 13 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nPan, X., Li, E., Li, Q., Liang, S., Shan, Y., Zhou, K., Luo,\nY., Wang, X., and Zhang, J. Instinfer: In-storage attention\noffloading for cost-effective long-context llm inference.\narXiv preprint arXiv:2409.04992, 2024.\nPatel, P., Choukse, E., Zhang, C., Shah, A., Goiri, \u00b4I., Maleki,\nS., and Bianchini, R. Splitwise: Efficient generative llm\ninference using phase splitting. In 2024 ACM/IEEE 51st\nAnnual International Symposium on Computer Architec-\nture (ISCA), pp. 118\u2013132. IEEE, 2024.\nPCI-SIG.\nPCI-SIG\nReleases\nPCIe\n4.0,\nVer-\nsion\n1.0,\n2017.\nURL\nhttps://pcisig.\ncom/pci-sig-releases-pcie%C2%\nAE-40-version-10.\nPope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury,\nJ., Heek, J., Xiao, K., Agrawal, S., and Dean, J. Efficiently\nscaling transformer inference. Proceedings of Machine\nLearning and Systems, 5:606\u2013624, 2023.\nQin, R., Li, Z., He, W., Zhang, M., Wu, Y., Zheng, W., and\nXu, X. Mooncake: Kimi\u2019s kvcache-centric architecture\nfor llm serving. arXiv preprint arXiv:2407.00079, 2024.\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero:\nMemory optimizations toward training trillion parameter\nmodels. In SC20: International Conference for High Per-\nformance Computing, Networking, Storage and Analysis,\npp. 1\u201316. IEEE, 2020.\nRen, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O.,\nYang, S., Zhang, M., Li, D., and He, Y. {Zero-offload}:\nDemocratizing {billion-scale} model training. In 2021\nUSENIX Annual Technical Conference (USENIX ATC\n21), pp. 551\u2013564, 2021.\nRoziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I.,\nTan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., et al.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950, 2023.\nShareGPT.\nSharegpt\nvicuna\nunfiltered\ndataset.\nhttps://huggingface.co/datasets/\nanon8231489123/ShareGPT_Vicuna_\nunfiltered, 2023. Apache 2.0 License.\nSheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen,\nB., Liang, P., R\u00b4e, C., Stoica, I., and Zhang, C. Flexgen:\nHigh-throughput generative inference of large language\nmodels with a single gpu. In International Conference\non Machine Learning, pp. 31094\u201331116. PMLR, 2023.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,\nJ., and Catanzaro, B.\nMegatron-lm: Training multi-\nbillion parameter language models using model paral-\nlelism. arXiv preprint arXiv:1909.08053, 2019.\nSong, Y., Mi, Z., Xie, H., and Chen, H. Powerinfer: Fast\nlarge language model serving with a consumer-grade gpu.\narXiv preprint arXiv:2312.12456, 2023.\nSrivatsa, V., He, Z., Abhyankar, R., Li, D., and Zhang, Y.\nPreble: Efficient distributed prompt scheduling for llm\nserving. 2024.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023b.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\nvLLM Team. Performance update: Bringing vllm to the\nnext level.\nhttps://blog.vllm.ai/2024/09/\n05/perf-update.html, 2024. Accessed: 2024-10-\n14.\nXuanlei, Z., Jia, B., Zhou, H., Liu, Z., Cheng, S., and You,\nY. Hetegen: Efficient heterogeneous parallel inference for\nlarge language models on resource-constrained devices.\nProceedings of Machine Learning and Systems, 6:162\u2013\n172, 2024.\nXue, F., Chen, Y., Li, D., Hu, Q., Zhu, L., Li, X., Fang, Y.,\nTang, H., Yang, S., Liu, Z., et al. Longvila: Scaling long-\ncontext visual language models for long videos. arXiv\npreprint arXiv:2408.10188, 2024.\nYu,\nC.,\nLee,\nS.,\nXu,\nR.,\nLin,\nW.,\nGorthy,\nP.,\nand Liaw, R.\nBatch llm inference on anyscale\nslashes aws bedrock costs by up to 6x, October\n2024. URL https://www.anyscale.com/blog/\nbatch-llm-inference-announcement.\nAc-\ncessed: 2024-10-30.\nYu, G.-I., Jeong, J. S., Kim, G.-W., Kim, S., and Chun, B.-\nG. Orca: A distributed serving system for {Transformer-\nBased} generative models. In 16th USENIX Symposium\non Operating Systems Design and Implementation (OSDI\n22), pp. 521\u2013538, 2022.\nYuan, Z., Shang, Y., Zhou, Y., Dong, Z., Xue, C., Wu, B.,\nLi, Z., Gu, Q., Lee, Y. J., Yan, Y., et al. Llm inference\nunveiled: Survey and roofline model insights.\narXiv\npreprint arXiv:2402.16363, 2024.\n\n=== Page 14 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nZhao, J., Wan, B., Peng, Y., Lin, H., and Wu, C. Llm-\npq: Serving llm on heterogeneous clusters with phase-\naware partition and adaptive quantization. arXiv preprint\narXiv:2403.01136, 2024.\nZhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M.,\nWright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al.\nPytorch fsdp: experiences on scaling fully sharded data\nparallel. arXiv preprint arXiv:2304.11277, 2023.\nZheng, L., Li, Z., Zhang, H., Zhuang, Y., Chen, Z., Huang,\nY., Wang, Y., Xu, Y., Zhuo, D., Xing, E. P., et al. Alpa:\nAutomating inter-and {Intra-Operator} parallelism for\ndistributed deep learning. In 16th USENIX Symposium\non Operating Systems Design and Implementation (OSDI\n22), pp. 559\u2013578, 2022.\nZheng, L., Yin, L., Xie, Z., Huang, J., Sun, C., Yu, C. H.,\nCao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., et al.\nEfficiently programming large language models using\nsglang. arXiv preprint arXiv:2312.07104, 2023.\nZhong, Y., Liu, S., Chen, J., Hu, J., Zhu, Y., Liu, X., Jin,\nX., and Zhang, H. Distserve: Disaggregating prefill and\ndecoding for goodput-optimized large language model\nserving. arXiv preprint arXiv:2401.09670, 2024.\nZhu, K., Zhao, Y., Zhao, L., Zuo, G., Gu, Y., Xie, D., Gao,\nY., Xu, Q., Tang, T., Ye, Z., et al. Nanoflow: Towards\noptimal large language model serving throughput. arXiv\npreprint arXiv:2408.12757, 2024.\nA\nPERFORMANCE MODEL\nIn this section, we examine the trade-offs of various paral-\nlelism strategies by developing an analytical performance\nmodel. We break down the model\u2019s inference time into\nmultiple components and analyze the impact of each paral-\nlelism type on these components. The results reveal that the\nproportion of these components differs across workloads,\nresulting in distinct scaling behaviors for each parallelism\nstrategy. Table 2 lists the notations used in our analysis. We\nassume the data type is float16.\nTable 2. Notations\nN\nnumber of output tokens\nT linear\ndm\ntime of moving weights\nr\nnumber of requests\nT attn\ndm\ntime of moving kvcache\nb\n(global) batch size\nT linear\nc\ntime of computation\ns\naverage sequence length\nT attn\nc\ncomputation of attention\nhq\nnumber of heads\nTnw\ntime of communication\nd\nhead dimension\nhkv\nnumber of KV heads\nL\nnumber of layers\nW\n#parameters of one layer\nPP\npipeline parallel degree\nDP\ndata parallel degree\nTP\ntensor parallel degree\nTable 3. Different components of the runtime of a forward pass.\nThe batch size b representing the batching effect is emphasized.\nT linear\ndm\nT linear\ncomp\nT attn\ndm\nT attn\ncomp\nTnw(TP)\nPrefill\n2W\nBHBM\n2bW s\nFLOPS\n2bs(hq+2hkv)d\nBHBM\nbhqs2d2\nFLOPS\n4bshqd\nBar(T P )\nDecode\n2W\nBHBM\n2bW\nFLOPS\n4bshkvd\nBHBM\n2bhqsd2\nFLOPS\n4bhqd\nBar(T P )\nA.1\nRuntime Break-Down\nThe runtime of each decoding layer can be divided into three\ncomponents: 1) data movement (Tdm) from GPU global\nmemory (HBM) to compute units, which includes transfer-\nring weights (T linear\ndm ) and KV cache (T attn\ndm), 2) computation\nTcomp, including T linear\ncomp and T attn\ncomp, and 3) communication\ncost Tnw (nw stands for network), primarily arising from the\nall-reduce operation in tensor parallelism. Based on the roof-\nline model, the runtime of each layer can be approximated\nas TL = max(T linear\ndm , T linear\ncomp) + max(T attn\ndm, T attn\ncomp) + Tnw.\nData Movement.\nThe runtime of data movement can be\napproximated as transferred data volume divided by the\nbandwidth, which is the HBM bandwidth for GPUs. For lin-\near layers, the transferred data is mostly weight matrices, of\nwhich the size is 2W bytes, which is constant. For attention\nlayers, the transferred data is most the Q, K, and V matrices,\nwhich is 2bs(hq + 2hkv)d bytes in prefilling and 4bshkvd\nin decoding.\nCompute.\nThe computation time can be approximated as\nthe number of floating operations (FLOPs) divided by the\nnumber of floating operations per second of the hardware\n(FLOP/s). For linear layers, the FLOPs is proportional to\nthe weight parameters times the number of tokens, which\nis 2Wbs in prefilling and 2Wb in decoding. For attention\nlayers, most operations come from computing the attention\nscore, which is approximated as bhqs2d2 in prefilling and\n2bhqsd2 in decoding.\nCommunication.\nThe communication cost mostly comes\nfrom the all-reduce operation in tensor parallelism. It can\nbe modeled as the transferred data volume divided by the\nbandwidth. We denote it as Tnw(TP), and approximate it as\nb \u00b7 A/Bar(TP) where A is the size of the activation of one\nrequest within a batch and Bar(TP) is the all-reduce band-\nwidth. Tnw(TP) is monotonically increasing with TP as\nadditional GPUs and more replicas of activations are added\nto all-reduce. We omit the peer-to-peer communication over\nin pipeline parallel since it is negligible compared to the\nall-reduce operation of tensor parallel.\nA.2\nBatching Analysis\nBatching is critical in decoding. It significantly affects the\nlatency and throughput. Batch size represents how many\n\n=== Page 15 ===\nSeesaw: High-throughput LLM Inference via Model Re-sharding\nTP1DP8\nTP2DP4\nTP4DP2\nTP8DP1\n0.00\n0.25\n0.50\n0.75\n1.00\nRuntime per Request\nOOM\nload weight\ncompute\nallreduce\n0\n50\n100\n150\nBatch Size\nBatch Size\nFigure 15. How data parallelism affects the decoding throughput.\nData parallelism has minimal communication overhead but suffers\nfrom caused by inefficient memory access caused by duplicating\nmodel weights. Model duplicates occupy more GPU memory,\nleaving less space for KV cache and smaller batch sizes. With\nmore data parallelism, the overhead of loading data from GPU\nglobal memory to compute units significantly increases.\nrequests are processed in one forward pass, and larger batch\nsizes can amortize the cost of transferring weights, thus\nimproving the throughput.\nGlobal and micro-batch size.\nIn distributed inference\nsuch as multi-GPU settings, we define the global batch size\nb as the number of requests being actively processed by the\nwhole cluster. It is a tunable hyper-parameter that represents\nthe overall workload of the system. It is bounded by the\nmaximal batch size, which is determined by the memory\nbudget. On the other side, the micro batch size is defined\nat the device level as the batch size processed during each\nforward pass. Tensor parallelism does not affect the batch\nsize while DP and PP shrink the micro batch size.\nA.3\nParallelism Analysis\nWe consider three types of parallelism: data parallelism,\ntensor parallelism, and pipeline parallelism, and denote their\ndegree of parallelism as DP, TP, and PP respectively.\nTensor parallelism\ncan accelerate both data moving\n(T linear\ndm\nand T attn\ndm are reduced to 1/TP) and computation\nTcomp (reduced to Tcomp/TP), at the cost of all reduce over-\nhead Tnw.\nData parallelism\ndistributes the global batch size b onto\nDP micro-batches processed in parallel. The model is du-\nplicated so T linear\ndm\nremains unchanged. T attn\ndw , T linear\ncomp, T attn\ncomp,\nTnw are reduced as the batch size is smaller. Due to the\nneed to duplicate model weights, the GPU memory left for\nthe KV cache is smaller. The spare space for KV cache on\neach GPU is Mkv = M \u2212\n2LW\nT P \u00b7P P . The maximal batch size\nis\nbmax = DP \u00b7 Mkv \u00b7 TP \u00b7 PP\n4Lhkvds\n= DP \u00b7 M \u00b7 TP \u00b7 PP \u22122LW\n4Lhkvds\nWhile TP and PP can super-linearly scale the batch size, DP\ncan only linearly scale the batch size. The trade-off between\nlimited batch sizes and reduced communication overhead is\nshown in Figure 15.\nPipeline parallelism\ndistributes different layers to dif-\nferent devices, and each device will have L/PP layers. It\ncannot reduce single-request latency but is more suitable for\nthroughput-oriented scenarios as it introduces less commu-\nnication overhead. However, it is not the ultimate answer\nof high-throughput applications because of an important\nobservation that pipeline parallelism harms maximal batch\nsize. A tricky nuance is that given a batch size b, pipeline\nparallelism can only process b/PP of them simultaneously\nin order to utilize and pipeline all PP GPUs, which is harm-\nful to batching. If the workload is not uniformly distributed\nacross GPUs, there will be bubbles, or in the worst case,\nsome GPUs might be idle. When the pipeline is fully and\nstably pipelining, each time the last pipeline stage finishes\nits L/PP layers of forward pass, a micro-batch of b/PP will\nbe finished.\nThroughput.\nThe micro-batch size on each GPU is\nb/(PP \u00b7 DP). The total runtime of generating one micro\nbatch with size b/(PP \u00b7 DP) on one DP replica (or more\nspecifically, the time of the last pipeline stage finishing a\nmicro-batch) is\nTstage = L\nPP \u00b7\n\"\nmax(T linear\ndm\nTP ,\nT linear\ncomp\nDP \u00b7 TP \u00b7 PP)+\n+ max(T attn\ndm, T attn\ncomp)\nDP \u00b7 TP \u00b7 PP\n+ Tnw(TP)\nPP \u00b7 DP\n\u0015\nThe throughput (number of processed requests per unit time)\nis b/PP/T. For simplicity, we calculate the inverse of it as\nthroughput\u22121 = Tstage\nb/PP = L\nb \u00b7\n\"\nmax(T linear\ndm\nTP ,\nT linear\ncomp\nDP \u00b7 TP \u00b7 PP)\n+ max(T attn\ndm, T attn\ncomp)\nDP \u00b7 TP \u00b7 PP\n+ Tnw(TP)\nPP \u00b7 DP\n\u0015\n(1)\nIf we approximate the roof-line model with a simplified\nadditional model, this expression can be simplified as:\nthroughput\u22121 \u221dT linear\ndm\nTP\n+ T linear\ncomp + T attn\ndm + T attn\ncomp\nDP \u00b7 TP \u00b7 PP\n+ Tnw(TP)\nPP \u00b7 DP\n(2)\n",
      "searchable_content": "page 1 seesaw high-throughput llm inference via model re-sharding qidong su123 wei zhao34 xin li3 muralidhar andoorveedu3 chenhao jiang12 zhanda zhu123 kevin song12 christina giannoula123 gennady pekhimenko123 abstract to improve the efficiency of distributed large language model llm inference various parallelization strategies such as tensor and pipeline parallelism have been proposed. however the distinct computational characteristics inherent in the two stages of llm inference prefilling and decoding render a single static parallelization strategy insufficient for the effective optimization of both stages. in this work we present seesaw an llm inference engine optimized for throughput-oriented tasks. the key idea behind seesaw is dynamic model re- sharding a technique that facilitates the dynamic reconfiguration of parallelization strategies across stages thereby maximizing throughput at both phases. to mitigate re-sharding overhead and optimize computational efficiency we employ tiered kv cache buffering and transition-minimizing scheduling. these approaches work synergistically to reduce the overhead caused by frequent stage transitions while ensuring maximum batching efficiency. our evaluation demonstrates that seesaw achieves a throughput increase of up to 1.78 1.36 on average compared to vllm the most widely used state-of-the-art llm inference engine. 1 introduction large language models llms such as the llama tou- vron et al. 2023a and gpt achiam et al. 2023 families have demonstrated exceptional performance across a wide range of tasks. beyond their prevalent use in interactive applications like chatbots openai 2024 llms are also gaining high interest in throughput-oriented offline inference workloads such as information extraction narayan et al. 2022 database querying liu et al. 2024 and knowledge graph processing edge et al. 2024 . unlike interactive applications where low latency is crucial these offline in- ference tasks prioritize high throughput over response time. these offline inference workloads are widely adopted in in- dustry kamsetty et al. 2023 yu et al. 2024 dell technolo- gies 2024 chan et al. 2024 leading mlperf to develop benchmarks specifically for them mlcommons 2024 . in this work we focus on improving inference efficiency for offline throughput-oriented llm inference workloads. as llms often exceed the memory capacity of individual gpus parallelization is essential for their deployment ben- nun hoefler 2019 shoeybi et al. 2019 . several paral- lelization strategies including tensor parallelism shoeybi et al. 2019 and pipeline parallelism narayanan et al. 2019 huang et al. 2019 have been proposed each presenting distinct trade-offs in memory efficiency inter-device com- munication and computational efficiency. tensor paral- lelism distributes model weights across devices but suffers 1 university of toronto 2 vector institute 3 centml 4 stan- ford university tp1pp8 tp2pp4 tp4pp2 tp8pp1 0.00 0.25 0.50 0.75 1.00 normalized time communication compute weight transfer a prefill tp1pp8 tp2pp4 tp4pp2 tp8pp1 0.00 0.25 0.50 0.75 1.00 normalized time communication compute weight transfer b decode figure 1. breakdown of execution time for the prefill and decode stages for llama2-13b inference on 8 l4 gpus the global batch size is 16. pipeline parallelism further divides the data into micro-batches of size 16 pp to fully utilize pipelining . from high communication costs due to frequent all-reduce operations at each layer pope et al. 2023 chang et al. 2024 . the communication cost becomes particularly severe in systems connected via pcie dell technologies 2023 or with partial high-speed connections nvidia corporation 2020 . in contrast pipeline parallelism partitions the model into sequential stages reducing inter-device communica- tion by passing only activations between them. however to enable pipelining each data batch needs to be divided into micro-batches leading to extra execution overheads since every micro-batch repeatedly loads weights into the compute units see section 3.1 for details . while numerous studies have proposed methods to optimize parallelization strategies for llms miao et al. 2023 kwon et al. 2023 li et al. 2023 pope et al. 2023 prior works typically rely on a single static configuration throughout arxiv 2503.06433v1 cs.dc 9 mar 2025 page 2 seesaw high-throughput llm inference via model re-sharding the entire generation process. however our findings indi- cate that this one-size-fits-all approach is often inefficient for throughput-oriented llm inference because it fails to leverage the distinct patterns between the two stages in llm generation the prefill stage where the input sequence is pro- cessed at once to produce the initial token and the decode stage where subsequent tokens are generated sequentially based on prior tokens. these two stages exhibit fundamen- tally different computational characteristics yuan et al. 2024 . during the prefill stage multiple tokens from the input prompt are processed simultaneously making com- putation and communication the dominant contributors to runtime. in contrast the decode stage processes one token at a time for each sequence increasing the relative time spent on weight transfer. this difference indicates that the optimal parallelization strategy for each stage may also vary. to illustrate the performance limitations of applying a uni- form parallelization strategy for both prefill and decode we measure the execution time of each stage under various combinations of tensor and pipeline parallelism as shown in figure 1. in the prefill stage as the degree of tensor par- allelism increases the communication overhead increases significantly due to additional gpus participating in all- reduce operations. as a result tensor parallelism performs significantly worse than pipeline parallelism. in contrast during the decode stage pipeline parallelism is slower than tensor parallelism largely due to increased weight trans- ferring overhead caused by micro-batching required for pipelining see section 3.1 for more details . therefore we need stage-specific parallelization strategies to provide better llm inference throughput. an existing approach is disaggregated prefill-decode zhong et al. 2024 qin et al. 2024 which assigns prefill and de- code computation to different gpu instances. the prefill instances and decode instances form a two-stage pipeline to serve inference requests. therefore the overall through- put of disaggregated prefill-decode is constrained by the slower of the two stages and balancing throughput between these two stages is essential. the key drawback of disag- gregated prefill-decode is that it can cause large amounts of pipeline bubbles under resource-constrained environments. for example when deploying a 70b model on 8 40gb gpus even the most balanced configuration results in a 6 difference in throughput between the prefill and decode stages. in this setup the decode stage operates at one-sixth the throughput of the prefill stage resulting in a significant bottleneck at the prefill stage that slows down the entire system see section 3.2 for details . to address these challenges we present seesaw a high- throughput llm inference engine that dynamically recon- figures parallelization strategies between the prefill and de- code stages. the key idea behind seesaw is model re- time gpu max seqs time gpu max seqs time cpu max seqs seqs seqs seqs under-utilize too frequent transitions a prefill-prioritizing b decode-prioritizing c tiered kv cache buffering transition-minimizing scheduling p d p d p d p d p d p d p d p d gpu max seqs p d p d p figure 2. different scheduling policies considering transition over- head. decoding throughput is positively correlated with the num- ber of sequences in gpu memory the maximal batch size which is highlighted as light green area. sharding a novel technique that dynamically re-partitions model weights and kv cache 1 between prefill and decode stages. by tailoring parallelization strategies to the dis- tinct computational demands of each stage seesaw reduces communication overhead during the prefill stage while en- hancing memory efficiency in the decode stage resulting in a substantial increase in overall throughput. however the overhead associated with model re-sharding can be high due to frequent transitions between prefill and decode. to maximize throughput existing systems typi- cally adopt prefill-prioritized scheduling yu et al. 2022 kwon et al. 2023 which interleaves prefill and decode stages across batches to achieve continuous batching. yet as illustrated in figure 2 a integrating this approach with model re-sharding can result in significant overhead due to frequent transitions between prefill and decode. on the other hand decode-prioritized scheduling nvidia 2024a completes all decode steps for a batch before proceeding to the next resulting in lower re-sharding overhead. however as depicted in figure 2 b this method suffers from low resource utilization due to smaller batch sizes. to overcome this constraint and achieve both minimal re-sharding overhead and large batch size we propose two synergetic techniques tiered kv cache buffering and transition-minimizing scheduling. tiered kv cache buffering leverages cpu memory as auxiliary storage for the kv cache enabling seesaw to store the kv cache for a large number of prefill requests. transition-minimizing scheduling reduces re-sharding overhead by minimizing the number of transitions to the decode stage. seesaw transi- tions from prefill to decode only after the cpu kv cache is full. during decoding the large number of kv cache in the cpu buffer enables seesaw to perform decode with large batch sizes and thus enabling high throughput. as depicted 1 the tensors cached for each sequence s decoding steps. page 3 seesaw high-throughput llm inference via model re-sharding in figure 2 c this approach maintains the maximal batch size during the decode stage while significantly reducing the frequency of stage transitions thereby minimizing re- sharding overhead. additionally to mitigate the overhead of kv cache transfers between cpu and gpu seesaw em- ploys asynchronous pipelining to overlap data transfers with computation. in summary we make the following contributions. we identify and quantitatively analyze the different pref- erences for parallelisms in the prefill and decode stages of throughput-oriented llm inference tasks. our analysis comprehensively accounts for data movement computa- tion and communication costs. we propose dynamic model re-sharding a novel technique that dynamically reconfigures the parallelization strategies for prefill and decode stages. we address the challenge of transition overhead in model re-sharding with continuous batching by introducing tiered kv cache buffering and transition-minimizing scheduling. based on these tech- niques we implement seesaw a high-throughput offline inference system that optimizes parallelization strategies for each llm inference stage. we conduct a comprehensive evaluation of seesaw across a variety of workloads and hardware configurations. our results show seesaw achieves an average speedup of 1.36 and a throughput improvement of up to 1.78 compared to the state-of-the-art llm inference engines. 2 background 2.1 llm inference transformer architecture. modern large language mod- els are based on the transformer architecture vaswani et al. 2017 which typically consists of multiple identical decoder layers openai 2024 . each layer includes several linear layers and an attention layer. the weights of the linear layers account for the majority of the model s parameters. auto-regressive generation. llm inference follows an auto-regressive paradigm bengio et al. 2000 which takes an input prompt and generates a sequence of output tokens. this process is divided into two stages prefilling which processes the input tokens and decoding which generates a token per step. these stages exhibit distinct computational properties zhong et al. 2024 yuan et al. 2024 . prefilling processes the prompt that are typically hundreds to thou- sands of tokens long. the computation and communication costs both of which scale with the number of tokens domi- nate the runtime during this stage. since the cost of loading weights is amortized over a larger set of tokens the overall performance is primarily bound by compute and or commu- nication. in contrast decoding processes only the newly generated tokens in each auto-regressive step and has com- paratively smaller computation in each step. therefore the cost for loading the weight data from off-chip memory to computation units has a relatively higher percentage. in each generation step the intermediate tensors k and v in each attention operator can be cached for reuse in the future gen- eration which is called key-value cache kv cache pope et al. 2023 . while being able to accelerate computation it occupies a substantial amount of gpu memory which is proportional to the total number of tokens. 2.2 llm inference optimization parallelism. as the size of llms grows the memory capacity on a single gpu becomes insufficient. conse- quently various techniques are developed to partition mod- els onto multiple gpus zheng et al. 2022 . these paral- lelization strategies can be classified as 1 inter-operator which places different operators or layers across multiple gpus overlapping them with pipelining known as pipeline parallelism pp huang et al. 2019 narayanan et al. 2019 li et al. 2023 and 2 intra-operator which partitions different dimensions of tensors involved in computation including data parallelism srivatsa et al. 2024 tensor parallelism shoeybi et al. 2019 etc. data parallelism du- plicates models on different devices and dispatches requests among them. tensor parallelism shards model weights and each device performs a portion of the computation then aggregates these partial results to produce the final output. batching. batching more tokens in a single forward pass increases inference efficiency by for example amortizing the time required to load model weights sheng et al. 2023 fang et al. 2021 . however its effectiveness differs be- tween the prefilling and decoding stages yuan et al. 2024 he zhai 2024 agrawal et al. 2023 . in decoding where weight-loading overhead occupies a larger portion of the runtime batching significantly boosts throughput by effec- tively amortizing this overhead. conversely in the prefilling stage batching has a less pronounced impact since the token count in input prompts is generally sufficient to keep the process compute-bound. overall larger batch sizes yield higher throughput though the maximum batch size is lim- ited by available gpu memory as it requires additional space for activations and the kv cache. continuous batching and scheduling. continuous batching is an essential optimization for throughput-oriented llm inference yu et al. 2022 kwon et al. 2023 . by batching multiple sequences at the token level it allows the system to onboard new sequences and clear the kv cache of completed sequences at any generation step. this approach enables prefill-prioritizing scheduling which removes se- quences as they finish frees up their kv cache and eagerly page 4 seesaw high-throughput llm inference via model re-sharding tensor parallel pipeline parallel l1 l1 gpu1 gpu2 gpu1 gpu2 gpu1 gpu2 gpu1 gpu2 l1 1 2 l1 2 2 l1 l1 prefill decode s1 s2 s1 s2 s2 s3 l1 s1 l2 s1 l2 s2 0.5x load weight 0.5x compute allreduce per-sequence time time 1x load weight 0.5x compute s2 s1 s1 s2 s1 s1 s2 s1 s2 s2 l2 s1 l2 s2 l1 s1 time 0.5x load weight 0.5x compute allreduce per-sequence time 1x load weight 0.5x compute layer token sequence batch allreduce pp is better because of lower allreduce overhead tp is better because of more effective batching proportion of allreduce is larger in prefilling proportion of loading weights is higher in decoding tp shards weights so load weights is parallelized l2 1 2 l2 2 2 s2 s1 s2 s1 l1 1 2 l1 2 2 l2 1 2 l2 2 2 pp has smaller batch sizes figure 3. different effects of tensor and pipeline parallelisms on prefilling and decoding. tensor parallelism incurs all-reduce overhead which has a higher percentage in prefilling therefore pipeline parallelism is better for prefilling. conversely pipeline parallelism splits batches into smaller micro-batches which leads to more forward passes and repetitive loading weights which is insufficient in decoding. schedules the prefilling of new sequences whenever gpu memory becomes available. this strategy maximizes the number of concurrent sequences being processed resulting in higher throughput. another alternative is to use decode- prioritizing scheduling which minimizes the frequency of transitions. instead of scheduling to prefilling eagerly this approach waits until all sequences in a batch have finished decoding before initiating the next round of prefilling. how- ever this scheduling policy results in suboptimal decoding throughput agrawal et al. 2024 . 3 motivation and analysis in this section we provide an in-depth analysis of two key observations we identify from figure 1 in section 1 1 tensor parallelism often exhibits significantly worse per- formance than pipeline parallelism during the prefill stage due to its substantial communication overhead 2 pipeline parallelism tends to fall short in the decode stage owing to the considerable weight loading overhead it incurs. we then argue that a dynamic parallelization strategy is essential to attain optimal performance across both stages. given the importance of batching in throughput-oriented tasks it can be useful to consider how different paralleliza- tion strategies impact the maximum batch size rather than assuming batch size as a tunable parameter as is often done in online-serving contexts such as distserve zhong et al. 2024 and sarathi-serve agrawal et al. 2024 . 3.1 parallelism analysis observation 1 tensor parallelism incurs substantial communication overhead during the prefill stage. in tensor parallelism each device performs a part of computa- tion and aggregate the partial result. the activations at each layer are synchronized across all gpus using all-reduce operations. the overhead associated with this operation can be quantified as tokens activation size all-reduce bandwidth where all-reduce bandwidth refers to the rate of data transfer during all-reduce operations calculated as the size of the tensor being all-reduced divided by the all-reduce runtime. as the degree of tensor parallelism increases the proportion of execution time of all-reduce operations grows substan- tially. this growth is attributed to two main factors. first while model weights are partitioned activations in tensor parallelism remain fully replicated across gpus leading to a constant activation size regardless of the degree of tensor parallelism. second all-reduce bandwidth decreases as the number of gpus grows due to more complex communi- cation schemes. therefore increasing the degree of tensor parallelism not only fails to reduce the traffic of all-reduce operations but further limits the communication bandwidth resulting in escalated communication overhead. this is- sue is particularly pronounced in the prefill stage where a large number of tokens are processed simultaneously mak- ing communication overhead the primary bottleneck. thus tensor parallelism tends to perform worse than pipeline par- allelism due to its large communication overhead. observation 2 pipeline parallelism suffers from signifi- cant weight transferring overhead in the decode stage. pipeline parallelism distributes model layers sequentially across devices with each device responsible for processing a set of consecutive layers before passing the output to the next device. due to the auto-regressive nature of llm infer- ence a sequence cannot enter the pipeline until its preceding token is generated. as a result at any given time step a sequence can appear in only one stage of the pipeline mak- ing the batches processed by each device mutually exclusive. however the total number of sequences that the pipeline can handle at a time referred to as the global batch size page 5 seesaw high-throughput llm inference via model re-sharding is constrained by the size of kv cache. given the mutual exclusion of batches at each device pipeline parallelism can process only approximately 1 pp of the global batch per forward pass. we denote this reduced batch size in pipeline parallelism as the micro-batch size. dividing batches into micro-batches increases the number of llm forward passes required to process the same amount of requests. specifically a pipeline parallelism degree of pp necessitates pp times more forward passes for a given global batch. this repeated execution degrades inference performance as model weight matrices must be loaded from global memory repeatedly. this inefficiency is especially significant in the decode stage where weight-loading over- head accounts for a substantial portion of total execution time. as a result pipeline parallelism generally underper- forms relative to tensor parallelism in the decode stage due to the amplified weight loading overhead. discussion on data parallelism. unlike tensor and pipeline parallelism which distribute the model across de- vices data parallelism distributes the data while duplicating the model. while data parallelism has minimal commu- nication overhead it has two key disadvantages 1 the volume of weight transferring is higher by the number of duplicates compared to tensor parallelism and 2 it occu- pies more gpu memory reducing the available space for the kv cache and thus limiting the maximum batch size re- sulting in lower throughput. data parallelism can be applied orthogonally alongside both tensor and pipeline parallelism. we do not dynamically adjust data parallelism which will be explained in section 4.1. conclusion no one-size-fits-all when comparing these three parallelism strategies for high-throughput llm infer- ence a key observation is that prefilling and decoding stages benefit from different parallelism approaches. this differ- ence arises from the distinct characteristics of each stage as illustrated in figure 3. tensor parallelism is preferred for decoding due to its ability to efficiently accelerate weight matrix loading. however it incurs significant communica- tion overhead as it requires all-reduce operations at each layer. in contrast pipeline and data parallelism have much lower communication overhead making them preferable for prefilling. however their decoding throughput is limited by inefficient batching and additional weight-loading overhead. to quantitatively analyze the trade-offs across different par- allelisms we model the average runtime per sequence the inverse of throughput as follows. derivations and further details are provided in the appendix a. t t linear dm tp t attn dm tcomp dp tp pp tcomm tp pp dp here t linear dm represents data movement for linear layers pri- gpu0 gpu1 gpu2 gpu3 gpu4 gpu5 gpu6 gpu7 prefill worker decode worker prefill throughput decode throughput 0.0 0.5 1.0 1.5 2.0 throughput reqs sec decode 8 gpus decode 4 gpus prefill 4 gpus throughput mismatch figure 4. an example of spatially disaggregating prefilling and decoding has a restricted search space. deploying a 70b model on eight 40gib gpus allows only one disaggregation strategy four gpus for prefilling and four for decoding. however this causes severe throughput mismatch between the two stages. marily model weights t attn dm represents data movement for attention layers primarily kv cache tcomp represents computation time tcomm represents communication time. note that tcomm is a monotonically increasing function with respect to tp as all-reduce operations require more time as tp increases. tensor parallelism can effectively accelerate loading model weights which is t linear dm while pipeline and data parallelism cannot. on the other hand pipeline and data parallelism effectively reduce the overhead of communication while tensor parallelism contrarily increases the communication overhead. in prefilling t linear dm is negligible and tcomm be- comes larger so pipeline and data parallelisms are more preferred while in decoding t linear dm occupies a larger pro- portion so tensor parallelism is more advantageous. 3.2 why not disaggregate prefilling and decoding spatially disaggregating prefilling and decoding with sepa- rate hardware resources as done in online serving systems such as distserve zhong et al. 2024 and mooncake qin et al. 2024 is one approach to separately select paralleliza- tion strategies for prefilling and decoding. sequences are first processed by the devices dedicated for prefilling before being transferred to decoding devices. however there are two obstacles when applying prefill- decode disaggregation to purely throughput-oriented sce- narios. first since the overall throughput is bound by the slower stage the throughput of prefilling and decod- ing needs to be matched by adjusting the devices allocated for each stage. however it can be impractical in resource- constrained scenarios. as shown in figure 4 to deploy a 70b model which takes 140gib memory for model weights on eight 40gib gpus there is only one disag- gregation strategy that is four gpus for prefilling and four page 6 seesaw high-throughput llm inference via model re-sharding l1 kv1 l2 kv2 l1 1 2 kv1 1 2 l2 1 2 kv2 1 2 l1 2 2 kv1 2 2 l2 2 2 kv2 2 2 model resharding gpu 1 gpu 2 prefill pipeline parallelism decode tensor parallelism figure 5. model weights and kv cache need to be re-sharded when switching between different parallelism. for decoding2. however it causes severe throughput mis- match where prefilling has more than 6 higher throughput than decoding. second disaggregation duplicates the model weights similarly to data parallelism bringing similar draw- backs such as limited kv cache space and increased weight transfer. as a result decoding throughput with four gpus is only 15 of that with eight gpus. in conclusion although disaggregation allows for select- ing different parallelization strategies for each stage the throughput mismatch between stages and limited resources allocated to each can lead to suboptimal performance. this calls for a method that offers flexibility in parallelization while maximizing hardware resource utilization. 4 seesaw key ideas 4.1 dynamic model re-sharding observing that prefilling and decoding have distinct pref- erences for parallelism we propose a technique called dy- namic model re-sharding. this technique enables the se- lection of different parallelism strategies for each stage and automatically transitions between them. this approach ex- pands the configuration space allowing for separate opti- mization of the two stages potentially improving overall throughput compared to using a single configuration. in the following paragraphs we denote the parallelization strategy used in prefilling as cp and that in decoding as cd. to support transitions between different parallelization con- figurations the cluster must rearrange the data stored on each device to align with the new parallelism which involves both model weights and kv cache as illustrated in figure 5. in seesaw model weights are re-sharded by reloading the re- quired shards from cpu memory and kv cache re-sharding is performed through cpu shared memory. the inter-device movement of tensors incurs overhead. to mitigate this re-sharding cost we design an asynchronous pipeline to overlap data transfer with computation as de- tailed in section 5.2. discussion on data parallelism. unlike switching be- tween tensor and pipeline parallelism adjusting the degree 2 at least four gpus 160 gib memory are needed to fit the model weights. asynchronous swap in cpu gpus prefill cpu gpus cpu memory is empty cpu gpus decoding cpu gpus cpu memory is filled cpu gpus prefill warm up figure 6. tiered kv cache buffering and transition-minimizing scheduling and the change of kv cache occupancy. request cp cd gpu 1 worker 1 gpu 2 worker 2 cpu write back kv after prefill load kv before decode cp cp prefill cd cd decode cpu kv cache is empty cpu kv cache is full scheduler figure 7. kv cache re-sharding is completed during swapping leveraging cpu shared memory. of data parallelism alters the proportion of gpu memory al- located to model weights versus kv cache. this adjustment increases system complexity or necessitates additional data movement between the cpu and gpu. therefore we only dynamically adjust tensor and pipeline parallelism. 4.2 tiered kv cache buffering and transition-minimizing scheduling challenge transition overhead. in practice dynamic model resharding encounters an obstacle of transition overhead which is amplified by the widely-used contin- uous batching and prefill-prioritizing scheduling. prefill- prioritizing scheduling eagerly schedules new prefilling tasks causing frequent transitions between the two stages. as a result directly applying model re-sharding with this in- terleaved prefill-decode scheduling policy would introduce significant re-sharding overhead. on the other hand decode- prioritizing scheduling minimizes the frequency of transi- tions but results in suboptimal decoding throughput. other compromise solutions involve setting a threshold-based ap- proach for managing the prefill-decode transition cheng et al. 2024 . however they still involve a trade-off be- tween reducing transition overhead and maximizing decod- ing throughput. to address this problem we propose 1 tiered kv cache buffering which leverages cpu memory offloading 2 transition-minimizing scheduling policy. these two syn- ergistic techniques prevent frequent stage transitions and maintain a high decoding throughput. tiered kv cache buffering uses cpu memory as auxiliary storage for the kv cache enabling the pre-computation of a large batch of prefilling consecutively. during the page 7 seesaw high-throughput llm inference via model re-sharding prefill stage the generated kv cache is offloaded to cpu kv cache storage freeing it from the limitations of gpu memory space. during decoding continuous batching runs as normal except that new sequences are on-boarded by swapping in its kv cache from the cpu memory. transition-minimizing scheduling controls the transition to only happen when the cpu kv storage is either full or empty. during prefill once the cpu kv cache storage is fully utilized re-sharding is triggered and the cluster transitions to decoding. during decoding gpus continue processing requests and loading kv cache from cpu mem- ory keeping gpu kv cache fully utilized for high decod- ing throughput. when the entire cpu kv cache has been transferred to gpu memory the cluster switches back to prefilling. the whole process is illustrated in figure 6. kv cache re-sharding occurs throughout this process. as illustrated in figure 7 in a multi-gpu setup the cpu kv cache storage is shared among all gpus. during swap-out each gpu pushes its shard based on cp of the generated kv cache to the shared cpu storage where these shards collectively form the complete kv cache. during swap- in each gpu retrieves its required kv shard based on cd from the shared storage. we implement the shared kv cache using shared memory of the operating system. 5 system design and implementation 5.1 scheduler-worker architecture in order to support dynamically switching parallelization configurations for prefilling and decoding we build seesaw a new llm inference engine designed for high-throughput llm inference. the overall architecture of seesaw fol- lows a single-scheduler multi-worker design. the sched- uler manages all generation requests organizes them into batches and sends instructions to the workers. to fully utilize pipelining each decoding step processes 1 pp of the sequences in gpu kv storage. once a batch is formed it is sent to workers through shared queues. each worker is re- sponsible for controlling a single gpu and maintains a task queue to receive and execute instructions sequentially. this architecture facilitates the implementation of asynchronous features such as pipeline parallelism and the asynchronous pipeline for tiered kv cache buffering. 5.2 asynchronous pipeline while re-sharding and tiered kv cache buffering offer sub- stantial benefits they also introduce new overhead related to moving model weights and kv cache. the overhead of reloading model weights remains constant relative to batch size allowing it to be amortized with larger batches. in con- trast swapping the kv cache incurs overhead proportional to batch size making it harder to amortize. fortunately swap out qkv_proj attn ffn swap in main thread prefetcher thread scheduler prefill scheduler decode decode cpu kv gpu kv non-blocking copy cpu kv is empty cpu kv is full figure 8. async pipeline of seesaw swap-in overlaps with prefill computation while swap-out occurs in a separate asynchronous prefetcher thread. these overheads can be mitigated through computation- communication overlap. we implement an asynchronous pipeline to overlap kv cache transfer with ongoing compu- tation as illustrated in figure 8. overlap swap-out with computation. the kv cache generated during the prefilling stage is not used until decod- ing begins allowing the kv cache swap-out to overlap with other computations during prefilling. although cpu-gpu data transfer is relatively slow due to pcie bandwidth limi- tations it can still be overlapped with computation given the high flops involved in prefilling. in practice cpu-gpu data transfer can only overlap with computation when using pinned memory but shared mem- ory cannot be pinned alband 2023 . to address this we split the transfer into two stages gpu to pinned memory overlapped with computation and then pinned to shared memory which is a host-side operation that also runs con- currently with gpu kernels. asynchronous swap-in. we implement swap-in using a background thread called the prefetcher on each worker op- erating in a fully asynchronous paradigm. the prefetcher is controlled directly by the scheduler and runs independently of the main thread whether the main thread is handling pre- filling or decoding. in each iteration the scheduler creates new prefetching tasks when there are free slots in the gpu kv store. once the prefetcher completes moving the kv cache for certain sequences it notifies the scheduler via a shared queue allowing those sequences to be scheduled for decoding tasks later. as long as the output length is not too short the swap-in can also be well overlapped. bandwidth-aware kv cache layout. the data layout of the kv cache significantly impacts the bandwidth efficiency of data movement. there are two common layouts for stor- ing kv cache seq len num heads head dim nhd and num heads seq len head dim hnd . nhd is less opti- mal for memory access because tensor parallelism shards the kv cache along the h dimension number of heads which is the second-to-last dimension leading to more non- contiguous memory access. therefore we use the hnd page 8 seesaw high-throughput llm inference via model re-sharding layout for storing the kv cache in cpu memory. 6 evaluation in this section we evaluate the performance of seesaw under a variety of hardware configurations and workloads. 6.1 experiment settings hardware. we use three types of gpus nvidia a10 l4 and a100. the a10 and l4 are deployed on aws ec2 instances g5.48xlarge and g6.48xlarge amazon web services 2024 and the a100 is used on gcp google cloud 2024 . gpu specifications are listed in table 1. the pcie connection for each gpu is pcie 4.0 8 providing 16 gib s bandwidth pci-sig 2017 while nvlink nvidia corporation 2024 offers a bandwidth of 600 gib s. addi- tionally we allocate 80 gib of cpu memory per gpu. model. we use three different llms with different sizes 1 a 15b variety of llama3 elinas 2024 2 codellama-34b roziere et al. 2023 3 llama2- 70b touvron et al. 2023b . they all use grouped query attention gqa ainslie et al. 2023 . for brevity we refer to them as 15b 34b and 70b respectively in the following sections. we use float16 as the data type. workload. we use two different datasets in our eval- uation namely sharegpt sharegpt 2023 and arxiv-summarization cohan et al. 2018 . they correspond to two different distributions of work- load. sharegpt is a dataset of chatting history so its input and output have comparable lengths while arxiv-summarization dataset is a summarization dataset where inputs are much longer than outputs. the characteristics of these two datasets are shown in figure 9. we sample 2000 requests from the sharegpt dataset and 500 requests from arxiv-summarization and also use constant-length workloads in section 6.5. since seesaw is purely throughput-oriented we measure the end-to-end throughput as the metrics. baselines. we use vllm 0.5.4 kwon et al. 2023 as the baseline. it is the most widely used open-source llm serving engine with wide support for different parallelisms. we also directly use the vllm s model implementation for a straightforward comparison. sglang zheng et al. 2023 and deepspeed-fastgen holmes et al. 2024 do not sup- port pipeline parallelism. tensorrt-llm nvidia 2024b is not included in the comparison because it uses a simi- lar scheduling policy as vllm and vllm demonstrates comparable performance vllm team 2024 in throughput- oriented tasks. the techniques proposed in seesaw can also be applied to modifying tensorrt-llm. table 1. gpu hardware specification gpu model memory size memory bandwidth flops nvlink a10 24 gib 600 gib s 125t l4 24 gib 300 gib s 121t a100 40 gib 1 555 gib s 312t 0 2000 4000 tokens 0 2 4 density 1e 3 input tokens output tokens a arxiv-summarization 0 2000 4000 tokens 0.0 0.5 1.0 density 1e 2 input tokens output tokens b sharegpt figure 9. input and output length distributions of the datasets we enable chunked prefill and tune the chunk size for vllm to get the optimal throughput following the practice of sarathi-serve agrawal et al. 2024 . otherwise suboptimal chunk sizes would cause severe throughput degradation. 6.2 end-to-end throughput on pcie systems first we measure the end-to-end throughput of seesaw. we sweep over all available single parallelism configurations for vllm and show the result of the best configuration. we use four gpus for the 15b model and eight gpus for the 34b and 70b models. the result is shown in figure 10 with the used parallelism labeled above each bar. on a10 compared with the highest single parallelism base- line seesaw achieves a geometrically average speedup of 1.45 with up to 1.78 speedup. on l4 seesaw achieves a geometrically average speedup of 1.29 with up to 1.52 speedup. the overall average speedup is 1.36 . the speedup is more significant on a10 because a10 has better single gpu performance than l4 while they have similar pcie inter-connection bandwidth causing a higher percent- age of communication overhead. 6.3 speedup breakdown an example figure 12 illustrates how seesaw merges the advantages of different parallelisms. using codellama34b on the arxiv-summarization dataset with four a10 gpus as an example we measured the runtime of each stage. tp4 is optimal for decoding but significantly slower for prefilling while pp4 excels at prefilling but is slower during decoding. seesaw uses a mixed parallelism strategy applying pp4 for prefilling and tp4 for decoding achieving performance comparable to the best configuration for each stage. compared to the optimal single parallelism configuration tp2pp2 with chunked prefill seesaw is still faster because 1 chunked prefill does not piggy-back all decoding steps page 9 seesaw high-throughput llm inference via model re-sharding 15b 34b 70b 15b 34b 70b 0 1 2 normalized throughput d2t2 p4- t4 d2t2p2 d2p4- d2t4 t2p4 p8- t4p2 d2p2 p4- t4 d2t2p2 d2p4- d2t4 t4p2 p8- t4p2 arxiv sharegpt vllm seesaw a end-to-end throughput on a10 15b 34b 70b 15b 34b 70b 0 1 2 normalized throughput t2p2 p4- t4 d2t4 d2p4- d2t4 t4p2 p8- t4p2 p4 p4- t4 d2t4 p8- t4p2 t4p2 p8- t4p2 arxiv sharegpt vllm seesaw b end-to-end throughput on l4 figure 10. end-to-end throughput comparison on pcie systems. the used parallelization strategies are labelled above each bar. labels such as p4 d4 represent the parallelization strategies for prefilling and decoding respectively in seesaw. arxiv sharegpt 0.0 0.5 1.0 throughput normalized 0.61 0.62 0.89 0.82 1.00 1.00 1.00 1.13 vllm pcie seesaw pcie vllm nvlink seesaw nvlink figure 11. throughput comparison on a100. leaving some purely decoding steps and 2 chunked prefill with tp2pp2 is slower than prefilling with pp4. 6.4 end-to-end throughput on a100 speedup on a100 nvlink the nvlink interconnec- tion across a100 gpus significantly reduces the all-reduce overhead and further scales tensor parallelism. usually tensor parallelism alone is enough to achieve optimal per- formance when there are no more than four gpus. never- theless there is still a noticeable percentage of all-reduce overhead in prefilling when tensor parallelism scales be- yond four gpus. seesaw can still provide speedup in this case. as shown in figure 11 seesaw still achieves a 13 throughput increase over vllm for the sharegpt dataset on llama3-70b on eight a100s. speedup on a100 pcie besides a100 sxm with nvlink inter-connection there is also another version of a100 that is inter-connected with pcie links where seesaw can achieve noticeable speedup. as shown in figure 11 see- saw provides 46 speedup on arxiv-summarization and 30 speedup on sharegpt. seesaw brings the per- formance of the a100 pcie version much closer to the per- formance level of the nvlink version. vllm gets roughly 60 throughput on a100 pcie compared with a100 sxm while seesaw boosts it up to 82 89 . tp4 pp4 p4- t4 tp2pp2 chunked prefill 0 500 1000 1500 end-to-end time s prefill mix decode other figure 12. speedup breakdown. mix represents batches contain- ing both prefilling and decoding when chunked prefill is enabled. we disable chunked prefill for tp4 and pp4 in order to show the reference prefilling and decoding time. tp2pp2 with chunked prefill is the optimal parallelism for vllm. 0.0 0.1 0.2 0.3 d p 0.5 1.0 throughput normalized tp4pp2 tp2pp4 pp8 pp8- tp4pp2 figure 13. throughput of various parallelization strategies with different ratios between output and input lengths d p mea- sured on 70b model and eight a10 gpus. 6.5 sensitivity study ratio between input and output length the speedup of seesaw depends on the ratio between the input and output length or p d. model re-sharding has the opportunity to provide speedup when prefilling and decoding have bal- anced time. to investigate to what extent model re-sharding would be effective we measure the throughput of various parallelization strategies on synthesized datasets with uni- form lengths and different p d ratios. we fix the input length as 3000 and vary the output length. as shown in figure 13 pp8 achieves the highest throughput during prefilling while tp4pp2 excels in decoding. when the output length equals one prefilling only seesaw and pp8 show similar throughput and tp4pp2 performs worse due to high communication overhead. as output length increases the inefficiency of pp in decoding outweighs its advantage in prefilling causing pp8 s throughput to drop rapidly. there is a range where tp2pp4 becomes optimal before decoding dominates the runtime and tp4pp2 takes over as the fastest. nonetheless seesaw achieves the highest overall throughput across all data points. in real scenarios with variable input and output lengths seesaw is even more advantageous due to its adaptive capabilities. inter-connection bandwidth the effectiveness of see- saw also depends on the inter-connection bandwidth. we investigate this by measuring the runtime and tracing all- reduce operations of running arxiv-summarization and 34b model on eight a10s. we then mutate the all- reduce time to project the end-to-end throughput with dif- page 10 seesaw high-throughput llm inference via model re-sharding 10 1 100 101 bandwidth scale all_reduce through pcie 0.0 0.5 1.0 throughput normalized d2t1p4 d2t2p2 d2t4p1 d1t1p8 d1t2p4 d1t4p2 d1t8p1 d2p4- d2t4 figure 14. projected throughput of various parallelization strate- gies with different inter-connection bandwidth measured and traced on 34b model and eight a10 gpus. ferent inter-connection bandwidths. as shown in figure 14 when the inter-connection bandwidth is slow for example among geographically distributed devices borzunov et al. 2022 pipeline parallelism is optimal when the bandwidth is very high tensor parallelism is optimal. the throughput of seesaw is superior to fixed parallelization strategies on a wide range from 0.1 to 50 of pcie bandwidth. 7 related work 7.1 heterogenity between prefilling and decoding due to the different computational characteristics between prefilling and decoding leading to under-utilization of hard- ware resources prior research has investigated two direc- tions to address this problem namely disaggregating or merging the two stages. disaggregation places prefilling and decoding onto different devices to avoid their interfer- ence while merging processes prefilling and decoding in one batch. disaggregate prefill and decoding distserve zhong et al. 2024 proposed placing prefilling and decoding on different devices to prevent interference and leverage dif- ferent characteristics of the two stages. mooncake qin et al. 2024 uses similar through a distributed kv cache pool. p d-serve jin et al. 2024 uses the device-to-device network to transfer the kv cache between prefill and decode devices. splitwise patel et al. 2024 proposes using dif- ferent gpu models for the two stages. tetriinfer hu et al. 2024 further disaggregates different downstream tasks to avoid interference. these works are designed for online serving while seesaw focuses on offline inference. more- over they are usually designed for large clusters. merge prefill and decode chunked prefill as proposed by splitfuse holmes et al. 2024 sarathi agrawal et al. 2023 and sarathi-serve agrawal et al. 2024 splits long prompts in the prefilling stage into smaller chunks combin- ing them with decoding steps to strike a balance between data movement and computation and reduce pipeline bub- bles in pipeline parallelism. however determining the opti- mal chunk size is challenging. a chunk size that s too large results in excessive decode-only steps closely resembling traditional prefill-decode scheduling. conversely a chunk size that s too small reduces kernel efficiency. 7.2 parallel and distributed llm inference aside from tensor parallelism pipeline parallelism and data parallelism discussed in section 2.2 there are also other types of parallelisms such as sequence parallelism sp li et al. 2021 liu et al. 2023 lin et al. 2024 brandon et al. 2023 xue et al. 2024 and fully sharded data parallelism fsdp zhao et al. 2023 rajbhandari et al. 2020 . se- quence parallelism is especially designed for long sequence lengths and is orthogonal with our work. fsdp requires frequently transferring weight matrices across gpus thus mainly used in training. hexgen jiang et al. 2023 llm-pq zhao et al. 2024 helix mei et al. 2024 investigate parallelisms in hetero- geneous clusters. intra-device parallelism leverages over- lapping functions using different resources within each de- vice including nanoflow zhu et al. 2024 and liger du et al. 2024 . petals borzunov et al. 2022 explores llm inference in geographically distributed setups em- ploying pipeline parallelism to minimize communication costs. spotserve miao et al. 2024 runs llm inference on preemptible instances. 7.3 offloading in llm inference offloading is a widely used technique to run llm applica- tions in resource-constrained scenarios ren et al. 2021 . flexgen sheng et al. 2023 swaps tensors across gpu memory cpu memory and disks. fiddler kamahori et al. 2024 hetegen xuanlei et al. 2024 powerinfer song et al. 2023 and fastdecoder he zhai 2024 perform part of computation in cpu which require cpus with strong compute capability or external cpu nodes connected with high-bandwidth networking. instinfer pan et al. 2024 offloads computation to computational storage drives. 8 conclusion this paper proposes seesaw a high-throughput llm infer- ence engine to address the inefficiencies of fixed paralleliza- tion by selecting different parallelization strategies for the prefilling and decoding stages and switching between them using model re-sharding. it uses tiered kv cache buffering to minimize re-sharding overheads. our experiments show that seesaw outperforms widely-used open-source inference engines with a throughput increase of 1.06-1.78 and an average throughput improvement of 1.36 . these results highlight seesaw s effectiveness and adaptability. page 11 seesaw high-throughput llm inference via model re-sharding references achiam j. adler s. agarwal s. ahmad l. akkaya i. aleman f. l. almeida d. altenschmidt j. altman s. anadkat s. et al. gpt-4 technical report. arxiv preprint arxiv 2303.08774 2023. agrawal a. panwar a. mohan j. kwatra n. gulavani b. s. and ramjee r. sarathi efficient llm inference by piggybacking decodes with chunked prefills. arxiv preprint arxiv 2308.16369 2023. agrawal a. kedia n. panwar a. mohan j. kwatra n. gulavani b. s. tumanov a. and ramjee r. taming throughput-latency tradeoff in llm inference with sarathi- serve. arxiv preprint arxiv 2403.02310 2024. ainslie j. lee-thorp j. de jong m. zemlyanskiy y. lebr on f. and sanghai s. gqa training generalized multi-query transformer models from multi-head check- points. arxiv preprint arxiv 2305.13245 2023. alband. why not multiprocess pin memory in data loader https discuss.pytorch.org t why-not- multiprocess-pin-memory-in-data-loader 197345 2 2023. accessed 2024-10-14. amazon web services. amazon ec2 instance types 2024. url https aws.amazon.com ec2 instance-types . accessed 2024-10-26. ben-nun t. and hoefler t. demystifying parallel and dis- tributed deep learning an in-depth concurrency analysis. acm computing surveys csur 52 4 1 43 2019. bengio y. ducharme r. and vincent p. a neural proba- bilistic language model. advances in neural information processing systems 13 2000. borzunov a. baranchuk d. dettmers t. ryabinin m. belkada y. chumachenko a. samygin p. and raffel c. petals collaborative inference and fine-tuning of large models. arxiv preprint arxiv 2209.01188 2022. brandon w. nrusimha a. qian k. ankner z. jin t. song z. and ragan-kelley j. striped attention faster ring attention for causal transformers. arxiv preprint arxiv 2311.09431 2023. chan v. zhang h. and wang f. snowflake llm inference optimizing gpu capacity for interactive workloads. https www.snowflake.com engineering- blog snowflake-llm-inference-interactive-workloads september 2024. accessed 2024-10-30. chang l. bao w. hou q. jiang c. zheng n. zhong y. zhang x. song z. jiang z. lin h. et al. flux fast software-based communication overlap on gpus through kernel fusion. arxiv preprint arxiv 2406.06858 2024. cheng k. hu w. wang z. peng h. li j. and zhang s. slice-level scheduling for high throughput and load balanced llm serving. arxiv preprint arxiv 2406.13511 2024. cohan a. dernoncourt f. kim d. s. bui t. kim s. chang w. and goharian n. a discourse-aware attention model for abstractive summarization of long documents. arxiv preprint arxiv 1804.05685 2018. dell technologies. poweredge server gpu matrix 2023. url https www. delltechnologies.com asset en-ca products servers briefs-summaries poweredge-server-gpu-matrix.pdf. ac- cessed 2024-10-24. dell technologies. inferencing performance for gen- erative ai in the enterprise with amd accelerators. https infohub.delltechnologies.com en-au l generative- ai-in-the-enterprise-with-amd-accelerators inferencing- performance 2024. accessed 2024-10-30. du j. wei j. jiang j. cheng s. huang d. chen z. and lu y. liger interleaving intra-and inter-operator parallelism for distributed large model inference. in pro- ceedings of the 29th acm sigplan annual symposium on principles and practice of parallel programming pp. 42 54 2024. edge d. trinh h. cheng n. bradley j. chao a. mody a. truitt s. and larson j. from local to global a graph rag approach to query-focused summarization. arxiv preprint arxiv 2404.16130 2024. elinas. llama-3-15b instruct-zeroed. https huggingface.co elinas llama-3-15b-instruct-zeroed 2024. fang j. yu y. zhao c. and zhou j. turbotransformers an efficient gpu serving system for transformer models. in proceedings of the 26th acm sigplan symposium on principles and practice of parallel programming pp. 389 402 2021. google cloud. gpu platforms a100 gpus 2024. url https cloud.google.com compute docs gpus a100-gpus. accessed 2024-10-26. he j. and zhai j. fastdecode high-throughput gpu- efficient llm serving using heterogeneous pipelines. arxiv preprint arxiv 2403.11421 2024. holmes c. tanaka m. wyatt m. awan a. a. rasley j. rajbhandari s. aminabadi r. y. qin h. bakhtiari a. kurilenko l. et al. deepspeed-fastgen high-throughput text generation for llms via mii and deepspeed-inference. arxiv preprint arxiv 2401.08671 2024. page 12 seesaw high-throughput llm inference via model re-sharding hu c. huang h. xu l. chen x. xu j. chen s. feng h. wang c. wang s. bao y. et al. inference without interference disaggregate llm inference for mixed down- stream workloads. arxiv preprint arxiv 2401.11181 2024. huang y. cheng y. bapna a. firat o. chen d. chen m. lee h. ngiam j. le q. v. wu y. et al. gpipe efficient training of giant neural networks using pipeline parallelism. advances in neural information processing systems 32 2019. jiang y. yan r. yao x. zhou y. chen b. and yuan b. hexgen generative inference of large language model over heterogeneous environment. in forty-first interna- tional conference on machine learning 2023. jin y. wang t. lin h. song m. li p. ma y. shan y. yuan z. li c. sun y. et al. p d-serve serving disag- gregated large language model at scale. arxiv preprint arxiv 2408.08147 2024. kamahori k. gu y. zhu k. and kasikci b. fiddler cpu-gpu orchestration for fast inference of mixture-of- experts models. arxiv preprint arxiv 2402.07033 2024. kamsetty a. chen h. and xie l. how bytedance scales offline inference with multi-modal llms to 200tb data. https www.anyscale.com blog how-bytedance-scales- offline-inference-with-multi-modal-llms-to-200tb-data august 2023. accessed 2024-10-30. kwon w. li z. zhuang s. sheng y. zheng l. yu c. h. gonzalez j. zhang h. and stoica i. efficient memory management for large language model serving with pagedattention. in proceedings of the 29th sym- posium on operating systems principles pp. 611 626 2023. li s. xue f. baranwal c. li y. and you y. sequence parallelism long sequence training from system perspec- tive. arxiv preprint arxiv 2105.13120 2021. li z. zheng l. zhong y. liu v. sheng y. jin x. huang y. chen z. zhang h. gonzalez j. e. et al. alpaserve statistical multiplexing with model paral- lelism for deep learning serving. in 17th usenix sympo- sium on operating systems design and implementation osdi 23 pp. 663 679 2023. lin b. peng t. zhang c. sun m. li l. zhao h. xiao w. xu q. qiu x. li s. et al. infinite-llm efficient llm service for long context with distattention and distributed kvcache. arxiv preprint arxiv 2401.02669 2024. liu h. zaharia m. and abbeel p. ring attention with blockwise transformers for near-infinite context. arxiv preprint arxiv 2310.01889 2023. liu s. biswal a. cheng a. mo x. cao s. gonzalez j. e. stoica i. and zaharia m. optimizing llm queries in relational workloads. arxiv preprint arxiv 2403.05821 2024. mei y. zhuang y. miao x. yang j. jia z. and vinayak r. helix distributed serving of large language models via max-flow on heterogeneous gpus. arxiv preprint arxiv 2406.01566 2024. miao x. oliaro g. zhang z. cheng x. jin h. chen t. and jia z. towards efficient generative large language model serving a survey from algorithms to systems. arxiv preprint arxiv 2312.15234 2023. miao x. shi c. duan j. xi x. lin d. cui b. and jia z. spotserve serving generative large language models on preemptible instances. in proceedings of the 29th acm international conference on architectural support for programming languages and operating systems volume 2 pp. 1112 1127 2024. mlcommons. mlperf inference datacenter benchmark suite. https mlcommons.org benchmarks inference-datacenter 2024. accessed 2024- 10-30. narayan a. chami i. orr l. arora s. and r e c. can foundation models wrangle your data arxiv preprint arxiv 2205.09911 2022. narayanan d. harlap a. phanishayee a. seshadri v. devanur n. r. ganger g. r. gibbons p. b. and za- haria m. pipedream generalized pipeline parallelism for dnn training. in proceedings of the 27th acm symposium on operating systems principles pp. 1 15 2019. nvidia. fastertransformer transformer related optimiza- tion including bert gpt. https github.com nvidia fastertransformer 2024a. nvidia. tensorrt-llm optimized inference for large lan- guage models. https github.com nvidia tensorrt-llm 2024b. nvidia corporation. nvidia a100 pcie product brief 2020. url https www.nvidia.com content dam en-zz solutions data-center a100 pdf a100-pcie-prduct-brief.pdf. accessed 2024-10-24. nvidia corporation. nvidia nvlink high-speed gpu interconnect 2024. url https www.nvidia. com en-us data-center nvlink . accessed 2024-10-26. openai. chatgpt gpt-4 2024. url https www. openai.com research gpt-4. accessed 2024- 08-02. page 13 seesaw high-throughput llm inference via model re-sharding pan x. li e. li q. liang s. shan y. zhou k. luo y. wang x. and zhang j. instinfer in-storage attention offloading for cost-effective long-context llm inference. arxiv preprint arxiv 2409.04992 2024. patel p. choukse e. zhang c. shah a. goiri i. maleki s. and bianchini r. splitwise efficient generative llm inference using phase splitting. in 2024 acm ieee 51st annual international symposium on computer architec- ture isca pp. 118 132. ieee 2024. pci-sig. pci-sig releases pcie 4.0 ver- sion 1.0 2017. url https pcisig. com pci-sig-releases-pcie c2 ae-40-version-10. pope r. douglas s. chowdhery a. devlin j. bradbury j. heek j. xiao k. agrawal s. and dean j. efficiently scaling transformer inference. proceedings of machine learning and systems 5 606 624 2023. qin r. li z. he w. zhang m. wu y. zheng w. and xu x. mooncake kimi s kvcache-centric architecture for llm serving. arxiv preprint arxiv 2407.00079 2024. rajbhandari s. rasley j. ruwase o. and he y. zero memory optimizations toward training trillion parameter models. in sc20 international conference for high per- formance computing networking storage and analysis pp. 1 16. ieee 2020. ren j. rajbhandari s. aminabadi r. y. ruwase o. yang s. zhang m. li d. and he y. zero-offload democratizing billion-scale model training. in 2021 usenix annual technical conference usenix atc 21 pp. 551 564 2021. roziere b. gehring j. gloeckle f. sootla s. gat i. tan x. e. adi y. liu j. sauvestre r. remez t. et al. code llama open foundation models for code. arxiv preprint arxiv 2308.12950 2023. sharegpt. sharegpt vicuna unfiltered dataset. https huggingface.co datasets anon8231489123 sharegpt_vicuna_ unfiltered 2023. apache 2.0 license. sheng y. zheng l. yuan b. li z. ryabinin m. chen b. liang p. r e c. stoica i. and zhang c. flexgen high-throughput generative inference of large language models with a single gpu. in international conference on machine learning pp. 31094 31116. pmlr 2023. shoeybi m. patwary m. puri r. legresley p. casper j. and catanzaro b. megatron-lm training multi- billion parameter language models using model paral- lelism. arxiv preprint arxiv 1909.08053 2019. song y. mi z. xie h. and chen h. powerinfer fast large language model serving with a consumer-grade gpu. arxiv preprint arxiv 2312.12456 2023. srivatsa v. he z. abhyankar r. li d. and zhang y. preble efficient distributed prompt scheduling for llm serving. 2024. touvron h. lavril t. izacard g. martinet x. lachaux m.-a. lacroix t. rozi ere b. goyal n. hambro e. azhar f. et al. llama open and efficient foundation lan- guage models. arxiv preprint arxiv 2302.13971 2023a. touvron h. martin l. stone k. albert p. almahairi a. babaei y. bashlykov n. batra s. bhargava p. bhosale s. et al. llama 2 open foundation and fine- tuned chat models. arxiv preprint arxiv 2307.09288 2023b. vaswani a. shazeer n. parmar n. uszkoreit j. jones l. gomez a. n. kaiser \u0142. and polosukhin i. at- tention is all you need. advances in neural information processing systems 30 2017. vllm team. performance update bringing vllm to the next level. https blog.vllm.ai 2024 09 05 perf-update.html 2024. accessed 2024-10- 14. xuanlei z. jia b. zhou h. liu z. cheng s. and you y. hetegen efficient heterogeneous parallel inference for large language models on resource-constrained devices. proceedings of machine learning and systems 6 162 172 2024. xue f. chen y. li d. hu q. zhu l. li x. fang y. tang h. yang s. liu z. et al. longvila scaling long- context visual language models for long videos. arxiv preprint arxiv 2408.10188 2024. yu c. lee s. xu r. lin w. gorthy p. and liaw r. batch llm inference on anyscale slashes aws bedrock costs by up to 6x october 2024. url https www.anyscale.com blog batch-llm-inference-announcement. ac- cessed 2024-10-30. yu g.-i. jeong j. s. kim g.-w. kim s. and chun b.- g. orca a distributed serving system for transformer- based generative models. in 16th usenix symposium on operating systems design and implementation osdi 22 pp. 521 538 2022. yuan z. shang y. zhou y. dong z. xue c. wu b. li z. gu q. lee y. j. yan y. et al. llm inference unveiled survey and roofline model insights. arxiv preprint arxiv 2402.16363 2024. page 14 seesaw high-throughput llm inference via model re-sharding zhao j. wan b. peng y. lin h. and wu c. llm- pq serving llm on heterogeneous clusters with phase- aware partition and adaptive quantization. arxiv preprint arxiv 2403.01136 2024. zhao y. gu a. varma r. luo l. huang c.-c. xu m. wright l. shojanazeri h. ott m. shleifer s. et al. pytorch fsdp experiences on scaling fully sharded data parallel. arxiv preprint arxiv 2304.11277 2023. zheng l. li z. zhang h. zhuang y. chen z. huang y. wang y. xu y. zhuo d. xing e. p. et al. alpa automating inter-and intra-operator parallelism for distributed deep learning. in 16th usenix symposium on operating systems design and implementation osdi 22 pp. 559 578 2022. zheng l. yin l. xie z. huang j. sun c. yu c. h. cao s. kozyrakis c. stoica i. gonzalez j. e. et al. efficiently programming large language models using sglang. arxiv preprint arxiv 2312.07104 2023. zhong y. liu s. chen j. hu j. zhu y. liu x. jin x. and zhang h. distserve disaggregating prefill and decoding for goodput-optimized large language model serving. arxiv preprint arxiv 2401.09670 2024. zhu k. zhao y. zhao l. zuo g. gu y. xie d. gao y. xu q. tang t. ye z. et al. nanoflow towards optimal large language model serving throughput. arxiv preprint arxiv 2408.12757 2024. a performance model in this section we examine the trade-offs of various paral- lelism strategies by developing an analytical performance model. we break down the model s inference time into multiple components and analyze the impact of each paral- lelism type on these components. the results reveal that the proportion of these components differs across workloads resulting in distinct scaling behaviors for each parallelism strategy. table 2 lists the notations used in our analysis. we assume the data type is float16. table 2. notations n number of output tokens t linear dm time of moving weights r number of requests t attn dm time of moving kvcache b global batch size t linear c time of computation s average sequence length t attn c computation of attention hq number of heads tnw time of communication d head dimension hkv number of kv heads l number of layers w parameters of one layer pp pipeline parallel degree dp data parallel degree tp tensor parallel degree table 3. different components of the runtime of a forward pass. the batch size b representing the batching effect is emphasized. t linear dm t linear comp t attn dm t attn comp tnw tp prefill 2w bhbm 2bw s flops 2bs hq 2hkv d bhbm bhqs2d2 flops 4bshqd bar t p decode 2w bhbm 2bw flops 4bshkvd bhbm 2bhqsd2 flops 4bhqd bar t p a.1 runtime break-down the runtime of each decoding layer can be divided into three components 1 data movement tdm from gpu global memory hbm to compute units which includes transfer- ring weights t linear dm and kv cache t attn dm 2 computation tcomp including t linear comp and t attn comp and 3 communication cost tnw nw stands for network primarily arising from the all-reduce operation in tensor parallelism. based on the roof- line model the runtime of each layer can be approximated as tl max t linear dm t linear comp max t attn dm t attn comp tnw. data movement. the runtime of data movement can be approximated as transferred data volume divided by the bandwidth which is the hbm bandwidth for gpus. for lin- ear layers the transferred data is mostly weight matrices of which the size is 2w bytes which is constant. for attention layers the transferred data is most the q k and v matrices which is 2bs hq 2hkv d bytes in prefilling and 4bshkvd in decoding. compute. the computation time can be approximated as the number of floating operations flops divided by the number of floating operations per second of the hardware flop s . for linear layers the flops is proportional to the weight parameters times the number of tokens which is 2wbs in prefilling and 2wb in decoding. for attention layers most operations come from computing the attention score which is approximated as bhqs2d2 in prefilling and 2bhqsd2 in decoding. communication. the communication cost mostly comes from the all-reduce operation in tensor parallelism. it can be modeled as the transferred data volume divided by the bandwidth. we denote it as tnw tp and approximate it as b a bar tp where a is the size of the activation of one request within a batch and bar tp is the all-reduce band- width. tnw tp is monotonically increasing with tp as additional gpus and more replicas of activations are added to all-reduce. we omit the peer-to-peer communication over in pipeline parallel since it is negligible compared to the all-reduce operation of tensor parallel. a.2 batching analysis batching is critical in decoding. it significantly affects the latency and throughput. batch size represents how many page 15 seesaw high-throughput llm inference via model re-sharding tp1dp8 tp2dp4 tp4dp2 tp8dp1 0.00 0.25 0.50 0.75 1.00 runtime per request oom load weight compute allreduce 0 50 100 150 batch size batch size figure 15. how data parallelism affects the decoding throughput. data parallelism has minimal communication overhead but suffers from caused by inefficient memory access caused by duplicating model weights. model duplicates occupy more gpu memory leaving less space for kv cache and smaller batch sizes. with more data parallelism the overhead of loading data from gpu global memory to compute units significantly increases. requests are processed in one forward pass and larger batch sizes can amortize the cost of transferring weights thus improving the throughput. global and micro-batch size. in distributed inference such as multi-gpu settings we define the global batch size b as the number of requests being actively processed by the whole cluster. it is a tunable hyper-parameter that represents the overall workload of the system. it is bounded by the maximal batch size which is determined by the memory budget. on the other side the micro batch size is defined at the device level as the batch size processed during each forward pass. tensor parallelism does not affect the batch size while dp and pp shrink the micro batch size. a.3 parallelism analysis we consider three types of parallelism data parallelism tensor parallelism and pipeline parallelism and denote their degree of parallelism as dp tp and pp respectively. tensor parallelism can accelerate both data moving t linear dm and t attn dm are reduced to 1 tp and computation tcomp reduced to tcomp tp at the cost of all reduce over- head tnw. data parallelism distributes the global batch size b onto dp micro-batches processed in parallel. the model is du- plicated so t linear dm remains unchanged. t attn dw t linear comp t attn comp tnw are reduced as the batch size is smaller. due to the need to duplicate model weights the gpu memory left for the kv cache is smaller. the spare space for kv cache on each gpu is mkv m 2lw t p p p . the maximal batch size is bmax dp mkv tp pp 4lhkvds dp m tp pp 2lw 4lhkvds while tp and pp can super-linearly scale the batch size dp can only linearly scale the batch size. the trade-off between limited batch sizes and reduced communication overhead is shown in figure 15. pipeline parallelism distributes different layers to dif- ferent devices and each device will have l pp layers. it cannot reduce single-request latency but is more suitable for throughput-oriented scenarios as it introduces less commu- nication overhead. however it is not the ultimate answer of high-throughput applications because of an important observation that pipeline parallelism harms maximal batch size. a tricky nuance is that given a batch size b pipeline parallelism can only process b pp of them simultaneously in order to utilize and pipeline all pp gpus which is harm- ful to batching. if the workload is not uniformly distributed across gpus there will be bubbles or in the worst case some gpus might be idle. when the pipeline is fully and stably pipelining each time the last pipeline stage finishes its l pp layers of forward pass a micro-batch of b pp will be finished. throughput. the micro-batch size on each gpu is b pp dp . the total runtime of generating one micro batch with size b pp dp on one dp replica or more specifically the time of the last pipeline stage finishing a micro-batch is tstage l pp max t linear dm tp t linear comp dp tp pp max t attn dm t attn comp dp tp pp tnw tp pp dp the throughput number of processed requests per unit time is b pp t. for simplicity we calculate the inverse of it as throughput 1 tstage b pp l b max t linear dm tp t linear comp dp tp pp max t attn dm t attn comp dp tp pp tnw tp pp dp 1 if we approximate the roof-line model with a simplified additional model this expression can be simplified as throughput 1 t linear dm tp t linear comp t attn dm t attn comp dp tp pp tnw tp pp dp 2",
      "keywords": [
        "page",
        "seesaw",
        "high-throughput",
        "llm",
        "inference",
        "via",
        "model",
        "re-sharding",
        "qidong",
        "su123",
        "wei",
        "zhao34",
        "xin",
        "li3",
        "muralidhar",
        "andoorveedu3",
        "chenhao",
        "jiang12",
        "zhanda",
        "zhu123",
        "kevin",
        "song12",
        "christina",
        "giannoula123",
        "gennady",
        "pekhimenko123",
        "abstract",
        "improve",
        "efficiency",
        "distributed",
        "large",
        "language",
        "various",
        "parallelization",
        "strategies",
        "such",
        "tensor",
        "pipeline",
        "parallelism",
        "proposed.",
        "however",
        "distinct",
        "computational",
        "characteristics",
        "inherent",
        "two",
        "stages",
        "prefilling",
        "decoding",
        "render",
        "single",
        "static",
        "strategy",
        "insufficient",
        "effective",
        "optimization",
        "both",
        "stages.",
        "work",
        "present",
        "engine",
        "optimized",
        "throughput-oriented",
        "tasks.",
        "key",
        "idea",
        "behind",
        "dynamic",
        "re-",
        "sharding",
        "technique",
        "facilitates",
        "reconfiguration",
        "across",
        "thereby",
        "maximizing",
        "throughput",
        "phases.",
        "mitigate",
        "overhead",
        "optimize",
        "employ",
        "tiered",
        "cache",
        "buffering",
        "transition-minimizing",
        "scheduling.",
        "approaches",
        "synergistically",
        "reduce",
        "caused",
        "frequent",
        "stage",
        "transitions",
        "while",
        "ensuring",
        "maximum",
        "batching",
        "efficiency.",
        "evaluation"
      ],
      "metadata": {
        "filename": "2503.06433v1.pdf",
        "original_filename": "2503.06433v1.pdf",
        "file_extension": ".pdf",
        "content_type": "application/pdf",
        "file_size": 825289,
        "upload_file_id": "upload_1751636156875_fseq924lp",
        "upload_timestamp": "2025-07-04T13:35:56.966689",
        "parsing_status": "completed"
      },
      "processing_stage": "immediate",
      "indexed_at": "2025-07-04T13:35:57.090321+00:00",
      "content_length": 73234,
      "content_type": ".pdf",
      "filename": "2503.06433v1.pdf"
    }
  },
  "file_index": {
    "5": {
      "filename": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-04T13:26:23.938343+00:00",
      "content_length": 40912,
      "processing_stage": "immediate"
    },
    "3": {
      "filename": "New Account - Toronto Hydro.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-04T13:32:44.928821+00:00",
      "content_length": 1597,
      "processing_stage": "immediate"
    },
    "14": {
      "filename": "New Account - Toronto Hydro.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-04T20:18:35.702004+00:00",
      "content_length": 1597,
      "processing_stage": "immediate"
    },
    "15": {
      "filename": "2503.06433v1.pdf",
      "content_type": ".pdf",
      "indexed_at": "2025-07-04T13:35:57.090321+00:00",
      "content_length": 73234,
      "processing_stage": "immediate"
    }
  },
  "last_updated": "2025-07-04T20:18:35.702033+00:00"
}