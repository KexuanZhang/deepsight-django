[
  {
    "conference": "cvpr",
    "year": "2024",
    "title": "Ahmed_DeepCompress-ViT_Rethinking_Model_Compression_to_Enhance_Efficiency_of_Vision_Transformers_CVPR_2025_paper",
    "summary": "The paper \"DeepCompress-ViT: Rethinking Model Compression to Enhance Efficiency of Vision Transformers at the Edge\" presents DeepCompress-ViT, a specialized model compression framework aimed at optimizing Vision Transformers (ViTs) for deployment on resource-constrained edge devices. ViTs, while powerful for complex vision tasks, often exceed the memory limits of edge hardware due to their large model sizes. DeepCompress-ViT addresses this challenge by achieving high compression ratios with minimal performance degradation.\n\nThe framework employs a **Unified Compression Training (UCT)** approach, utilizing an encoder-decoder architecture where the encoder compresses model weights into a lower-dimensional representation, and the decoder reconstructs the original weights. This training minimizes the mean squared error (MSE) between original and reconstructed weights, while also integrating cross-entropy loss and knowledge distillation to preserve model accuracy. Additionally, the framework features **Optimized Test-Time Decoding**, which reorders matrix multiplication operations to reduce computational overhead during inference, ensuring that decoding does not increase overall floating-point operations (FLOPs).\n\nResults demonstrate that DeepCompress-ViT achieves compression ratios exceeding 14×, with the DeepCompress-DeiT-S model reducing its size from 84.1 MB to 5.64 MB while maintaining 78.74% accuracy on the ImageNet dataset. The DeepCompress-DeiT-B model achieves a 17.4× compression ratio with only a slight accuracy drop. The method also significantly reduces energy consumption (up to 1475×) and latency (up to 68×) compared to uncompressed models, making it feasible to deploy large ViTs on edge devices. In object detection tasks on the COCO dataset, the framework shows a 15.7× reduction in model size while maintaining competitive accuracy.\n\nOverall, DeepCompress-ViT offers a promising solution for deploying large-scale ViT models in edge environments, addressing critical issues of memory constraints, energy efficiency, and latency. This work not only enhances ViT efficiency but also contributes to the field of model compression by introducing a dual encoding-decoding strategy that mitigates performance loss during aggressive compression, paving the way for real-time applications in various domains such as autonomous vehicles, healthcare, and industrial automation."
  },
  {
    "conference": "cvpr",
    "year": "2024",
    "title": "Ahmed_Efficient_Event-Based_Object_Detection_A_Hybrid_Neural_Network_with_Spatial_CVPR_2025_paper",
    "summary": "The paper introduces a hybrid neural network architecture that integrates Spiking Neural Networks (SNNs) and Artificial Neural Networks (ANNs) for efficient event-based object detection. This architecture capitalizes on the high temporal resolution and dynamic range of event cameras, which capture asynchronous pixel-level illumination changes, thereby reducing motion blur. Key components include an SNN block for low-level feature extraction, an attention-based SNN-ANN bridge module (βasab) that transforms sparse spatiotemporal data into dense feature maps, and an ANN block for high-level spatial feature extraction. The βasab module employs Event-Rate Spatial (ERS) Attention to emphasize active spatial areas and Spatial-aware Temporal (SAT) Attention to capture temporal relationships. Additionally, a variant of the model utilizes Depth-Wise Convolutional LSTMs (DWConvLSTMs) for multi-timescale processing, accommodating both fast and slow dynamics.\n\nThe model is trained end-to-end using the Adam optimizer and evaluated on the Gen1 and Gen4 Automotive Detection datasets, achieving a mean Average Precision (mAP) of 0.35 on Gen1 with only 6.6 million parameters, outperforming existing SNN-based methods. Comparisons with ANN and RNN approaches yield competitive mAP values of 0.27 on Gen4. Ablation studies validate the significance of each component in the βasab module, confirming that the complete model configuration delivers optimal performance.\n\nThis hybrid architecture not only demonstrates the feasibility of achieving ANN-like performance in object detection with reduced parameter count, latency, and power consumption but also shows efficient real-time operation on digital neuromorphic hardware (Intel's Loihi 2). The findings suggest promising avenues for future research in hybrid neural networks, particularly for applications requiring efficient processing of event-based data in energy-constrained environments."
  },
  {
    "conference": "cvpr",
    "year": "2025",
    "title": "Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper",
    "summary": "The paper \"CLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object Representation\" critically examines the limitations of the Contrastive Language-Image Pre-training (CLIP) model in complex multi-object scenarios. It introduces the ComCO (Complex COCO Objects) dataset, which consists of 72 objects from the COCO dataset, designed to facilitate controlled experiments with images containing 2 to 5 objects, each paired with specific captions.\n\nThe study employs two primary tasks: Text-based Object Retrieval (TOR) and Image-based Object Retrieval (IOR). TOR assesses the text encoder's ability to identify individual objects based on their position in multi-object captions, while IOR evaluates the image encoder's performance concerning object size in images. The analysis reveals significant biases in both encoders: the text encoder shows a strong preference for the first-mentioned object, while the image encoder favors larger objects. These biases were confirmed through experiments on both the ComCO and COCO datasets, indicating their generalizability.\n\nThe study attributes these biases to the training process, where larger objects are often prioritized in captions, leading to a reinforcement of these tendencies. The implications of these findings are substantial, as they affect image-text matching tasks, with performance varying based on caption structure. The paper calls for balanced training approaches to mitigate these biases, suggesting that addressing these issues could enhance the robustness of vision-language models and inform future developments in multimodal AI, including text-to-image generation models like Stable Diffusion."
  },
  {
    "conference": "cvpr",
    "year": "2025",
    "title": "Abdelsamad_Multi-Scale_Neighborhood_Occupancy_Masked_Autoencoder_for_Self-Supervised_Learning_in_LiDAR_CVPR_2025_paper",
    "summary": "The paper presents the Neighborhood Occupancy MAE (NOMAE), a self-supervised learning framework tailored for LiDAR point clouds, which often exhibit sparsity and large empty regions. NOMAE addresses limitations of existing masked autoencoders (MAEs) by employing a multi-scale voxel masking and occupancy reconstruction strategy that mitigates information leakage and reduces computational complexity. \n\nThe architecture comprises an encoder based on PTv3, a token upsampling module, and multiple decoders for hierarchical voxel reconstruction. A hierarchical mask generation (HMG) technique ensures consistent masking ratios across scales, allowing only visible voxels to influence the reconstruction loss, thus facilitating localized reconstruction tasks. The Binary Cross Entropy (BCE) loss is utilized for occupancy prediction, with the final loss being the average across all scales. Additionally, a multi-scale pretext task (MSP) enhances the model's capacity to learn representations for objects of varying sizes.\n\nEvaluated on the nuScenes and Waymo Open datasets, NOMAE achieved state-of-the-art results in semantic segmentation and 3D object detection, surpassing both existing self-supervised methods and some fully supervised models. Notably, it attained a mean Intersection over Union (mIoU) of 81.8 on nuScenes and 72.3 on Waymo, along with significant improvements in detection scores (60.9 NDS and 54.4 mAP).\n\nNOMAE's ability to leverage self-supervised learning on sparse LiDAR data reduces dependence on large annotated datasets, enhancing performance in downstream tasks like semantic segmentation and object detection. This positions NOMAE as a valuable asset for autonomous driving applications, with potential future work exploring its integration with temporal data and other sparse 3D sensors."
  },
  {
    "conference": "neurips",
    "year": "2024",
    "title": "Aiello_DreamCache_Finetuning-Free_Lightweight_Personalized_Image_Generation_via_Feature_Caching_CVPR_2025_paper",
    "summary": "The paper introduces DreamCache, an innovative method for personalized image generation that utilizes feature caching from a pretrained diffusion model, enabling efficient image creation of a reference subject across various contexts without extensive fine-tuning or computational demands. DreamCache employs a lightweight conditioning mechanism that integrates cached features from select U-Net layers into the generation process, resulting in high-quality images with significantly fewer parameters than traditional methods.\n\nKey components of DreamCache include:\n\n1. **Feature Caching:** Features from a reference image are cached by executing a forward pass through the diffusion model's denoiser at a single timestep (t=1) using a null text prompt, capturing multi-resolution representations while separating visual content from textual input.\n\n2. **Conditioning Mechanism:** The conditioning adapters consist of a cross-attention block that merges cached features with the features of the generated image, followed by concatenation and a projection layer to ensure compatibility with the original U-Net architecture.\n\n3. **Training of Adapters:** These adapters are pretrained on a synthetic dataset created using a large language model and segmentation techniques, employing a standard score matching loss for parameter optimization.\n\nResults indicate that DreamCache achieves state-of-the-art performance in personalized image generation, surpassing fine-tuning and zero-shot methods in image quality and computational efficiency. It requires only 25 million additional parameters and offers faster inference times, making it suitable for resource-constrained environments. \n\nDreamCache's lightweight and plug-and-play design facilitates easy integration into existing systems while maintaining the integrity of the original model. Although effective for single-subject personalization, the authors note potential challenges in multi-subject scenarios and propose future exploration of adaptive caching techniques. Overall, DreamCache enhances the accessibility and efficiency of personalized image generation, particularly for mobile and real-time applications."
  },
  {
    "conference": "neurips",
    "year": "2024",
    "title": "Akkerman_InterDyn_Controllable_Interactive_Dynamics_with_Video_Diffusion_Models_CVPR_2025_paper",
    "summary": "**Summary: InterDyn: Controllable Interactive Dynamics with Video Diffusion Models**\n\nInterDyn is an innovative framework that synthesizes realistic interactive dynamics in videos by leveraging implicit physics knowledge from large-scale video generative models, specifically extending the Stable Video Diffusion (SVD) model. It utilizes a U-Net-based latent diffusion architecture enhanced with a control signal branch that encodes the motion of driving objects (e.g., hand masks) to generate temporally coherent videos without explicit 3D reconstruction or physics simulation.\n\nThe model is fine-tuned on the CLEVRER and Something-Something-v2 (SSV2) datasets, employing the Adam optimizer and a tailored noise distribution for the diffusion process. Evaluation metrics include image quality (SSIM, PSNR, LPIPS, FID, KID), spatio-temporal similarity (FVD, KVD), and motion fidelity, which assesses the accuracy of object trajectories.\n\nInterDyn demonstrates superior performance, significantly outperforming state-of-the-art methods like CosHand by 37.5% on LPIPS and 77% on FVD in the SSV2 dataset. It effectively generates plausible videos of complex interactions, including force propagation and counterfactual dynamics, even with noisy control signals. The framework's robustness and ability to generalize to unseen objects suggest its potential applications in robotics, animation, and virtual reality, facilitating advancements in intelligent systems that require a nuanced understanding of physical interactions."
  },
  {
    "conference": "neurips",
    "year": "2025",
    "title": "Ahmed_Towards_Source-Free_Machine_Unlearning_CVPR_2025_paper",
    "summary": "The paper \"Towards Source-Free Machine Unlearning\" tackles the challenge of machine unlearning in scenarios where the original training data is unavailable, introducing a novel algorithm that estimates the Hessian of the remaining data. This method, designed for linear classifiers and adaptable to mixed linear scenarios, approximates the Hessian using only the data intended for removal and the trained model, under the assumption that loss differences at perturbed points for remaining and forgotten data are similar.\n\nThe Hessian estimation is framed as a semi-definite programming (SDP) problem, minimizing an objective function that quantifies the discrepancy between the estimated and true Hessians, with theoretical guarantees on accuracy. The unlearning process is assessed through parameter indistinguishability, ensuring that the outputs of the unlearned model closely resemble those of a model retrained from scratch without the forgotten data.\n\nExperimental validation on benchmark datasets (CIFAR-10, CIFAR-100, Stanford Dogs, and CalTech-256) demonstrated that the unlearned models performed comparably to retrained models and those using the exact Hessian. Performance metrics, including classification accuracy and Membership Inference Attack (MIA) scores, indicated successful unlearning, with MIA scores nearing 50% for unlearned models. The results also revealed that increasing the percentage of forget data adversely affected performance, consistent with theoretical predictions.\n\nThe proposed algorithm outperformed existing source-free unlearning methods, such as NegGrad and Adversarial techniques, in terms of accuracy on both remaining and forgotten data. This robust solution enhances compliance with data privacy regulations like GDPR, offering flexibility across various convex loss functions, thereby broadening its applicability in machine learning contexts."
  },
  {
    "conference": "neurips",
    "year": "2025",
    "title": "Aiello_DreamCache_Finetuning-Free_Lightweight_Personalized_Image_Generation_via_Feature_Caching_CVPR_2025_paper",
    "summary": "The paper introduces DreamCache, an innovative method for personalized image generation that utilizes feature caching from a pretrained diffusion model, enabling efficient image creation of a reference subject in diverse contexts without extensive fine-tuning or computational demands. DreamCache employs a lightweight conditioning mechanism that integrates cached features from select U-Net layers into the generation process, resulting in high-quality images with significantly fewer parameters than existing approaches.\n\nKey components of DreamCache include:\n\n1. **Feature Caching:** Features from a reference image are cached by executing a forward pass through the diffusion model's denoiser at a single timestep (t=1) using a null text prompt, capturing multi-resolution representations while decoupling visual content from textual input.\n\n2. **Conditioning Mechanism:** The method incorporates conditioning adapters featuring a cross-attention block that merges cached features with the generated image's features, followed by concatenation and projection to ensure compatibility with the original U-Net architecture.\n\n3. **Training of Adapters:** These adapters are pretrained on a synthetic dataset created using a large language model and segmentation techniques, employing a standard score matching loss for parameter optimization.\n\nResults indicate that DreamCache achieves state-of-the-art performance in personalized image generation, surpassing fine-tuning and zero-shot methods in image quality and computational efficiency. Evaluations on the DreamBooth dataset reveal competitive DINO and CLIP scores, demonstrating strong alignment between generated images and reference subjects. The method significantly reduces inference time and model size, requiring only 25M additional parameters, making it suitable for resource-constrained environments.\n\nDreamCache marks a substantial advancement in personalized image generation, offering a scalable and efficient alternative to traditional fine-tuning methods. Its lightweight architecture facilitates easy integration into existing systems for real-time personalized content generation, although challenges remain in multi-subject scenarios and highly abstract images, indicating potential areas for future research. Overall, DreamCache enhances the accessibility and efficiency of personalized image generation across various applications."
  }
]